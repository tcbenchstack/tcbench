{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>This website documents code and data artifacts related to the IMC23 submission #132 titled</p> <p>Contrastive Learning and Data Augmentation in Traffic Classification via a Flowpic Representation Replicating and Reproducing \u201cA Few Shots Traffic Classification with mini-FlowPic Augmentations\u201d from IMC\u201922</p> <p>Our submission investigates the role of data augmentation by using both supervised and contrastive learning techniques across 4 datasets.</p> <p>It replicates and reproduces the following paper from the IMC22 program</p> <pre><code>@inproceedings{10.1145/3517745.3561436,\nauthor = {Horowicz, Eyal and Shapira, Tal and Shavitt, Yuval},\ntitle = {A Few Shots Traffic Classification with Mini-FlowPic Augmentations},\nyear = {2022},\nisbn = {9781450392594},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3517745.3561436},\ndoi = {10.1145/3517745.3561436},\nbooktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},\npages = {647\u2013654},\nnumpages = {8},\nlocation = {Nice, France},\nseries = {IMC '22}\n}\n</code></pre> <p>We adopt the same traffic representation used in <code>imc22-paper</code>, namely a Flowpic -- a summarization of the packet size time series of a flow by means of  frequency histograms extracted from consecutive time windows of the flow --  applied on the <code>ucdavis-icdm19</code>.</p> <p>In the first part of the submission we investigate how augmentations affect classification performance -- the study considers 3 image transformations (rotation,  color jitter, horizontal flip) and 3 time series transformations (time shift, packet drop, change rtt) applied to packets timestamps -- when used either in a fully supervised setting or via contrastive learning.</p> <p>Key takeaways from reproducibility</p> <ol> <li> <p>We can only partially reproduce the results from <code>imc22-paper</code> on <code>ucdavis-icdm19</code>.    Specifically, we uncover a data shift present in the dataset itself which justifies our results;     yet, we cannot comment on why this was not detected in <code>imc22-paper</code>.</p> </li> <li> <p>Simply based on the <code>ucdavis-icdm19</code> dataset, and differently    from the argumentation presented in <code>imc22-paper</code>,     we do not find statistical significance differences across the different augmentations.</p> </li> <li> <p>Contrastive learning can help to \"bootstrap\" a model in an unsupervised fashion, yet    relying on more samples is beneficial to boost performance.</p> </li> </ol> <p>Then, in the second part of the submission we replicate the  analysis testing the same 6 augmentations across 3 other datasets.</p> <p>Key takeaways from replicability</p> <p>Using multiple datasets allow to confirm the argument of the  <code>imc22-paper</code>, i.e., Change RTT augmentation used in <code>ucdavis-icdm19</code> is superior to the alternative transformations presented in the paper.</p>"},{"location":"#website-conventions","title":"Website conventions","text":"<ul> <li> <p><code>imc22-paper</code> is used to the reference the replicated/reproduced paper.</p> </li> <li> <p>WIP (Work in progress) and  suggest documentation that is incomplete or not yet available.</p> </li> <li> <p> suggests a link is expected to be added but is not yet available.</p> </li> </ul>"},{"location":"artifacts/","title":"Artifacts","text":"<p>The submission is associated to three types of artifacts</p> <ul> <li> <p> Website: This website serves as a primary source of documentation. It collects</p> <ul> <li>Documentation about datasets .</li> <li>Documentation about our modeling framework called <code>tcbench</code>.</li> <li>Guides on how to run experiments  via <code>tcbench</code>.</li> </ul> </li> <li> <p> Code: This includes </p> <ul> <li>All source code related to <code>tcbench</code> .</li> <li>A collection of  Jupyter notebooks  used for the tables and figures of the submission.</li> </ul> </li> <li> <p> Data: This includes </p> <ul> <li>The datasets install, curation and split generation  used in our modeling</li> <li>All models and logs  generated through our modeling campaigns.</li> </ul> </li> </ul>"},{"location":"artifacts/#figshare-material","title":"Figshare material","text":"<p>A key objective of our submission is to made available all artifacts to the research community.  For instance, all code will be pushed to a  github repository, this website will be published on github pages or similar solutions, and data artifacts will be on a public cloud storage solution.</p> <p>Yet, due to double-blind policy, we temporarily uploaded our artifacts to a  figshare repository.</p> <p>More specifically, on figshare you find the following tarball.</p> <ul> <li> <p><code>website_documentation.tgz</code>: Well...if you are reading this page you already know the tarball contains this website .</p> </li> <li> <p><code>code_artifacts_paper132.tgz</code>: All code developed. See </p> <ul> <li>Quick tour for <code>tcbench</code>.</li> <li>Table and figures for jupyter notebooks.</li> </ul> </li> <li> <p><code>curated_datasets.tgz</code>: The preprocessed version of the datasets.  Please see the datasets pages in this website.</p> </li> <li> <p><code>ml_artifacts.tgz</code>: All output data generated via modeling campaigns. For fine grained view, those can be explored via AIM web UI  while results are generated via  Jupyter notebooks.</p> </li> </ul>"},{"location":"artifacts/#unpack-artifacts","title":"Unpack artifacts","text":"<p>In the figshare folder we also provide a <code>unpack_scripts.tgz</code>  tarball containing the following scripts</p> <pre><code>unpack_all.sh\n_unpack_code_artifacts_paper132.sh\n_unpack_curated_datasets.sh\n_unpack_ml_artifacts.sh\n</code></pre> <p>These are simple bash scripts to simplify the  extraction and installation of all material.</p> <p>Use the following process</p> <ol> <li> <p>First of all, prepare a python virtual environment, for example via  conda     <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> </li> <li> <p>Download all figshare tarballs in the same folder and run     <pre><code>tar -xzvf unpack_script.tgz\nbash ./unpack_all.sh\n</code></pre></p> </li> </ol>"},{"location":"install/","title":"Install and config","text":""},{"location":"install/#download-code-and-artifacts","title":"Download code and artifacts","text":"<p>If you see this documentation it means you downloaded the file from figshare so you already have the code in your hand :)</p> <p>Note</p> <p>It is our intent to push all the code into a proper repository</p>"},{"location":"install/#configure-a-python-environment","title":"Configure a python environment","text":"<p>We first create a <code>conda</code> environment to install all required dependencies</p> <pre><code>conda create -n replicating-imc22-flowpic python=3.10 pip\nconda activate replicating-imc22-flowpic\npython -m pip install -r ./requirements.txt\n</code></pre> <p>The code artifacts are also a python package that can be installed inside the environment. From inside <code>/replicate_imc22_flowpic</code> run</p> <pre><code>python -m pip install .\n</code></pre>"},{"location":"quick_tour/","title":"Quick tour","text":"<p>The code base is collected into a python package named <code>tcbench</code> which is designed to cover two functionalities</p> <ol> <li>Easy install and access to a curated set of traffic classification datasets</li> <li>Use the datasets to train/test ML and DL models</li> </ol>"},{"location":"quick_tour/#install-tcbench","title":"Install <code>tcbench</code>","text":"<p>If you unpacked the artifacts...</p> <p>When unpacking the artifacts, <code>tcbench</code> has been already installed</p> <p>First prepare a python virtual environment, for example via  conda <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> <p>Grab the latest <code>code_artifacts_paper132.tgz</code> from  figshare and unpack it. It contains a folder <code>/code_artifacts_paper132</code> from which you can trigger the installation.</p> <pre><code>cd code_artifacts_paper132\npython -m pip install .\n</code></pre> <p>All dependecies are automatically installed.</p>"},{"location":"quick_tour/#tcbench-cli","title":"<code>tcbench</code> cli","text":"<p>When installing the package you also install a <code>tcbench</code> CLI script which  acts as a universal entry point to interact with the framework via a nested commands structure.</p> <p>For instance <pre><code>tcbench --help\n</code></pre></p> <p>Output</p> <pre><code> Usage: tcbench [OPTIONS] COMMAND [ARGS]...\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version      Show tcbench version and exit.                                            \u2502\n\u2502 --help         Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 aimrepo         Investigate AIM repository content.                                      \u2502\n\u2502 campaign        Triggers a modeling campaign.                                            \u2502\n\u2502 datasets        Install/Remove traffic classification datasets.                          \u2502\n\u2502 run             Triggers a modeling run.                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>For instance command <code>datasets</code> offers the following sub-commands</p> <pre><code>tcbench datasets --help\n</code></pre> <p>Output</p> <pre><code>Usage: tcbench datasets [OPTIONS] COMMAND [ARGS]...\n\n Install/Remove traffic classification datasets\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 delete                Delete a dataset                                                   \u2502\n\u2502 import                Import datasets                                                    \u2502\n\u2502 info                  Show the meta-data related to supported datasets                   \u2502\n\u2502 install               Install a dataset                                                  \u2502\n\u2502 lsparquet             Tree view of the datasets parquet files                            \u2502\n\u2502 samples-count         Show report on number of samples per class                         \u2502\n\u2502 schema                Show datasets schemas                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Those sub-commands in turn offers different options. For instance for <code>install</code></p> <pre><code>tcbench datasets install --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench datasets install [OPTIONS]\n\n Install a dataset\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --name          -n  [ucdavis-icdm19|utmobilenet21|m  Dataset to install. [required]   \u2502\n\u2502                        irage19|mirage22]                                                 \u2502\n\u2502    --input-folder  -i  PATH                             Folder where to find             \u2502\n\u2502                                                         pre-downloaded tarballs.         \u2502\n\u2502    --help                                               Show this message and exit.      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"datasets/datasets/","title":"Datasets","text":""},{"location":"datasets/datasets/#datasets-references","title":"Datasets references","text":"<p>We refer to the following datasets</p> Name Classes PDF Data Code Auto-download ucdavis-icdm19 5 pdf data code mirage19 20 pdf data - mirage22 9 pdf data - utmobilenet21 17 pdf data code <p>Warning</p> <p>Run the following commands from the root folder of the code repository (one of the scripts, namely mirage22_json_to_parquet.py has an harded dependency from mirage19_json_to_parquet.py).</p>"},{"location":"datasets/datasets/#why-and-how-to-preprocess-the-raw-data-from-each-dataset","title":"Why and how to preprocess the raw data from each dataset","text":"<p>Each dataset comes as either CSV or JSON files, with a mixed preference between per-packet and per-flow formating. Moreover, files can be organized in subfolders---namely partitions--- to reflect some aspect of the measuring campaign.</p> <p>We preprocess all dataset to create monolitich per-flow parquet files, associating to each flow numpy arrays for the packets time series used for modeling.</p> <p>The scripts for the conversion are collected in the <code>/datasets</code>  subfolder of the repository. The same folder is expected to gather the output parquet files and (later) the splits used for modeling.</p> <p>Note</p> <p>Our modeling framework provides some flexibility to bypass this limitation but, as of now, this is not fully supported yet.</p> <p>Note</p> <p>The code for generating the charts is in <code>/notebooks/datasets_properties.ipynb</code></p>"},{"location":"datasets/datasets/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<p>The dataset comprises 3 partitions with the following structure</p> <pre><code>&lt;root folder&gt;\n\u251c\u2500\u2500 pretraining\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u251c\u2500\u2500 Retraining(human-triggered)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u2514\u2500\u2500 Retraining(script-triggered)\n    \u251c\u2500\u2500 Google Doc\n    \u251c\u2500\u2500 Google Drive\n    \u251c\u2500\u2500 Google Music\n    \u251c\u2500\u2500 Google Search\n    \u2514\u2500\u2500 Youtube\n</code></pre> <p>Inside each nested folder there is a collection CSV files. Each file corresponds to a different flow,  where each row represent individual packet information.</p> <p>The aim of the pre-processing is to load all CSV into a single parquet file <code>ucdavis-icdm19.parquet</code>, and \"transpose\" the representation--- rather than having indivial packet for each row, we create one row per flow (the flow_id is the filename itself) encoding the packet time series into numpy arrays.</p> <p>The final dataset has the following columns</p> <ul> <li><code>row_id</code>: a unique row id</li> <li><code>app</code>: the label of the flow, encoded as pandas <code>category</code></li> <li><code>flow_id</code>: the original filename</li> <li><code>partition</code>: the partition related to the flow</li> <li><code>num_pkts</code>: number of packets in the flow</li> <li><code>duration</code>: the duration of the flow</li> <li><code>bytes</code>: the number of bytes of the flow</li> <li><code>unixtime</code>: numpy array with the absolute time of each packet</li> <li><code>timetofirst</code>: numpy array with the delta between a packet the first packet of the flow</li> <li><code>pkts_size</code>: numpy array for the packet size time series</li> <li><code>pkts_dir</code>: numpy array for the packet direction time series</li> <li><code>pkts_iat</code>: numpy array for the packet inter-arrival time series</li> </ul> <pre><code>python datasets/ucdavis-icdm19_csv-to-parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-csvs&gt; \\\n    --output-folder datasets/ucdavis-icdm19\n</code></pre> output <pre><code>found 6672 files to load\n\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\nloading complete\nsaving datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#mirage19","title":"mirage19","text":"<p>The dataset is a collection of JSON files for 20 applications</p> <p>Note</p> <p>Despite the website and the related paper mention that the dataset contains 40 application, the public version has only 20. With separate communication with the authors of the dataset, we understood that the remaining 20 are available only upon request (altough not explicitly specified). As result, we considered only the 20 publicly available.</p> <p>The files collection is organized as follows <pre><code>/mnt/storage/nfs/TLS/mirage-unina/mirage-19\n\u2514\u2500 MIRAGE-2019_traffic_dataset_downloadable\n\u00a0\u00a0 \u251c\u2500\u2500 Mi5_38_a4_ed_18_cc_bf\n\u00a0\u00a0 \u2514\u2500\u2500 Nexus7_bc_ee_7b_a4_09_47\n</code></pre></p> <p>Inside each nested folder there is a collection of JSON files with some semantical information embedded in the names themselves.</p> <p>Each JSON has fairly complicated nested structure which makes it very difficult to work with.</p> <p>The purpose of the preprocessing is to</p> <ol> <li>Combine all JSON into a monolithic parquet file <code>mirage19.parquet</code></li> <li>Flatten the nested structure. For instance, a dictionary     such as {\"a\":{\"b\":1, \"c\":2}} is transformed into two separate     columns \"a_b\" and \"a_c\" with the respective values</li> <li>Add a <code>\"background\"</code> class by processing the original     label compared the JSON filenames. More specifically,     each JSON file detail the name of the app used during     a measurement campaign. But the traffic in an experiment     can be related to a different app/service running in parallel.     The decoupling of the two is facilitated by the column     <code>flow_metadata_bf_label</code> which is collected using <code>netstat</code>     from the mobile phone: if <code>flow_metadata_bf_label</code> !=      the expected app name, we mark the flow as <code>background</code></li> <li>The dataset contains raw packet bytes across multiple packets     of a flow. We process these series to search for ASCII strings.     This can be usefull for extract (in a lazy way) TLS     handshake information (e.g., SNI or certificate info)</li> </ol> <p>The final parquet files has 127 columns, and most of them comes from the original dataset itself. They are not documented but fairly easy to understand based on the name.</p> <p>The most important one are</p> <ul> <li><code>packet_data_packet_dir</code>: the time series of the packet direction</li> <li><code>packet_data_l4_payload_bytes</code>: the time series of the packet size</li> <li><code>packet_data_iat</code>: the time series of the packet inter-arrival time</li> <li><code>flow_metadata_bf_label</code>: the label gathered via netstat</li> <li><code>strings</code>: the ASCII string recovered from the payload analysis</li> <li><code>android_name</code>: the app used for an experiment</li> <li><code>app</code>: the final label encoded as a pandas <code>category</code></li> </ul> <pre><code>python datasets/mirage19_json_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-json&gt;/MIRAGE-2019_traffic_dataset_downloadable \\\n    --output-f0lder datasets/mirage19 \\\n    --workers 30\n</code></pre> output <pre><code>found 1642 files\n..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done\nsaving: mirage19/mirage19.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#mirage22","title":"mirage22","text":"<p>The data has the same format as mirage-19, i.e., a collection of JSON files.</p> <p>The dataset contains the following structure</p> <pre><code>/mnt/storage/nfs/TLS/mirage-unina/mirage-22\n\u2514\u2500\u2500 MIRAGE-COVID-CCMA-2022\n\u00a0\u00a0 \u251c\u2500\u2500 Preprocessed_pickle\n\u00a0\u00a0 \u2514\u2500\u2500 Raw_JSON\n\u00a0\u00a0     \u251c\u2500\u2500 Discord\n\u00a0\u00a0     \u251c\u2500\u2500 GotoMeeting\n\u00a0\u00a0     \u251c\u2500\u2500 Meet\n\u00a0\u00a0     \u251c\u2500\u2500 Messenger\n\u00a0\u00a0     \u251c\u2500\u2500 Skype\n\u00a0\u00a0     \u251c\u2500\u2500 Slack\n\u00a0\u00a0     \u251c\u2500\u2500 Teams\n\u00a0\u00a0     \u251c\u2500\u2500 Webex\n\u00a0\u00a0     \u2514\u2500\u2500 Zoom\n</code></pre> <p>!!! warning:     We disegarded the pickle preprocessed version because (from what we reverse engineered)      encodes a series of object in the same pickle, but we found it cumbersome to work with.</p> <p>Please refer to mirage-19 for details about pre-processing</p> <pre><code>python datasets/mirage22_json_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-json&gt;/MIRAGE-COVID-CCMA-2022/Raw_JSON/ \\\n    --output-f0lder datasets/mirage22\n    --workers 30\n</code></pre> output <pre><code>found 998 files\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done\nsaving: datasets/mirage22/mirage22.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#utmobilenet21","title":"utmobilenet21","text":"<p>The dataset is a collection of per-packet CSV files divided into 4 partitions representing 4 different measurement campaigns.</p> <p>The files are organized as follows <pre><code>&lt;root&gt;\n\u2514\u2500\u2500 csvs\n \u00a0\u00a0 \u251c\u2500\u2500 Action-Specific Wild Test Data\n \u00a0\u00a0 \u251c\u2500\u2500 Deterministic Automated Data\n \u00a0\u00a0 \u251c\u2500\u2500 Randomized Automated Data\n \u00a0\u00a0 \u2514\u2500\u2500 Wild Test Data\n</code></pre></p> <p>Broadly speaking, the dataset has the same preprocessing needs of ucdavid-icmd19, i.e., being formatted per-packet, we pre-process it into per-flow and create numpy time series.</p> <p>The final <code>utmobilenet21.parquet</code> files contains the following columns</p> <ul> <li><code>row_id</code>: a unique flow id</li> <li><code>src_ip</code>: the source ip of the flow</li> <li><code>src_port</code>: the source port of the flow</li> <li><code>dst_ip</code>: the destination ip of the flow</li> <li><code>dst_port</code>: the destination port of the flow</li> <li><code>ip_proto</code>: the protocol of the flow (TCP or UDP)</li> <li><code>first</code>: timestamp of the first packet</li> <li><code>last</code>: timestamp of the last packet</li> <li><code>duration</code>: duration of the flow</li> <li><code>packets</code>: number of packets in the flow</li> <li><code>bytes</code>: number of bytes in the flow</li> <li><code>partition</code>: from which folder the flow was originally stored</li> <li><code>location</code>: a label originally provided by the dataset (see the related paper for details)</li> <li><code>fname</code>: the original filename where the packets of the flow come from </li> <li><code>app</code>: the final label of the flow, encoded as pandas <code>category</code></li> <li><code>pkts_size</code>: the numpy array for the packet size time series</li> <li><code>pkts_dir</code>: the numpy array for the packet diretion time series</li> <li><code>timetofirst</code>: the numpy array for the delta between the each packet timestamp the first packet of the flow</li> </ul> <pre><code>python datasets/utmobilenet21_csv_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-dataset&gt;/csvs \\\n    --output-folder ./datasets/utmobilenet21 \\\n    --tmp-staging-folder /tmp/processing-utmobilenet21 \\\n    --num-workers 10\n</code></pre> output <pre><code>processing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Wild Test Data\nfound 14 files\n..............\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/wild_test_data.parquet\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Deterministic Automated Data\nfound 3438 files\n..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/deterministic_automated_data.parquet\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Action-Specific Wild Test Data\nfound 43 files\n...........................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/action-specific_wild_test_data.parquet\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Randomized Automated Data\nfound 288 files\n................................................................................................................................................................................................................................................................................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/randomized_automated_data.parquet\nmerging all partitions\nsaving: datasets/utmobilenet21/utmobilenet21.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets_splits/","title":"Datasets splits","text":"<p>The splits described here are specific for our submission and the aim to replicate the previous IMC22 paper.</p>"},{"location":"datasets/datasets_splits/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<p>Differently from the other datasets inhere described, <code>ucdavis-icdm19</code> does NOT require any filtering/adaptation after transforming the original CSV into a monolithic parquet.</p> <p>The testing partition are also predefined (\"human\" and \"script\").</p> <p>We need however to define splits of 100 samples per class for modeling. To do so we perform a random shuffle of  the data and generate 5 non overlapping groups of 100 samples.</p> <pre><code>python datasets/generate_splits.py --config config.yml\n</code></pre> output <pre><code>loading: datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\nsaving: datasets/ucdavis-icdm19/train_split_0.parquet\nsaving: datasets/ucdavis-icdm19/train_split_1.parquet\nsaving: datasets/ucdavis-icdm19/train_split_2.parquet\nsaving: datasets/ucdavis-icdm19/train_split_3.parquet\nsaving: datasets/ucdavis-icdm19/train_split_4.parquet\nloading: datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\nsaving: datasets/ucdavis-icdm19/test_split_human.parquet\nsaving: datasets/ucdavis-icdm19/test_split_script.parquet\n</code></pre>"},{"location":"datasets/import/","title":"Import/Delete","text":"<p>The <code>datasets</code> command offers also the option to import a pre-computed curation of datasets.</p> <p>This is </p> <ul> <li> <p>To avoid spending computation.  Some of the preprocessing requires ingenuity and multiprocessing/multicore architecture. </p> </li> <li> <p>Further strength replicability (although the curation process is deterministic).</p> </li> </ul>"},{"location":"datasets/import/#the-import-subcommand","title":"The <code>import</code> subcommand","text":"<p>To <code>import</code> sub-command enables to  add to <code>tcbench</code> a pre-created curated datasets, e.g., the artifacts available on figshare.</p> <p>It requires the data to be in a folder (so  unpack the tarball if you use prepared artifacts). For instance, assuming the data is stored under <code>./curated-datasets</code></p> <pre><code>tcbench datasets import --input-folder ./curated-datasets\n</code></pre> <p>Info</p> <pre><code>tcbench datasets info --name ucdavis-icdm19\nDatasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n         \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n         \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n         \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm1\n                           9/preprocessed\n         \ud83d\udcc1 data splits:   /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm1\n                           9/preprocessed/imc23\n</code></pre> <p>Notice that <code>installed</code> is not set. Indeed the prepared curated datasets do NOT repack the original datasets, just the preprocessed ones (see the meta-data page).</p>"},{"location":"datasets/import/#the-delete-subcommand","title":"The <code>delete</code> subcommand","text":"<p>You can use the <code>delete</code> subcommand to remove installed/imported datasets.</p> <p>For instance, continuing the example above</p> <pre><code>tcbench datasets delete --name ucdavis-icdm19\n</code></pre> <p>...now <code>info</code> stats all data for <code>ucdavis-icdm19</code> has been removed</p> <pre><code>tcbench datasets info --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n         \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n         \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n         \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  None\n         \ud83d\udcc1 data splits:   None\n</code></pre>"},{"location":"datasets/install/","title":"Install","text":""},{"location":"datasets/install/#supported-datasets","title":"Supported datasets","text":"<p>TCBench integrates and curates the following traffic classification datasets</p>"},{"location":"datasets/install/#table-datasets-properties","title":"Table : Datasets properties","text":"Name No. Classes Links Auto-download <code>ucdavis-icdm19</code> 5 <code>mirage19</code> 20 <code>mirage22</code> 9 <code>utmobilenet21</code> 17 <p>Unfortunately, there is no single format used when preparing datasets for public release.</p> <ul> <li> <p>Datasets come as either CSV or JSON files collections with either per-packet or per-flow records. </p> </li> <li> <p>Files can be organized in subfolders, namely partitions, named to reflect the related measurement campaign (see <code>ucdavis-icdm19</code>, <code>utmobilenet21</code>).</p> </li> <li> <p>File names can carry semantic and/or the classification  require preprocessing to be obtained by separating it from background (see <code>mirage19</code> and <code>mirage22</code>).</p> </li> <li> <p>Datasets typically do not have native splits (i.e., train/validation/test splits) nor are ready to use for app modeling (e.g., ICMP packets, short flows, etc., are not filtered).</p> </li> </ul>"},{"location":"datasets/install/#datasets-curation-at-glance","title":"Datasets curation at-glance","text":"<p> We target per-flow classification where each flow is associated to packet time series input. </p> <p>To do so, we take an opinionated view on how datasets should be formatted and handled.</p> <p>Specifically, when installing a dataset, the dataset raw data goes through the following steps:</p> <ol> <li> <p>Download and unpack: <code>tcbench</code> enables you to directly fetch the raw data from the Internet or install it from a folder where data was pre-downloaded, and unpack it into a predefined destination folder which,  for simplicity, corresponds to a subfolder of the python environment where  <code>tcbench</code> is installed. This folder is mantained and can accessed for futher ad-hoc processing outside the functionalities of <code>tcbench</code>.</p> </li> <li> <p>Preprocess: Once unpacked, the dataset raw data is converted into monolithic packet files. Such files are left untouched, i.e., they simply serve as a re-organization of the original data (with a per-flow view where needed) with an homogeneous format across datasets.</p> </li> <li> <p>Filter and split: The monolithic parquet files are first filtered (e.g., removing very short flows or flow related to invalid IP addresses) and then used to train/validation/test splits. Both steps are necessary to enable traffic modeling and replicability; yet they reflect our opinionated view on how to handle dataset for traffic classification.</p> </li> </ol>"},{"location":"datasets/install/#datasets-meta-data","title":"Datasets meta-data","text":"<p>As part of the curation process,  <code>tcbench</code> enables you to show meta-data related to the datasets.</p> <p>For instance, the refences collected in the summary table reported at the top of this page can be visualized issuing</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  None\n         \ud83d\udcc1 data splits:   None\n</code></pre> <p>Beside showing the a set of static properties (e.g., URL links),  the 3 properties <code>installed</code>, <code>preprocessed</code> nd <code>data_splits</code>  reports the absolute path where the related data is stored. The example refers to the initial setup where no dataset is yet installed.</p> <p>However, when unpacking artifacts with the  provided scripts,  the curated datasets are automatically installed</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed\n         \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23\n</code></pre> <p>Notice that * installed is <code>None</code> because this refers to the     original raw data of the datasets (which is not     provided with the curated artifacts).</p> <ul> <li>preprocessed and data splits are two specific types     of curation of the datasets.</li> </ul> <p>In fact, the artifacts unpacking uses a dataset import process.</p> <p>When installing a dataset, <code>tcbench</code> also shows two types of reports as formatted tables.</p> <ul> <li> <p>Samples count: This tables collect the number of samples (i.e., flows) available.</p> </li> <li> <p>Stats: The curation process can filter out flows (e.g., based on a minum number of packets or remove classes without a minimum number of flows). As such, when  installing, <code>tcbench</code> is showing general stats (mean, std, percentiles) about number of packets for each flow across classes.</p> </li> </ul> <p>Please check out the datasets meta-data page for more details.</p>"},{"location":"datasets/install/#ucdavis-icdm19","title":"<code>ucdavis-icdm19</code>","text":"<p>This dataset cannot be downloaded directly (as it is stored on Google Drive). So, it needs to be manually downloaded.</p> <p>More specifically, the dataset is composed of 3 zip files that need to be kept in a single folder.</p> <p>For instance <pre><code>downloads/\n\u251c\u2500\u2500 pretraining.zip\n\u251c\u2500\u2500 Retraining(human-triggered).zip\n\u2514\u2500\u2500 Retraining(script-triggered).zip\n</code></pre></p>"},{"location":"datasets/install/#original-structure","title":"Original structure","text":"<p>The 3 files correspond to 3 partitions with different scopes: <code>pretraining</code> is  meant for training while the other two for testing.</p> <p>When all zips are unpacked, the folder structure becomes <pre><code>downloads/\n\u251c\u2500\u2500 pretraining\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u251c\u2500\u2500 Retraining(human-triggered)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u2514\u2500\u2500 Retraining(script-triggered)\n    \u251c\u2500\u2500 Google Doc\n    \u251c\u2500\u2500 Google Drive\n    \u251c\u2500\u2500 Google Music\n    \u251c\u2500\u2500 Google Search\n    \u2514\u2500\u2500 Youtube\n</code></pre></p> <p>Inside each nested folder there is a collection of CSV files. Each file corresponds to a different flow,  where each row represents individual packet information.</p>"},{"location":"datasets/install/#curation","title":"Curation","text":"<ul> <li> <p>No filtering is applied to the original data.</p> </li> <li> <p>The only processing applied is to \"transpose\"  the data with respect to the original representation. Specifically, original CSV are per-packet while we create pandas DataFrames where one row  represents one flow and we gather all packets of the flow into numpy arrays.</p> </li> <li> <p>During the conversion the original folder structure is preserved via extra columns (<code>partition</code> and <code>flow_id</code>).</p> </li> </ul>"},{"location":"datasets/install/#splits","title":"Splits","text":"<p>This dataset was used in <code>imc22-paper</code>, hence we follow the splits described in the paper</p> <ul> <li>From <code>pretraining</code> we generate 5 random splits, each with 100 samples per class.</li> <li>The other two partitions are left as is and are used for testing.</li> </ul> <p>Both training and testing splits are \"materialized\", i.e., differently from <code>mirage19</code>, <code>mirage22</code>, and <code>utmobilenet21</code>, the splits are NOT collection or row indexes but rather already filtered views of the monolithic  parquet files.</p> <p>Hence, all splits have the same columns.</p> Field Description <code>row_id</code> A unique row id <code>app</code> The label of the flow, encoded as pandas <code>category</code> <code>flow_id</code> The original filename <code>partition</code> The partition related to the flow <code>num_pkts</code> Number of packets in the flow <code>duration</code> The duration of the flow <code>bytes</code> The number of bytes of the flow <code>unixtime</code> Numpy array with the absolute time of each packet <code>timetofirst</code> Numpy array with the delta between a packet the first packet of the flow <code>pkts_size</code> Numpy array for the packet size time series <code>pkts_dir</code> Numpy array for the packet direction time series <code>pkts_iat</code> Numpy array for the packet inter-arrival time series"},{"location":"datasets/install/#install","title":"Install","text":"<p>To install the dataset run (assuming data was pre-downloaded under <code>/downloads</code>)</p> <pre><code>tcbench datasets install \\\n    --name ucdavis-icdm19 \\\n    --input-folder ./downloads/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nopening: downloads/pretraining.zip\nopening: downloads/Retraining(human-triggered).zip\nopening: downloads/Retraining(script-triggered).zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 6672 CSV files to load\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00\nconcatenating files\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_1.parquet\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_2.parquet\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_3.parquet\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_4.parquet\nsamples count : train_split = 0 to 4\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet\nsamples count : test_split_human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet\nsamples count : test_split_script\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502      30 \u2502\n\u2502 google-drive  \u2502      30 \u2502\n\u2502 google-music  \u2502      30 \u2502\n\u2502 google-search \u2502      30 \u2502\n\u2502 youtube       \u2502      30 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Focusing on the reports...</p> <p>Notice the following:</p> <ul> <li>There is a monolithic parquet file containing the original partitions. For readability, the samples count report is groupped by partition</li> <li>There are 5 train splits (with 100 samples per class)</li> <li>There are 2 test splits (i.e., <code>human</code> and <code>script</code>) matching the related partitions from the raw dataset.</li> </ul>"},{"location":"datasets/install/#mirage19","title":"<code>mirage19</code>","text":"<p>The dataset is a collection of JSON files gathering per-flow data from 20 Android apps.</p> <p>...but why not 40 apps?</p> <p>Despite the <code>mirage19</code> website and the related paper mention the availability of 40 applications, the public version has only 20. With separate communication with the authors of the dataset, we understood that the remaining 20 are available only upon request (altough not explicitly specified). As result, we considered only the 20 publicly available.</p> <p>The dataset can be downloaded directly by <code>tcbench</code> or can be pre-downloaded and placed into a folder.</p> <p>For instance <pre><code>downloads/\n\u2514\u2500\u2500 MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz \n</code></pre></p>"},{"location":"datasets/install/#original-structure_1","title":"Original structure","text":"<p>Once unpacked the dataset has the following structure <pre><code>downloads/\n\u2514\u2500 MIRAGE-2019_traffic_dataset_downloadable\n\u00a0\u00a0 \u251c\u2500\u2500 Mi5_38_a4_ed_18_cc_bf\n\u00a0\u00a0 \u2514\u2500\u2500 Nexus7_bc_ee_7b_a4_09_47\n</code></pre></p> <p>The subfolders contain collections of JSON files, each representing a different experiment. The application is encoded both in the filenames as well as extra metadata in the JSON structure (<code>flow</code> <code>metadata</code> <code>bf</code> <code>label</code>).</p> <p>Each JSON file is already per-flow, but they have a fairly complicated nested structure. For example, aggregate flow metrics are hierarchially separated from packet time series, which are further separated from other metadata.</p>"},{"location":"datasets/install/#curation_1","title":"Curation","text":"<ol> <li> <p>Combine all JSON into a monolithic parquet file.</p> </li> <li> <p>Flatten the JSON nested structure.      For instance, the nested input dictionary     <code>{\"layer1\":{\"col1\":1, \"col2\":2}}</code>      would be flattened into a table     with columns \"layer1_col1\" and \"layer1_col2\"      with the respective values \"1\" and \"2\".</p> </li> <li> <p>Add a <code>\"background\"</code> class. More specifically,     each JSON file details the Android      app name in the file name. But the traffic in an experiment     can be related to a different app/service running in parallel.     However, the dataset offers the column     <code>flow_metadata_bf_label</code> which contains the     Android app name that <code>netstat</code> linked to each     network socket during an experiment.     This implies that, by knowing the expected app     of an experiment, one can define as \"background\"       <code>flow_metadata_bf_label</code> !=      expected Android app name.</p> </li> <li> <p>The dataset contains raw packet bytes across multiple packets     of a flow. We process these series to search for      ASCII strings.     This can be usefull for extract (in a lazy way) TLS     handshake information (e.g., SNI or certificate info).</p> </li> </ol> <p>The final parquet files has 127 columns, and most of them comes from the original dataset itself. They are not documented but fairly easy to understand based on the name. Thus, we prune some columns in the final filtered files.</p> <p>The most important ones are</p> Field Description <code>packet_data_packet_dir</code> The time series of the packet direction <code>packet_data_l4_payload_bytes</code> The time series of the packet size <code>packet_data_iat</code> The time series of the packet inter-arrival time <code>flow_metadata_bf_label</code> The label gathered via netstat <code>strings</code> The ASCII string recovered from the payload analysis <code>android_name</code> The app used for an experiment <code>app</code> The final label encoded as a pandas <code>category</code> <code>row_id</code> A unique row identifier <p>Please refer to the datasets schema page for more details.</p>"},{"location":"datasets/install/#splits_1","title":"Splits","text":"<p>Once preprocessed, the monolithic dataset is further processed to:</p> <ul> <li>Remove ACK packets from time series.</li> <li>Remove flows with &lt; 10 samples.</li> <li>Remove apps with &lt; 100 samples.</li> </ul> <p>From the remaining traffic we define 5 train/val/test splits with the following logic</p> <ol> <li>Shuffle the rows.</li> <li>Perform a 90/10 split where the 10-part is used for testing.</li> <li>From the 90-part, do a second 90/10 to define train and validation.</li> </ol> <p>The splits are NOT materialized, i.e.,  splits are a collection of row indexes that needs to be applied on the filtered monolithic parquet in order to obtain the data for modeling</p> <p>The structure of the splits table is as follows</p> Field Description <code>train_indexes</code> A numpy array with the <code>row_id</code> related to the train split <code>val_indexes</code> ... validation split <code>test_indexes</code> ... test split <code>split_index</code> The index of the split (0..4)"},{"location":"datasets/install/#install_1","title":"Install","text":"<p>To install the dataset run</p> <pre><code>tcbench datasets install --name mirage19\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502download &amp; unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDownloading... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5 GB / 1.5 GB eta 0:00:00\nopening: /tmp/tmpxcdzy8tw/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 1642 JSON files to load\nConverting JSONs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1642/1642 0:00:11\nmerging files...\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/mirage19.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/mirage19.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.waze                    \u2502   11737 \u2502\n\u2502 de.motain.iliga             \u2502   10810 \u2502\n\u2502 com.accuweather.android     \u2502   10631 \u2502\n\u2502 com.duolingo                \u2502    8319 \u2502\n\u2502 it.subito                   \u2502    8167 \u2502\n\u2502 com.contextlogic.wish       \u2502    6507 \u2502\n\u2502 com.spotify.music           \u2502    6431 \u2502\n\u2502 com.joelapenna.foursquared  \u2502    6399 \u2502\n\u2502 com.google.android.youtube  \u2502    6346 \u2502\n\u2502 com.iconology.comics        \u2502    5516 \u2502\n\u2502 com.facebook.katana         \u2502    5368 \u2502\n\u2502 com.dropbox.android         \u2502    4815 \u2502\n\u2502 com.twitter.android         \u2502    4734 \u2502\n\u2502 background                  \u2502    4439 \u2502\n\u2502 com.pinterest               \u2502    4078 \u2502\n\u2502 com.facebook.orca           \u2502    4018 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502    3572 \u2502\n\u2502 air.com.hypah.io.slither    \u2502    3088 \u2502\n\u2502 com.viber.voip              \u2502    2740 \u2502\n\u2502 com.trello                  \u2502    2306 \u2502\n\u2502 com.groupon                 \u2502    1986 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502  122007 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503    value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502 122007.0 \u2502\n\u2502 mean  \u2502    23.11 \u2502\n\u2502 std   \u2502     9.73 \u2502\n\u2502 min   \u2502      1.0 \u2502\n\u2502 25%   \u2502     17.0 \u2502\n\u2502 50%   \u2502     26.0 \u2502\n\u2502 75%   \u2502     32.0 \u2502\n\u2502 max   \u2502     32.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=10...\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23/mirage19_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 de.motain.iliga             \u2502    7505 \u2502\n\u2502 com.waze                    \u2502    7214 \u2502\n\u2502 com.duolingo                \u2502    4583 \u2502\n\u2502 it.subito                   \u2502    4299 \u2502\n\u2502 com.contextlogic.wish       \u2502    3927 \u2502\n\u2502 com.accuweather.android     \u2502    3737 \u2502\n\u2502 com.joelapenna.foursquared  \u2502    3627 \u2502\n\u2502 com.spotify.music           \u2502    3300 \u2502\n\u2502 com.dropbox.android         \u2502    3189 \u2502\n\u2502 com.facebook.katana         \u2502    2878 \u2502\n\u2502 com.iconology.comics        \u2502    2812 \u2502\n\u2502 com.twitter.android         \u2502    2805 \u2502\n\u2502 com.google.android.youtube  \u2502    2728 \u2502\n\u2502 com.pinterest               \u2502    2450 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502    2052 \u2502\n\u2502 com.facebook.orca           \u2502    1783 \u2502\n\u2502 com.viber.voip              \u2502    1618 \u2502\n\u2502 com.trello                  \u2502    1478 \u2502\n\u2502 com.groupon                 \u2502    1174 \u2502\n\u2502 air.com.hypah.io.slither    \u2502    1013 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502   64172 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503   value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502 64172.0 \u2502\n\u2502 mean  \u2502   17.01 \u2502\n\u2502 std   \u2502    4.43 \u2502\n\u2502 min   \u2502    11.0 \u2502\n\u2502 25%   \u2502    14.0 \u2502\n\u2502 50%   \u2502    17.0 \u2502\n\u2502 75%   \u2502    19.0 \u2502\n\u2502 max   \u2502    32.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23/mirage19_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 de.motain.iliga             \u2502          6079 \u2502         675 \u2502          751 \u2502        7505 \u2502\n\u2502 com.waze                    \u2502          5844 \u2502         649 \u2502          721 \u2502        7214 \u2502\n\u2502 com.duolingo                \u2502          3712 \u2502         413 \u2502          458 \u2502        4583 \u2502\n\u2502 it.subito                   \u2502          3482 \u2502         387 \u2502          430 \u2502        4299 \u2502\n\u2502 com.contextlogic.wish       \u2502          3181 \u2502         353 \u2502          393 \u2502        3927 \u2502\n\u2502 com.accuweather.android     \u2502          3027 \u2502         336 \u2502          374 \u2502        3737 \u2502\n\u2502 com.joelapenna.foursquared  \u2502          2938 \u2502         326 \u2502          363 \u2502        3627 \u2502\n\u2502 com.spotify.music           \u2502          2673 \u2502         297 \u2502          330 \u2502        3300 \u2502\n\u2502 com.dropbox.android         \u2502          2583 \u2502         287 \u2502          319 \u2502        3189 \u2502\n\u2502 com.facebook.katana         \u2502          2331 \u2502         259 \u2502          288 \u2502        2878 \u2502\n\u2502 com.iconology.comics        \u2502          2278 \u2502         253 \u2502          281 \u2502        2812 \u2502\n\u2502 com.twitter.android         \u2502          2272 \u2502         252 \u2502          281 \u2502        2805 \u2502\n\u2502 com.google.android.youtube  \u2502          2209 \u2502         246 \u2502          273 \u2502        2728 \u2502\n\u2502 com.pinterest               \u2502          1984 \u2502         221 \u2502          245 \u2502        2450 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502          1662 \u2502         185 \u2502          205 \u2502        2052 \u2502\n\u2502 com.facebook.orca           \u2502          1444 \u2502         161 \u2502          178 \u2502        1783 \u2502\n\u2502 com.viber.voip              \u2502          1310 \u2502         146 \u2502          162 \u2502        1618 \u2502\n\u2502 com.trello                  \u2502          1197 \u2502         133 \u2502          148 \u2502        1478 \u2502\n\u2502 com.groupon                 \u2502           951 \u2502         106 \u2502          117 \u2502        1174 \u2502\n\u2502 air.com.hypah.io.slither    \u2502           821 \u2502          91 \u2502          101 \u2502        1013 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502         51978 \u2502        5776 \u2502         6418 \u2502       64172 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Focusing on the reports...</p> <p>Notice the following:</p> <ul> <li>The unfiltered dataset has many flows but observe (see packets stats) that those are mostly short.</li> <li>The splits are formed so to have roughly 100 samples for validation/test.</li> </ul>"},{"location":"datasets/install/#install-mirage22","title":"Install <code>mirage22</code>","text":"<p>The dataset is from the same authors of <code>mirage19</code> so  the two datasets have many commonalities.</p> <p>The major difference is the dataset aim: It focuses on video meeting Android apps with experiments annotated with respect to different interactions the the apps (voice, chat, etc.)</p>"},{"location":"datasets/install/#original-structure_2","title":"Original structure","text":"<p>The dataset can be downloaded automatically  (or can be pre-downloaded into a folder).</p> <p>For instance <pre><code>downloads/\n\u2514\u2500\u2500 MIRAGE-COVID-CCMA-2022.zip\n</code></pre></p> <p>Once unpacked it has the following structure <pre><code>downloads/\n\u2514\u2500\u2500 MIRAGE-COVID-CCMA-2022\n\u00a0\u00a0 \u251c\u2500\u2500 Preprocessed_pickle\n\u00a0\u00a0 \u2514\u2500\u2500 Raw_JSON\n\u00a0\u00a0     \u251c\u2500\u2500 Discord\n\u00a0\u00a0     \u251c\u2500\u2500 GotoMeeting\n\u00a0\u00a0     \u251c\u2500\u2500 Meet\n\u00a0\u00a0     \u251c\u2500\u2500 Messenger\n\u00a0\u00a0     \u251c\u2500\u2500 Skype\n\u00a0\u00a0     \u251c\u2500\u2500 Slack\n\u00a0\u00a0     \u251c\u2500\u2500 Teams\n\u00a0\u00a0     \u251c\u2500\u2500 Webex\n\u00a0\u00a0     \u2514\u2500\u2500 Zoom\n</code></pre></p> <p>Notice the two subfolders:</p> <ul> <li> <p><code>Raw_JSON</code> gathers the nested JSON files for each experiment.</p> </li> <li> <p><code>Preprocessed_pickle</code> is a pickle serialization of the  data (undocumented by the authors).</p> </li> </ul>"},{"location":"datasets/install/#curation_2","title":"Curation","text":"<p>Same as for <code>mirage19</code> curation</p>"},{"location":"datasets/install/#splits_2","title":"Splits","text":"<p>Again, similar to <code>mirage19</code> splits.</p> <p>The only difference is that we apply two filtering on flow length (at least 10 packets and at least 1000 packets).</p>"},{"location":"datasets/install/#install_2","title":"Install","text":"<p>To install the dataset run</p> <pre><code>tcbench datasets install --name mirage22\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502download &amp; unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDownloading... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1 GB / 3.1 GB eta 0:00:00\nopening: /tmp/tmp3marsp7l/MIRAGE-COVID-CCMA-2022.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Discord.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/GotoMeeting.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Meet.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Messenger.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Skype.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Slack.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Teams.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Webex.zip\nopening: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Zoom.zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 998 JSON files to load\nConverting JSONs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 998/998 0:00:28\nmerging files...\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/preprocessed/mirage22.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/mirage22.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 background                       \u2502   18882 \u2502\n\u2502 com.microsoft.teams              \u2502    6541 \u2502\n\u2502 com.skype.raider                 \u2502    6203 \u2502\n\u2502 us.zoom.videomeetings            \u2502    5066 \u2502\n\u2502 com.cisco.webex.meetings         \u2502    4789 \u2502\n\u2502 com.discord                      \u2502    4337 \u2502\n\u2502 com.facebook.orca                \u2502    4321 \u2502\n\u2502 com.gotomeeting                  \u2502    3695 \u2502\n\u2502 com.Slack                        \u2502    2985 \u2502\n\u2502 com.google.android.apps.meetings \u2502    2252 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   59071 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   59071.0 \u2502\n\u2502 mean  \u2502   3068.32 \u2502\n\u2502 std   \u2502  25416.43 \u2502\n\u2502 min   \u2502       1.0 \u2502\n\u2502 25%   \u2502      20.0 \u2502\n\u2502 50%   \u2502      27.0 \u2502\n\u2502 75%   \u2502      42.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=10...\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.cisco.webex.meetings         \u2502    4437 \u2502\n\u2502 com.skype.raider                 \u2502    4117 \u2502\n\u2502 com.microsoft.teams              \u2502    3857 \u2502\n\u2502 us.zoom.videomeetings            \u2502    3587 \u2502\n\u2502 com.discord                      \u2502    3387 \u2502\n\u2502 com.facebook.orca                \u2502    2623 \u2502\n\u2502 com.gotomeeting                  \u2502    2557 \u2502\n\u2502 com.google.android.apps.meetings \u2502    1238 \u2502\n\u2502 com.Slack                        \u2502     970 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   26773 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   26773.0 \u2502\n\u2502 mean  \u2502   6598.23 \u2502\n\u2502 std   \u2502  37290.08 \u2502\n\u2502 min   \u2502      11.0 \u2502\n\u2502 25%   \u2502      15.0 \u2502\n\u2502 50%   \u2502      21.0 \u2502\n\u2502 75%   \u2502     186.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.cisco.webex.meetings         \u2502          3594 \u2502         399 \u2502          444 \u2502        4437 \u2502\n\u2502 com.skype.raider                 \u2502          3334 \u2502         371 \u2502          412 \u2502        4117 \u2502\n\u2502 com.microsoft.teams              \u2502          3124 \u2502         347 \u2502          386 \u2502        3857 \u2502\n\u2502 us.zoom.videomeetings            \u2502          2905 \u2502         323 \u2502          359 \u2502        3587 \u2502\n\u2502 com.discord                      \u2502          2743 \u2502         305 \u2502          339 \u2502        3387 \u2502\n\u2502 com.facebook.orca                \u2502          2125 \u2502         236 \u2502          262 \u2502        2623 \u2502\n\u2502 com.gotomeeting                  \u2502          2072 \u2502         230 \u2502          255 \u2502        2557 \u2502\n\u2502 com.google.android.apps.meetings \u2502          1002 \u2502         112 \u2502          124 \u2502        1238 \u2502\n\u2502 com.Slack                        \u2502           786 \u2502          87 \u2502           97 \u2502         970 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502         21685 \u2502        2410 \u2502         2678 \u2502       26773 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=1000...\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts1000.parquet\nsamples count : filtered (min_pkts=1000)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502    2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502     425 \u2502\n\u2502 com.google.android.apps.meetings \u2502     379 \u2502\n\u2502 com.microsoft.teams              \u2502     321 \u2502\n\u2502 com.gotomeeting                  \u2502     297 \u2502\n\u2502 com.facebook.orca                \u2502     280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502     259 \u2502\n\u2502 com.Slack                        \u2502     198 \u2502\n\u2502 com.skype.raider                 \u2502     190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502    4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=1000)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502    4569.0 \u2502\n\u2502 mean  \u2502  38321.32 \u2502\n\u2502 std   \u2502   83282.0 \u2502\n\u2502 min   \u2502    1001.0 \u2502\n\u2502 25%   \u2502    2863.0 \u2502\n\u2502 50%   \u2502    6303.0 \u2502\n\u2502 75%   \u2502   35392.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts1000_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502          1798 \u2502         200 \u2502          222 \u2502        2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502           344 \u2502          39 \u2502           42 \u2502         425 \u2502\n\u2502 com.google.android.apps.meetings \u2502           307 \u2502          34 \u2502           38 \u2502         379 \u2502\n\u2502 com.microsoft.teams              \u2502           260 \u2502          29 \u2502           32 \u2502         321 \u2502\n\u2502 com.gotomeeting                  \u2502           240 \u2502          27 \u2502           30 \u2502         297 \u2502\n\u2502 com.facebook.orca                \u2502           227 \u2502          25 \u2502           28 \u2502         280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502           210 \u2502          23 \u2502           26 \u2502         259 \u2502\n\u2502 com.Slack                        \u2502           160 \u2502          18 \u2502           20 \u2502         198 \u2502\n\u2502 com.skype.raider                 \u2502           154 \u2502          17 \u2502           19 \u2502         190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502          3700 \u2502         412 \u2502          457 \u2502        4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Focusing on the reports...</p> <p>Notice the following:</p> <ul> <li>The unfiltered dataset shows a lot of small flows, but this bias reduces when applying the filtering.</li> </ul>"},{"location":"datasets/install/#utmobilenet21","title":"<code>utmobilenet21</code>","text":"<p>The dataset is a collection of per-packet CSV files  related to 17 Android apps across  4 different partitions each corresponding to a different interaction with each app.</p> <p>The dataset cannot be downloaded directly (because it is stored into a Box cloud storage) so you need to pre-download it and save it into a folder.</p> <p>For instance <pre><code>downloads/\n\u2514\u2500\u2500 UTMobileNet2021.zip\n</code></pre></p>"},{"location":"datasets/install/#original-structure_3","title":"Original structure","text":"<p>Once unpacked, the datasets is organized as follows <pre><code>downloads/\n\u2514\u2500\u2500 csvs\n \u00a0\u00a0 \u251c\u2500\u2500 Action-Specific Wild Test Data\n \u00a0\u00a0 \u251c\u2500\u2500 Deterministic Automated Data\n \u00a0\u00a0 \u251c\u2500\u2500 Randomized Automated Data\n \u00a0\u00a0 \u2514\u2500\u2500 Wild Test Data\n</code></pre></p> <p>Each subfolders of <code>csvs/</code> corresponds to a different partition (the folder name is informing you about the semantic of the original measurement campaign).</p> <p>Within each subfolder there is a collection of CSVs generated running <code>tshark</code>, i.e., they are per-packet logs.</p> <p>Differently from <code>mirage19</code> and <code>mirage22</code>, the only label available is provided by CSV file names.</p>"},{"location":"datasets/install/#curation_3","title":"Curation","text":"<ul> <li> <p>Some of the original CSVs files have rows which break the parsing via common utilities such as <code>pandas.read_csv()</code>. Moreover, some columns have missing values, while others have missing types between files (e.g., ports can be either int of floats). So extra care is taken to properly ingest the CSVs.</p> </li> <li> <p>We Filter out packets which are not TCP or UDP.</p> </li> <li> <p>Then, packets are organized into flows using the traditional network 5-tuple.</p> </li> </ul> <p>The final monolithic parquet files has the following columns</p> Field Description <code>row_id</code> A unique flow id <code>src_ip</code> The source ip of the flow <code>src_port</code> The source port of the flow <code>dst_ip</code> The destination ip of the flow <code>dst_port</code> The destination port of the flow <code>ip_proto</code> The protocol of the flow (TCP or UDP) <code>first</code> Timestamp of the first packet <code>last</code> Timestamp of the last packet <code>duration</code> Duration of the flow <code>packets</code> Number of packets in the flow <code>bytes</code> Number of bytes in the flow <code>partition</code> From which folder the flow was originally stored <code>location</code> A label originally provided by the dataset (see the related paper for details) <code>fname</code> The original filename where the packets of the flow come from <code>app</code> The final label of the flow, encoded as pandas <code>category</code> <code>pkts_size</code> The numpy array for the packet size time series <code>pkts_dir</code> The numpy array for the packet diretion time series <code>timetofirst</code> The numpy array for the delta between the each packet timestamp the first packet of the flow"},{"location":"datasets/install/#splits_3","title":"Splits","text":"<p>Once preprocessed, the monolithic dataset is further processed to:</p> <ul> <li>Remove flows with &lt; 10 samples</li> <li>Remove apps with &lt; 100 samples</li> </ul> <p>From the remaining traffic we define 5 train/val/test splits with the following logic</p> <ol> <li>Shuffle the rows</li> <li>Perform a 90/10 split where the 10-part is used for testing</li> <li>From the 90-part, do a second 90/10 to define train and validation</li> </ol> <p>The splits are NOT materialized, i.e.,  splits are a collection of row indexes that needs to be applied on the filtered monolithic parquet in order to obtain the data for modeling</p> <p>The structure of the split table is</p> Field Description <code>train_indexes</code> A numpy array with the <code>row_id</code> related to the train split <code>val_indexes</code> ... validation split <code>test_indexes</code> ... test split <code>split_index</code> The index of the split (0..4)"},{"location":"datasets/install/#install_3","title":"Install","text":"<p>To install the dataset run (assuming data was pre-downloaded under <code>/downloads</code>)</p> <pre><code>tcbench datasets install \\\n    --name utmobilenet21 \\\n    --input-folder downloads/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nopening: downloads/UTMobileNet2021.zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nprocessing: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Action-Specific Wild Test Data\nfound 43 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43/43 0:01:15\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/action-specific_wild_test_data.parquet\n\nprocessing: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Wild Test Data\nfound 14 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14/14 0:03:12\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/wild_test_data.parquet\n\nprocessing: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Randomized Automated Data\nfound 288 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 288/288 0:01:35\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/randomized_automated_data.parquet\n\nprocessing: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Deterministic Automated Data\nfound 3438 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3438/3438 0:08:26\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/deterministic_automated_data.parquet\nmerging all partitions\nsaving: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/preprocessed/utmobilenet21.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: /opt/anaconda/anaconda3/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/utmobilenet21.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502    4716 \u2502\n\u2502 reddit       \u2502    3622 \u2502\n\u2502 google-maps  \u2502    3475 \u2502\n\u2502 netflix      \u2502    1804 \u2502\n\u2502 pinterest    \u2502    1702 \u2502\n\u2502 dropbox      \u2502    1609 \u2502\n\u2502 instagram    \u2502    1426 \u2502\n\u2502 gmail        \u2502     848 \u2502\n\u2502 google-drive \u2502     709 \u2502\n\u2502 messenger    \u2502     690 \u2502\n\u2502 hangout      \u2502     483 \u2502\n\u2502 facebook     \u2502     364 \u2502\n\u2502 twitter      \u2502     308 \u2502\n\u2502 hulu         \u2502     294 \u2502\n\u2502 spotify      \u2502     252 \u2502\n\u2502 pandora      \u2502      70 \u2502\n\u2502 skype        \u2502      57 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502   22429 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   22429.0 \u2502\n\u2502 mean  \u2502    716.33 \u2502\n\u2502 std   \u2502  22271.93 \u2502\n\u2502 min   \u2502       1.0 \u2502\n\u2502 25%   \u2502       2.0 \u2502\n\u2502 50%   \u2502       2.0 \u2502\n\u2502 75%   \u2502      15.0 \u2502\n\u2502 max   \u2502 1973657.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: /opt/anaconda/anaconda3/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23/utmobilenet21_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502    2153 \u2502\n\u2502 google-maps  \u2502    1391 \u2502\n\u2502 reddit       \u2502     654 \u2502\n\u2502 netflix      \u2502     317 \u2502\n\u2502 pinterest    \u2502     312 \u2502\n\u2502 dropbox      \u2502     211 \u2502\n\u2502 instagram    \u2502     205 \u2502\n\u2502 hangout      \u2502     176 \u2502\n\u2502 hulu         \u2502     162 \u2502\n\u2502 google-drive \u2502     104 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502    5685 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502    5685.0 \u2502\n\u2502 mean  \u2502   2740.55 \u2502\n\u2502 std   \u2502  44152.43 \u2502\n\u2502 min   \u2502      11.0 \u2502\n\u2502 25%   \u2502      25.0 \u2502\n\u2502 50%   \u2502      44.0 \u2502\n\u2502 75%   \u2502     156.0 \u2502\n\u2502 max   \u2502 1973657.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: /opt/anaconda/anaconda3/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23/utmobilenet21_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502          1743 \u2502         194 \u2502          216 \u2502        2153 \u2502\n\u2502 google-maps  \u2502          1127 \u2502         125 \u2502          139 \u2502        1391 \u2502\n\u2502 reddit       \u2502           530 \u2502          59 \u2502           65 \u2502         654 \u2502\n\u2502 netflix      \u2502           256 \u2502          29 \u2502           32 \u2502         317 \u2502\n\u2502 pinterest    \u2502           253 \u2502          28 \u2502           31 \u2502         312 \u2502\n\u2502 dropbox      \u2502           171 \u2502          19 \u2502           21 \u2502         211 \u2502\n\u2502 instagram    \u2502           166 \u2502          18 \u2502           21 \u2502         205 \u2502\n\u2502 hangout      \u2502           142 \u2502          16 \u2502           18 \u2502         176 \u2502\n\u2502 hulu         \u2502           131 \u2502          15 \u2502           16 \u2502         162 \u2502\n\u2502 google-drive \u2502            85 \u2502           9 \u2502           10 \u2502         104 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502          4604 \u2502         512 \u2502          569 \u2502        5685 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Focusing on the reports...</p> <p>Notice the following:</p> <ul> <li> <p>From the packet stats of the original (unfiltered) dataset we can see there are a lot of small flows. Those are removed when considering a minimum flow length of 10.</p> </li> <li> <p>On top of this filtering we also remove apps with less than 100 flows, i.e., only 10 of the original 17 apps can be used for modeling.</p> </li> </ul>"},{"location":"datasets/metadata/","title":"Meta-data","text":"<p>The <code>tcbench</code> CLI can show 4 types of meta-data:</p> <ul> <li> <p> Static information: A collection of  URL links with datasets documentation and folders path related the installation.</p> </li> <li> <p> List of curated files: An organized view of the files generated during installation.</p> </li> <li> <p> Schemas: A formatted view of the schemas of installed files.</p> </li> <li> <p> Samples count report: A per-app breakdown of the number of samples.</p> </li> </ul>"},{"location":"datasets/metadata/#static-information","title":"Static information","text":"<p>The static information corresponds to the information displayed in the  datasets properties shown in the installation page.</p> <p>To show it in the console run</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  None\n         \ud83d\udcc1 data splits:   None\n</code></pre> <p>The example above corresponds to the case when no dataset is installed.</p> <p>After a dataset is installed, the remaining properties are filled. Specifically, as suggested by the icon, the last 3 properties of each dataset  correspond to folders generated via the curation:</p> <ul> <li> <p><code>\"installed\"</code> is the path where the raw data of the dataset is unpacked.</p> </li> <li> <p><code>\"preprocessed\"</code> is the path where the preprocessed data is stored, i.e., the monolithic per-flow parquet files (with no filtering applied).</p> </li> <li> <p><code>\"data splits\"</code> is the folder where filtered data and data splits are stored, i.e., the data used for modeling.</p> </li> </ul> <p>The <code>datasets info</code> sub-command supports the option <code>--name</code> to filter out information for just one dataset.</p> <p>For instance, after installing <code>ucdavis-icdm19</code>, its information are:</p> <pre><code>tcbench datasets info --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n         \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n         \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n         \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n         \ud83d\udcc1 installed:     /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/raw\n         \ud83d\udcc1 preprocessed:  /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed\n         \ud83d\udcc1 data splits:   /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23\n</code></pre>"},{"location":"datasets/metadata/#list-of-curated-files","title":"List of curated files","text":"<p>As reported by <code>datasets info</code>,  both datasets raw data and curated parquet files  are stored into a subfolder of the python environment.</p> <p>Specifically, the folder is structure is as follows: <pre><code>/datasets\n  \u2514\u2500\u2500 &lt;dataset-name&gt; \n        \u2514\u2500\u2500 /raw\n        \u2514\u2500\u2500 /preprocessed\n             \u2514\u2500\u2500 /imc23\n</code></pre></p> <p>where</p> <ul> <li><code>/raw</code> contains the raw data of the datasets.</li> <li><code>/preprocessed</code> contains the monolithic parquet files.</li> <li><code>/imc23</code> contains the filtererd monolithic parquet files and the splits generated for the submission.</li> </ul> <p>One can better inspect this structure via the <code>datasets lsparquet</code> sub-command</p> <pre><code>tcbench datasets lsparquet\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 ucdavis-icdm19.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 test_split_human.parquet\n\u2502           \u251c\u2500\u2500 test_split_script.parquet\n\u2502           \u251c\u2500\u2500 train_split_0.parquet\n\u2502           \u251c\u2500\u2500 train_split_1.parquet\n\u2502           \u251c\u2500\u2500 train_split_2.parquet\n\u2502           \u251c\u2500\u2500 train_split_3.parquet\n\u2502           \u2514\u2500\u2500 train_split_4.parquet\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 mirage19.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 mirage19_filtered_minpkts10.parquet\n\u2502           \u2514\u2500\u2500 mirage19_filtered_minpkts10_splits.parquet\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 mirage22.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts10.parquet\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts1000.parquet\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts1000_splits.parquet\n\u2502           \u2514\u2500\u2500 mirage22_filtered_minpkts10_splits.parquet\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 utmobilenet21.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 utmobilenet21_filtered_minpkts10.parquet\n            \u2514\u2500\u2500 utmobilenet21_filtered_minpkts10_splits.parquet\n</code></pre> <p>While all datasets have a file <code>&lt;dataset-name&gt;.parquet</code>  which corresponds to the monolithic version of the raw data, the content of the <code>/imc23</code> folder is slightly different between datasets</p> <ul> <li> <p>For <code>ucdavis-icdm19</code> split are \"materialized\". This means that the files <code>train_split_[0-4].parquet</code> contains the data to use for training (the actual train/val split is operated at runtime) while <code>test_split_human.parquet</code> and <code>text_split_script.parquet</code> are predefined test split already available in the raw dataset.</p> </li> <li> <p>For all other datasets, the files <code>xyz_minpkts&lt;N&gt;.parquet</code> contains a filtered version of the monolithic data (see install page for more details on the filtering) and the related <code>xyz_minpkts&lt;N&gt;_split.parquet</code> contains the index of the rows to use for train/val/test splits.</p> </li> </ul> <p>The tutorial about load and explore data offers more details about how to handle those differences.</p>"},{"location":"datasets/metadata/#schemas","title":"Schemas","text":"<p>Via the <code>datasets schema</code> sub-command is possible to visualize the schema of the curated parquet files.</p> <pre><code>tcbench datasets schema --help\n\n Usage: tcbench datasets schema [OPTIONS]\n\n Show datasets schemas\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --name  -n  [ucdavis-icdm19|utmobilenet21|mirage19|mirage22]  Dataset to install                                         \u2502\n\u2502 --type  -t  [unfiltered|filtered|splits]                      Schema type (unfiltered: original raw data; filtered:      \u2502\n\u2502                                                               curated data; splits: train/val/test splits)               \u2502\n\u2502 --help                                                        Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Beside the dataset name <code>--name</code>, the selection of the schema is simplified via a single parameter <code>--type</code> which matches the parquet files as follows</p> <ul> <li> <p><code>\"unfiltered\"</code> corresponds to the monolithic  before any filtering (i.e., the files under <code>/preprocessed</code>)</p> </li> <li> <p><code>\"filtered\"</code> corresponds to the filtered  version of the monolithic files (i.e., the files having <code>minpkts&lt;N&gt;</code> in the filename).</p> </li> <li> <p><code>\"splits\"</code> corresponds to the split files (i.e., the files having <code>xyz_split.parquet</code> in the filename).</p> </li> </ul> <p>While for <code>ucdavis-icdm19</code> the three schema types are the same, for the other datasets there are differences.</p> <p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p>"},{"location":"datasets/metadata/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<pre><code>tcbench datasets schema --name ucdavis-icdm19 --type unfiltered\n</code></pre> <p>Output</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type filtered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type splits <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#mirage19","title":"<code>mirage19</code>","text":"tcbench datasets schema --name mirage19 <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 float    \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 float    \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 float    \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 float    \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 float    \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 float    \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 float    \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 float    \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 float    \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protocol                                                \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 float    \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#mirage22","title":"<code>mirage22</code>","text":"tcbench datasets schema --name mirage22 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_timestamp                                     \u2502 np.array \u2502 Time series of packet unixtime                             \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_ip_packet_bytes                               \u2502 np.array \u2502 Time series pkts bytes (as from IP len field)              \u2502\n\u2502 packet_data_ip_header_bytes                               \u2502 np.array \u2502 Time series of IP header bytes                             \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_l4_header_bytes                               \u2502 np.array \u2502 Time series of L4 header bytes                             \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_tcp_flags                                     \u2502 np.array \u2502 Time series of TCP flags                                   \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 packet_data_is_clear                                      \u2502 np.array \u2502 n.a.                                                       \u2502\n\u2502 packet_data_heuristic                                     \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 packet_data_annotations                                   \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_device                                   \u2502 str      \u2502 Ethernet address                                           \u2502\n\u2502 flow_metadata_bf_label_source                             \u2502 str      \u2502 Constant value 'netstate'                                  \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_sublabel                                 \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_code                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_name                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 int      \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 int      \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 int      \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 int      \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 int      \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 int      \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_uf_mss                                      \u2502 float    \u2502 Upload maximum segment size                                \u2502\n\u2502 flow_metadata_uf_ws                                       \u2502 float    \u2502 Upload window scaling                                      \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 int      \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 int      \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 int      \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 flow_metadata_df_mss                                      \u2502 float    \u2502 Download maximum segment size                              \u2502\n\u2502 flow_metadata_df_ws                                       \u2502 float    \u2502 Download window scaling                                    \u2502\n\u2502 flow_metadata_bf_activity                                 \u2502 str      \u2502 Experiment activity                                        \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protol                                                  \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_activity         \u2502 str      \u2502 Experiment activity                                                  \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 int      \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#utmobilenet21","title":"<code>utmobilenet21</code>","text":"tcbench datasets schema --name utmobilenet21 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#samples-count-report","title":"Samples count report","text":"<p>As the name suggests, a samples count report details how many samples (i.e., flows) are available for each app. These reports are shown during installation, but can  be retrieved at any time using the subcommand <code>datasets samples-count</code>.</p> <p>They can be generated for unfiltered, filtered or based on splits, but the command requires familiarity with the parametrization semantic.</p>"},{"location":"datasets/metadata/#ucdavis-icdm19_1","title":"ucdavis-icdm19","text":"<p>For instance, the following provides the unfitered view for the <code>ucdavis-icdm19</code> dataset</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>While to obtain the breakdown of the first train split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split 0\n</code></pre> <p>Output</p> <pre><code>filtered, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>...or the <code>human</code> test split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split human\n</code></pre> <p>Output</p> <pre><code>filtered, split: human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#examples-for-other-datasets","title":"Examples for other datasets","text":"<p>Other datasets can be filtered based on the <code>--min_pkts</code> options.</p> <p>For instance, the following is the overall view for <code>mirage22</code></p> <pre><code>tcbench datasets samples-count --name mirage22\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 background                       \u2502   18882 \u2502\n\u2502 com.microsoft.teams              \u2502    6541 \u2502\n\u2502 com.skype.raider                 \u2502    6203 \u2502\n\u2502 us.zoom.videomeetings            \u2502    5066 \u2502\n\u2502 com.cisco.webex.meetings         \u2502    4789 \u2502\n\u2502 com.discord                      \u2502    4337 \u2502\n\u2502 com.facebook.orca                \u2502    4321 \u2502\n\u2502 com.gotomeeting                  \u2502    3695 \u2502\n\u2502 com.Slack                        \u2502    2985 \u2502\n\u2502 com.google.android.apps.meetings \u2502    2252 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   59071 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This counts reduce when filtering by <code>--min-pkts 1000</code></p> <pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 1000\n</code></pre> <p>Output</p> <pre><code>min_pkts: 1000\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502    2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502     425 \u2502\n\u2502 com.google.android.apps.meetings \u2502     379 \u2502\n\u2502 com.microsoft.teams              \u2502     321 \u2502\n\u2502 com.gotomeeting                  \u2502     297 \u2502\n\u2502 com.facebook.orca                \u2502     280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502     259 \u2502\n\u2502 com.Slack                        \u2502     198 \u2502\n\u2502 com.skype.raider                 \u2502     190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502    4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>...and you can also obtain the breakdown from a specific split <pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 1000 --split 0\n</code></pre></p> <p>Output</p> <pre><code>min_pkts: 1000, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502          1798 \u2502         200 \u2502          222 \u2502        2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502           344 \u2502          39 \u2502           42 \u2502         425 \u2502\n\u2502 com.google.android.apps.meetings \u2502           307 \u2502          34 \u2502           38 \u2502         379 \u2502\n\u2502 com.microsoft.teams              \u2502           260 \u2502          29 \u2502           32 \u2502         321 \u2502\n\u2502 com.gotomeeting                  \u2502           240 \u2502          27 \u2502           30 \u2502         297 \u2502\n\u2502 com.facebook.orca                \u2502           227 \u2502          25 \u2502           28 \u2502         280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502           210 \u2502          23 \u2502           26 \u2502         259 \u2502\n\u2502 com.Slack                        \u2502           160 \u2502          18 \u2502           20 \u2502         198 \u2502\n\u2502 com.skype.raider                 \u2502           154 \u2502          17 \u2502           19 \u2502         190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502          3700 \u2502         412 \u2502          457 \u2502        4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/tutorial_load_parquet/","title":"Tutorial - Load parquet files","text":"<p>Let's import <code>tcbench</code> and map its alias <code>tcb</code></p> <p>The module automatically import a few functions and constants.</p> In\u00a0[1]: Copied! <pre>import tcbench as tcb\n</pre> import tcbench as tcb <p>You can first discover the  path where the datasets are installed using <code>.get_datasets_root_folder()</code> In\u00a0[2]: Copied! <pre>root_folder = tcb.get_datasets_root_folder()\nroot_folder\n</pre> root_folder = tcb.get_datasets_root_folder() root_folder Out[2]: <pre>PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets')</pre> <p>The function returns a <code>pathlib</code> path so you can take advantage of it to navigate the subfolders structure.</p> <p>For instance:</p> In\u00a0[3]: Copied! <pre>list(root_folder.iterdir())\n</pre> list(root_folder.iterdir()) Out[3]: <pre>[PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19')]</pre> <p>As from the output, each dataset is mapped to a different folder  named after the dataset itself. Meaning, again taking advantage of <code>pathlib</code>,  you can compose path based on strings.</p> <p>For instance:</p> In\u00a0[4]: Copied! <pre>list((root_folder / 'ucdavis-icdm19').iterdir())\n</pre> list((root_folder / 'ucdavis-icdm19').iterdir()) Out[4]: <pre>[PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/raw'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed')]</pre> <p>A more polished way to reference datasets is via the <code>tcbench.DATASETS</code> attribute which corresponds to a python enumeration object</p> In\u00a0[5]: Copied! <pre>type(tcb.DATASETS), list(tcb.DATASETS)\n</pre> type(tcb.DATASETS), list(tcb.DATASETS) Out[5]: <pre>(enum.EnumMeta,\n [&lt;DATASETS.UCDAVISICDM19: 'ucdavis-icdm19'&gt;,\n  &lt;DATASETS.UTMOBILENET21: 'utmobilenet21'&gt;,\n  &lt;DATASETS.MIRAGE19: 'mirage19'&gt;,\n  &lt;DATASETS.MIRAGE22: 'mirage22'&gt;])</pre> <p>For instance, you can bypass the composition of a dataset folder path and call directly <code>.get_dataset_folder()</code> to find the specific  dataset folder you look for.</p> In\u00a0[6]: Copied! <pre>dataset_folder = tcb.get_dataset_folder(tcb.DATASETS.UCDAVISICDM19)\ndataset_folder\n</pre> dataset_folder = tcb.get_dataset_folder(tcb.DATASETS.UCDAVISICDM19) dataset_folder Out[6]: <pre>PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19')</pre> <p>Via <code>pathlib</code> you can easily discover all parquet files composing a dataset</p> In\u00a0[7]: Copied! <pre>list(dataset_folder.rglob('*.parquet'))\n</pre> list(dataset_folder.rglob('*.parquet')) Out[7]: <pre>[PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_3.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_4.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_2.parquet'),\n PosixPath('/opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_1.parquet')]</pre> <p>But you can also programmatically call the the <code>datasets lsparquet</code> subcommand of the CLI using <code>get_rich_tree_parquet_files()</code></p> In\u00a0[8]: Copied! <pre>from tcbench.libtcdatasets.datasets_utils import get_rich_tree_parquet_files\nget_rich_tree_parquet_files(tcb.DATASETS.UCDAVISICDM19)\n</pre> from tcbench.libtcdatasets.datasets_utils import get_rich_tree_parquet_files get_rich_tree_parquet_files(tcb.DATASETS.UCDAVISICDM19) Out[8]: <pre>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 ucdavis-icdm19.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 test_split_human.parquet\n            \u251c\u2500\u2500 test_split_script.parquet\n            \u251c\u2500\u2500 train_split_0.parquet\n            \u251c\u2500\u2500 train_split_1.parquet\n            \u251c\u2500\u2500 train_split_2.parquet\n            \u251c\u2500\u2500 train_split_3.parquet\n            \u2514\u2500\u2500 train_split_4.parquet\n</pre> <p>Finally, the generic <code>.load_parquet()</code> can be used to load one of the parquet files.</p> <p>For instance, the following load the unfiltered monolithic file of the <code>ucdavis-icdm19</code> dataset</p> In\u00a0[9]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n</pre> df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19) In\u00a0[10]: Copied! <pre>df.head(2)\n</pre> df.head(2) Out[10]: row_id app flow_id partition num_pkts duration bytes unixtime timetofirst pkts_size pkts_dir pkts_iat 0 0 google-doc GoogleDoc-100 pretraining 2925 116.348 816029 [1527993495.652867, 1527993495.685678, 1527993... [0.0, 0.0328109, 0.261392, 0.262656, 0.263943,... [354, 87, 323, 1412, 1412, 107, 1412, 180, 141... [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ... [0.0, 0.0328109, 0.2285811, 0.0012639999999999... 1 1 google-doc GoogleDoc-1000 pretraining 2813 116.592 794628 [1527987720.40456, 1527987720.422811, 15279877... [0.0, 0.0182509, 0.645106, 0.646344, 0.647689,... [295, 87, 301, 1412, 1412, 1412, 180, 113, 141... [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0.0, 0.0182509, 0.6268551, 0.0012380000000000... In\u00a0[11]: Copied! <pre>df.groupby(['partition', 'app'])['app'].value_counts()\n</pre> df.groupby(['partition', 'app'])['app'].value_counts() Out[11]: <pre>partition                    app          \npretraining                  google-doc       1221\n                             google-drive     1634\n                             google-music      592\n                             google-search    1915\n                             youtube          1077\nretraining-human-triggered   google-doc         15\n                             google-drive       18\n                             google-music       15\n                             google-search      15\n                             youtube            20\nretraining-script-triggered  google-doc         30\n                             google-drive       30\n                             google-music       30\n                             google-search      30\n                             youtube            30\nName: count, dtype: int64</pre> <p>Beside the dataset name, the function only has 2 other parameters, but their semantic and values are \"mingled\" with the curation process adopted.</p> In\u00a0[12]: Copied! <pre>tcb.load_parquet?\n</pre> tcb.load_parquet? <pre>Signature:\ntcb.load_parquet(\n    dataset_name: 'str | DATASETS',\n    min_pkts: 'int' = -1,\n    split: 'str' = None,\n) -&gt; 'pd.DataFrame'\nDocstring:\nLoad and returns a dataset parquet file\n\nArguments:\n    dataset_name: The name of the dataset\n    min_pkts: the filtering rule applied when curating the datasets.\n        If -1, load the unfiltered dataset\n    split: if min_pkts!=-1, is used to request the loading of \n        the split file. For DATASETS.UCDAVISICDM19 \n        values can be \"human\", \"script\" or a number \n        between 0 and 4.\n        For all other dataset split can be anything \n        which is not None (e.g., True)\n\nReturns:\n    A pandas dataframe\nFile:      /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets_utils.py\nType:      function</pre> <p>The logic to follow to load specific files can be confusing. The table below report a global view across datasets:</p> Dataset min_pkts=-1 min_pkts=10 min_pkts=1000 split=True split=0..4 split=human split=script <code>ucdavis-icdm19</code> yes - - - yes (train+val) yes (test) yes (test) <code>mirage19</code> yes yes - yes (train/val/test) - - - <code>mirage22</code> yes yes yes yes (train/val/test) - - - <code>utmobilenet21</code> yes yes - yes (train/val/test) - - - <ul> <li><p><code>min_pkts=-1</code> is set by default and corresponds to loading the unfiltered parquet files, i.e., the files stored immediately under <code>/preprocessed</code>. All other files are stored under the <code>imc23</code> subfolders</p> </li> <li><p>For <code>ucdavis-icdm19</code>, the parameter <code>min_pkts</code> is not used. The loading of training(+validation) and test data is controlled by <code>split</code></p> </li> <li><p>For all other datasets, <code>min_pkts</code> specifies which filtered version of the data to use, while <code>split=True</code> load the split indexes</p> </li> </ul> <p>For instance, to load the <code>human</code> test split of <code>ucdavid-icdm19</code> you can run</p> In\u00a0[13]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='human')\ndf['app'].value_counts()\n</pre> df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='human') df['app'].value_counts() Out[13]: <pre>app\nyoutube          20\ngoogle-drive     18\ngoogle-doc       15\ngoogle-music     15\ngoogle-search    15\nName: count, dtype: int64</pre> <p>And the logic is very similar for the <code>script</code> partition</p> In\u00a0[14]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='script')\ndf['app'].value_counts()\n</pre> df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='script') df['app'].value_counts() Out[14]: <pre>app\ngoogle-doc       30\ngoogle-drive     30\ngoogle-music     30\ngoogle-search    30\nyoutube          30\nName: count, dtype: int64</pre> <p>However to load a specific train split</p> In\u00a0[16]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='0')\ndf['app'].value_counts()\n</pre> df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='0') df['app'].value_counts() Out[16]: <pre>app\ngoogle-doc       100\ngoogle-drive     100\ngoogle-music     100\ngoogle-search    100\nyoutube          100\nName: count, dtype: int64</pre> <p>By default, without any parameter beside the dataset name, the function loads the unfiltered version of a dataset</p> In\u00a0[17]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.MIRAGE19)\ndf.shape\n</pre> df = tcb.load_parquet(tcb.DATASETS.MIRAGE19) df.shape Out[17]: <pre>(122007, 135)</pre> <p>Recall the structure of the <code>mirage19</code> dataset</p> In\u00a0[18]: Copied! <pre>get_rich_tree_parquet_files(tcb.DATASETS.MIRAGE19)\n</pre> get_rich_tree_parquet_files(tcb.DATASETS.MIRAGE19) Out[18]: <pre>Datasets\n\u2514\u2500\u2500 mirage19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 mirage19.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 mirage19_filtered_minpkts10.parquet\n            \u2514\u2500\u2500 mirage19_filtered_minpkts10_splits.parquet\n</pre> <p>So there is only one filtering with <code>min_pkts=10</code></p> In\u00a0[19]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)\ndf.shape\n</pre> df = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10) df.shape Out[19]: <pre>(64172, 20)</pre> <p>Based on the dataframe shape, we can see that (indeed) we loaded a reduced version of the unfiltered dataset.</p> <p>While for <code>ucdavis-icdm19</code> the \"split\" files represent 100 samples selected for training (because there are two ad-hoc test split), for all other dataset the \"split\" files contains indexes indicating the rows to use for train/val/test.</p> <p>Thus, issuing <code>split=True</code> is enough to indicate the need to load the split table.</p> In\u00a0[20]: Copied! <pre>df_split = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10, split=True)\n</pre> df_split = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10, split=True) Out[20]: train_indexes val_indexes test_indexes split_index 0 [18965, 24694, 59797, 35708, 42030, 356, 39052... [36752, 7114, 48500, 39083, 44382, 58758, 2001... [20363, 36256, 24604, 11752, 40529, 50086, 470... 0 1 [8741, 55715, 47053, 37161, 59608, 6777, 47281... [41506, 56625, 18344, 23114, 10634, 44785, 130... [21524, 19560, 41837, 57207, 35174, 38440, 563... 1 2 [58596, 59589, 26514, 56766, 51386, 20802, 453... [11552, 34447, 16180, 21248, 28195, 16763, 387... [43026, 28228, 29243, 27753, 50389, 48093, 85,... 2 3 [22303, 11403, 53901, 919, 54389, 22144, 51538... [26990, 50118, 45109, 29126, 16420, 10965, 257... [10721, 35420, 47187, 51800, 30736, 44707, 134... 3 4 [21918, 7887, 5426, 22788, 40262, 34857, 58966... [30232, 23269, 16058, 30390, 60505, 26499, 258... [7366, 48552, 27092, 40144, 19834, 15065, 5229... 4"},{"location":"datasets/tutorial_load_parquet/#the-get_datasets_root_folder-method","title":"The <code>.get_datasets_root_folder()</code> method\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#the-datasets-enum","title":"The <code>.DATASETS</code> enum\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#the-get_dataset_folder-method","title":"The <code>.get_dataset_folder()</code> method\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#listing-files","title":"Listing files\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#the-load_parquet-method","title":"The <code>.load_parquet()</code> method\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#how-load_parquet-maps-to-parquet-files","title":"How <code>.load_parquet()</code> maps to parquet files\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#loading-ucdavis-icdm19","title":"Loading <code>ucdavis-icdm19</code>\u00b6","text":""},{"location":"datasets/tutorial_load_parquet/#loading-other-datasets","title":"Loading other datasets\u00b6","text":""},{"location":"modeling/aim_repositories_content/","title":"Aim repositories content","text":"<p>Exploring AIM repositories via the web UI or run artifacts are great ways as long as a precise target is already defined.</p> <p>To complement those options, <code>tcbench</code> offers a few sub-commands to overview an AIM repository  content.</p> <pre><code>tcbench aimrepo --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo [OPTIONS] COMMAND [ARGS]...\n\n Investigate AIM repository content.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ls                    List a subset of properties of each run.                                  \u2502\n\u2502 properties            List properties across all runs.                                          \u2502\n\u2502 report                Summarize runs performance metrics.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In the following we illustrate each sub-command using the  <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository</p>"},{"location":"modeling/aim_repositories_content/#ls","title":"<code>ls</code>","text":"<p>The <code>ls</code> sub-command simply list  the hash, creation time and end time of each run.</p> <pre><code> tcbench aimrepo ls \\\n    --aim-repo code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code> hash                      creation_time      end_time\n 8ba8f05ec1604574b71139b4  1684486220.186745  1684487165.465486\n 67503d171467435091714d45  1684485354.995328  1684486219.662343\n 9a106b1a209b4a47abb5c17f  1684484451.951014  1684485354.490275\n bfe6d326e299461cb56cdc5f  1684483871.600086  1684484451.518072\n 87fa4af7e99d478aaf22f4da  1684483362.006482  1684483871.393643\n c13844d1704c45fc95653b1f  1684482566.637334  1684483361.812976\n c7bdc0d409c4403bbe5b9cc4  1684482432.924283  1684484618.028195\n 043419ccd36541fd8a4057cb  1684482146.1981    1684482566.387968\n 963f7797f19d4976a3efcca3  1684481692.788042  1684482145.885839\n 4d81379229814f409b7647d3  1684481202.271289  1684481692.537144\n fe35f9380c344b8aa9c23f03  1684481101.181884  1684482432.517077\n 2f6d76f632c64391b7cb4973  1684480123.949839  1684481201.997798\n b3df56698ec6416cac21add4  1684479475.427219  1684481101.055126\n 72ac1602329b415dbf17524f  1684479393.541195  1684480123.490255\n 68832122ee434d799da3ec6c  1684478564.503901  1684479474.865144\n 98033a73d4d1433ab3779742  1684478415.758284  1684479393.015683\n c6aea7ff760b4ad090d9dd36  1684477658.120382  1684478415.53678\n 7e0386cdcbaa454e892b5fb9  1684477439.749558  1684478563.989332\n d985dfe5a5a6422293ff34eb  1684476688.49235   1684477657.471275\n 889dc5128693473f9fd79299  1684476501.215957  1684477439.58382\n 922fef07ff264188a2e370a5  1684475858.118653  1684476688.059573\n cfb5812f3ca84c2bb440e6c4  1684475376.248467  1684476501.070375\n 99fc7d6feea64a07a9f4499d  1684475282.908762  1684475857.977606\n 728dfe6ff0584b0db57b8f22  1684474888.182631  1684475282.672584\n 9dde5426e9424f9aa0eda8ef  1684474417.41854   1684474887.912377\n d8845f81ebbc4fbdbb7efc6e  1684474280.592608  1684475375.693128\n 4ca5aff6b9c442808f39d2c3  1684473415.875285  1684474417.168511\n f24dfd8ef3e44cd6ae37c4d7  1684473045.421477  1684474280.137564\n 9d530cacbbfc4300846c412a  1684472369.171916  1684473415.375791\n b222cda5054744ee923cc1a5  1684471606.606575  1684473045.147482\n 5c9c03c663ea493480ff176c  1684471433.960361  1684472736.412599\n 1e13e4b8c60a4dd084ca607b  1684471290.69116   1684472368.905358\n 4c42b65f85e444579adabd51  1684470601.838798  1684471289.976878\n 4f759432db0743c7b3d95f6b  1684470452.527724  1684471433.756871\n 1a71035742a940c7a74c6a9b  1684470331.022178  1684471606.113162\n 1f2bc263b9ed450abcf1c1d3  1684469705.56874   1684470601.122689\n 9951ad2c1f894efcb04d39bb  1684469472.837658  1684470451.971661\n dd4ec191cd0945a184c9aef8  1684469140.912997  1684469472.087474\n 60b55181ebaa4ccebc214875  1684468906.545817  1684470330.375135\n d74b79343de8419b874f5210  1684468808.347594  1684469705.18729\n 0a30bf4e578445239c3e91cb  1684468778.110371  1684469140.702514\n 1d0327a026444eee800dc9a1  1684468500.060794  1684468777.756142\n e40d612c892644f59370cd68  1684468394.226275  1684468808.078585\n 67c4857856d445d4809e1fd2  1684467913.759307  1684468394.08247\n cad4cd97d854419e91869d85  1684467543.981347  1684468905.978667\n a7018c0846304408bc75e8c8  1684467526.249917  1684467913.503346\n 39c042def7b94710bc211b0e  1684466951.957943  1684468499.463829\n e61e21d87ccd4aa8b8a2ed35  1684466335.110533  1684467526.054343\n b1869271d50644c696fb5246  1684466131.933304  1684467543.395299\n e0dac9c82bf04bafb4aade4c  1684465619.566809  1684466951.462375\n ed199b3b6a71488893e1abfb  1684465434.028724  1684466333.918455\n 7dc2a753e3f74ed69f069ea8  1684464725.062243  1684466131.24298\n 03a5cdeb32354bfdba1de136  1684464569.689175  1684465433.542933\n 37c00f6c1ebe44fda8a9618d  1684464407.673333  1684465618.991646\n f620bcb6afe24d2a87a778c0  1684464064.526061  1684464406.941772\n 9cdde43cddd640e2b440b1c4  1684463741.103362  1684464064.054385\n ffb1a45a1fb249c0bf3f463c  1684463607.054282  1684464569.411643\n 395fc5799efc4f34823b3655  1684463449.453486  1684463740.94038\n 5a0a0a5c3bb244d497538de4  1684463312.066392  1684464724.410855\n 0638ad838f4a429b91131a5b  1684462917.872626  1684463606.433408\n 51aa03f39a3e42e2b99db486  1684462233.665966  1684463311.043898\n 57eb602e1dad4933b5eca155  1684462207.043093  1684463448.792002\n 01fea90ec67544d3a39ef997  1684461943.457484  1684462917.499634\n d792e20d40604c78956ab5a2  1684461464.25762   1684461943.313054\n 637badf3dfec4b93a78d9919  1684461042.264266  1684461464.126894\n f87edacfdacf4b5faee625b4  1684460961.382713  1684462233.005103\n 444c2e1ebfaa43a78c6b4c96  1684460467.126398  1684461042.125671\n 9e5135774a89458b8dae96ef  1684459980.097812  1684462206.830038\n ca77db360e8f45d5a8d2f3b7  1684459693.93209   1684460466.346116\n adf0905c3c8349d590574c38  1684459300.071951  1684460960.695716\n 916583de1a524ed092408d72  1684459069.160377  1684459693.416057\n 7c25e7eca05e49ffa82e1dda  1684458823.182091  1684459979.576512\n e77cf8e464b2496a94229ca8  1684458538.286254  1684458823.017745\n 70fef369b9e741e59a9cb092  1684458437.369592  1684459067.97458\n 73c779ffb561448281772cca  1684458244.2756    1684458537.96287\n c21d1b9a093c42cbb1421b99  1684458241.060798  1684459299.580839\n e0c07fd0f3664789a68a97fd  1684457885.407864  1684458243.499636\n 7b52359ef3ef42219b6297ef  1684457777.173311  1684458436.747236\n aecb143222764ac2badda996  1684457079.7536    1684457776.789753\n 83850279a1fd43e1892771ea  1684457024.45417   1684458240.375002\n c80ccdfec89b4ac68013dc57  1684456500.647965  1684457079.174438\n bd8b6929f7bb41eeb00e8451  1684456429.302551  1684457884.834434\n cca820ec5ae24ee793b45ef3  1684456007.859746  1684456500.321697\n ad89e8df0be34133bb61e589  1684455782.341968  1684457023.582\n d9613c247ffc49ae83b1864d  1684455514.437144  1684456007.704841\n f4f28ffbd85041d78d032bf7  1684455198.659235  1684456428.772103\n 17aa80f35ba240d6a3c27681  1684454983.894229  1684455514.144682\n 376d83d39ee34bf6a5f14553  1684454900.637969  1684454978.036799\n d66de4d4c0f043289d0f9671  1684454828.019291  1684454900.501477\n 53e969d8626c41df92814c61  1684454756.332229  1684454827.858425\n 988b1457caba480b817ba385  1684454684.082903  1684454756.197079\n f60fc0f56310461ebd15ccb1  1684454602.697385  1684454683.934212\n 4231173e05fe4466bb3592e5  1684454519.796744  1684454602.561087\n b7bcc2cb30054c9d89b81e71  1684454436.86775   1684454519.639057\n 0eb416e6597f4205a4eb30e5  1684454350.202147  1684454436.706402\n 01d4bb96f3d547b0b67f3219  1684454275.221716  1684454350.058379\n 2863d94709a241549ea43d1e  1684454180.176362  1684454275.095384\n 608b4ff892424b909c37195b  1684454131.059445  1684455781.74821\n 2f4819882cab4dbc8bb4e599  1684454101.90471   1684454180.044018\n dc3e8dfccc964658b1094227  1684454031.000489  1684454101.748034\n 95e5b09ba7d44fabbb055f93  1684453973.055474  1684454030.873004\n 3ab8c90f935845bc85d9e7c0  1684453916.309955  1684453972.93122\n 8ed7f1729fd5456cbf9bf9f0  1684453858.633202  1684453916.18064\n 2838f1c9925a4276b596f29a  1684453857.763272  1684453999.474313\n fb37ebad53be4b8bb0922c43  1684453854.938354  1684455198.416701\n dbc9c91611dd40d18da3a563  1684453800.945448  1684453857.692718\n 0922df9981cb4dada7cb8e70  1684453783.490234  1684453858.502442\n 577b94c1058543fbbbac556c  1684453740.054494  1684453800.871211\n 1e8636dca105456d96c9d911  1684453708.58691   1684453783.36737\n 2927b88ec0064c1b96f2420d  1684453669.588055  1684453739.989276\n 1f79705713004475b0f4b423  1684453641.653473  1684453708.452254\n 816d410679ce4d3d93c9fb13  1684453609.559607  1684453669.528333\n f20a0864a0154d14bc62bef9  1684453603.103946  1684453641.526248\n 7ab9bc7b21f84284a8c06e2c  1684453562.770793  1684453602.990837\n b7dbd5917875476a9f331342  1684453554.43456   1684453609.487829\n e711aaf96ecc418cb1629a36  1684453522.761069  1684453562.665403\n c4d648e857fa404dbec12d64  1684453500.782744  1684453554.364838\n 810da0118ed7417db7c4771e  1684453469.291014  1684453854.683895\n 5d8a70cabc464fcbbd0a904d  1684453444.704697  1684453500.709896\n 5634546f547945849b466cf5  1684453420.998128  1684453522.623544\n c4ed1533ddbf43b5ba8f83ab  1684453389.201033  1684453444.618217\n cc13b218cbfd4cf08e5d20a4  1684453337.641638  1684453389.137557\n 610b507713cb47f4963ef671  1684453316.818646  1684453420.841483\n 36f2e4b6045b4f01b70e4680  1684453287.041721  1684453337.567413\n 37f99c3ed79a45f3b7d40a5f  1684453246.21599   1684453316.660344\n 5fe2730f842e416c8af9c9b3  1684453237.076659  1684453286.959855\n e25a532da42c4327bc11528a  1684453190.214313  1684453237.018725\n 7aea23e113cc4a289d7bfa79  1684453161.004546  1684453246.06685\n 45429199088f4c0c8b705537  1684453142.490356  1684453190.154943\n ab079de736834eef899c21bb  1684453101.882433  1684454130.262774\n fa468f7b03e3475daf215746  1684453095.863897  1684453142.425248\n add75d33ef8b4d638e770aaf  1684453075.861901  1684453468.361486\n 2aeb5a16085f42e9ab4094df  1684453065.579356  1684453160.874607\n 8e87b04207e54931b865d237  1684453016.800851  1684453095.802078\n 76f30367c8ab44b0bbe67d03  1684452977.614285  1684453065.424347\n deaffa25f04b4aae856c632f  1684452942.818858  1684453016.74139\n 5cae649ac7bb46d5a60c72ac  1684452877.834861  1684452942.748545\n 4a0efe2a9ecb499983245191  1684452870.028023  1684452977.473543\n 2e0474d626b94b5e957a3774  1684452847.893095  1684452877.790754\n 50f15777a7174531bbf50a1c  1684452818.958093  1684452847.832531\n b924f0f921e04b64828975ba  1684452789.569578  1684452818.902016\n b418ebaa6576432290bfe7ae  1684452774.458869  1684452869.895356\n b9f6b0815edf4ddfb34dc177  1684452728.81128   1684452789.506862\n 01b5d3ec52da4424a37125df  1684452700.006671  1684452774.319969\n f93d353566c24408bb3fc997  1684452663.618535  1684452728.740588\n dca1cc1dc9cc45f69bf45471  1684452629.495522  1684453074.989142\n 61a54b25733740b586ae3788  1684452604.757812  1684452663.541514\n 762d10ed26ae46ab8752cbfb  1684452601.345309  1684452699.879312\n 6cbe5bb9f82141b8abb5d2c8  1684452537.019976  1684452604.693619\n f2d476a93fef4c7abc727453  1684452508.867201  1684452601.21922\n 3d69205834a84112b65fcaef  1684452472.610688  1684452536.957576\n f2c44a9bc9f14ad4b3b827b8  1684452419.986378  1684452508.722681\n 4d2f061684c842d793d53458  1684452405.098763  1684452472.542425\n f05cbbb211a545d8b726a301  1684452365.427215  1684452419.85173\n 5f451dce01f04ee9b0169828  1684452346.577028  1684452405.012775\n 234669be443d4f08ac6dd900  1684452308.986717  1684452365.299411\n e8d6da0defd049028aaea9d5  1684452290.203461  1684452346.517978\n bd57bec0663e479ea2934678  1684452252.92531   1684452308.85739\n 352f015ecc004b7f922e9675  1684452225.314651  1684452290.121976\n 47a4a0891c024ff9ad3e10a5  1684452168.422513  1684452252.796179\n 06da93cc03a44040b0c74178  1684452157.739649  1684452225.250497\n e3d645b0dc16421ca3209896  1684452137.461206  1684453101.39133\n 7bdb5341a2614a888c577a09  1684452097.493495  1684452157.677333\n 4bbd8caafebe4379b130c6b0  1684452090.542063  1684452168.28824\n 7b25cd869f00439fafbed535  1684452029.411407  1684452097.43651\n 92c9588486534986a7a5eacf  1684451984.658156  1684452029.353263\n 61d85dcf712b434fac9a5b1f  1684451983.953664  1684452090.41075\n a12b3724c60d4958996de377  1684451944.910572  1684451983.848708\n 66dda78e98484ac695ac28c2  1684451937.460639  1684451984.606509\n 6693f2a4a645455cada34389  1684451908.020474  1684451944.789062\n e1169972d26140e18d897023  1684451886.957104  1684451937.392489\n 822c84da50f74667920f03bf  1684451871.324137  1684451907.890575\n 77bb1cec15af4c23b8be0f2f  1684451821.675738  1684451886.891239\n cf3545083d794dee8e0d2e9a  1684451812.399059  1684451871.18422\n 38171ed6da8f44e1a555b9d8  1684451748.611645  1684451821.617548\n a94ec554111b4a1a89cca0f6  1684451744.945461  1684451812.24693\n f20aca505c37459abb281e08  1684451656.412687  1684451744.789916\n 5c4aa95035c841d4a5e96b0c  1684451556.350265  1684451656.276747\n 6aa44846db614702867c9623  1684451546.981732  1684451748.381051\n c727a871d64b4b84ab39ce7b  1684451472.885078  1684451556.217995\n f1444a30a0b64ddebb0ee893  1684451451.528096  1684451546.751202\n 83ecc5c955ec4023bc82b30d  1684451382.832542  1684451472.73465\n 6c7b1cbe72004a0898b331e1  1684451373.367619  1684451451.294249\n 55a0901ebd8c4a14a006bda0  1684451332.395285  1684452629.058694\n 9c0d10c9e8f04194b6cf322c  1684451332.169061  1684451373.247294\n 1fe3f3cff5364183b91b0161  1684451311.409357  1684451382.69011\n e64349682cac4e96bdb3a195  1684451305.523624  1684452136.669685\n b6f5f861bfd045b79910163f  1684451268.06238   1684451332.097281\n 24db445e86664a5688d5b058  1684451244.011303  1684451311.247374\n 707b1d8f242c4c569397818f  1684451201.961014  1684451267.986849\n b5870e5dd6ac4433a59e949e  1684451151.07773   1684451243.875269\n 972de7c3956d4d96b27fa2e7  1684451147.277512  1684451201.870307\n 007ff3eb7d3644c9bfa97590  1684451089.133811  1684451147.206315\n be295d1045b64e5da2dad253  1684451086.977876  1684451150.932065\n 008ab3a058424242b2c74721  1684451024.406991  1684451089.065439\n 8587efea57d444b0a37a4d87  1684451012.747797  1684451086.845493\n 46cfa3ec77604b18b5cbb952  1684450970.955052  1684451024.350877\n 473c10c9d83b4a2f9f7aaacd  1684450935.840727  1684451012.61568\n 9749d96f77754ee3a1dd82dc  1684450910.230388  1684450970.876716\n bd622c8c454a43b2a526e255  1684450878.062955  1684450935.71623\n 1b69360731f64dbabd3e6b4a  1684450856.297448  1684450910.156479\n f1c8afdee5f843dea2cd6326  1684450820.288095  1684450877.926006\n 8719913ab19c470090e2cbba  1684450789.546818  1684450856.227493\n 64cc636a24de4af085a90012  1684450764.779803  1684450820.161206\n 38f7ad203bd3441cb27e8477  1684450726.736646  1684450789.489278\n 2006e773a42a45169c8af288  1684450700.498349  1684450764.63308\n f48a8088282a4f22a3060aad  1684450628.084315  1684450726.679969\n 07741993d3ff4d4c85cd2378  1684450625.213713  1684450700.367589\n 78897d14594d4d8ab0511b09  1684450532.651023  1684450625.066728\n c058300f4d55480183fe7d0a  1684450500.728736  1684450627.9136\n 70457b6d2ca34c9ca54faa9c  1684450494.905926  1684450532.530795\n fae0d0098235451582379e49  1684450457.158045  1684450494.787555\n eb0656be43d64f4abc07d913  1684450420.256119  1684450457.035452\n 3f6b15bdbb4c4f7990139390  1684450373.169881  1684450500.508234\n 37fb6128cf0940b0a8e02a6d  1684450346.729378  1684450420.123028\n 59850e2e072c4315bcfd44f9  1684450317.249483  1684451331.988945\n eb23366973d54adcb7ae41dc  1684450309.69558   1684450372.983297\n 4c51c9d6344741a3b4167ace  1684450270.958624  1684450346.587809\n 0adf8ec7b2d44e83b9d1ecd6  1684450264.164186  1684450309.641298\n 7663b098261c42b6ae806446  1684450255.519111  1684451304.891808\n 8d87effdeaf045bd8e1793be  1684450191.755371  1684450270.815829\n 4afff20583a7446a87f532bb  1684450174.269007  1684450264.10297\n a12f8c41aa7645fb8dce11db  1684450113.72971   1684450191.620866\n 278b560f285e4b3e968d2500  1684450112.787798  1684450174.213632\n 8232054671804f4a9f049ac8  1684450038.59175   1684450113.590469\n 8a2aaa0359c14bee82d7a01b  1684450038.553256  1684450112.718496\n cc14c289c387497299647aff  1684450009.504617  1684450038.503996\n c1d541e31bf04c34a8361277  1684449981.075677  1684450009.447578\n f6ae1ffbf09b4b829845768a  1684449960.174022  1684450038.453731\n e55525d57f8f4c0fbe62925c  1684449951.684003  1684449981.027956\n 3b3c950211ec4ac1861d2ef2  1684449896.906696  1684449951.618761\n a6156d0fedfb4da58d7ed157  1684449886.685146  1684449960.026201\n 34fb83c203bb4950bd629d93  1684449847.082951  1684449896.838466\n 32315d3421a643f8a60e3cc6  1684449808.13473   1684449886.545201\n 6db2eb1e1dd347acba48e369  1684449790.974563  1684449847.000392\n 63c5de9bc5ae4bca84a04a12  1684449737.9346    1684449807.970432\n ec202059cb874f4f9ca4205e  1684449725.824672  1684449790.911293\n ae692ba4c260457d9a3e442a  1684449668.959527  1684449725.763858\n a96434d27b8e45d3ad8b81ea  1684449659.840383  1684449737.633578\n 5c28fb0c4cae4a5eb3e2bf37  1684449609.2497    1684449668.911\n 401d87dc0f9d4f7fb3aa52df  1684449583.918668  1684449659.708532\n c9c4b346ff394c8488e9790d  1684449545.541382  1684449609.186583\n 74ba1657c61644e2a5e7da30  1684449520.169214  1684449583.771104\n ae7a6eb3e85a48228d3371b4  1684449492.929299  1684449545.465121\n 52615c37cb4c403a8bf7a025  1684449464.493325  1684449520.040155\n 4969fb8c374141028594d962  1684449437.814941  1684449492.858044\n 1e96377382f443ccbe7e85f0  1684449405.594734  1684449464.356868\n 776699b253614175b7f827ff  1684449387.696488  1684449437.748665\n 5687406675c34ef8bead5633  1684449346.890198  1684449405.47788\n 387e01ef08454e2eb2cff697  1684449333.183408  1684449387.6353\n 685af93f85ac4b67bb99144e  1684449302.386924  1684450255.027677\n 4ea944b1be754ab48bcb7d45  1684449276.832263  1684449346.737273\n db1c87ebb5494363a2fb2d3b  1684449237.813694  1684449333.12579\n 3e1533ac0f164dc092b7b681  1684449204.473322  1684449276.677444\n 61882760258d44d793c721dd  1684449151.709934  1684449237.749622\n 385286c19cb14a0c9571e0e9  1684449129.216098  1684449204.342291\n 63a50b8c64884ec5aa61cfa1  1684449091.168293  1684449129.090858\n a9b40ede978e4fb4bfc120b7  1684449055.11729   1684449091.054672\n cb78f86b48284696aa938fcc  1684449029.36249   1684449151.468267\n 47595fc5c64146e9805b51e6  1684449017.959478  1684449054.990691\n bf2c9394ea3742e683452a22  1684448936.264374  1684449029.147291\n c8f67a99057c49a2944bf39a  1684448932.684268  1684449017.804864\n a4cf32694e6e411ea84aa47a  1684448931.706416  1684450316.541687\n 4b3570a3a50c41c78f0000f4  1684448858.610413  1684448936.197166\n b21d99a3202e4da3b484ae4a  1684448837.720911  1684448932.548398\n f099eb327fd7494db384bb14  1684448765.679447  1684448858.545372\n 47eccc984daf4834aec19f67  1684448756.36372   1684448837.570065\n 7aaac8ecd2e644eda14968cf  1684448674.903293  1684448765.625554\n 43a503bf40aa4f518a97cf4e  1684448667.212994  1684448756.204382\n 230c734960ea4c849e3af2e1  1684448645.431733  1684448674.860142\n a6a20facc3f34662aa660fee  1684448615.158763  1684448645.376438\n cd65b0169cf441bfb6e3dcd8  1684448585.157683  1684448615.106011\n a0bc2e31538249aab969fc0d  1684448580.386622  1684448667.054529\n 3ee8b28573704e4ba79cf698  1684448527.514667  1684448585.087628\n 0712bcf076c44d239f8def39  1684448505.486607  1684448580.237805\n 45cf18b3d09a40098e621079  1684448465.274798  1684448527.439574\n 40cde1e6fa674cc3bfd82bb6  1684448423.393276  1684448931.366076\n d59fb822d0c346e496361cc6  1684448421.81773   1684448505.32149\n 13d14051be32424d92613289  1684448410.573326  1684448465.203814\n f05bc193e27d4178986a5662  1684448379.497623  1684449301.840416\n 3e378ebe488442e59997f8d2  1684448349.959204  1684448421.651724\n e5fc7761ea6a4e04bbb16b74  1684448346.800847  1684448410.504434\n 7538f4623894469ea5db1024  1684448284.44457   1684448346.71941\n c81913cba557469a90999dfa  1684448266.979889  1684448349.801789\n 6a742ccb640e4594a353c49a  1684448229.388576  1684448284.376865\n 75d1aa21751a4d0085f7c78c  1684448190.969628  1684448266.848555\n 321f7acb616040cda1232be3  1684448178.230579  1684448229.301771\n f325127637554978a28c7c4c  1684448123.997501  1684448178.150143\n ba4003ab5d674388913ad196  1684448122.292348  1684448190.811933\n ecf0408ff75948cdad056c2a  1684448069.866839  1684448123.924316\n a96a951613e947c596cd48fa  1684448051.908431  1684448122.161052\n 274ed9aae2324ebdb6b0e59a  1684448023.663688  1684448423.171355\n 81e993bee47043319ff1ab6e  1684448007.683417  1684448069.801989\n a8bdcf0e5d6f4645a7a76382  1684447991.732175  1684448051.786494\n a1f84df603a3455b8bc91007  1684447955.241488  1684448007.621067\n df5e5a6eed6542739b67f833  1684447937.451409  1684447991.609256\n f1e61e62d6fb484c9122547e  1684447898.394327  1684447955.175681\n c59bc63eb8f041049543f1d2  1684447880.325154  1684447937.315866\n 1e5a6ac24e9540f89c0c54cf  1684447853.491841  1684447898.333804\n c21a103dfa35473390e9d1fe  1684447807.994382  1684447853.423596\n e5f3fc273b114ace8bcb8478  1684447807.815813  1684447880.187383\n 0d92ca4591404ba889239d2f  1684447760.682189  1684447807.938637\n 53046971da764da1b343c6c5  1684447727.930714  1684447807.672719\n f65eae9d11414922a5cc531e  1684447662.420696  1684447760.621397\n 5e9105ff533e4b44a0dfc577  1684447636.207182  1684447727.776517\n 7e7fd7824169464b9602d7cc  1684447598.500283  1684447636.086839\n a669d0df5eaa4e47a20f5d94  1684447572.937839  1684448379.216737\n 426257bb89474ca8a339523b  1684447565.763093  1684447662.363762\n 8cc32087458448c7bd165ea0  1684447561.149784  1684447598.372651\n caaffcdf9919419cb3616619  1684447520.023255  1684447561.032779\n 3ace3b501ffc4d3d9f006a4c  1684447477.174045  1684447565.696864\n 2ec24496c7b44bc1abb9d07d  1684447446.502455  1684447477.122609\n d74763a98ac54a899facc16e  1684447417.775803  1684447446.44637\n 7e702018ebfe4d96b96b84c5  1684447395.8736    1684448023.427643\n 18dd6fca9c6944fb9ac0b77f  1684447384.730554  1684447417.721719\n</code></pre>"},{"location":"modeling/aim_repositories_content/#properties","title":"<code>properties</code>","text":"<p>The <code>properties</code> sub-command show aggregate information and the list of unique values of the properties of an AIM repo (i.e., run hyper params and other meta data).</p> <pre><code>tcbench aimrepo properties \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name                   \u2502 No. unique \u2502 Value                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 runs                   \u2502          - \u2502 315                                                       \u2502\n\u2502 duration (mean \u00b1 std)  \u2502          - \u2502 5m44s \u00b1 7m40s                                             \u2502\n\u2502 metrics                \u2502          4 \u2502 ['acc', 'best_epoch', 'best_loss', 'loss']                \u2502\n\u2502 contexts               \u2502          5 \u2502 ['test-human', 'test-script', 'test-train-val-leftover',  \u2502\n\u2502                        \u2502            \u2502 'train', 'val']                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 experiment             \u2502          1 \u2502 ['augmentation-at-loading']                               \u2502\n\u2502 aug_name               \u2502          7 \u2502 ['changertt', 'colorjitter', 'horizontalflip', 'noaug',   \u2502\n\u2502                        \u2502            \u2502 'packetloss', 'rotate', 'timeshift']                      \u2502\n\u2502 campaign_exp_idx       \u2502        105 \u2502 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,   \u2502\n\u2502                        \u2502            \u2502 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,   \u2502\n\u2502                        \u2502            \u2502 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,   \u2502\n\u2502                        \u2502            \u2502 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,   \u2502\n\u2502                        \u2502            \u2502 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,   \u2502\n\u2502                        \u2502            \u2502 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,   \u2502\n\u2502                        \u2502            \u2502 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,  \u2502\n\u2502                        \u2502            \u2502 101, 102, 103, 104, 105]                                  \u2502\n\u2502 campaign_id            \u2502          1 \u2502 ['1684447037']                                            \u2502\n\u2502 dataset                \u2502          1 \u2502 ['ucdavis-icdm19']                                        \u2502\n\u2502 flowpic_block_duration \u2502          1 \u2502 [15]                                                      \u2502\n\u2502 flowpic_dim            \u2502          3 \u2502 [32, 64, 1500]                                            \u2502\n\u2502 patience_steps         \u2502          1 \u2502 [5]                                                       \u2502\n\u2502 seed                   \u2502          3 \u2502 [42, 666, 12345]                                          \u2502\n\u2502 split_index            \u2502          5 \u2502 [0, 1, 2, 3, 4]                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The table is split into two parts to separate general properties (top) from hyper parameters (bottom). General properties are common across repositories while hyper parameters vary depending of the campaign.</p> <p>What is a context?</p> <p>The term is borrored from AIM terminology and refers to ability to group metadata into categories. </p> <p>For instance, in the example above, the 4 listed metrics are separately stored for each listed context.</p>"},{"location":"modeling/aim_repositories_content/#report","title":"<code>report</code>","text":"<p>The <code>report</code> sub-command provide an aggregated summary across metrics grouped by properties</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                                    hparams                           acc                  duration\n                   split        aug_name   flowpic_dim  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              test-human       changertt            32    15  70.04   4.41   2.44    83.93      7.8     4.32\n                                                    64    15  72.05    2.1   1.16    61.57      5.2     2.88\n                                                  1500    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  68.84   4.69   2.59    78.01     10.9     6.03\n                                                    64    15  71.33   3.35   1.86    67.15    22.49    12.45\n                                                  1500    15  68.59   3.17   1.76   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                                    64    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                                  1500    15  73.82   1.47   0.82    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  69.48   2.12   1.17    37.97     1.46     0.81\n                                                    64    15  69.88   2.28   1.26    38.06    20.21    11.19\n                                                  1500    15  68.67   1.93   1.07   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                                    64    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                                  1500    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                                    64    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                                  1500    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  70.36   2.98   1.65    80.08    12.72     7.04\n                                                    64    15  72.53   1.83   1.02    64.23    21.91    12.13\n                                                  1500    15  70.84   2.42   1.34   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             test-script       changertt            32    15  97.33   0.71   0.39    83.93      7.8     4.32\n                                                    64    15  97.29   0.64   0.35    61.57      5.2     2.88\n                                                  1500    15   96.8   0.63   0.35  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.87    0.8   0.45    78.01     10.9     6.03\n                                                    64    15  97.42    1.2   0.67    67.15    22.49    12.45\n                                                  1500    15  94.89    1.5   0.83   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.11   0.74   0.41     56.9     1.64     0.91\n                                                    64    15  95.96   0.89   0.49    63.86    28.97    16.04\n                                                  1500    15  95.11   1.23   0.68    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.73   0.49   0.27    37.97     1.46     0.81\n                                                    64    15  95.96   0.53   0.29    38.06    20.21    11.19\n                                                  1500    15  94.44   1.63    0.9   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.98   0.87   0.48    80.83    11.11     6.15\n                                                    64    15  96.89   0.96   0.53    57.08     4.75     2.63\n                                                  1500    15  95.96   1.27    0.7  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.36   0.71   0.39    78.52     11.1     6.15\n                                                    64    15  96.89    0.7   0.39    88.49    33.41     18.5\n                                                  1500    15  95.47   0.84   0.47  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  96.71   0.92   0.51    80.08    12.72     7.04\n                                                    64    15  97.11   0.65   0.36    64.23    21.91    12.13\n                                                  1500    15   96.8   0.57   0.32   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-train-val-leftover       changertt            32    15  98.24   0.56   0.31    83.93      7.8     4.32\n                                                    64    15  98.29   0.71   0.39    61.57      5.2     2.88\n                                                  1500    15  98.43   0.22   0.12  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.46    0.6   0.33    78.01     10.9     6.03\n                                                    64    15  96.82   0.74   0.41    67.15    22.49    12.45\n                                                  1500    15  95.79   0.91    0.5   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.88   0.46   0.25     56.9     1.64     0.91\n                                                    64    15  96.38   0.91    0.5    63.86    28.97    16.04\n                                                  1500    15  96.47   1.02   0.57    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  96.05   0.35   0.19    37.97     1.46     0.81\n                                                    64    15  96.22   0.57   0.31    38.06    20.21    11.19\n                                                  1500    15  95.62   0.91   0.51   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  97.47   0.64   0.35    80.83    11.11     6.15\n                                                    64    15  97.48    0.5   0.28    57.08     4.75     2.63\n                                                  1500    15  97.29   0.49   0.27  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  97.01   0.43   0.24    78.52     11.1     6.15\n                                                    64    15  97.28   0.62   0.34    88.49    33.41     18.5\n                                                  1500    15  95.93   0.74   0.41  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  97.44   0.75   0.42    80.08    12.72     7.04\n                                                    64    15  97.78   0.68   0.38    64.23    21.91    12.13\n                                                  1500    15  97.94   0.34   0.19   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The console output is informing about the <code>campain_id</code>  found in the repository and the total number of runs. If the repository was containing more than one campaign, a different report table would have been created for each campaign.</p> <p>The report is always grouped by <code>test-&lt;XYZ&gt;</code> contexts. By default all hyper parameters with more than one value are also added (see the previous description about properties).</p> <p>In the example, the <code>acc</code> metric is measured with mean, standard deviation and 95 %tile confidence intervals. Next to the metric is reported also <code>duration</code> which  corresponds to the overall time for train/validation/test  in the run execution.</p> <p>One can rearrange the table composition using the <code>--groupby</code> and <code>--contexts</code> options For instance, in the following we swap the order of the <code>hparams</code> used and report only on one context.</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/ \\\n    --groupby flowpic_dim,aug_name \\\n    --contexts test-human\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                       hparams                           acc                  duration\n      split  flowpic_dim         aug_name  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-human           32        changertt    15  70.04   4.41   2.44    83.93      7.8     4.32\n                              colorjitter    15  68.84   4.69   2.59    78.01     10.9     6.03\n                           horizontalflip    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                    noaug    15  69.48   2.12   1.17    37.97     1.46     0.81\n                               packetloss    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                   rotate    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                timeshift    15  70.36   2.98   1.65    80.08    12.72     7.04\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      64        changertt    15  72.05    2.1   1.16    61.57      5.2     2.88\n                              colorjitter    15  71.33   3.35   1.86    67.15    22.49    12.45\n                           horizontalflip    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                    noaug    15  69.88   2.28   1.26    38.06    20.21    11.19\n                               packetloss    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                   rotate    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                timeshift    15  72.53   1.83   1.02    64.23    21.91    12.13\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    1500        changertt    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                              colorjitter    15  68.59   3.17   1.76   765.43   152.53    84.47\n                           horizontalflip    15  73.82   1.47   0.82    471.8     58.6    32.45\n                                    noaug    15  68.67   1.93   1.07   374.88    94.41    52.28\n                               packetloss    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                                   rotate    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                                timeshift    15  70.84   2.42   1.34   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The <code>report</code> sub-command also creates output artifacts.</p> <ul> <li> <p>An output folder is created based on the <code>campaign_id</code> value.</p> </li> <li> <p>A set of <code>runinfo_&lt;XYZ&gt;.parquet</code> files collect runs     hyper param and metrics. </p> </li> <li> <p>A set of <code>summary_&lt;XYZ&gt;.csv</code> files collect the     aggregate table reported on the console.</p> </li> </ul>"},{"location":"modeling/campaigns/","title":"Campaigns","text":"<p>Individual modeling campaings can be triggered from the subcommand <code>campaign</code> sub-command.</p> <pre><code>tcbench campaign --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench campaign [OPTIONS] COMMAND [ARGS]...\n\n Triggers a modeling campaign.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 augment-at-loading        Modeling by applying data augmentation when loading the training set.  \u2502\n\u2502 contralearn-and-finetune  Modeling by pre-training via constrative learning and then finetune    \u2502\n\u2502                           the final classifier from the pre-trained model.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The <code>campaign</code> sub-command is an \"opinionated version\" of <code>run</code> sub-command. Meaning, is currently targetting only the needs for the campaigns which are part of the submission. So, the options exposed by the <code>campaign</code> sub-commands are a selected subset of the one options available for the related <code>run</code> sub-commands.</p> <p>For <code>augment-at-loading</code> campaign supports the following options <pre><code>tcbench campaign augment-at-loading --help\n</code></pre></p> <p>Output</p> <pre><code> Usage: tcbench campaign augment-at-loading [OPTIONS]\n\n Modeling by applying data augmentation when loading the training set.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                  \u2502\n\u2502                                   [default: augmentations-at-loading]                           \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                \u2502\n\u2502                                   [default: aim-repo]                                           \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]               \u2502\n\u2502 --campaign-id            TEXT     A campaign id to mark all experiments.                        \u2502\n\u2502 --dry-run                         Show the number of experiments and then quit.                 \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).    \u2502\n\u2502                                   [default: 0]                                                  \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20] \u2502\n\u2502 --seeds                  TEXT     Coma separated list of seed for experiments.                  \u2502\n\u2502                                   [default: 12345,42,666]                                       \u2502\n\u2502 --help                            Show this message and exit.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --augmentations       TEXT                                 Coma separated list of augmentations \u2502\n\u2502                                                            for experiments. Choices:            \u2502\n\u2502                                                            [noaug|rotate|horizontalflip|colorj\u2026 \u2502\n\u2502                                                            [default:                            \u2502\n\u2502                                                            noaug,rotate,horizontalflip,colorji\u2026 \u2502\n\u2502 --dataset             [ucdavis-icdm19|utmobilenet21|mirag  Dataset to use for modeling.         \u2502\n\u2502                       e19|mirage22]                        [default: ucdavis-icdm19]            \u2502\n\u2502 --dataset-minpkts     [-1|10|100|1000]                     In combination with --dataset,       \u2502\n\u2502                                                            refines preprocessed and split       \u2502\n\u2502                                                            dataset to use.                      \u2502\n\u2502                                                            [default: -1]                        \u2502\n\u2502 --flowpic-dims        TEXT                                 Coma separated list of flowpic       \u2502\n\u2502                                                            dimensions for experiments.          \u2502\n\u2502                                                            [default: 32,64,1500]                \u2502\n\u2502 --max-train-splits    INTEGER                              The maximum number of training       \u2502\n\u2502                                                            splits to experiment with. If -1,    \u2502\n\u2502                                                            use all available.                   \u2502\n\u2502                                                            [default: -1]                        \u2502\n\u2502 --split-indexes       TEXT                                 Coma separted list of split indexes  \u2502\n\u2502                                                            (by default all splits are used).    \u2502\n\u2502 --no-test-leftover                                         Skip test on leftover split          \u2502\n\u2502                                                            (specific for ucdavis-icdm19, and    \u2502\n\u2502                                                            default enabled for all other        \u2502\n\u2502                                                            datasets).                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Modeling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --method    [monolithic|xgboost]  Method to use for training. [default: monolithic]             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 DL hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size        INTEGER  Training batch size. [default: 32]                                 \u2502\n\u2502 --epochs            INTEGER  Number of epochs for training. [default: 50]                       \u2502\n\u2502 --learning-rate     FLOAT    Training learning rate. [default: 0.001]                           \u2502\n\u2502 --patience-steps    INTEGER  Max. number of epochs without improvement before stopping          \u2502\n\u2502                              training.                                                          \u2502\n\u2502                              [default: 5]                                                       \u2502\n\u2502 --no-dropout                 Mask dropout layers with Identity layers.                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 XGBoost hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --input-repr       TEXT     Input representation. [default: pktseries]                          \u2502\n\u2502 --pktseries-len    INTEGER  Number of packets (when using time series as input).                \u2502\n\u2502                             [default: 10,30]                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --max-samples-per-class    INTEGER  Activated when --split-indexes is -1 to define how many     \u2502\n\u2502                                     samples to select for train+val (with a 80/20 split between \u2502\n\u2502                                     train and val).                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>For <code>contralearn-and-finetune</code> campaign supports the following options <pre><code>tcbench campaign contralearn-and-finetune --help\n</code></pre></p> <p>Output</p> <pre><code>tcbench campaign contralearn-and-finetune --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench campaign contralearn-and-finetune [OPTIONS]\n\n Modeling by pre-training via constrative learning and then finetune the final classifier from the\n pre-trained model.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                  \u2502\n\u2502                                   [default: contrastive-learning-and-finetune]                  \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                \u2502\n\u2502                                   [default: aim-repo]                                           \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]               \u2502\n\u2502 --campaign-id            TEXT     A campaign id to mark all experiments.                        \u2502\n\u2502 --dry-run                         Show the number of experiments and then quit.                 \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).    \u2502\n\u2502                                   [default: 0]                                                  \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 50] \u2502\n\u2502 --help                            Show this message and exit.                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --augmentations       TEXT     Coma separated list of augmentations. Choices:                   \u2502\n\u2502                                [noaug|rotate|horizontalflip|colorjitter|packetloss|changertt|t\u2026 \u2502\n\u2502                                [default: changertt,timeshift]                                   \u2502\n\u2502 --flowpic-dims        TEXT     Coma separated list of flowpic dimensions for experiments.       \u2502\n\u2502                                [default: 32]                                                    \u2502\n\u2502 --max-train-splits    INTEGER  The maximum number of training splits to experiment with. If -1, \u2502\n\u2502                                use all available.                                               \u2502\n\u2502                                [default: -1]                                                    \u2502\n\u2502 --split-indexes       TEXT     Coma separted list of split indexes (by default all splits are   \u2502\n\u2502                                used).                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Training hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size                  INTEGER  Training batch size. [default: 32]                       \u2502\n\u2502 --cl-projection-layer-dims    TEXT     Coma separate list of contrastive learning projection    \u2502\n\u2502                                        layer dimensions.                                        \u2502\n\u2502                                        [default: 30]                                            \u2502\n\u2502 --cl-seeds                    TEXT     Coma separated list of seeds to use for contrastive      \u2502\n\u2502                                        learning pretraining.                                    \u2502\n\u2502                                        [default: 12345,1,2,3,4]                                 \u2502\n\u2502 --ft-seeds                    TEXT     Coma separated list of seeds to use for finetune         \u2502\n\u2502                                        training.                                                \u2502\n\u2502                                        [default: 12345,1,2,3,4]                                 \u2502\n\u2502 --dropout                     TEXT     Coma separated list. Choices:[enable|disable].           \u2502\n\u2502                                        [default: disable]                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Going beyond the scope of the submission</p> <p>Considering the <code>tcbench</code> projection it self, it is clearly in the road map to extend the current functionalities with a full-fledged variety of options/controls beside adding more datasets and training methodologies.</p>"},{"location":"modeling/campaigns/#campaign-composition-and-progress","title":"Campaign composition and progress","text":"<p>As mentioned, campaigns are essentially just an array of runs.</p>"},{"location":"modeling/campaigns/#using-dry-run","title":"Using <code>--dry-run</code>","text":"<p>The <code>--dry-run</code> option allows to verify the composition of a campaign.</p> <p>For instance <pre><code> tcbench campaign augment-at-loading --dry-run --method monolithic\n</code></pre></p> <p>Output</p> <pre><code>##########\n# campaign_id: 1688582575 | experiment 1/315 - time to completion 0:00:00\n##########\n\nexperiment grid with 315 experiments\n---\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (3): [32, 64, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre> <p>From the output we can see that the  default configuration for an <code>augment-at-loading</code> campaign corresponds to 315 runs = 5 splits x 7 augmentations x 3 flowpic dimensions x 3 seeds.</p> <p>This can be adapted based on the campaign sub-commands options.</p> <p>For instance <pre><code>tcbench campaign augment-at-loading \\\n    --dry-run \\\n    --method monolithic \\\n    --split-indexes 0,3 \\\n    --seeds 12345 \\\n    --flowpic-dims 32 \\\n    --augmentations noaug,rotate\n</code></pre></p> <p>Output</p> <pre><code>##########\n# campaign_id: 1688582864 | experiment 1/4 - time to completion 0:00:00\n##########\n\nsplit_indexes (2): [0, 3]\naugmentations (2): ['noaug', 'rotate']\nflowpic_dims  (1): [32]\nseeds         (1): [12345]\n</code></pre>"},{"location":"modeling/campaigns/#estimated-completion","title":"Estimated completion","text":"<p>Notice also the <code>campaign_id</code> and <code>time to completion</code>.</p> <ul> <li> <p><code>campaign_id</code> by default corresponds to the unixtime of when the campaign is triggered. This can be changed using the <code>--campaign-id</code> option.</p> </li> <li> <p><code>time to completion</code> is an estimate of the remaning time to complete the campaign based on the average duration of the runs already completed. As such, it is simply a rough estimate rather than a precise estimate.</p> </li> </ul>"},{"location":"modeling/campaigns/#split-campaigns-across-serversrepositories","title":"Split campaigns across servers/repositories","text":"<p>For simplicity, the runs are executed in sequence on a single-server environment. This is clearly not the best option in a multi-server environment, neither using a sequence can likely take advantange of available GPUs/CPUs on a single-server.</p> <p>It is however possible to use <code>run</code> and <code>campaign</code> commands to manually split the workload and merge ML artifact folders and AIM repositories a posteriori.</p> <p>How to merge AIM repositories</p> <p>The <code>aim</code> command line utility allows to copy run between repositories provided the hash of the runs to copy are known.</p> <pre><code>Usage: aim runs cp [OPTIONS] [HASHES]...\n\n  Copy Run data for given run hashes to destination Repo.\n\n  Options:\n    --destination TEXT  [required]\n    --help              Show this message and exit.\n</code></pre> <p>For instance, this is a minimal bash script to copy runs  <pre><code>#!/bin/bash\nSRC=/path/to/source/aim/repo\nDST=/path/to/destionation/aim/repo\n\ncd $SRC\nfor run_hash in `aim runs --repo $SRC ls | grep -v Total | tr '\\t' ' '`; do\naim runs cp --destination $DST $run_hash\ndone\n</code></pre></p>"},{"location":"modeling/campaigns/#submission-campaigns-commands","title":"Submission campaigns commands","text":"<p>We report below the command used to trigger the campaigns collected in the ML artifacts</p>"},{"location":"modeling/campaigns/#ucdavis-icdm19xgboostnoaugmentation-flowpic","title":"<code>ucdavis-icdm19/xgboost/noaugmentation-flowpic</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method xgboost \\\n    --augmentations noaug \\\n    --input-repr flowpic \\\n    --flowpic-dims 32,64,1500 \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/xgboost/noaugmentation-flowpic\n    --artifacts-folder ucdavis-icdm19/xgboost/noaugmentation-flowpic/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaigns has 45 runs</p> <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nflowpic_dims  (3): [32, 64, 1500]\n</code></pre> <p>In the submission we just reported results for flowpic with 32x32 resolution.</p>"},{"location":"modeling/campaigns/#ucdavis-icdm19xgboostnoaugmentation-timeseries","title":"<code>ucdavis-icdm19/xgboost/noaugmentation-timeseries</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method xgboost \\\n    --augmentations noaug \\\n    --input-repr pktseries \\\n    --pktseries-len 10,30 \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/xgboost/noaugmentation-timeseries \\\n    --artifacts-folder ucdavis-icdm19/xgboost/noaugmentation-timeseries/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 30 runs</p> <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nmax_n_pkts    (2): [10, 30]\n</code></pre> <p>In the submission we just reported results for time series with 10 packets</p>"},{"location":"modeling/campaigns/#ucdavis-icdm19augmentation-at-loading-with-dropout","title":"<code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/augmentation-at-loading-with-dropout \\\n    --artifacts-folder ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 315 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (3): [32, 64, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#mirage19augmentation-at-loading-no-dropoutminpkts10","title":"<code>mirage19/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage19 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage19/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage19/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#mirage22augmentation-at-loading-no-dropoutminpkts10","title":"<code>mirage22/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage22/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage22/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#mirage22augmentation-at-loading-no-dropoutminpkts1000","title":"<code>mirage22/augmentation-at-loading-no-dropout/minpkts1000</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage22/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage22/augmentation-at-loading-no-dropout/minpkts10\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#utmobilenet21augmentation-at-loading-no-dropoutminpkts10","title":"<code>utmobilenet21/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset utmobilenet21 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo utmobilenet21/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-changertt","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,changertt \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-packetloss","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,packetloss \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-rotate","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,rotate \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairsrotate-changertt","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations rotate,changertt \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairsrotate-packetloss","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations rotate,packetloss \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 215 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19simclr-dropout-and-projection","title":"<code>ucdavis-icdm19/simclr-dropout-and-projection</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations changertt,timeshift \\\n    --flowpic-dims 32 \\\n    --cl-projection-layer-dims 30,84 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --dropout disable,enable \\\n    --aim-repo ucdavis-icdm19/simclr-dropout-and-projection \\\n    --artifacts-folder ucdavis-icdm19/simclr-dropout-and-projection/artifacts\n</code></pre> <p>Run grid</p> <p>The campaign has 500 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (2): [30, 84]\ndropout                    (2): ['disable', 'enable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19augmentation-at-loading-suppress-dropout","title":"<code>ucdavis-icdm19/augmentation-at-loading-suppress-dropout</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --flowpic-dims 32,1500 \\\n    --seeds 12345,42,666 \\\n    --no-dropout \\\n    --aim-repo ucdavis-icdm19/simclr-dropout-and-projection \\\n    --artifacts-folder ucdavis-icdm19/simclr-dropout-and-projection/artifacts\n</code></pre> <p>Run grid</p> <p>The campaign has 210 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (2): [32, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"modeling/campaigns/#ucdavis-icdm19larger-trainsetaugmentation-at-loading","title":"<code>ucdavis-icdm19/larger-trainset/augmentation-at-loading</code>","text":"<p>This campaign is a composition of two sub-campaigns  stored in the same AIM repository.</p> <p>The first is for augmentation at loading <pre><code>tcbench campaign augment-at-loading \\\n    --seeds 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 \\\n    --split-indexes -1 \\\n    --method monolithic \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo ucdavis-icdm19/larger-trainset/augmentation-at-loading \\\n    --artifacts-folder ucdavis-icdm19/larger-trainset/augmentation-at-loading/artifacts\n</code></pre></p> <p>Run grid</p> <p>The campaign has 140 runs <pre><code>split_indexes (1): [-1]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (20): [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n</code></pre></p> <p>The second is for contrastive learning created with the following script to manually compose 20 runs.</p> <pre><code>#!/bin/bash\nCONTRALEARN_SEEDS=(32 33 34 35 6 7 8 9 10 11 12 13 14 15 16 17 18 20 43 64)\nFINETUNE_SEEDS=(2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 22 43 64)\nfor idx in {0..19}; do\ntcbench run contralearn-and-finetune \\\n--dataset ucdavis-icdm19 \\\n--batch-size 32 \\\n--flowpic-dim 32 \\\n--no-dropout \\\n--split-index -1 \\\n--cl-projection-layer-dim 30 \\\n--cl-seed ${CONTRALEARN_SEEDS[$idx]} \\\n--ft-seed ${FINETUNE_SEEDS[$idx]} \\\n--aim-repo ucdavis-icdm19/larger-trainset/augmentation-at-loading \\\n--artifacts-folder ucdavis-icdm19/larger-trainset/augmentation-at-loading/artifacts\ndone\n</code></pre>"},{"location":"modeling/campaigns/#ucdavis-icdm19-git-repo-forked","title":"<code>ucdavis-icdm19-git-repo-forked</code>","text":"<p>This campaign is different from all the others because is just repeating the experiments of the following paper ICDM19.</p> <pre><code>@misc{rezaei2020achieve,\ntitle={How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets}, \nauthor={Shahbaz Rezaei and Xin Liu},\nyear={2020},\neprint={1812.09761},\narchivePrefix={arXiv},\nprimaryClass={cs.NI}\n}\n</code></pre> <p>The related code is available at this   repository</p> <p>We just did minor modifications (mostly for changing output folders) without affecting how to execute the code.</p> <p>In other words, to generate the result run <pre><code>python dataProcessInMemoryQUIC.py \npython pre-training.py\npython re-training.py\n</code></pre></p> <p>All results are collected in an output <code>/artifacts</code> folder (but now AIM repository is generated).</p>"},{"location":"modeling/exploring_artifacts/","title":"Exploring ML artifacts","text":"<p>The ML artifacts provided with the submission  (<code>ml_artifacts.tgz</code> on  figshare) corresponds to data gathered by the modeling campaigns created for the submission.</p> <p>You can explore/investigate ML artifacts in three ways</p> <ol> <li> Using AIM Web UI.</li> <li> Inspecting individual run artifacts.</li> <li> Inspecting AIM repository reports.</li> </ol>"},{"location":"modeling/exploring_artifacts/#ml-artifacts-overview","title":"ML artifacts overview","text":"<p>First of all, make sure to install <code>tcbench</code> and unpack the ML artifacts.</p> <p>You can verify that everything is ok by running two checks.</p> <p>Check #1: When installing <code>tcbench</code> you also installed <code>aim</code> so you can invoke it on the command line. </p> <p>For instance <pre><code>aim version\n</code></pre></p> <p>Output</p> <p><pre><code>Aim v3.17.5\n</code></pre> Note: You might get a different version as  we explicitly do NOT pin dependency packages version, as <code>tcbench</code> does not require strict dependencies.</p> <p>Check #2: You should have all ML artifacts located under  <code>code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/</code> with the following structure.</p> <pre><code>&lt;root&gt;\n\u2514\u2500\u2500 campaigns\n    \u251c\u2500\u2500 mirage19\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts10\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 mirage22\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 minpkts10\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts1000\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 ucdavis-icdm19\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-suppress-dropout\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-with-dropout\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 larger-trainset\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-dropout-and-projection\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-other-augmentation-pairs\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter-packetloss\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter-rotate\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rotate-changertt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rotate-packetloss\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 xgboost\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 noaugmentation-flowpic\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 noaugmentation-timeseries\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 ucdavis-icdm19-git-repo-forked\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(script-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_20\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(script-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 RandomSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 RandomSampling_Retraining(script-triggered)_10\n    \u2514\u2500\u2500 utmobilenet21\n        \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n            \u2514\u2500\u2500 minpkts10\n                \u251c\u2500\u2500 .aim\n                \u251c\u2500\u2500 artifacts\n                \u2514\u2500\u2500 campaign_summary\n</code></pre> <p>Each subfolder relates to a different campaign with some semantic encoded in the folder names themselves.</p> <ul> <li> <p>Subfolders containing an <code>.aim/</code> folder are AIM repositories and can be explored via the AIM web UI.</p> </li> <li> <p>Subfolders named <code>artifacts/</code> are ML file artifacts (see below).</p> </li> <li> <p>Subfolders named <code>campaign_summary/</code> contains reports summarizing a campaign (see below).</p> </li> </ul> <p>The following reference table details how the different campaigns map to the results of the submission.</p>"},{"location":"modeling/exploring_artifacts/#mapping-campaigns-folder-to-submission-results","title":"Mapping campaigns folder to submission results","text":"Subfolder Submission reference CLI trigger <code>ucdavis-icdm19/xgboost/noaugmentation-flowpic</code> Table 2 <code>ucdavis-icdm19/xgboost/noaugmentation-timeseries</code> Table 2 <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> Table 3,9; Figure 1,3,9 <code>ucdavis-icdm19/simclr-dropout-and-projection</code> Table 4 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss</code> Table 5 <code>ucdavis-icdm19/larger-trainset/augmentation-at-loading</code> Table 6 <code>mirage19/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>mirage22/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>mirage22/augmentation-at-loading-no-dropout/minpkts1000</code> Table 7; Figure 4,5 <code>utmobilenet21/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>ucdavis-icdm19-git-repo-forked</code> Table 8; Figure 8 <code>ucdavis-icdm19/augmentation-at-loading-suppress-dropout</code> Figure 9"},{"location":"modeling/exploring_artifacts/#aim-web-ui","title":"AIM Web UI","text":"<p>AIM web interface is quite intuitive and  the official documentation already provides  a general purpose tutorial.</p> <p>In this mini guide we limit to showcase a basic set  of operations to navigate the ML artifacts using the <code>campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository as example. The same operations can be  performed on all other campaings too.</p> <p>First of all, we need to spawn the web UI for the repository.</p> <p>Assuming we are in the root folder of the jupyter notebooks <code>notebooks/</code></p> <pre><code>aim up --repo submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>Running Aim UI on repo `&lt;Repo#-3653246895908991301 path=/home/johndoe/code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/.aim read_only=None&gt;`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n</code></pre> <p>Run <code>aim up --help</code> for more options (e.g., specifying a different port or hostname).</p> <p>When visiting the URL reported in the output  you land on the home page of the AIM repository.</p> <p>This collects a variety of aggregate metrics  and track activity over time.  Hence, in our scenario the home page of the ML artifacts are mostly empty because all campaigns were generated in a specific moment in time.</p> <p></p> <p>The left side bar allows switch the view. In particular, \"Runs\" show a tabular view of the runs collected in the repository.</p> <p></p> <p>From the view you can see the hash of each run and scrolling horizontally you can glance  over the metadata stored for each run.</p> <p></p> <p>The search bar on the top of the page allows to filter runs. It accept python expression bounded to a <code>run</code> entry point.</p> <p>For instance, in the following example we filter one specific run based on hyper parameters.</p> <p></p> <p>When clicking the hash of a run (e.g., the one we filtered) we switch to a per-run view which further details the collected metadata of the selected run.</p> <p></p> <p>For instance, when scrolling at the bottom of the per-run page we can see that AIM details</p> <ul> <li> <p>The specific git commit used when executing the run.</p> </li> <li> <p>The specific python packages and related versions available in the environment when executing the run.</p> </li> </ul> <p>Both are automatically tracked by AIM with no extra code required (beside activating the  their collection when creating the run).</p> <p></p> <p>The per-run view offers a variety of information organized in multiple tabs.</p> <p>For instance, the tab \"Logs\" details the console output.</p> <p></p>"},{"location":"modeling/exploring_artifacts/#run-artifacts","title":"Run artifacts","text":"<p>Unfortunately, AIM has little support for  tracking general file artifacts and no native support for storing trained models.</p> <p>For instance from the last screenshot above we can see that we could bind to a run some images, audio files and some text.  While the first two are out of the scope for network traffic-related datasets, text tracking is meant for free form \"blob\" output.</p> <p>Hence, <code>tcbench</code> associates to a run  an \"artifacts folder\", named as the hash of the run, where a variety of files are saved.</p> <p>Specifically, each run is associated to the following files:</p> <ul> <li> <p><code>params.yml</code> is a YAML file collecting ALL parameters used when triggering a run, i.e., both the arguments explicitly defined on the command line, as well the ones with default values.</p> </li> <li> <p><code>log.txt</code> collects the console output generated by the run.</p> </li> <li> <p><code>best_model_weights_split_&lt;split-index&gt;.pt</code> stores the weights of the best  trained pytorch model (for a deep learning model). The filename is bounded to the specific split index configured when triggering the run.</p> </li> <li> <p><code>xgb_model_split_&lt;split-index&gt;.json</code> stores an XGBoost model (when training  via xgboost). The filename is bounded to the specific split index configured when triggering the run.</p> </li> <li> <p><code>&lt;context&gt;_class_rep.csv</code> contains a classification report. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> <li> <p><code>&lt;context&gt;_conf_mtx.csv</code> contains confusion matrix. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> </ul> <p>Why saving extra files?</p> <p>Future version of <code>tcbench</code> will likely integrate with solutions such as DVC to further strenghtening file artifacts handling and tracking also the input data version.</p>"},{"location":"modeling/exploring_artifacts/#aim-repository-reports","title":"AIM repository reports","text":"<p>To complement AIM web UI and manual inspection of ML  artifacts, <code>tcbench</code> offers a few sub-commands to overview an AIM repository  content.</p> <pre><code>tcbench aimrepo --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo [OPTIONS] COMMAND [ARGS]...\n\n Investigate AIM repository content.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ls                    List a subset of properties of each run.                                  \u2502\n\u2502 properties            List properties across all runs.                                          \u2502\n\u2502 report                Summarize runs performance metrics.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In the following we illustrate each sub-command using the  <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository</p>"},{"location":"modeling/exploring_artifacts/#ls","title":"<code>ls</code>","text":"<p>The <code>ls</code> sub-command simply list  the hash, creation time and end time of each run.</p> <pre><code> tcbench aimrepo ls \\\n    --aim-repo code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code> hash                      creation_time      end_time\n 8ba8f05ec1604574b71139b4  1684486220.186745  1684487165.465486\n 67503d171467435091714d45  1684485354.995328  1684486219.662343\n 9a106b1a209b4a47abb5c17f  1684484451.951014  1684485354.490275\n bfe6d326e299461cb56cdc5f  1684483871.600086  1684484451.518072\n 87fa4af7e99d478aaf22f4da  1684483362.006482  1684483871.393643\n c13844d1704c45fc95653b1f  1684482566.637334  1684483361.812976\n c7bdc0d409c4403bbe5b9cc4  1684482432.924283  1684484618.028195\n 043419ccd36541fd8a4057cb  1684482146.1981    1684482566.387968\n 963f7797f19d4976a3efcca3  1684481692.788042  1684482145.885839\n 4d81379229814f409b7647d3  1684481202.271289  1684481692.537144\n fe35f9380c344b8aa9c23f03  1684481101.181884  1684482432.517077\n 2f6d76f632c64391b7cb4973  1684480123.949839  1684481201.997798\n b3df56698ec6416cac21add4  1684479475.427219  1684481101.055126\n 72ac1602329b415dbf17524f  1684479393.541195  1684480123.490255\n 68832122ee434d799da3ec6c  1684478564.503901  1684479474.865144\n 98033a73d4d1433ab3779742  1684478415.758284  1684479393.015683\n c6aea7ff760b4ad090d9dd36  1684477658.120382  1684478415.53678\n 7e0386cdcbaa454e892b5fb9  1684477439.749558  1684478563.989332\n d985dfe5a5a6422293ff34eb  1684476688.49235   1684477657.471275\n 889dc5128693473f9fd79299  1684476501.215957  1684477439.58382\n 922fef07ff264188a2e370a5  1684475858.118653  1684476688.059573\n cfb5812f3ca84c2bb440e6c4  1684475376.248467  1684476501.070375\n 99fc7d6feea64a07a9f4499d  1684475282.908762  1684475857.977606\n 728dfe6ff0584b0db57b8f22  1684474888.182631  1684475282.672584\n 9dde5426e9424f9aa0eda8ef  1684474417.41854   1684474887.912377\n d8845f81ebbc4fbdbb7efc6e  1684474280.592608  1684475375.693128\n 4ca5aff6b9c442808f39d2c3  1684473415.875285  1684474417.168511\n f24dfd8ef3e44cd6ae37c4d7  1684473045.421477  1684474280.137564\n 9d530cacbbfc4300846c412a  1684472369.171916  1684473415.375791\n b222cda5054744ee923cc1a5  1684471606.606575  1684473045.147482\n 5c9c03c663ea493480ff176c  1684471433.960361  1684472736.412599\n 1e13e4b8c60a4dd084ca607b  1684471290.69116   1684472368.905358\n 4c42b65f85e444579adabd51  1684470601.838798  1684471289.976878\n 4f759432db0743c7b3d95f6b  1684470452.527724  1684471433.756871\n 1a71035742a940c7a74c6a9b  1684470331.022178  1684471606.113162\n 1f2bc263b9ed450abcf1c1d3  1684469705.56874   1684470601.122689\n 9951ad2c1f894efcb04d39bb  1684469472.837658  1684470451.971661\n dd4ec191cd0945a184c9aef8  1684469140.912997  1684469472.087474\n 60b55181ebaa4ccebc214875  1684468906.545817  1684470330.375135\n d74b79343de8419b874f5210  1684468808.347594  1684469705.18729\n 0a30bf4e578445239c3e91cb  1684468778.110371  1684469140.702514\n 1d0327a026444eee800dc9a1  1684468500.060794  1684468777.756142\n e40d612c892644f59370cd68  1684468394.226275  1684468808.078585\n 67c4857856d445d4809e1fd2  1684467913.759307  1684468394.08247\n cad4cd97d854419e91869d85  1684467543.981347  1684468905.978667\n a7018c0846304408bc75e8c8  1684467526.249917  1684467913.503346\n 39c042def7b94710bc211b0e  1684466951.957943  1684468499.463829\n e61e21d87ccd4aa8b8a2ed35  1684466335.110533  1684467526.054343\n b1869271d50644c696fb5246  1684466131.933304  1684467543.395299\n e0dac9c82bf04bafb4aade4c  1684465619.566809  1684466951.462375\n ed199b3b6a71488893e1abfb  1684465434.028724  1684466333.918455\n 7dc2a753e3f74ed69f069ea8  1684464725.062243  1684466131.24298\n 03a5cdeb32354bfdba1de136  1684464569.689175  1684465433.542933\n 37c00f6c1ebe44fda8a9618d  1684464407.673333  1684465618.991646\n f620bcb6afe24d2a87a778c0  1684464064.526061  1684464406.941772\n 9cdde43cddd640e2b440b1c4  1684463741.103362  1684464064.054385\n ffb1a45a1fb249c0bf3f463c  1684463607.054282  1684464569.411643\n 395fc5799efc4f34823b3655  1684463449.453486  1684463740.94038\n 5a0a0a5c3bb244d497538de4  1684463312.066392  1684464724.410855\n 0638ad838f4a429b91131a5b  1684462917.872626  1684463606.433408\n 51aa03f39a3e42e2b99db486  1684462233.665966  1684463311.043898\n 57eb602e1dad4933b5eca155  1684462207.043093  1684463448.792002\n 01fea90ec67544d3a39ef997  1684461943.457484  1684462917.499634\n d792e20d40604c78956ab5a2  1684461464.25762   1684461943.313054\n 637badf3dfec4b93a78d9919  1684461042.264266  1684461464.126894\n f87edacfdacf4b5faee625b4  1684460961.382713  1684462233.005103\n 444c2e1ebfaa43a78c6b4c96  1684460467.126398  1684461042.125671\n 9e5135774a89458b8dae96ef  1684459980.097812  1684462206.830038\n ca77db360e8f45d5a8d2f3b7  1684459693.93209   1684460466.346116\n adf0905c3c8349d590574c38  1684459300.071951  1684460960.695716\n 916583de1a524ed092408d72  1684459069.160377  1684459693.416057\n 7c25e7eca05e49ffa82e1dda  1684458823.182091  1684459979.576512\n e77cf8e464b2496a94229ca8  1684458538.286254  1684458823.017745\n 70fef369b9e741e59a9cb092  1684458437.369592  1684459067.97458\n 73c779ffb561448281772cca  1684458244.2756    1684458537.96287\n c21d1b9a093c42cbb1421b99  1684458241.060798  1684459299.580839\n e0c07fd0f3664789a68a97fd  1684457885.407864  1684458243.499636\n 7b52359ef3ef42219b6297ef  1684457777.173311  1684458436.747236\n aecb143222764ac2badda996  1684457079.7536    1684457776.789753\n 83850279a1fd43e1892771ea  1684457024.45417   1684458240.375002\n c80ccdfec89b4ac68013dc57  1684456500.647965  1684457079.174438\n bd8b6929f7bb41eeb00e8451  1684456429.302551  1684457884.834434\n cca820ec5ae24ee793b45ef3  1684456007.859746  1684456500.321697\n ad89e8df0be34133bb61e589  1684455782.341968  1684457023.582\n d9613c247ffc49ae83b1864d  1684455514.437144  1684456007.704841\n f4f28ffbd85041d78d032bf7  1684455198.659235  1684456428.772103\n 17aa80f35ba240d6a3c27681  1684454983.894229  1684455514.144682\n 376d83d39ee34bf6a5f14553  1684454900.637969  1684454978.036799\n d66de4d4c0f043289d0f9671  1684454828.019291  1684454900.501477\n 53e969d8626c41df92814c61  1684454756.332229  1684454827.858425\n 988b1457caba480b817ba385  1684454684.082903  1684454756.197079\n f60fc0f56310461ebd15ccb1  1684454602.697385  1684454683.934212\n 4231173e05fe4466bb3592e5  1684454519.796744  1684454602.561087\n b7bcc2cb30054c9d89b81e71  1684454436.86775   1684454519.639057\n 0eb416e6597f4205a4eb30e5  1684454350.202147  1684454436.706402\n 01d4bb96f3d547b0b67f3219  1684454275.221716  1684454350.058379\n 2863d94709a241549ea43d1e  1684454180.176362  1684454275.095384\n 608b4ff892424b909c37195b  1684454131.059445  1684455781.74821\n 2f4819882cab4dbc8bb4e599  1684454101.90471   1684454180.044018\n dc3e8dfccc964658b1094227  1684454031.000489  1684454101.748034\n 95e5b09ba7d44fabbb055f93  1684453973.055474  1684454030.873004\n 3ab8c90f935845bc85d9e7c0  1684453916.309955  1684453972.93122\n 8ed7f1729fd5456cbf9bf9f0  1684453858.633202  1684453916.18064\n 2838f1c9925a4276b596f29a  1684453857.763272  1684453999.474313\n fb37ebad53be4b8bb0922c43  1684453854.938354  1684455198.416701\n dbc9c91611dd40d18da3a563  1684453800.945448  1684453857.692718\n 0922df9981cb4dada7cb8e70  1684453783.490234  1684453858.502442\n 577b94c1058543fbbbac556c  1684453740.054494  1684453800.871211\n 1e8636dca105456d96c9d911  1684453708.58691   1684453783.36737\n 2927b88ec0064c1b96f2420d  1684453669.588055  1684453739.989276\n 1f79705713004475b0f4b423  1684453641.653473  1684453708.452254\n 816d410679ce4d3d93c9fb13  1684453609.559607  1684453669.528333\n f20a0864a0154d14bc62bef9  1684453603.103946  1684453641.526248\n 7ab9bc7b21f84284a8c06e2c  1684453562.770793  1684453602.990837\n b7dbd5917875476a9f331342  1684453554.43456   1684453609.487829\n e711aaf96ecc418cb1629a36  1684453522.761069  1684453562.665403\n c4d648e857fa404dbec12d64  1684453500.782744  1684453554.364838\n 810da0118ed7417db7c4771e  1684453469.291014  1684453854.683895\n 5d8a70cabc464fcbbd0a904d  1684453444.704697  1684453500.709896\n 5634546f547945849b466cf5  1684453420.998128  1684453522.623544\n c4ed1533ddbf43b5ba8f83ab  1684453389.201033  1684453444.618217\n cc13b218cbfd4cf08e5d20a4  1684453337.641638  1684453389.137557\n 610b507713cb47f4963ef671  1684453316.818646  1684453420.841483\n 36f2e4b6045b4f01b70e4680  1684453287.041721  1684453337.567413\n 37f99c3ed79a45f3b7d40a5f  1684453246.21599   1684453316.660344\n 5fe2730f842e416c8af9c9b3  1684453237.076659  1684453286.959855\n e25a532da42c4327bc11528a  1684453190.214313  1684453237.018725\n 7aea23e113cc4a289d7bfa79  1684453161.004546  1684453246.06685\n 45429199088f4c0c8b705537  1684453142.490356  1684453190.154943\n ab079de736834eef899c21bb  1684453101.882433  1684454130.262774\n fa468f7b03e3475daf215746  1684453095.863897  1684453142.425248\n add75d33ef8b4d638e770aaf  1684453075.861901  1684453468.361486\n 2aeb5a16085f42e9ab4094df  1684453065.579356  1684453160.874607\n 8e87b04207e54931b865d237  1684453016.800851  1684453095.802078\n 76f30367c8ab44b0bbe67d03  1684452977.614285  1684453065.424347\n deaffa25f04b4aae856c632f  1684452942.818858  1684453016.74139\n 5cae649ac7bb46d5a60c72ac  1684452877.834861  1684452942.748545\n 4a0efe2a9ecb499983245191  1684452870.028023  1684452977.473543\n 2e0474d626b94b5e957a3774  1684452847.893095  1684452877.790754\n 50f15777a7174531bbf50a1c  1684452818.958093  1684452847.832531\n b924f0f921e04b64828975ba  1684452789.569578  1684452818.902016\n b418ebaa6576432290bfe7ae  1684452774.458869  1684452869.895356\n b9f6b0815edf4ddfb34dc177  1684452728.81128   1684452789.506862\n 01b5d3ec52da4424a37125df  1684452700.006671  1684452774.319969\n f93d353566c24408bb3fc997  1684452663.618535  1684452728.740588\n dca1cc1dc9cc45f69bf45471  1684452629.495522  1684453074.989142\n 61a54b25733740b586ae3788  1684452604.757812  1684452663.541514\n 762d10ed26ae46ab8752cbfb  1684452601.345309  1684452699.879312\n 6cbe5bb9f82141b8abb5d2c8  1684452537.019976  1684452604.693619\n f2d476a93fef4c7abc727453  1684452508.867201  1684452601.21922\n 3d69205834a84112b65fcaef  1684452472.610688  1684452536.957576\n f2c44a9bc9f14ad4b3b827b8  1684452419.986378  1684452508.722681\n 4d2f061684c842d793d53458  1684452405.098763  1684452472.542425\n f05cbbb211a545d8b726a301  1684452365.427215  1684452419.85173\n 5f451dce01f04ee9b0169828  1684452346.577028  1684452405.012775\n 234669be443d4f08ac6dd900  1684452308.986717  1684452365.299411\n e8d6da0defd049028aaea9d5  1684452290.203461  1684452346.517978\n bd57bec0663e479ea2934678  1684452252.92531   1684452308.85739\n 352f015ecc004b7f922e9675  1684452225.314651  1684452290.121976\n 47a4a0891c024ff9ad3e10a5  1684452168.422513  1684452252.796179\n 06da93cc03a44040b0c74178  1684452157.739649  1684452225.250497\n e3d645b0dc16421ca3209896  1684452137.461206  1684453101.39133\n 7bdb5341a2614a888c577a09  1684452097.493495  1684452157.677333\n 4bbd8caafebe4379b130c6b0  1684452090.542063  1684452168.28824\n 7b25cd869f00439fafbed535  1684452029.411407  1684452097.43651\n 92c9588486534986a7a5eacf  1684451984.658156  1684452029.353263\n 61d85dcf712b434fac9a5b1f  1684451983.953664  1684452090.41075\n a12b3724c60d4958996de377  1684451944.910572  1684451983.848708\n 66dda78e98484ac695ac28c2  1684451937.460639  1684451984.606509\n 6693f2a4a645455cada34389  1684451908.020474  1684451944.789062\n e1169972d26140e18d897023  1684451886.957104  1684451937.392489\n 822c84da50f74667920f03bf  1684451871.324137  1684451907.890575\n 77bb1cec15af4c23b8be0f2f  1684451821.675738  1684451886.891239\n cf3545083d794dee8e0d2e9a  1684451812.399059  1684451871.18422\n 38171ed6da8f44e1a555b9d8  1684451748.611645  1684451821.617548\n a94ec554111b4a1a89cca0f6  1684451744.945461  1684451812.24693\n f20aca505c37459abb281e08  1684451656.412687  1684451744.789916\n 5c4aa95035c841d4a5e96b0c  1684451556.350265  1684451656.276747\n 6aa44846db614702867c9623  1684451546.981732  1684451748.381051\n c727a871d64b4b84ab39ce7b  1684451472.885078  1684451556.217995\n f1444a30a0b64ddebb0ee893  1684451451.528096  1684451546.751202\n 83ecc5c955ec4023bc82b30d  1684451382.832542  1684451472.73465\n 6c7b1cbe72004a0898b331e1  1684451373.367619  1684451451.294249\n 55a0901ebd8c4a14a006bda0  1684451332.395285  1684452629.058694\n 9c0d10c9e8f04194b6cf322c  1684451332.169061  1684451373.247294\n 1fe3f3cff5364183b91b0161  1684451311.409357  1684451382.69011\n e64349682cac4e96bdb3a195  1684451305.523624  1684452136.669685\n b6f5f861bfd045b79910163f  1684451268.06238   1684451332.097281\n 24db445e86664a5688d5b058  1684451244.011303  1684451311.247374\n 707b1d8f242c4c569397818f  1684451201.961014  1684451267.986849\n b5870e5dd6ac4433a59e949e  1684451151.07773   1684451243.875269\n 972de7c3956d4d96b27fa2e7  1684451147.277512  1684451201.870307\n 007ff3eb7d3644c9bfa97590  1684451089.133811  1684451147.206315\n be295d1045b64e5da2dad253  1684451086.977876  1684451150.932065\n 008ab3a058424242b2c74721  1684451024.406991  1684451089.065439\n 8587efea57d444b0a37a4d87  1684451012.747797  1684451086.845493\n 46cfa3ec77604b18b5cbb952  1684450970.955052  1684451024.350877\n 473c10c9d83b4a2f9f7aaacd  1684450935.840727  1684451012.61568\n 9749d96f77754ee3a1dd82dc  1684450910.230388  1684450970.876716\n bd622c8c454a43b2a526e255  1684450878.062955  1684450935.71623\n 1b69360731f64dbabd3e6b4a  1684450856.297448  1684450910.156479\n f1c8afdee5f843dea2cd6326  1684450820.288095  1684450877.926006\n 8719913ab19c470090e2cbba  1684450789.546818  1684450856.227493\n 64cc636a24de4af085a90012  1684450764.779803  1684450820.161206\n 38f7ad203bd3441cb27e8477  1684450726.736646  1684450789.489278\n 2006e773a42a45169c8af288  1684450700.498349  1684450764.63308\n f48a8088282a4f22a3060aad  1684450628.084315  1684450726.679969\n 07741993d3ff4d4c85cd2378  1684450625.213713  1684450700.367589\n 78897d14594d4d8ab0511b09  1684450532.651023  1684450625.066728\n c058300f4d55480183fe7d0a  1684450500.728736  1684450627.9136\n 70457b6d2ca34c9ca54faa9c  1684450494.905926  1684450532.530795\n fae0d0098235451582379e49  1684450457.158045  1684450494.787555\n eb0656be43d64f4abc07d913  1684450420.256119  1684450457.035452\n 3f6b15bdbb4c4f7990139390  1684450373.169881  1684450500.508234\n 37fb6128cf0940b0a8e02a6d  1684450346.729378  1684450420.123028\n 59850e2e072c4315bcfd44f9  1684450317.249483  1684451331.988945\n eb23366973d54adcb7ae41dc  1684450309.69558   1684450372.983297\n 4c51c9d6344741a3b4167ace  1684450270.958624  1684450346.587809\n 0adf8ec7b2d44e83b9d1ecd6  1684450264.164186  1684450309.641298\n 7663b098261c42b6ae806446  1684450255.519111  1684451304.891808\n 8d87effdeaf045bd8e1793be  1684450191.755371  1684450270.815829\n 4afff20583a7446a87f532bb  1684450174.269007  1684450264.10297\n a12f8c41aa7645fb8dce11db  1684450113.72971   1684450191.620866\n 278b560f285e4b3e968d2500  1684450112.787798  1684450174.213632\n 8232054671804f4a9f049ac8  1684450038.59175   1684450113.590469\n 8a2aaa0359c14bee82d7a01b  1684450038.553256  1684450112.718496\n cc14c289c387497299647aff  1684450009.504617  1684450038.503996\n c1d541e31bf04c34a8361277  1684449981.075677  1684450009.447578\n f6ae1ffbf09b4b829845768a  1684449960.174022  1684450038.453731\n e55525d57f8f4c0fbe62925c  1684449951.684003  1684449981.027956\n 3b3c950211ec4ac1861d2ef2  1684449896.906696  1684449951.618761\n a6156d0fedfb4da58d7ed157  1684449886.685146  1684449960.026201\n 34fb83c203bb4950bd629d93  1684449847.082951  1684449896.838466\n 32315d3421a643f8a60e3cc6  1684449808.13473   1684449886.545201\n 6db2eb1e1dd347acba48e369  1684449790.974563  1684449847.000392\n 63c5de9bc5ae4bca84a04a12  1684449737.9346    1684449807.970432\n ec202059cb874f4f9ca4205e  1684449725.824672  1684449790.911293\n ae692ba4c260457d9a3e442a  1684449668.959527  1684449725.763858\n a96434d27b8e45d3ad8b81ea  1684449659.840383  1684449737.633578\n 5c28fb0c4cae4a5eb3e2bf37  1684449609.2497    1684449668.911\n 401d87dc0f9d4f7fb3aa52df  1684449583.918668  1684449659.708532\n c9c4b346ff394c8488e9790d  1684449545.541382  1684449609.186583\n 74ba1657c61644e2a5e7da30  1684449520.169214  1684449583.771104\n ae7a6eb3e85a48228d3371b4  1684449492.929299  1684449545.465121\n 52615c37cb4c403a8bf7a025  1684449464.493325  1684449520.040155\n 4969fb8c374141028594d962  1684449437.814941  1684449492.858044\n 1e96377382f443ccbe7e85f0  1684449405.594734  1684449464.356868\n 776699b253614175b7f827ff  1684449387.696488  1684449437.748665\n 5687406675c34ef8bead5633  1684449346.890198  1684449405.47788\n 387e01ef08454e2eb2cff697  1684449333.183408  1684449387.6353\n 685af93f85ac4b67bb99144e  1684449302.386924  1684450255.027677\n 4ea944b1be754ab48bcb7d45  1684449276.832263  1684449346.737273\n db1c87ebb5494363a2fb2d3b  1684449237.813694  1684449333.12579\n 3e1533ac0f164dc092b7b681  1684449204.473322  1684449276.677444\n 61882760258d44d793c721dd  1684449151.709934  1684449237.749622\n 385286c19cb14a0c9571e0e9  1684449129.216098  1684449204.342291\n 63a50b8c64884ec5aa61cfa1  1684449091.168293  1684449129.090858\n a9b40ede978e4fb4bfc120b7  1684449055.11729   1684449091.054672\n cb78f86b48284696aa938fcc  1684449029.36249   1684449151.468267\n 47595fc5c64146e9805b51e6  1684449017.959478  1684449054.990691\n bf2c9394ea3742e683452a22  1684448936.264374  1684449029.147291\n c8f67a99057c49a2944bf39a  1684448932.684268  1684449017.804864\n a4cf32694e6e411ea84aa47a  1684448931.706416  1684450316.541687\n 4b3570a3a50c41c78f0000f4  1684448858.610413  1684448936.197166\n b21d99a3202e4da3b484ae4a  1684448837.720911  1684448932.548398\n f099eb327fd7494db384bb14  1684448765.679447  1684448858.545372\n 47eccc984daf4834aec19f67  1684448756.36372   1684448837.570065\n 7aaac8ecd2e644eda14968cf  1684448674.903293  1684448765.625554\n 43a503bf40aa4f518a97cf4e  1684448667.212994  1684448756.204382\n 230c734960ea4c849e3af2e1  1684448645.431733  1684448674.860142\n a6a20facc3f34662aa660fee  1684448615.158763  1684448645.376438\n cd65b0169cf441bfb6e3dcd8  1684448585.157683  1684448615.106011\n a0bc2e31538249aab969fc0d  1684448580.386622  1684448667.054529\n 3ee8b28573704e4ba79cf698  1684448527.514667  1684448585.087628\n 0712bcf076c44d239f8def39  1684448505.486607  1684448580.237805\n 45cf18b3d09a40098e621079  1684448465.274798  1684448527.439574\n 40cde1e6fa674cc3bfd82bb6  1684448423.393276  1684448931.366076\n d59fb822d0c346e496361cc6  1684448421.81773   1684448505.32149\n 13d14051be32424d92613289  1684448410.573326  1684448465.203814\n f05bc193e27d4178986a5662  1684448379.497623  1684449301.840416\n 3e378ebe488442e59997f8d2  1684448349.959204  1684448421.651724\n e5fc7761ea6a4e04bbb16b74  1684448346.800847  1684448410.504434\n 7538f4623894469ea5db1024  1684448284.44457   1684448346.71941\n c81913cba557469a90999dfa  1684448266.979889  1684448349.801789\n 6a742ccb640e4594a353c49a  1684448229.388576  1684448284.376865\n 75d1aa21751a4d0085f7c78c  1684448190.969628  1684448266.848555\n 321f7acb616040cda1232be3  1684448178.230579  1684448229.301771\n f325127637554978a28c7c4c  1684448123.997501  1684448178.150143\n ba4003ab5d674388913ad196  1684448122.292348  1684448190.811933\n ecf0408ff75948cdad056c2a  1684448069.866839  1684448123.924316\n a96a951613e947c596cd48fa  1684448051.908431  1684448122.161052\n 274ed9aae2324ebdb6b0e59a  1684448023.663688  1684448423.171355\n 81e993bee47043319ff1ab6e  1684448007.683417  1684448069.801989\n a8bdcf0e5d6f4645a7a76382  1684447991.732175  1684448051.786494\n a1f84df603a3455b8bc91007  1684447955.241488  1684448007.621067\n df5e5a6eed6542739b67f833  1684447937.451409  1684447991.609256\n f1e61e62d6fb484c9122547e  1684447898.394327  1684447955.175681\n c59bc63eb8f041049543f1d2  1684447880.325154  1684447937.315866\n 1e5a6ac24e9540f89c0c54cf  1684447853.491841  1684447898.333804\n c21a103dfa35473390e9d1fe  1684447807.994382  1684447853.423596\n e5f3fc273b114ace8bcb8478  1684447807.815813  1684447880.187383\n 0d92ca4591404ba889239d2f  1684447760.682189  1684447807.938637\n 53046971da764da1b343c6c5  1684447727.930714  1684447807.672719\n f65eae9d11414922a5cc531e  1684447662.420696  1684447760.621397\n 5e9105ff533e4b44a0dfc577  1684447636.207182  1684447727.776517\n 7e7fd7824169464b9602d7cc  1684447598.500283  1684447636.086839\n a669d0df5eaa4e47a20f5d94  1684447572.937839  1684448379.216737\n 426257bb89474ca8a339523b  1684447565.763093  1684447662.363762\n 8cc32087458448c7bd165ea0  1684447561.149784  1684447598.372651\n caaffcdf9919419cb3616619  1684447520.023255  1684447561.032779\n 3ace3b501ffc4d3d9f006a4c  1684447477.174045  1684447565.696864\n 2ec24496c7b44bc1abb9d07d  1684447446.502455  1684447477.122609\n d74763a98ac54a899facc16e  1684447417.775803  1684447446.44637\n 7e702018ebfe4d96b96b84c5  1684447395.8736    1684448023.427643\n 18dd6fca9c6944fb9ac0b77f  1684447384.730554  1684447417.721719\n</code></pre>"},{"location":"modeling/exploring_artifacts/#properties","title":"<code>properties</code>","text":"<p>The <code>properties</code> sub-command shows aggregate information and the list of unique values of the properties of an AIM repo (i.e., run hyper params and other meta data).</p> <pre><code>tcbench aimrepo properties \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name                   \u2502 No. unique \u2502 Value                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 runs                   \u2502          - \u2502 315                                                       \u2502\n\u2502 duration (mean \u00b1 std)  \u2502          - \u2502 5m44s \u00b1 7m40s                                             \u2502\n\u2502 metrics                \u2502          4 \u2502 ['acc', 'best_epoch', 'best_loss', 'loss']                \u2502\n\u2502 contexts               \u2502          5 \u2502 ['test-human', 'test-script', 'test-train-val-leftover',  \u2502\n\u2502                        \u2502            \u2502 'train', 'val']                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 experiment             \u2502          1 \u2502 ['augmentation-at-loading']                               \u2502\n\u2502 aug_name               \u2502          7 \u2502 ['changertt', 'colorjitter', 'horizontalflip', 'noaug',   \u2502\n\u2502                        \u2502            \u2502 'packetloss', 'rotate', 'timeshift']                      \u2502\n\u2502 campaign_exp_idx       \u2502        105 \u2502 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,   \u2502\n\u2502                        \u2502            \u2502 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,   \u2502\n\u2502                        \u2502            \u2502 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,   \u2502\n\u2502                        \u2502            \u2502 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,   \u2502\n\u2502                        \u2502            \u2502 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,   \u2502\n\u2502                        \u2502            \u2502 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,   \u2502\n\u2502                        \u2502            \u2502 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,  \u2502\n\u2502                        \u2502            \u2502 101, 102, 103, 104, 105]                                  \u2502\n\u2502 campaign_id            \u2502          1 \u2502 ['1684447037']                                            \u2502\n\u2502 dataset                \u2502          1 \u2502 ['ucdavis-icdm19']                                        \u2502\n\u2502 flowpic_block_duration \u2502          1 \u2502 [15]                                                      \u2502\n\u2502 flowpic_dim            \u2502          3 \u2502 [32, 64, 1500]                                            \u2502\n\u2502 patience_steps         \u2502          1 \u2502 [5]                                                       \u2502\n\u2502 seed                   \u2502          3 \u2502 [42, 666, 12345]                                          \u2502\n\u2502 split_index            \u2502          5 \u2502 [0, 1, 2, 3, 4]                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The table is split into two parts to separate general properties (top) from hyper parameters (bottom). General properties are common across repositories while hyper parameters vary depending of the campaign.</p> <p>What is a context?</p> <p>The term is borrored from AIM terminology and refers to ability to group metadata into categories. </p> <p>For instance, in the example above, the 4 listed metrics are separately stored for each listed context.</p>"},{"location":"modeling/exploring_artifacts/#report","title":"<code>report</code>","text":"<p>The <code>report</code> sub-command provides an aggregated summary across metrics grouped by properties</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                                    hparams                           acc                  duration\n                   split        aug_name   flowpic_dim  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              test-human       changertt            32    15  70.04   4.41   2.44    83.93      7.8     4.32\n                                                    64    15  72.05    2.1   1.16    61.57      5.2     2.88\n                                                  1500    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  68.84   4.69   2.59    78.01     10.9     6.03\n                                                    64    15  71.33   3.35   1.86    67.15    22.49    12.45\n                                                  1500    15  68.59   3.17   1.76   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                                    64    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                                  1500    15  73.82   1.47   0.82    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  69.48   2.12   1.17    37.97     1.46     0.81\n                                                    64    15  69.88   2.28   1.26    38.06    20.21    11.19\n                                                  1500    15  68.67   1.93   1.07   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                                    64    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                                  1500    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                                    64    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                                  1500    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  70.36   2.98   1.65    80.08    12.72     7.04\n                                                    64    15  72.53   1.83   1.02    64.23    21.91    12.13\n                                                  1500    15  70.84   2.42   1.34   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             test-script       changertt            32    15  97.33   0.71   0.39    83.93      7.8     4.32\n                                                    64    15  97.29   0.64   0.35    61.57      5.2     2.88\n                                                  1500    15   96.8   0.63   0.35  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.87    0.8   0.45    78.01     10.9     6.03\n                                                    64    15  97.42    1.2   0.67    67.15    22.49    12.45\n                                                  1500    15  94.89    1.5   0.83   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.11   0.74   0.41     56.9     1.64     0.91\n                                                    64    15  95.96   0.89   0.49    63.86    28.97    16.04\n                                                  1500    15  95.11   1.23   0.68    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.73   0.49   0.27    37.97     1.46     0.81\n                                                    64    15  95.96   0.53   0.29    38.06    20.21    11.19\n                                                  1500    15  94.44   1.63    0.9   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.98   0.87   0.48    80.83    11.11     6.15\n                                                    64    15  96.89   0.96   0.53    57.08     4.75     2.63\n                                                  1500    15  95.96   1.27    0.7  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.36   0.71   0.39    78.52     11.1     6.15\n                                                    64    15  96.89    0.7   0.39    88.49    33.41     18.5\n                                                  1500    15  95.47   0.84   0.47  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  96.71   0.92   0.51    80.08    12.72     7.04\n                                                    64    15  97.11   0.65   0.36    64.23    21.91    12.13\n                                                  1500    15   96.8   0.57   0.32   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-train-val-leftover       changertt            32    15  98.24   0.56   0.31    83.93      7.8     4.32\n                                                    64    15  98.29   0.71   0.39    61.57      5.2     2.88\n                                                  1500    15  98.43   0.22   0.12  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.46    0.6   0.33    78.01     10.9     6.03\n                                                    64    15  96.82   0.74   0.41    67.15    22.49    12.45\n                                                  1500    15  95.79   0.91    0.5   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.88   0.46   0.25     56.9     1.64     0.91\n                                                    64    15  96.38   0.91    0.5    63.86    28.97    16.04\n                                                  1500    15  96.47   1.02   0.57    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  96.05   0.35   0.19    37.97     1.46     0.81\n                                                    64    15  96.22   0.57   0.31    38.06    20.21    11.19\n                                                  1500    15  95.62   0.91   0.51   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  97.47   0.64   0.35    80.83    11.11     6.15\n                                                    64    15  97.48    0.5   0.28    57.08     4.75     2.63\n                                                  1500    15  97.29   0.49   0.27  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  97.01   0.43   0.24    78.52     11.1     6.15\n                                                    64    15  97.28   0.62   0.34    88.49    33.41     18.5\n                                                  1500    15  95.93   0.74   0.41  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  97.44   0.75   0.42    80.08    12.72     7.04\n                                                    64    15  97.78   0.68   0.38    64.23    21.91    12.13\n                                                  1500    15  97.94   0.34   0.19   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The console output is informing about the <code>campaign_id</code>  found in the repository and the total number of runs. If the repository was containing more than one campaign, a different report table would have been created for each campaign.</p> <p>The report is always grouped by <code>test-&lt;XYZ&gt;</code> contexts. By default all hyper parameters with more than one value are also added (see the previous description about properties).</p> <p>In the example, the <code>acc</code> metric is measured with mean, standard deviation and 95 %tile confidence intervals. Next to the metric is reported also <code>duration</code> which  corresponds to the overall time for train/validation/test  in the run execution.</p> <p>One can rearrange the table composition using the <code>--groupby</code> and <code>--contexts</code> options For instance, in the following we swap the order of the <code>hparams</code> used and report only on one context.</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/ \\\n    --groupby flowpic_dim,aug_name \\\n    --contexts test-human\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                       hparams                           acc                  duration\n      split  flowpic_dim         aug_name  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-human           32        changertt    15  70.04   4.41   2.44    83.93      7.8     4.32\n                              colorjitter    15  68.84   4.69   2.59    78.01     10.9     6.03\n                           horizontalflip    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                    noaug    15  69.48   2.12   1.17    37.97     1.46     0.81\n                               packetloss    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                   rotate    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                timeshift    15  70.36   2.98   1.65    80.08    12.72     7.04\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      64        changertt    15  72.05    2.1   1.16    61.57      5.2     2.88\n                              colorjitter    15  71.33   3.35   1.86    67.15    22.49    12.45\n                           horizontalflip    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                    noaug    15  69.88   2.28   1.26    38.06    20.21    11.19\n                               packetloss    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                   rotate    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                timeshift    15  72.53   1.83   1.02    64.23    21.91    12.13\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    1500        changertt    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                              colorjitter    15  68.59   3.17   1.76   765.43   152.53    84.47\n                           horizontalflip    15  73.82   1.47   0.82    471.8     58.6    32.45\n                                    noaug    15  68.67   1.93   1.07   374.88    94.41    52.28\n                               packetloss    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                                   rotate    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                                timeshift    15  70.84   2.42   1.34   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The <code>report</code> sub-command also creates output artifacts.</p> <ul> <li> <p>An output folder is created based on the <code>campaign_id</code> value.</p> </li> <li> <p>A set of <code>runinfo_&lt;XYZ&gt;.parquet</code> files collect runs     hyper param and metrics. </p> </li> <li> <p>A set of <code>summary_&lt;XYZ&gt;.csv</code> files collect the     aggregate table reported on the console.</p> </li> </ul>"},{"location":"modeling/overview/","title":"Modeling overview","text":"<p>When training ML/DL models,  finding the right combination of data preprocessing/splitting, algorithms and hyper-parameters can be challenging. Even more so when the modeling process  aims to be repeatable/replicable/reproducible.</p> <p>To ease this process is key to</p> <ul> <li> <p>Collect telemetry and metadata. This includes both the parameters used to create models as well as lower level metrics such as the evolution of the training loss over time.</p> </li> <li> <p>Generate artifacts such as  reports about the overall performance (e.g., confusion matrixes).</p> </li> </ul>"},{"location":"modeling/overview/#aim-stack-tracking","title":"AIM stack tracking","text":"<p><code>tcbench</code> integrates with AIM stack, an open-source and self-hosted model tracking framework enabling logging of metrics  related to model training. Such telemetry  can later be explored via a web interface or programmatically extracted via AIM SKD.</p> <p>Why not using more popular frameworks?</p> <p>There are many solutions for model tracking. While frameworks such as Weights &amp; Biases or Neptune.ai are extremely rich with features, unfortunately they typically  are cloud-based solutions and not necessarily open-sourced.</p> <p>Alternative frameworks such as Tensorboard and MLFlow have only primitive functionalities with respect to AIM stack.</p> <p>Aim stack is sitting in the middle of this spectrum: It is self-hosted (i.e., no need to push data to the cloud) and provides nice data exploration features.</p>"},{"location":"modeling/overview/#runs-and-campaigns","title":"Runs and campaigns","text":"<p>AIM collects modeling metadata into repositories which are fully controlled by end-users:</p> <ul> <li> <p>Repositories are not tied to specific projects. In other words, the end-user can store in a repository models completely unrelated to each other.</p> </li> <li> <p>There is no limit on the amount of repositories  can be created. </p> </li> </ul> <p><code>tcbench</code> tracks in an AIM repository two types of tasks, namely runs and campaigns:</p> <ul> <li> <p>A run corresponds to the training of an individual ML/DL model and is \"minimal experiment object\" used by AIM, i.e., any tracked metadata need to be associated to an AIM run.</p> </li> <li> <p>A campaign corresponds to a collection of runs. </p> </li> </ul> <p>AIM assign a unique hash code to a run, but a run object be further enriched with  extra metadata using AIM SDK or web UI.</p> <p>A run can be enriched with both individual values (e.g., best validation loss observed or the final accuracy score) as well as series (e.g., loss value for each epoch). Morever, values can have a context to further specify semantic (e.g., define if a registered metric relates to trainining, validation or test).</p> <p>While run is at term borrowed from AIM terminology, <code>tcbench</code> introduces campaign to  group runs which are semantically related and need to be summarized together (e.g., results collected across different train/val/test splits).</p> <p>It follows that:</p> <ul> <li> <p>Runs are the fundamental building block for collecting modeling results. But they are also the fundamental unit when developing/debugging modeling tasks.</p> </li> <li> <p>Campaigns bind multiple runs together. Hence, are meant to be stored in separate AIM repositories (although this is NOT a strict requirement for <code>tcbench</code>).</p> </li> </ul>"},{"location":"modeling/runs/","title":"Runs","text":"<p>Individual modeling run can be triggered from the subcommand <code>run</code></p> <pre><code>tcbench run --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench run [OPTIONS] COMMAND [ARGS]...\n\n Trigger an individual modeling run.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 augment-at-loading        Modeling by applying data augmentation when loading the training set.       \u2502\n\u2502 contralearn-and-finetune  Modeling by pre-training via constrative learning and then finetune the     \u2502\n\u2502                           final classifier from the pre-trained model.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The submission focuses on two types of runs. As from the help string of the sub-commands</p> <ul> <li> <p><code>augment-at-loading</code> applies data augmentation to  boost the number of samples in the training set.</p> </li> <li> <p><code>contralearn-and-finetune</code> pre-train a model (via SimCLR) and then uses it to finetune the final classifier.</p> </li> </ul> <p>Both run types are associated to a variety of parameters which,  for readability, are organized in groups when printing the <code>--help</code>.</p> <p>Each parameter help string should be sufficient for understanding their purpose so we skip their detailed discussion.</p> <p>Yet, for each run type we report a reference example of how to trigger it.</p> <p>Runs and campaigns are repeatable</p> <p>When executing the reference examples (or any of the runs collected in the ML artifacts) we expect you to obtain the  exact same results! </p>"},{"location":"modeling/runs/#augment-at-loading","title":"<code>augment-at-loading</code>","text":"<pre><code>tcbench run augment-at-loading --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench run augment-at-loading [OPTIONS]\n\n Modeling by applying data augmentation when loading the training set.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: debug/artifacts]                        \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL). [default: debug]     \u2502\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                        \u2502\n\u2502                                   [default: augmentation-at-loading]                                  \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).          \u2502\n\u2502                                   [default: 0]                                                        \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20]       \u2502\n\u2502 --seed                   INTEGER  Seed to initialize random generators. [default: 12345]              \u2502\n\u2502 --help                            Show this message and exit.                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --dataset                   [ucdavis-icdm19|utmobilenet21|mirag  Dataset to use for modeling.         \u2502\n\u2502                             e19|mirage22]                        [default: ucdavis-icdm19]            \u2502\n\u2502 --dataset-minpkts           [-1|10|100|1000]                     In combination with --dataset,       \u2502\n\u2502                                                                  refines preprocessed and split       \u2502\n\u2502                                                                  dataset to use.                      \u2502\n\u2502                                                                  [default: -1]                        \u2502\n\u2502 --flowpic-dim               [32|64|1500]                         Flowpic dimension. [default: 32]     \u2502\n\u2502 --flowpic-block-duration    INTEGER                              Number of seconds for the head of a  \u2502\n\u2502                                                                  flow (i.e., block) to use for a      \u2502\n\u2502                                                                  flowpic.                             \u2502\n\u2502                                                                  [default: 15]                        \u2502\n\u2502 --split-index               INTEGER                              Data split index. [default: 0]       \u2502\n\u2502 --train-val-split-ratio     FLOAT                                If not predefined by the selected    \u2502\n\u2502                                                                  split, the ratio data to use for     \u2502\n\u2502                                                                  training (rest is for validation).   \u2502\n\u2502                                                                  [default: 0.8]                       \u2502\n\u2502 --aug-name                  [noaug|rotate|horizontalflip|colorj  Name of the augmentation to use.     \u2502\n\u2502                             itter|packetloss|timeshift|changert  [default: noaug]                     \u2502\n\u2502                             t]                                                                        \u2502\n\u2502 --no-test-leftover                                               Skip test on leftover split          \u2502\n\u2502                                                                  (specific for ucdavis-icdm19, and    \u2502\n\u2502                                                                  default enabled for all other        \u2502\n\u2502                                                                  datasets).                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Modeling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --method    [monolithic|xgboost]  Method to use for training. [default: monolithic]                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 DL hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size        INTEGER  Training batch size [default: 32]                                        \u2502\n\u2502 --learning-rate     FLOAT    Training learning rate. [default: 0.001]                                 \u2502\n\u2502 --patience-steps    INTEGER  Max. number of epochs without improvement before stopping training.      \u2502\n\u2502                              [default: 5]                                                             \u2502\n\u2502 --epochs            INTEGER  Number of epochs for training. [default: 50]                             \u2502\n\u2502 --no-dropout                 Mask dropout layers with Identity layers.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 XGBoost hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --input-repr       TEXT     Input representation. [default: pktseries]                                \u2502\n\u2502 --pktseries-len    INTEGER  Number of packets (when using time series as input). [default: 10]        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"modeling/runs/#reference-example","title":"Reference example","text":"<pre><code>tcbench run augment-at-loading \\\n    --dataset ucdavis-icdm19 \\\n    --learning-rate 0.001 \\\n    --batch-size 32 \\\n    --flowpic-dim 32 \\\n    --split-index 0 \\\n    --seed 12345 \\\n    --aug-name noaug \\\n    --method monolithic\n</code></pre> Output <pre><code>opened log at debug/artifacts/0179aa10fa7245d6bfd79b49/log.txt\n\nconnecting to AIM repo at: debug\ncreated aim run hash=0179aa10fa7245d6bfd79b49\nartifacts folder at: debug/artifacts/0179aa10fa7245d6bfd79b49\nWARNING: the artifact folder is not a subfolder of the AIM repo\n--- run hparams ---\nflowpic_dim: 32\nflowpic_block_duration: 15\nsplit_index: 0\nmax_samples_per_class: -1\naug_name: noaug\npatience_steps: 5\nsuppress_val_augmentation: False\ndataset: ucdavis-icdm19\ndataset_minpkts: -1\nseed: 12345\nwith_dropout: True\n-------------------\nloaded: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\nno augmentation\nno augmentation\ndataset samples count\n               train  val\napp\ngoogle-doc        80   20\ngoogle-drive      80   20\ngoogle-music      80   20\ngoogle-search     80   20\nyoutube           80   20\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n         Dropout2d-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n           Linear-11                   [-1, 84]          10,164\n             ReLU-12                   [-1, 84]               0\n        Dropout1d-13                   [-1, 84]               0\n           Linear-14                    [-1, 5]             425\n================================================================\nTotal params: 61,281\nTrainable params: 61,281\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.23\nEstimated Total Size (MB): 0.36\n----------------------------------------------------------------\n---\nWARNING: Detected Dropout layer!\nWARNING: During supervised training, the monitored train_acc will be inaccurate\n---\nepoch:   0 | train_loss: 1.608132 | train_acc:  38.5% | val_loss: 1.609477 | val_acc:  95.0% | *\nepoch:   1 | train_loss: 1.337334 | train_acc:  49.2% | val_loss: 1.043962 | val_acc:  96.0% | *\nepoch:   2 | train_loss: 1.108509 | train_acc:  56.8% | val_loss: 0.708749 | val_acc:  95.0% | *\nepoch:   3 | train_loss: 1.100363 | train_acc:  54.0% | val_loss: 0.544368 | val_acc:  95.0% | *\nepoch:   4 | train_loss: 1.031712 | train_acc:  53.5% | val_loss: 0.389431 | val_acc:  97.0% | *\nepoch:   5 | train_loss: 0.987201 | train_acc:  57.5% | val_loss: 0.479318 | val_acc:  96.0%\nepoch:   6 | train_loss: 0.996780 | train_acc:  59.8% | val_loss: 0.341755 | val_acc:  97.0% | *\nepoch:   7 | train_loss: 1.044375 | train_acc:  53.8% | val_loss: 0.311472 | val_acc:  97.0% | *\nepoch:   8 | train_loss: 0.941040 | train_acc:  59.5% | val_loss: 0.364296 | val_acc:  98.0%\nepoch:   9 | train_loss: 0.951338 | train_acc:  58.8% | val_loss: 0.255973 | val_acc:  97.0% | *\nepoch:  10 | train_loss: 0.882188 | train_acc:  59.5% | val_loss: 0.257684 | val_acc:  98.0%\nepoch:  11 | train_loss: 0.946672 | train_acc:  54.8% | val_loss: 0.239257 | val_acc:  97.0% | *\nepoch:  12 | train_loss: 0.963688 | train_acc:  57.0% | val_loss: 0.215193 | val_acc:  97.0% | *\nepoch:  13 | train_loss: 0.934703 | train_acc:  56.8% | val_loss: 0.268320 | val_acc:  98.0%\nepoch:  14 | train_loss: 1.032247 | train_acc:  51.8% | val_loss: 0.192979 | val_acc:  97.0% | *\nepoch:  15 | train_loss: 0.968257 | train_acc:  56.8% | val_loss: 0.173423 | val_acc:  98.0% | *\nepoch:  16 | train_loss: 0.904505 | train_acc:  59.2% | val_loss: 0.234691 | val_acc:  96.0%\nepoch:  17 | train_loss: 0.957420 | train_acc:  57.8% | val_loss: 0.172733 | val_acc:  98.0%\nepoch:  18 | train_loss: 0.890696 | train_acc:  59.5% | val_loss: 0.179425 | val_acc:  97.0%\nepoch:  19 | train_loss: 0.933493 | train_acc:  58.5% | val_loss: 0.156731 | val_acc:  98.0% | *\nepoch:  20 | train_loss: 0.927721 | train_acc:  60.2% | val_loss: 0.243990 | val_acc:  96.0%\nepoch:  21 | train_loss: 1.007217 | train_acc:  55.0% | val_loss: 0.193065 | val_acc:  98.0%\nepoch:  22 | train_loss: 0.873772 | train_acc:  60.8% | val_loss: 0.149504 | val_acc:  98.0% | *\nepoch:  23 | train_loss: 0.967534 | train_acc:  56.8% | val_loss: 0.147503 | val_acc:  98.0% | *\nepoch:  24 | train_loss: 0.881966 | train_acc:  62.0% | val_loss: 0.167102 | val_acc:  98.0%\nepoch:  25 | train_loss: 0.871421 | train_acc:  59.2% | val_loss: 0.158940 | val_acc:  98.0%\nepoch:  26 | train_loss: 0.940876 | train_acc:  57.8% | val_loss: 0.135381 | val_acc:  98.0% | *\nepoch:  27 | train_loss: 0.865655 | train_acc:  61.8% | val_loss: 0.140646 | val_acc:  98.0%\nepoch:  28 | train_loss: 0.912683 | train_acc:  58.0% | val_loss: 0.115086 | val_acc:  98.0% | *\nepoch:  29 | train_loss: 0.915417 | train_acc:  57.2% | val_loss: 0.179121 | val_acc:  98.0%\nepoch:  30 | train_loss: 0.792390 | train_acc:  64.2% | val_loss: 0.105904 | val_acc:  97.0% | *\nepoch:  31 | train_loss: 0.857908 | train_acc:  62.8% | val_loss: 0.120300 | val_acc:  98.0%\nepoch:  32 | train_loss: 0.884288 | train_acc:  62.2% | val_loss: 0.113660 | val_acc:  98.0%\nepoch:  33 | train_loss: 0.938388 | train_acc:  56.0% | val_loss: 0.107200 | val_acc:  98.0%\nepoch:  34 | train_loss: 0.845021 | train_acc:  62.2% | val_loss: 0.145654 | val_acc:  98.0%\nepoch:  35 | train_loss: 0.937906 | train_acc:  56.5% | val_loss: 0.102596 | val_acc:  98.0% | *\nepoch:  36 | train_loss: 0.920095 | train_acc:  58.8% | val_loss: 0.105251 | val_acc:  98.0%\nepoch:  37 | train_loss: 0.902691 | train_acc:  59.0% | val_loss: 0.136474 | val_acc:  98.0%\nepoch:  38 | train_loss: 0.883385 | train_acc:  59.8% | val_loss: 0.117455 | val_acc:  98.0%\nepoch:  39 | train_loss: 0.854439 | train_acc:  61.0% | val_loss: 0.103810 | val_acc:  97.0%\nepoch:  40 | train_loss: 0.854355 | train_acc:  61.5% | val_loss: 0.120359 | val_acc:  98.0%\nrun out of patience\n\n---train reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.975610   1.000  0.987654   80.000\ngoogle-drive    1.000000   0.900  0.947368   80.000\ngoogle-music    0.917647   0.975  0.945455   80.000\ngoogle-search   0.987654   1.000  0.993789   80.000\nyoutube         1.000000   1.000  1.000000   80.000\naccuracy        0.975000   0.975  0.975000    0.975\nmacro avg       0.976182   0.975  0.974853  400.000\nweighted avg    0.976182   0.975  0.974853  400.000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             80             0             0              0        0\ngoogle-drive            1            72             7              0        0\ngoogle-music            1             0            78              1        0\ngoogle-search           0             0             0             80        0\nyoutube                 0             0             0              0       80\n\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/train_class_rep.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/train_conf_mtx.csv\n\n---val reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.952381    1.00  0.975610    20.00\ngoogle-drive    1.000000    0.95  0.974359    20.00\ngoogle-music    0.952381    1.00  0.975610    20.00\ngoogle-search   1.000000    0.95  0.974359    20.00\nyoutube         1.000000    1.00  1.000000    20.00\naccuracy        0.980000    0.98  0.980000     0.98\nmacro avg       0.980952    0.98  0.979987   100.00\nweighted avg    0.980952    0.98  0.979987   100.00\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             20             0             0              0        0\ngoogle-drive            0            19             1              0        0\ngoogle-music            0             0            20              0        0\ngoogle-search           1             0             0             19        0\nyoutube                 0             0             0              0       20\n\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/val_class_rep.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/val_conf_mtx.csv\nloading: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet\nloading: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet\n               human  script\napp\nyoutube           20      30\ngoogle-drive      18      30\ngoogle-doc        15      30\ngoogle-music      15      30\ngoogle-search     15      30\nTest dataset human | loss: 1.471955 | acc: 68.7\n\n---test-human reports---\n\n               precision    recall  f1-score    support\ngoogle-doc      0.500000  1.000000  0.666667  15.000000\ngoogle-drive    0.736842  0.777778  0.756757  18.000000\ngoogle-music    0.764706  0.866667  0.812500  15.000000\ngoogle-search   0.000000  0.000000  0.000000  15.000000\nyoutube         0.937500  0.750000  0.833333  20.000000\naccuracy        0.686747  0.686747  0.686747   0.686747\nmacro avg       0.587810  0.678889  0.613851  83.000000\nweighted avg    0.614262  0.686747  0.632238  83.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             15             0             0              0        0\ngoogle-drive            0            14             3              0        1\ngoogle-music            0             1            13              1        0\ngoogle-search          15             0             0              0        0\nyoutube                 0             4             1              0       15\n\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-human_class_rep.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-human_conf_mtx.csv\nTest dataset script | loss: 0.178018 | acc: 95.3\n\n---test-script reports---\n\n               precision    recall  f1-score     support\ngoogle-doc      0.882353  1.000000  0.937500   30.000000\ngoogle-drive    1.000000  0.900000  0.947368   30.000000\ngoogle-music    0.933333  0.933333  0.933333   30.000000\ngoogle-search   1.000000  0.933333  0.965517   30.000000\nyoutube         0.967742  1.000000  0.983607   30.000000\naccuracy        0.953333  0.953333  0.953333    0.953333\nmacro avg       0.956686  0.953333  0.953465  150.000000\nweighted avg    0.956686  0.953333  0.953465  150.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             30             0             0              0        0\ngoogle-drive            1            27             2              0        0\ngoogle-music            1             0            28              0        1\ngoogle-search           2             0             0             28        0\nyoutube                 0             0             0              0       30\n\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-script_class_rep.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-script_conf_mtx.csv\nloaded: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet\n\nTest dataset train-val-leftover | loss: 0.155867 | acc: 96.3\n\n---test-train-val-leftover reports---\n\n               precision    recall  f1-score     support\ngoogle-doc      0.953997  0.999210  0.976080  1266.00000\ngoogle-drive    0.994190  0.915577  0.953265  1682.00000\ngoogle-music    0.857143  0.960754  0.905996   637.00000\ngoogle-search   0.979114  0.980612  0.979862  1960.00000\nyoutube         0.969643  0.963620  0.966622  1127.00000\naccuracy        0.962980  0.962980  0.962980     0.96298\nmacro avg       0.950817  0.963955  0.956365  6672.00000\nweighted avg    0.964904  0.962980  0.963151  6672.00000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc           1265             0             1              0        0\ngoogle-drive           25          1540            98              0       19\ngoogle-music            3             4           612              8       10\ngoogle-search          32             1             0           1922        5\nyoutube                 1             4             3             33     1086\n\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-train-val-leftover_class_rep.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/test-train-val-leftover_conf_mtx.csv\nsaving: debug/artifacts/0179aa10fa7245d6bfd79b49/params.yml\n</code></pre>"},{"location":"modeling/runs/#contralearn-and-finetune","title":"<code>contralearn-and-finetune</code>","text":"<pre><code>tcbench run contralearn-and-finetune --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench run contralearn-and-finetune [OPTIONS]\n\n Modeling by pre-training via constrative learning and then finetune the final\n classifier from the pre-trained model.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: debug/artifacts]                        \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL). [default: debug]     \u2502\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                        \u2502\n\u2502                                   [default: contrastive-learning-and-finetune]                        \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).          \u2502\n\u2502                                   [default: 0]                                                        \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20]       \u2502\n\u2502 --help                            Show this message and exit.                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --dataset                   [ucdavis-icdm19]  Dataset to use for modeling. [default: ucdavis-icdm19]  \u2502\n\u2502 --flowpic-dim               [32]              Flowpic dimension. [default: 32]                        \u2502\n\u2502 --flowpic-block-duration    INTEGER           Number of seconds for the head of a flow (i.e., block)  \u2502\n\u2502                                               to use for a flowpic.                                   \u2502\n\u2502                                               [default: 15]                                           \u2502\n\u2502 --split-index               INTEGER           Data split index. [default: 0]                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 General Deeplearning hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size    INTEGER  Training batch size [default: 32]                                            \u2502\n\u2502 --no-dropout             Mask dropout layers with Identity layers.                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Contrastive learning hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --cl-aug-names               TEXT     Coma separated list of augmentations pool for contrastive       \u2502\n\u2502                                       learning.                                                       \u2502\n\u2502                                       [default: changertt,timeshift]                                  \u2502\n\u2502 --cl-projection-layer-dim    INTEGER  The number of units in the contrastive learning projection      \u2502\n\u2502                                       layer.                                                          \u2502\n\u2502                                       [default: 30]                                                   \u2502\n\u2502 --cl-learning-rate           FLOAT    Learning rate for pretraining. [default: 0.001]                 \u2502\n\u2502 --cl-seed                    INTEGER  Seed for contrastive learning pretraining. [default: 12345]     \u2502\n\u2502 --cl-patience-steps          INTEGER  Max steps to wait before stopping training if the top5          \u2502\n\u2502                                       validation accuracy does not improve.                           \u2502\n\u2502                                       [default: 3]                                                    \u2502\n\u2502 --cl-temperature             FLOAT    Temperature for InfoNCE loss. [default: 0.07]                   \u2502\n\u2502 --cl-epochs                  INTEGER  Epochs for contrastive learning pretraining. [default: 50]      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Finetune hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --ft-learning-rate         FLOAT    Learning rate for finetune. [default: 0.01]                       \u2502\n\u2502 --ft-patience-steps        INTEGER  Max steps to wait before stopping finetune training loss does not \u2502\n\u2502                                     improve.                                                          \u2502\n\u2502                                     [default: 5]                                                      \u2502\n\u2502 --ft-patience-min-delta    FLOAT    Minimum decrease of training loss to be considered as             \u2502\n\u2502                                     improvement.                                                      \u2502\n\u2502                                     [default: 0.001]                                                  \u2502\n\u2502 --ft-train-samples         INTEGER  Number of samples per-class for finetune training. [default: 10]  \u2502\n\u2502 --ft-epochs                INTEGER  Epochs for finetune training. [default: 50]                       \u2502\n\u2502 --ft-seed                  INTEGER  Seed for finetune training. [default: 12345]                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"modeling/runs/#reference-example_1","title":"Reference example","text":"<pre><code>tcbench run contralearn-and-finetune \n    --dataset ucdavis-icdm19 \\\n    --batch-size 32 \\\n    --flowpic-dim 32 \\\n    --split-index 0 \\\n    --no-dropout \\\n    --cl-seed 12345 \\\n    --ft-seed 12345 \\\n    --cl-projection-layer-dim 30\n</code></pre> Output <pre><code>opened log at debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/log.txt\n\nconnecting to AIM repo at: debug\ncreated aim run hash=b6255a30a35d4f5daa0beab1\nartifacts folder at: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1\nWARNING: the artifact folder is not a subfolder of the AIM repo\n--- run hparams ---\nflowpic_dim: 32\nsplit_index: 0\ndataset: ucdavis-icdm19\ndataset_minpkts: -1\ncontrastive_learning_seed: 12345\nfinetune_seed: 12345\nfinetune_train_samples: 10\nwith_dropout: False\nprojection_layer_dim: 30\nfinetune_augmentation: none\naugmentations: ['changertt', 'timeshift']\n-------------------\nloaded: /opt/anaconda/anaconda3/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\ndataset samples count\n               train  val\napp\ngoogle-doc        80   20\ngoogle-drive      80   20\ngoogle-music      80   20\ngoogle-search     80   20\nyoutube           80   20\n\n==== network adapted for pretrain ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n           Linear-11                  [-1, 120]          14,520\n             ReLU-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                   [-1, 30]           3,630\n================================================================\nTotal params: 68,842\nTrainable params: 68,842\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.26\nEstimated Total Size (MB): 0.39\n----------------------------------------------------------------\nepoch:   0 | train_loss: 2.792354 | train_acc_top_1:  22.1% | train_acc_top_5:  58.2% | val_loss: 2.770429 | val_acc_top_1:  24.6% | val_acc_top_5:  51.2% | *\nepoch:   1 | train_loss: 2.389633 | train_acc_top_1:  27.4% | train_acc_top_5:  62.3% | val_loss: 2.722881 | val_acc_top_1:  31.2% | val_acc_top_5:  54.3% | *\nepoch:   2 | train_loss: 2.277134 | train_acc_top_1:  26.1% | train_acc_top_5:  64.1% | val_loss: 2.687306 | val_acc_top_1:  26.6% | val_acc_top_5:  52.3%\nepoch:   3 | train_loss: 2.155350 | train_acc_top_1:  30.5% | train_acc_top_5:  68.5% | val_loss: 2.353693 | val_acc_top_1:  25.4% | val_acc_top_5:  60.5% | *\nepoch:   4 | train_loss: 2.128647 | train_acc_top_1:  30.8% | train_acc_top_5:  70.3% | val_loss: 2.409730 | val_acc_top_1:  30.9% | val_acc_top_5:  57.8%\nepoch:   5 | train_loss: 2.033960 | train_acc_top_1:  32.5% | train_acc_top_5:  72.5% | val_loss: 2.468154 | val_acc_top_1:  30.5% | val_acc_top_5:  57.8%\nepoch:   6 | train_loss: 1.933769 | train_acc_top_1:  36.7% | train_acc_top_5:  74.2% | val_loss: 2.246096 | val_acc_top_1:  34.8% | val_acc_top_5:  64.1% | *\nepoch:   7 | train_loss: 1.913906 | train_acc_top_1:  37.7% | train_acc_top_5:  78.6% | val_loss: 2.082685 | val_acc_top_1:  45.7% | val_acc_top_5:  68.4% | *\nepoch:   8 | train_loss: 1.800368 | train_acc_top_1:  36.2% | train_acc_top_5:  81.1% | val_loss: 2.157686 | val_acc_top_1:  35.2% | val_acc_top_5:  67.6%\nepoch:   9 | train_loss: 1.739876 | train_acc_top_1:  43.5% | train_acc_top_5:  81.7% | val_loss: 2.286904 | val_acc_top_1:  39.5% | val_acc_top_5:  67.6%\nepoch:  10 | train_loss: 1.773573 | train_acc_top_1:  41.8% | train_acc_top_5:  79.8% | val_loss: 2.208454 | val_acc_top_1:  30.5% | val_acc_top_5:  68.4%\nrun out of patience\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/best_model_weights_pretrain_split_0.pt\n               human_test  human_train  script_test  script_train\napp\ngoogle-doc              5           10           20            10\ngoogle-drive            8           10           20            10\ngoogle-music            5           10           20            10\ngoogle-search           5           10           20            10\nyoutube                10           10           20            10\n\n--- finetune (train) on human ---\napp\ngoogle-doc       10\ngoogle-drive     10\ngoogle-music     10\ngoogle-search    10\nyoutube          10\nName: count, dtype: int64\n\n==== network adapted for fine-tuning ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n         Identity-11                  [-1, 120]               0\n         Identity-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                    [-1, 5]             605\n================================================================\nTotal params: 51,297\nTrainable params: 51,297\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.20\nEstimated Total Size (MB): 0.33\n----------------------------------------------------------------\nepoch:   0 | train_loss: 3.231672 | train_acc:   8.0% | *\nepoch:   1 | train_loss: 3.074481 | train_acc:  32.0% | *\nepoch:   2 | train_loss: 2.968672 | train_acc:  40.0% | *\nepoch:   3 | train_loss: 2.879148 | train_acc:  40.0% | *\nepoch:   4 | train_loss: 2.783693 | train_acc:  42.0% | *\nepoch:   5 | train_loss: 2.705268 | train_acc:  36.0% | *\nepoch:   6 | train_loss: 2.661375 | train_acc:  36.0% | *\nepoch:   7 | train_loss: 2.562433 | train_acc:  38.0% | *\nepoch:   8 | train_loss: 2.525108 | train_acc:  40.0% | *\nepoch:   9 | train_loss: 2.476060 | train_acc:  42.0% | *\nepoch:  10 | train_loss: 2.450299 | train_acc:  46.0% | *\nepoch:  11 | train_loss: 2.396669 | train_acc:  50.0% | *\nepoch:  12 | train_loss: 2.376229 | train_acc:  58.0% | *\nepoch:  13 | train_loss: 2.351827 | train_acc:  64.0% | *\nepoch:  14 | train_loss: 2.383254 | train_acc:  64.0%\nepoch:  15 | train_loss: 2.248392 | train_acc:  64.0% | *\nepoch:  16 | train_loss: 2.223375 | train_acc:  64.0% | *\nepoch:  17 | train_loss: 2.243016 | train_acc:  66.0%\nepoch:  18 | train_loss: 2.158619 | train_acc:  68.0% | *\nepoch:  19 | train_loss: 2.122373 | train_acc:  70.0% | *\nepoch:  20 | train_loss: 2.078161 | train_acc:  72.0% | *\nepoch:  21 | train_loss: 2.087560 | train_acc:  72.0%\nepoch:  22 | train_loss: 2.053180 | train_acc:  72.0% | *\nepoch:  23 | train_loss: 1.953339 | train_acc:  76.0% | *\nepoch:  24 | train_loss: 2.032256 | train_acc:  80.0%\nepoch:  25 | train_loss: 1.967944 | train_acc:  82.0%\nepoch:  26 | train_loss: 2.018130 | train_acc:  82.0%\nepoch:  27 | train_loss: 1.859777 | train_acc:  82.0% | *\nepoch:  28 | train_loss: 1.923519 | train_acc:  82.0%\nepoch:  29 | train_loss: 1.845623 | train_acc:  82.0% | *\nepoch:  30 | train_loss: 1.927216 | train_acc:  82.0%\nepoch:  31 | train_loss: 1.904452 | train_acc:  82.0%\nepoch:  32 | train_loss: 1.835915 | train_acc:  84.0% | *\nepoch:  33 | train_loss: 1.840065 | train_acc:  84.0%\nepoch:  34 | train_loss: 1.829454 | train_acc:  84.0% | *\nepoch:  35 | train_loss: 1.747133 | train_acc:  84.0% | *\nepoch:  36 | train_loss: 1.829719 | train_acc:  84.0%\nepoch:  37 | train_loss: 1.773501 | train_acc:  84.0%\nepoch:  38 | train_loss: 1.727183 | train_acc:  84.0% | *\nepoch:  39 | train_loss: 1.750620 | train_acc:  84.0%\nepoch:  40 | train_loss: 1.795435 | train_acc:  82.0%\nepoch:  41 | train_loss: 1.750014 | train_acc:  82.0%\nepoch:  42 | train_loss: 1.681466 | train_acc:  84.0% | *\nepoch:  43 | train_loss: 1.620332 | train_acc:  84.0% | *\nepoch:  44 | train_loss: 1.638735 | train_acc:  84.0%\nepoch:  45 | train_loss: 1.647057 | train_acc:  84.0%\nepoch:  46 | train_loss: 1.627543 | train_acc:  84.0%\nepoch:  47 | train_loss: 1.693797 | train_acc:  84.0%\nepoch:  48 | train_loss: 1.609901 | train_acc:  86.0% | *\nepoch:  49 | train_loss: 1.628688 | train_acc:  86.0%\nreached max epochs\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/best_model_weights_finetune_human_from_split_0.pt\n\n--- finetune (test) on human ---\napp\nyoutube          10\ngoogle-drive      8\ngoogle-doc        5\ngoogle-music      5\ngoogle-search     5\nName: count, dtype: int64\nTest dataset human | loss: 2.244415 | acc: 75.8\n\n---test-human reports---\n\n               precision    recall  f1-score    support\ngoogle-doc      0.714286  1.000000  0.833333   5.000000\ngoogle-drive    0.600000  0.750000  0.666667   8.000000\ngoogle-music    0.714286  1.000000  0.833333   5.000000\ngoogle-search   1.000000  0.600000  0.750000   5.000000\nyoutube         1.000000  0.600000  0.750000  10.000000\naccuracy        0.757576  0.757576  0.757576   0.757576\nmacro avg       0.805714  0.790000  0.766667  33.000000\nweighted avg    0.816450  0.757576  0.755051  33.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc              5             0             0              0        0\ngoogle-drive            0             6             2              0        0\ngoogle-music            0             0             5              0        0\ngoogle-search           2             0             0              3        0\nyoutube                 0             4             0              0        6\n\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/test-human_class_rep.csv\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/test-human_conf_mtx.csv\n\n--- finetune (train) on script ---\napp\ngoogle-doc       10\ngoogle-drive     10\ngoogle-music     10\ngoogle-search    10\nyoutube          10\nName: count, dtype: int64\n\n==== network adapted for fine-tuning ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n         Identity-11                  [-1, 120]               0\n         Identity-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                    [-1, 5]             605\n================================================================\nTotal params: 51,297\nTrainable params: 51,297\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.20\nEstimated Total Size (MB): 0.33\n----------------------------------------------------------------\nepoch:   0 | train_loss: 3.220116 | train_acc:  20.0% | *\nepoch:   1 | train_loss: 3.102323 | train_acc:  34.0% | *\nepoch:   2 | train_loss: 2.989245 | train_acc:  44.0% | *\nepoch:   3 | train_loss: 2.864427 | train_acc:  46.0% | *\nepoch:   4 | train_loss: 2.747027 | train_acc:  52.0% | *\nepoch:   5 | train_loss: 2.699277 | train_acc:  54.0% | *\nepoch:   6 | train_loss: 2.589165 | train_acc:  56.0% | *\nepoch:   7 | train_loss: 2.562590 | train_acc:  70.0% | *\nepoch:   8 | train_loss: 2.483140 | train_acc:  72.0% | *\nepoch:   9 | train_loss: 2.362642 | train_acc:  76.0% | *\nepoch:  10 | train_loss: 2.249433 | train_acc:  78.0% | *\nepoch:  11 | train_loss: 2.289871 | train_acc:  78.0%\nepoch:  12 | train_loss: 2.206161 | train_acc:  78.0% | *\nepoch:  13 | train_loss: 2.123558 | train_acc:  78.0% | *\nepoch:  14 | train_loss: 2.060765 | train_acc:  76.0% | *\nepoch:  15 | train_loss: 2.060799 | train_acc:  76.0%\nepoch:  16 | train_loss: 1.909777 | train_acc:  76.0% | *\nepoch:  17 | train_loss: 1.910333 | train_acc:  76.0%\nepoch:  18 | train_loss: 1.919481 | train_acc:  80.0%\nepoch:  19 | train_loss: 1.793242 | train_acc:  84.0% | *\nepoch:  20 | train_loss: 1.772380 | train_acc:  92.0% | *\nepoch:  21 | train_loss: 1.818070 | train_acc:  92.0%\nepoch:  22 | train_loss: 1.777959 | train_acc:  92.0%\nepoch:  23 | train_loss: 1.720130 | train_acc:  92.0% | *\nepoch:  24 | train_loss: 1.641118 | train_acc:  92.0% | *\nepoch:  25 | train_loss: 1.600633 | train_acc:  92.0% | *\nepoch:  26 | train_loss: 1.522475 | train_acc:  92.0% | *\nepoch:  27 | train_loss: 1.518417 | train_acc:  92.0% | *\nepoch:  28 | train_loss: 1.578722 | train_acc:  92.0%\nepoch:  29 | train_loss: 1.502436 | train_acc:  92.0% | *\nepoch:  30 | train_loss: 1.487650 | train_acc:  94.0% | *\nepoch:  31 | train_loss: 1.516318 | train_acc:  94.0%\nepoch:  32 | train_loss: 1.452326 | train_acc:  92.0% | *\nepoch:  33 | train_loss: 1.400150 | train_acc:  92.0% | *\nepoch:  34 | train_loss: 1.314930 | train_acc:  92.0% | *\nepoch:  35 | train_loss: 1.313898 | train_acc:  92.0% | *\nepoch:  36 | train_loss: 1.343631 | train_acc:  92.0%\nepoch:  37 | train_loss: 1.351552 | train_acc:  92.0%\nepoch:  38 | train_loss: 1.242814 | train_acc:  92.0% | *\nepoch:  39 | train_loss: 1.285116 | train_acc:  92.0%\nepoch:  40 | train_loss: 1.248655 | train_acc:  92.0%\nepoch:  41 | train_loss: 1.279359 | train_acc:  92.0%\nepoch:  42 | train_loss: 1.189178 | train_acc:  92.0% | *\nepoch:  43 | train_loss: 1.235253 | train_acc:  92.0%\nepoch:  44 | train_loss: 1.195394 | train_acc:  92.0%\nepoch:  45 | train_loss: 1.197707 | train_acc:  92.0%\nepoch:  46 | train_loss: 1.259920 | train_acc:  92.0%\nepoch:  47 | train_loss: 1.193645 | train_acc:  92.0%\nrun out of patience\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/best_model_weights_finetune_script_from_split_0.pt\n\n--- finetune (test) on script ---\napp\ngoogle-doc       20\ngoogle-drive     20\ngoogle-music     20\ngoogle-search    20\nyoutube          20\nName: count, dtype: int64\nTest dataset script | loss: 0.754386 | acc: 93.0\n\n---test-script reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.900000    0.90  0.900000    20.00\ngoogle-drive    1.000000    0.90  0.947368    20.00\ngoogle-music    0.904762    0.95  0.926829    20.00\ngoogle-search   0.904762    0.95  0.926829    20.00\nyoutube         0.950000    0.95  0.950000    20.00\naccuracy        0.930000    0.93  0.930000     0.93\nmacro avg       0.931905    0.93  0.930205   100.00\nweighted avg    0.931905    0.93  0.930205   100.00\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             18             0             0              2        0\ngoogle-drive            0            18             1              0        1\ngoogle-music            1             0            19              0        0\ngoogle-search           1             0             0             19        0\nyoutube                 0             0             1              0       19\n\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/test-script_class_rep.csv\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/test-script_conf_mtx.csv\nsaving: debug/artifacts/ucdavis-icdm19/b6255a30a35d4f5daa0beab1/params.yml\n</code></pre>"},{"location":"paper_tables_and_figures/figure1_confusion_matrix_supervised_setting/","title":"Figure1 : Average confusion matrixes for the 32x32 resolution across all experiments in Table 3","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[2]: Copied! <pre>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import pathlib  import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.preprocessing import normalize  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[3]: Copied! <pre>folder_artifacts = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts/\"\n)\n</pre> folder_artifacts = pathlib.Path(     \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts/\" ) In\u00a0[4]: Copied! <pre>filelists = [\n    list(folder_artifacts.glob(\"*/test-human_conf_mtx.csv\")),\n    list(folder_artifacts.glob(\"*/test-script_conf_mtx.csv\")),\n]\n\ntitles = [\"human\", \"script\"]\n\nclasses = [\"google-doc\", \"google-drive\", \"google-music\", \"google-search\", \"youtube\"]\n</pre> filelists = [     list(folder_artifacts.glob(\"*/test-human_conf_mtx.csv\")),     list(folder_artifacts.glob(\"*/test-script_conf_mtx.csv\")), ]  titles = [\"human\", \"script\"]  classes = [\"google-doc\", \"google-drive\", \"google-music\", \"google-search\", \"youtube\"] In\u00a0[5]: Copied! <pre>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 5))\ncbar_ax = fig.add_axes([0.93, 0.2, 0.02, 0.6])  # (left, bottom, width, height)\nfor i in range(2):\n    cm_mean = np.mean(\n        np.stack([pd.read_csv(file)[classes].values for file in filelists[i]]), axis=0\n    )\n\n    normed_cm_mean = normalize(cm_mean, axis=1, norm=\"l1\")\n\n    ax = axes[i]\n\n    sns.heatmap(\n        data=normed_cm_mean,\n        ax=ax,\n        square=True,\n        cmap=\"viridis\",\n        annot=True,\n        annot_kws={\"size\": 10},\n        fmt=\".2f\",\n        vmin=0,\n        vmax=1,\n        cbar=i == 0,\n        cbar_ax=None if i else cbar_ax,\n    )\n\n    ax.set_xticklabels(classes, rotation=90)\n    ax.set_yticklabels(classes, rotation=0)\n\n    ax.set_title(titles[i])\n\n    ax.set_ylabel(\"Ground Truth\")\n    ax.set_xlabel(\"Prediction\")\n\nfig.tight_layout(rect=[0, 0, 0.92, 1])\nplt.show()\n</pre> fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 5)) cbar_ax = fig.add_axes([0.93, 0.2, 0.02, 0.6])  # (left, bottom, width, height) for i in range(2):     cm_mean = np.mean(         np.stack([pd.read_csv(file)[classes].values for file in filelists[i]]), axis=0     )      normed_cm_mean = normalize(cm_mean, axis=1, norm=\"l1\")      ax = axes[i]      sns.heatmap(         data=normed_cm_mean,         ax=ax,         square=True,         cmap=\"viridis\",         annot=True,         annot_kws={\"size\": 10},         fmt=\".2f\",         vmin=0,         vmax=1,         cbar=i == 0,         cbar_ax=None if i else cbar_ax,     )      ax.set_xticklabels(classes, rotation=90)     ax.set_yticklabels(classes, rotation=0)      ax.set_title(titles[i])      ax.set_ylabel(\"Ground Truth\")     ax.set_xlabel(\"Prediction\")  fig.tight_layout(rect=[0, 0, 0.92, 1]) plt.show() <pre>/tmp/ipykernel_26554/3130660952.py:34: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout(rect=[0, 0, 0.92, 1])\n</pre>"},{"location":"paper_tables_and_figures/figure1_confusion_matrix_supervised_setting/#figure1-average-confusion-matrixes-for-the-32x32-resolution-across-all-experiments-in-table-3","title":"Figure1 : Average confusion matrixes for the 32x32 resolution across all experiments in Table 3\u00b6","text":""},{"location":"paper_tables_and_figures/figure1_confusion_matrix_supervised_setting/","title":"Figure1 : Average confusion matrixes for the 32x32 resolution across all experiments in Table 3","text":"<pre><code>import pathlib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import normalize\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>folder_artifacts = pathlib.Path(\n\"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts/\"\n)\n</code></pre> <pre><code>filelists = [\nlist(folder_artifacts.glob(\"*/test-human_conf_mtx.csv\")),\nlist(folder_artifacts.glob(\"*/test-script_conf_mtx.csv\")),\n]\ntitles = [\"human\", \"script\"]\nclasses = [\"google-doc\", \"google-drive\", \"google-music\", \"google-search\", \"youtube\"]\n</code></pre> <pre><code>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 5))\ncbar_ax = fig.add_axes([0.93, 0.2, 0.02, 0.6])  # (left, bottom, width, height)\nfor i in range(2):\ncm_mean = np.mean(\nnp.stack([pd.read_csv(file)[classes].values for file in filelists[i]]), axis=0\n)\nnormed_cm_mean = normalize(cm_mean, axis=1, norm=\"l1\")\nax = axes[i]\nsns.heatmap(\ndata=normed_cm_mean,\nax=ax,\nsquare=True,\ncmap=\"viridis\",\nannot=True,\nannot_kws={\"size\": 10},\nfmt=\".2f\",\nvmin=0,\nvmax=1,\ncbar=i == 0,\ncbar_ax=None if i else cbar_ax,\n)\nax.set_xticklabels(classes, rotation=90)\nax.set_yticklabels(classes, rotation=0)\nax.set_title(titles[i])\nax.set_ylabel(\"Ground Truth\")\nax.set_xlabel(\"Prediction\")\nfig.tight_layout(rect=[0, 0, 0.92, 1])\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_26554/3130660952.py:34: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout(rect=[0, 0, 0.92, 1])\n</code></pre>"},{"location":"paper_tables_and_figures/figure2_ucdavis_per_class_average_flowpic/","title":"Figure 2: Average 32x32 flowpic for each class across multiple data splits.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import itertools\n\nimport numpy as np\nimport pandas as pd\n</pre> import itertools  import numpy as np import pandas as pd In\u00a0[2]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import LogNorm, Normalize  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[4]: Copied! <pre>import tcbench as tcb\nfrom tcbench import dataprep\n</pre> import tcbench as tcb from tcbench import dataprep In\u00a0[5]: Copied! <pre>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</pre> FLOWPIC_DIM = 32 FLOWPIC_BLOCK_DURATION = 15 In\u00a0[7]: Copied! <pre># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</pre> # load unfiltered dataset dset = dataprep.FlowpicDataset(     data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),     timetofirst_colname=\"timetofirst\",     pkts_size_colname=\"pkts_size\",     pkts_dir_colname=\"pkts_dir\",     target_colname=\"app\",     flowpic_dim=FLOWPIC_DIM,     flowpic_block_duration=FLOWPIC_BLOCK_DURATION, ) In\u00a0[8]: Copied! <pre># load the first train split\ndset_train_split = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split=0),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</pre> # load the first train split dset_train_split = dataprep.FlowpicDataset(     data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split=0),     timetofirst_colname=\"timetofirst\",     pkts_size_colname=\"pkts_size\",     pkts_dir_colname=\"pkts_dir\",     target_colname=\"app\",     flowpic_dim=FLOWPIC_DIM,     flowpic_block_duration=FLOWPIC_BLOCK_DURATION, ) In\u00a0[9]: Copied! <pre>def compute_average_flowpic(df):\n    # gather all (precomputed) flowpic\n    # in a single tensor (n x dim x dim)\n    # and do an avereage across the 1st dimension\n    mtx = np.stack(df[\"flowpic\"], axis=0).mean(\n        axis=0, keepdims=True\n    )  # .astype(np.uint8)\n    return mtx\n</pre> def compute_average_flowpic(df):     # gather all (precomputed) flowpic     # in a single tensor (n x dim x dim)     # and do an avereage across the 1st dimension     mtx = np.stack(df[\"flowpic\"], axis=0).mean(         axis=0, keepdims=True     )  # .astype(np.uint8)     return mtx In\u00a0[10]: Copied! <pre>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\n\nTARGETS_LABEL, PARTITIONS_NAME\n</pre> TARGETS_LABEL = sorted(dset.df[\"app\"].unique()) PARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())  TARGETS_LABEL, PARTITIONS_NAME Out[10]: <pre>(['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'],\n ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'])</pre> In\u00a0[11]: Copied! <pre>average_flowpic = dict()\nfor partition_name in [\n    \"pretraining\",\n    \"train-split\",\n    \"retraining-script-triggered\",\n    \"retraining-human-triggered\",\n]:\n    if partition_name != \"train-split\":\n        df_partition = dset.df[dset.df[\"partition\"] == partition_name]\n    else:\n        df_partition = dset_train_split.df\n\n    average_flowpic[partition_name] = dict()\n\n    for target in TARGETS_LABEL:\n        df_app = df_partition[df_partition[\"app\"] == target]\n        mtx = compute_average_flowpic(df_app).squeeze()\n        average_flowpic[partition_name][target] = mtx\n</pre> average_flowpic = dict() for partition_name in [     \"pretraining\",     \"train-split\",     \"retraining-script-triggered\",     \"retraining-human-triggered\", ]:     if partition_name != \"train-split\":         df_partition = dset.df[dset.df[\"partition\"] == partition_name]     else:         df_partition = dset_train_split.df      average_flowpic[partition_name] = dict()      for target in TARGETS_LABEL:         df_app = df_partition[df_partition[\"app\"] == target]         mtx = compute_average_flowpic(df_app).squeeze()         average_flowpic[partition_name][target] = mtx In\u00a0[12]: Copied! <pre>mtx_min = 100\nmtx_max = -100\nfor partition_name, app in itertools.product(\n    [\n        \"pretraining\",\n        \"train-split\",\n        \"retraining-script-triggered\",\n        \"retraining-human-triggered\",\n    ],\n    TARGETS_LABEL,\n):\n    mtx = average_flowpic[partition_name][app]\n    nonzero = mtx.flatten()\n    nonzero = nonzero[nonzero &gt; 0]\n    if nonzero.min() &lt; mtx_min:\n        mtx_min = nonzero.min()\n    if mtx.max() &gt; mtx_max:\n        mtx_max = mtx.max()\n\nmtx_min, mtx_max\n</pre> mtx_min = 100 mtx_max = -100 for partition_name, app in itertools.product(     [         \"pretraining\",         \"train-split\",         \"retraining-script-triggered\",         \"retraining-human-triggered\",     ],     TARGETS_LABEL, ):     mtx = average_flowpic[partition_name][app]     nonzero = mtx.flatten()     nonzero = nonzero[nonzero &gt; 0]     if nonzero.min() &lt; mtx_min:         mtx_min = nonzero.min()     if mtx.max() &gt; mtx_max:         mtx_max = mtx.max()  mtx_min, mtx_max Out[12]: <pre>(0.0005221932114882506, 238.0)</pre> In\u00a0[13]: Copied! <pre>mpl.rcParams.update({\"font.size\": 10})\n\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(8, 6), sharex=True, sharey=True)\n\ncbar_ax = fig.add_axes([0.91, 0.2, 0.03, 0.6])  # (left, bottom, width, height)\n\nfor i, ax, (partition_name, app) in zip(\n    range(len(axes.flatten())),\n    axes.flatten(),\n    itertools.product(\n        [\n            \"pretraining\",\n            \"train-split\",\n            \"retraining-script-triggered\",\n            \"retraining-human-triggered\",\n        ],\n        TARGETS_LABEL,\n    ),\n):\n    mtx = average_flowpic[partition_name][app]\n\n    sns.heatmap(\n        ax=ax,\n        data=np.where(mtx == 0, np.nan, mtx),\n        vmin=mtx_min,\n        vmax=mtx_max,\n        cbar=i == 0, \n        cmap=plt.get_cmap(\"viridis_r\"),\n        square=True,\n        norm=LogNorm(mtx_min, mtx_max),\n        cbar_ax=None if i else cbar_ax,  # &lt;19\n    )\n\n    # ax.set_title(target)\n    for pos in (\"top\", \"bottom\", \"right\", \"left\"):\n        ax.spines[pos].set_color(\"lightgray\")\n        ax.spines[pos].set_visible(True)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_ticks([], None)\n    ax.set_ylabel(\"\")\n\nfor ax, app in zip(axes[0], TARGETS_LABEL):\n    ax.set_title(app, fontsize=10)\n\nfor ax, partition_name in zip(\n    axes[:, 0], [\"pretraining\", \"pretraining\\none split\", \"script\", \"human\"]\n):  \n    ax.set_ylabel(partition_name, fontsize=10)\n\nfig.tight_layout(rect=[0, 0, 0.9, 1])\n\nplt.show()\n</pre> mpl.rcParams.update({\"font.size\": 10})  fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(8, 6), sharex=True, sharey=True)  cbar_ax = fig.add_axes([0.91, 0.2, 0.03, 0.6])  # (left, bottom, width, height)  for i, ax, (partition_name, app) in zip(     range(len(axes.flatten())),     axes.flatten(),     itertools.product(         [             \"pretraining\",             \"train-split\",             \"retraining-script-triggered\",             \"retraining-human-triggered\",         ],         TARGETS_LABEL,     ), ):     mtx = average_flowpic[partition_name][app]      sns.heatmap(         ax=ax,         data=np.where(mtx == 0, np.nan, mtx),         vmin=mtx_min,         vmax=mtx_max,         cbar=i == 0,          cmap=plt.get_cmap(\"viridis_r\"),         square=True,         norm=LogNorm(mtx_min, mtx_max),         cbar_ax=None if i else cbar_ax,  # &lt;19     )      # ax.set_title(target)     for pos in (\"top\", \"bottom\", \"right\", \"left\"):         ax.spines[pos].set_color(\"lightgray\")         ax.spines[pos].set_visible(True)     ax.xaxis.set_visible(False)     ax.yaxis.set_ticks([], None)     ax.set_ylabel(\"\")  for ax, app in zip(axes[0], TARGETS_LABEL):     ax.set_title(app, fontsize=10)  for ax, partition_name in zip(     axes[:, 0], [\"pretraining\", \"pretraining\\none split\", \"script\", \"human\"] ):       ax.set_ylabel(partition_name, fontsize=10)  fig.tight_layout(rect=[0, 0, 0.9, 1])  plt.show() <pre>/tmp/ipykernel_30167/2059813758.py:54: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout(rect=[0, 0,\n</pre>"},{"location":"paper_tables_and_figures/figure2_ucdavis_per_class_average_flowpic/#figure-2-average-32x32-flowpic-for-each-class-across-multiple-data-splits","title":"Figure 2: Average 32x32 flowpic for each class across multiple data splits.\u00b6","text":""},{"location":"paper_tables_and_figures/figure2_ucdavis_per_class_average_flowpic/","title":"Figure 2: Average 32x32 flowpic for each class across multiple data splits.","text":"<pre><code>import itertools\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import tcbench as tcb\nfrom tcbench import dataprep\n</code></pre> <pre><code>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</code></pre> <pre><code># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\ndata=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\ntimetofirst_colname=\"timetofirst\",\npkts_size_colname=\"pkts_size\",\npkts_dir_colname=\"pkts_dir\",\ntarget_colname=\"app\",\nflowpic_dim=FLOWPIC_DIM,\nflowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code># load the first train split\ndset_train_split = dataprep.FlowpicDataset(\ndata=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split=0),\ntimetofirst_colname=\"timetofirst\",\npkts_size_colname=\"pkts_size\",\npkts_dir_colname=\"pkts_dir\",\ntarget_colname=\"app\",\nflowpic_dim=FLOWPIC_DIM,\nflowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code>def compute_average_flowpic(df):\n# gather all (precomputed) flowpic\n# in a single tensor (n x dim x dim)\n# and do an avereage across the 1st dimension\nmtx = np.stack(df[\"flowpic\"], axis=0).mean(\naxis=0, keepdims=True\n)  # .astype(np.uint8)\nreturn mtx\n</code></pre> <pre><code>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\nTARGETS_LABEL, PARTITIONS_NAME\n</code></pre> <pre><code>(['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'],\n ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'])\n</code></pre> <pre><code>average_flowpic = dict()\nfor partition_name in [\n\"pretraining\",\n\"train-split\",\n\"retraining-script-triggered\",\n\"retraining-human-triggered\",\n]:\nif partition_name != \"train-split\":\ndf_partition = dset.df[dset.df[\"partition\"] == partition_name]\nelse:\ndf_partition = dset_train_split.df\naverage_flowpic[partition_name] = dict()\nfor target in TARGETS_LABEL:\ndf_app = df_partition[df_partition[\"app\"] == target]\nmtx = compute_average_flowpic(df_app).squeeze()\naverage_flowpic[partition_name][target] = mtx\n</code></pre> <pre><code>mtx_min = 100\nmtx_max = -100\nfor partition_name, app in itertools.product(\n[\n\"pretraining\",\n\"train-split\",\n\"retraining-script-triggered\",\n\"retraining-human-triggered\",\n],\nTARGETS_LABEL,\n):\nmtx = average_flowpic[partition_name][app]\nnonzero = mtx.flatten()\nnonzero = nonzero[nonzero &gt; 0]\nif nonzero.min() &lt; mtx_min:\nmtx_min = nonzero.min()\nif mtx.max() &gt; mtx_max:\nmtx_max = mtx.max()\nmtx_min, mtx_max\n</code></pre> <pre><code>(0.0005221932114882506, 238.0)\n</code></pre> <pre><code>mpl.rcParams.update({\"font.size\": 10})\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(8, 6), sharex=True, sharey=True)\ncbar_ax = fig.add_axes([0.91, 0.2, 0.03, 0.6])  # (left, bottom, width, height)\nfor i, ax, (partition_name, app) in zip(\nrange(len(axes.flatten())),\naxes.flatten(),\nitertools.product(\n[\n\"pretraining\",\n\"train-split\",\n\"retraining-script-triggered\",\n\"retraining-human-triggered\",\n],\nTARGETS_LABEL,\n),\n):\nmtx = average_flowpic[partition_name][app]\nsns.heatmap(\nax=ax,\ndata=np.where(mtx == 0, np.nan, mtx),\nvmin=mtx_min,\nvmax=mtx_max,\ncbar=i == 0, \ncmap=plt.get_cmap(\"viridis_r\"),\nsquare=True,\nnorm=LogNorm(mtx_min, mtx_max),\ncbar_ax=None if i else cbar_ax,  # &lt;19\n)\n# ax.set_title(target)\nfor pos in (\"top\", \"bottom\", \"right\", \"left\"):\nax.spines[pos].set_color(\"lightgray\")\nax.spines[pos].set_visible(True)\nax.xaxis.set_visible(False)\nax.yaxis.set_ticks([], None)\nax.set_ylabel(\"\")\nfor ax, app in zip(axes[0], TARGETS_LABEL):\nax.set_title(app, fontsize=10)\nfor ax, partition_name in zip(\naxes[:, 0], [\"pretraining\", \"pretraining\\none split\", \"script\", \"human\"]\n):  \nax.set_ylabel(partition_name, fontsize=10)\nfig.tight_layout(rect=[0, 0, 0.9, 1])\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_30167/2059813758.py:54: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout(rect=[0, 0,\n</code></pre>"},{"location":"paper_tables_and_figures/figure3_ucdavis_augmentations_comparison/","title":"Figure 3: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[1]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[7]: Copied! <pre>import pathlib\n\nimport autorank\n</pre> import pathlib  import autorank In\u00a0[19]: Copied! <pre>df = pd.read_parquet(\n    \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet\"\n)\n</pre> df = pd.read_parquet(     \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet\" ) In\u00a0[20]: Copied! <pre>def prepare_data(dat, test_split):\n    res = dat.query(\"test_split_name == @test_split &amp; flowpic_dim != 1500\")\n    res = res[[\"aug_name\", \"split_index\", \"flowpic_dim\", \"seed\", \"acc\"]]\n    res[\"id\"] = (\n        \"split_index\"\n        + res[\"split_index\"].astype(str)\n        + \"_seed\"\n        + res[\"seed\"].astype(str)\n        + \"_flowpicdim\"\n        + res[\"flowpic_dim\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"acc\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df, test_split, force_ranks=True):\n    df1 = prepare_data(df, test_split)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement)\n    if force_ranks:\n        return autorank.autorank(new_df, force_mode=\"nonparametric\")\n    else:\n        return autorank.autorank(new_df)\n</pre> def prepare_data(dat, test_split):     res = dat.query(\"test_split_name == @test_split &amp; flowpic_dim != 1500\")     res = res[[\"aug_name\", \"split_index\", \"flowpic_dim\", \"seed\", \"acc\"]]     res[\"id\"] = (         \"split_index\"         + res[\"split_index\"].astype(str)         + \"_seed\"         + res[\"seed\"].astype(str)         + \"_flowpicdim\"         + res[\"flowpic_dim\"].astype(str)     )     res = res[[\"aug_name\", \"id\", \"acc\"]]     return res.sort_values([\"aug_name\", \"id\"])   def get_ranks(df, test_split, force_ranks=True):     df1 = prepare_data(df, test_split)     df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)     df1.columns = df1.columns.get_level_values(1)     new_df = pd.DataFrame(         {             \"changertt\": df1[\"changertt\"].values,             \"colorjitter\": df1[\"colorjitter\"].values,             \"horizontalflip\": df1[\"horizontalflip\"].values,             \"noaug\": df1[\"noaug\"].values,             \"packetloss\": df1[\"packetloss\"].values,             \"rotate\": df1[\"rotate\"].values,             \"timeshift\": df1[\"timeshift\"].values,         }     )     replacement = {         \"noaug\": \"No augmentation\",         \"horizontalflip\": \"Horizontal flip\",         \"rotate\": \"Rotate\",         \"timeshift\": \"Time shift\",         \"colorjitter\": \"Color jitter\",         \"changertt\": \"Change RTT\",         \"packetloss\": \"Packet Loss\",     }     new_df = new_df.rename(columns=replacement)     if force_ranks:         return autorank.autorank(new_df, force_mode=\"nonparametric\")     else:         return autorank.autorank(new_df) In\u00a0[21]: Copied! <pre>res_script = get_ranks(df, \"test-script\", force_ranks=True)\nres_human = get_ranks(df, \"test-human\", force_ranks=True)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 4))\nax = axes[0]\nautorank.plot_stats(res_script, ax=ax)\nax.set_title(\"Test-script\")\n\nax = axes[1]\nautorank.plot_stats(res_human, ax=ax)\nax.set_title(\"Test-human\")\nplt.tight_layout()\n\n# plt.savefig(\"figures/augmentations_rank_comparison.pdf\", bbox_inches=\"tight\")\n</pre> res_script = get_ranks(df, \"test-script\", force_ranks=True) res_human = get_ranks(df, \"test-human\", force_ranks=True)  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 4)) ax = axes[0] autorank.plot_stats(res_script, ax=ax) ax.set_title(\"Test-script\")  ax = axes[1] autorank.plot_stats(res_human, ax=ax) ax.set_title(\"Test-human\") plt.tight_layout()  # plt.savefig(\"figures/augmentations_rank_comparison.pdf\", bbox_inches=\"tight\") <pre>Tests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\nTests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\n</pre> In\u00a0[22]: Copied! <pre>autorank.create_report(res_human)\n</pre> autorank.create_report(res_human) <pre>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.010). Therefore, we assume that all populations are normal.\nWe applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.064) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\nBecause we have more than two populations and all populations are normal and homoscedastic, we should use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. However, the user decided to force the use of the less powerful Friedman test as omnibus test to determine if there are any significant differences between the mean values of the populations. We report the mean value (M), the standard deviation (SD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations No augmentation (MD=69.880+-1.033, MAD=1.205, MR=5.433), Color jitter (MD=69.880+-1.756, MAD=3.012, MR=4.883), Horizontal flip (MD=71.687+-1.281, MAD=1.807, MR=3.883), Change RTT (MD=71.084+-1.490, MAD=1.205, MR=3.867), Packet Loss (MD=72.289+-1.113, MAD=2.410, MR=3.633), Rotate (MD=72.289+-1.406, MAD=2.410, MR=3.467), and Time shift (MD=72.289+-1.179, MAD=1.205, MR=2.833). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: No augmentation, Color jitter, Horizontal flip, and Change RTT; Color jitter, Horizontal flip, Change RTT, Packet Loss, and Rotate; Horizontal flip, Change RTT, Packet Loss, Rotate, and Time shift. All other differences are significant.\n</pre> In\u00a0[23]: Copied! <pre>autorank.create_report(res_script)\n</pre> autorank.create_report(res_script) <pre>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe rejected the null hypothesis that the population is normal for the populations Horizontal flip (p=0.007), Rotate (p=0.006), Packet Loss (p=0.005), Change RTT (p=0.004), and Color jitter (p=0.000). Therefore, we assume that not all populations are normal.\nBecause we have more than two populations and the populations and some of them are not normal, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the median values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the median (MD), the median absolute deviation (MAD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Horizontal flip (MD=95.333+-0.667, MAD=0.667, MR=6.250), No augmentation (MD=96.000+-0.333, MAD=0.000, MR=5.800), Rotate (MD=96.667+-0.667, MAD=0.667, MR=4.217), Time shift (MD=97.000+-1.000, MAD=0.333, MR=3.400), Packet Loss (MD=97.333+-1.000, MAD=0.667, MR=3.200), Change RTT (MD=97.333+-0.667, MAD=0.667, MR=3.033), and Color jitter (MD=97.333+-1.000, MAD=0.667, MR=2.100). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: Horizontal flip and No augmentation; No augmentation and Rotate; Rotate, Time shift, Packet Loss, and Change RTT; Time shift, Packet Loss, Change RTT, and Color jitter. All other differences are significant.\n</pre>"},{"location":"paper_tables_and_figures/figure3_ucdavis_augmentations_comparison/#figure-3-critical-distance-plot-of-the-accuracy-obtained-with-each-augmentation-for-the-32x32-and-64x64-cases","title":"Figure 3: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.\u00b6","text":""},{"location":"paper_tables_and_figures/figure3_ucdavis_augmentations_comparison/","title":"Figure 3: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import pathlib\nimport autorank\n</code></pre> <pre><code>df = pd.read_parquet(\n\"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet\"\n)\n</code></pre> <pre><code>def prepare_data(dat, test_split):\nres = dat.query(\"test_split_name == @test_split &amp; flowpic_dim != 1500\")\nres = res[[\"aug_name\", \"split_index\", \"flowpic_dim\", \"seed\", \"acc\"]]\nres[\"id\"] = (\n\"split_index\"\n+ res[\"split_index\"].astype(str)\n+ \"_seed\"\n+ res[\"seed\"].astype(str)\n+ \"_flowpicdim\"\n+ res[\"flowpic_dim\"].astype(str)\n)\nres = res[[\"aug_name\", \"id\", \"acc\"]]\nreturn res.sort_values([\"aug_name\", \"id\"])\ndef get_ranks(df, test_split, force_ranks=True):\ndf1 = prepare_data(df, test_split)\ndf1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\ndf1.columns = df1.columns.get_level_values(1)\nnew_df = pd.DataFrame(\n{\n\"changertt\": df1[\"changertt\"].values,\n\"colorjitter\": df1[\"colorjitter\"].values,\n\"horizontalflip\": df1[\"horizontalflip\"].values,\n\"noaug\": df1[\"noaug\"].values,\n\"packetloss\": df1[\"packetloss\"].values,\n\"rotate\": df1[\"rotate\"].values,\n\"timeshift\": df1[\"timeshift\"].values,\n}\n)\nreplacement = {\n\"noaug\": \"No augmentation\",\n\"horizontalflip\": \"Horizontal flip\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n\"colorjitter\": \"Color jitter\",\n\"changertt\": \"Change RTT\",\n\"packetloss\": \"Packet Loss\",\n}\nnew_df = new_df.rename(columns=replacement)\nif force_ranks:\nreturn autorank.autorank(new_df, force_mode=\"nonparametric\")\nelse:\nreturn autorank.autorank(new_df)\n</code></pre> <pre><code>res_script = get_ranks(df, \"test-script\", force_ranks=True)\nres_human = get_ranks(df, \"test-human\", force_ranks=True)\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 4))\nax = axes[0]\nautorank.plot_stats(res_script, ax=ax)\nax.set_title(\"Test-script\")\nax = axes[1]\nautorank.plot_stats(res_human, ax=ax)\nax.set_title(\"Test-human\")\nplt.tight_layout()\n# plt.savefig(\"figures/augmentations_rank_comparison.pdf\", bbox_inches=\"tight\")\n</code></pre> <pre><code>Tests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\nTests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\n</code></pre> <pre><code>autorank.create_report(res_human)\n</code></pre> <pre><code>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.010). Therefore, we assume that all populations are normal.\nWe applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.064) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\nBecause we have more than two populations and all populations are normal and homoscedastic, we should use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. However, the user decided to force the use of the less powerful Friedman test as omnibus test to determine if there are any significant differences between the mean values of the populations. We report the mean value (M), the standard deviation (SD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations No augmentation (MD=69.880+-1.033, MAD=1.205, MR=5.433), Color jitter (MD=69.880+-1.756, MAD=3.012, MR=4.883), Horizontal flip (MD=71.687+-1.281, MAD=1.807, MR=3.883), Change RTT (MD=71.084+-1.490, MAD=1.205, MR=3.867), Packet Loss (MD=72.289+-1.113, MAD=2.410, MR=3.633), Rotate (MD=72.289+-1.406, MAD=2.410, MR=3.467), and Time shift (MD=72.289+-1.179, MAD=1.205, MR=2.833). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: No augmentation, Color jitter, Horizontal flip, and Change RTT; Color jitter, Horizontal flip, Change RTT, Packet Loss, and Rotate; Horizontal flip, Change RTT, Packet Loss, Rotate, and Time shift. All other differences are significant.\n</code></pre> <pre><code>autorank.create_report(res_script)\n</code></pre> <pre><code>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe rejected the null hypothesis that the population is normal for the populations Horizontal flip (p=0.007), Rotate (p=0.006), Packet Loss (p=0.005), Change RTT (p=0.004), and Color jitter (p=0.000). Therefore, we assume that not all populations are normal.\nBecause we have more than two populations and the populations and some of them are not normal, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the median values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the median (MD), the median absolute deviation (MAD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Horizontal flip (MD=95.333+-0.667, MAD=0.667, MR=6.250), No augmentation (MD=96.000+-0.333, MAD=0.000, MR=5.800), Rotate (MD=96.667+-0.667, MAD=0.667, MR=4.217), Time shift (MD=97.000+-1.000, MAD=0.333, MR=3.400), Packet Loss (MD=97.333+-1.000, MAD=0.667, MR=3.200), Change RTT (MD=97.333+-0.667, MAD=0.667, MR=3.033), and Color jitter (MD=97.333+-1.000, MAD=0.667, MR=2.100). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: Horizontal flip and No augmentation; No augmentation and Rotate; Rotate, Time shift, Packet Loss, and Change RTT; Time shift, Packet Loss, Change RTT, and Color jitter. All other differences are significant.\n</code></pre>"},{"location":"paper_tables_and_figures/figure4_augmentations_comparison_across_datasets_critical_distance/","title":"Figure 4 : Critical distance plot of the accuracy obtained withh each augmentationacross the four tested datasets","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[34]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[20]: Copied! <pre>import autorank\nimport pandas as pd\n</pre> import autorank import pandas as pd In\u00a0[21]: Copied! <pre>dat = {\n    \"mirage19\": pd.read_parquet(\n        \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_10\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_1000\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"utmobile19\": pd.read_parquet(\n        \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n}\n</pre> dat = {     \"mirage19\": pd.read_parquet(         \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"mirage22_10\": pd.read_parquet(         \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"mirage22_1000\": pd.read_parquet(         \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"utmobile19\": pd.read_parquet(         \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ), } In\u00a0[22]: Copied! <pre>def prepare_data(df):\n    res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\n    res.loc[:, \"id\"] = (\n        \"split_index\"\n        + res.loc[:, \"split_index\"].astype(str)\n        + \"_seed\"\n        + res.loc[:, \"seed\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"f1\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    rankmat = new_df.rank(axis=\"columns\", ascending=False)\n    return rankmat\n\n\ndef get_pivoted_df(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    return new_df\n</pre> def prepare_data(df):     res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]     res.loc[:, \"id\"] = (         \"split_index\"         + res.loc[:, \"split_index\"].astype(str)         + \"_seed\"         + res.loc[:, \"seed\"].astype(str)     )     res = res[[\"aug_name\", \"id\", \"f1\"]]     return res.sort_values([\"aug_name\", \"id\"])   def get_ranks(df):     df1 = prepare_data(df)     df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)     df1.columns = df1.columns.get_level_values(1)     new_df = pd.DataFrame(         {             \"changertt\": df1[\"changertt\"].values,             \"colorjitter\": df1[\"colorjitter\"].values,             \"horizontalflip\": df1[\"horizontalflip\"].values,             \"noaug\": df1[\"noaug\"].values,             \"packetloss\": df1[\"packetloss\"].values,             \"rotate\": df1[\"rotate\"].values,             \"timeshift\": df1[\"timeshift\"].values,         }     )     replacement = {         \"noaug\": \"No augmentation\",         \"horizontalflip\": \"Horizontal flip\",         \"rotate\": \"Rotate\",         \"timeshift\": \"Time shift\",         \"colorjitter\": \"Color jitter\",         \"changertt\": \"Change RTT\",         \"packetloss\": \"Packet Loss\",     }     new_df = new_df.rename(columns=replacement).dropna()     rankmat = new_df.rank(axis=\"columns\", ascending=False)     return rankmat   def get_pivoted_df(df):     df1 = prepare_data(df)     df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)     df1.columns = df1.columns.get_level_values(1)     new_df = pd.DataFrame(         {             \"changertt\": df1[\"changertt\"].values,             \"colorjitter\": df1[\"colorjitter\"].values,             \"horizontalflip\": df1[\"horizontalflip\"].values,             \"noaug\": df1[\"noaug\"].values,             \"packetloss\": df1[\"packetloss\"].values,             \"rotate\": df1[\"rotate\"].values,             \"timeshift\": df1[\"timeshift\"].values,         }     )     replacement = {         \"noaug\": \"No augmentation\",         \"horizontalflip\": \"Horizontal flip\",         \"rotate\": \"Rotate\",         \"timeshift\": \"Time shift\",         \"colorjitter\": \"Color jitter\",         \"changertt\": \"Change RTT\",         \"packetloss\": \"Packet Loss\",     }     new_df = new_df.rename(columns=replacement).dropna()     return new_df In\u00a0[23]: Copied! <pre>fig, ax = plt.subplots(figsize=(3, 3))\n\nperfs = pd.concat(\n    [\n        get_pivoted_df(dat[\"mirage19\"]),\n        get_pivoted_df(dat[\"mirage22_10\"]),\n        get_pivoted_df(dat[\"mirage22_1000\"]),\n        get_pivoted_df(dat[\"utmobile19\"]),\n    ]\n)\n\nautorank.plot_stats(autorank.autorank(perfs), ax=ax)\n</pre> fig, ax = plt.subplots(figsize=(3, 3))  perfs = pd.concat(     [         get_pivoted_df(dat[\"mirage19\"]),         get_pivoted_df(dat[\"mirage22_10\"]),         get_pivoted_df(dat[\"mirage22_1000\"]),         get_pivoted_df(dat[\"utmobile19\"]),     ] )  autorank.plot_stats(autorank.autorank(perfs), ax=ax) <pre>/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n</pre> Out[23]: <pre>&lt;Axes: &gt;</pre>"},{"location":"paper_tables_and_figures/figure4_augmentations_comparison_across_datasets_critical_distance/#figure-4-critical-distance-plot-of-the-accuracy-obtained-withh-each-augmentationacross-the-four-tested-datasets","title":"Figure 4 : Critical distance plot of the accuracy obtained withh each augmentationacross the four tested datasets\u00b6","text":""},{"location":"paper_tables_and_figures/figure4_augmentations_comparison_across_datasets_critical_distance/","title":"Figure 4 : Critical distance plot of the accuracy obtained withh each augmentationacross the four tested datasets","text":"<pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import autorank\nimport pandas as pd\n</code></pre> <pre><code>dat = {\n\"mirage19\": pd.read_parquet(\n\"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"mirage22_10\": pd.read_parquet(\n\"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"mirage22_1000\": pd.read_parquet(\n\"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"utmobile19\": pd.read_parquet(\n\"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n}\n</code></pre> <pre><code>def prepare_data(df):\nres = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\nres.loc[:, \"id\"] = (\n\"split_index\"\n+ res.loc[:, \"split_index\"].astype(str)\n+ \"_seed\"\n+ res.loc[:, \"seed\"].astype(str)\n)\nres = res[[\"aug_name\", \"id\", \"f1\"]]\nreturn res.sort_values([\"aug_name\", \"id\"])\ndef get_ranks(df):\ndf1 = prepare_data(df)\ndf1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\ndf1.columns = df1.columns.get_level_values(1)\nnew_df = pd.DataFrame(\n{\n\"changertt\": df1[\"changertt\"].values,\n\"colorjitter\": df1[\"colorjitter\"].values,\n\"horizontalflip\": df1[\"horizontalflip\"].values,\n\"noaug\": df1[\"noaug\"].values,\n\"packetloss\": df1[\"packetloss\"].values,\n\"rotate\": df1[\"rotate\"].values,\n\"timeshift\": df1[\"timeshift\"].values,\n}\n)\nreplacement = {\n\"noaug\": \"No augmentation\",\n\"horizontalflip\": \"Horizontal flip\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n\"colorjitter\": \"Color jitter\",\n\"changertt\": \"Change RTT\",\n\"packetloss\": \"Packet Loss\",\n}\nnew_df = new_df.rename(columns=replacement).dropna()\nrankmat = new_df.rank(axis=\"columns\", ascending=False)\nreturn rankmat\ndef get_pivoted_df(df):\ndf1 = prepare_data(df)\ndf1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\ndf1.columns = df1.columns.get_level_values(1)\nnew_df = pd.DataFrame(\n{\n\"changertt\": df1[\"changertt\"].values,\n\"colorjitter\": df1[\"colorjitter\"].values,\n\"horizontalflip\": df1[\"horizontalflip\"].values,\n\"noaug\": df1[\"noaug\"].values,\n\"packetloss\": df1[\"packetloss\"].values,\n\"rotate\": df1[\"rotate\"].values,\n\"timeshift\": df1[\"timeshift\"].values,\n}\n)\nreplacement = {\n\"noaug\": \"No augmentation\",\n\"horizontalflip\": \"Horizontal flip\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n\"colorjitter\": \"Color jitter\",\n\"changertt\": \"Change RTT\",\n\"packetloss\": \"Packet Loss\",\n}\nnew_df = new_df.rename(columns=replacement).dropna()\nreturn new_df\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(3, 3))\nperfs = pd.concat(\n[\nget_pivoted_df(dat[\"mirage19\"]),\nget_pivoted_df(dat[\"mirage22_10\"]),\nget_pivoted_df(dat[\"mirage22_1000\"]),\nget_pivoted_df(dat[\"utmobile19\"]),\n]\n)\nautorank.plot_stats(autorank.autorank(perfs), ax=ax)\n</code></pre> <pre><code>/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n/tmp/ipykernel_53164/244620128.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:,'id'] = 'split_index' + res.loc[:,'split_index'].astype(str) + '_seed' + res.loc[:,'seed'].astype(str)\n\n\n\n\n\n&lt;Axes: &gt;\n</code></pre>"},{"location":"paper_tables_and_figures/figure5_augmentations_comparison_across_datasets_average_rank/","title":"Figure 5: Average rank obtained per augmentation and dataset.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[2]: Copied! <pre>import autorank\nimport pandas as pd\n</pre> import autorank import pandas as pd In\u00a0[3]: Copied! <pre>dat = {\n    \"mirage19\": pd.read_parquet(\n        \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_10\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_1000\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"utmobile19\": pd.read_parquet(\n        \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n    ),\n}\n</pre> dat = {     \"mirage19\": pd.read_parquet(         \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"mirage22_10\": pd.read_parquet(         \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"mirage22_1000\": pd.read_parquet(         \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ),     \"utmobile19\": pd.read_parquet(         \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"     ), } In\u00a0[10]: Copied! <pre>def prepare_data(df):\n    res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\n    res.loc[:, \"id\"] = (\n        \"split_index\"\n        + res.loc[:, \"split_index\"].astype(str)\n        + \"_seed\"\n        + res.loc[:, \"seed\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"f1\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    rankmat = new_df.rank(axis=\"columns\", ascending=False)\n    return rankmat\n</pre> def prepare_data(df):     res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]     res.loc[:, \"id\"] = (         \"split_index\"         + res.loc[:, \"split_index\"].astype(str)         + \"_seed\"         + res.loc[:, \"seed\"].astype(str)     )     res = res[[\"aug_name\", \"id\", \"f1\"]]     return res.sort_values([\"aug_name\", \"id\"])   def get_ranks(df):     df1 = prepare_data(df)     df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)     df1.columns = df1.columns.get_level_values(1)     new_df = pd.DataFrame(         {             \"changertt\": df1[\"changertt\"].values,             \"colorjitter\": df1[\"colorjitter\"].values,             \"horizontalflip\": df1[\"horizontalflip\"].values,             \"noaug\": df1[\"noaug\"].values,             \"packetloss\": df1[\"packetloss\"].values,             \"rotate\": df1[\"rotate\"].values,             \"timeshift\": df1[\"timeshift\"].values,         }     )     replacement = {         \"noaug\": \"No augmentation\",         \"horizontalflip\": \"Horizontal flip\",         \"rotate\": \"Rotate\",         \"timeshift\": \"Time shift\",         \"colorjitter\": \"Color jitter\",         \"changertt\": \"Change RTT\",         \"packetloss\": \"Packet Loss\",     }     new_df = new_df.rename(columns=replacement).dropna()     rankmat = new_df.rank(axis=\"columns\", ascending=False)     return rankmat In\u00a0[12]: Copied! <pre>def prepare_ranks_data(dataset):\n    res = get_ranks(dat[dataset])\n    res[\"dataset\"] = dataset\n    return res\n\n\ntogether = pd.concat(\n    [\n        prepare_ranks_data(\"mirage19\"),\n        prepare_ranks_data(\"mirage22_10\"),\n        prepare_ranks_data(\"mirage22_1000\"),\n        prepare_ranks_data(\"utmobile19\"),\n    ]\n)\n\ndf_tmp = pd.melt(\n    together, id_vars=[\"dataset\"], var_name=\"augmentation\", value_name=\"rank\"\n)\n</pre> def prepare_ranks_data(dataset):     res = get_ranks(dat[dataset])     res[\"dataset\"] = dataset     return res   together = pd.concat(     [         prepare_ranks_data(\"mirage19\"),         prepare_ranks_data(\"mirage22_10\"),         prepare_ranks_data(\"mirage22_1000\"),         prepare_ranks_data(\"utmobile19\"),     ] )  df_tmp = pd.melt(     together, id_vars=[\"dataset\"], var_name=\"augmentation\", value_name=\"rank\" ) <pre>/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n</pre> In\u00a0[13]: Copied! <pre>df_tmp2 = df_tmp.groupby([\"dataset\", \"augmentation\"])[\"rank\"].mean().reset_index()\n</pre> df_tmp2 = df_tmp.groupby([\"dataset\", \"augmentation\"])[\"rank\"].mean().reset_index() In\u00a0[14]: Copied! <pre>df_tmp2.head()\n</pre> df_tmp2.head() Out[14]: dataset augmentation rank 0 mirage19 Change RTT 1.0 1 mirage19 Color jitter 5.2 2 mirage19 Horizontal flip 3.0 3 mirage19 No augmentation 4.0 4 mirage19 Packet Loss 5.3 In\u00a0[15]: Copied! <pre>sns.heatmap(\n    data=df_tmp2.pivot(index=\"augmentation\", columns=\"dataset\", values=\"rank\"),\n    annot=True,\n    fmt=\".1f\",\n)\n</pre> sns.heatmap(     data=df_tmp2.pivot(index=\"augmentation\", columns=\"dataset\", values=\"rank\"),     annot=True,     fmt=\".1f\", ) Out[15]: <pre>&lt;Axes: xlabel='dataset', ylabel='augmentation'&gt;</pre>"},{"location":"paper_tables_and_figures/figure5_augmentations_comparison_across_datasets_average_rank/#figure-5-average-rank-obtained-per-augmentation-and-dataset","title":"Figure 5: Average rank obtained per augmentation and dataset.\u00b6","text":""},{"location":"paper_tables_and_figures/figure5_augmentations_comparison_across_datasets_average_rank/","title":"Figure 5: Average rank obtained per augmentation and dataset.","text":"<pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import autorank\nimport pandas as pd\n</code></pre> <pre><code>dat = {\n\"mirage19\": pd.read_parquet(\n\"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"mirage22_10\": pd.read_parquet(\n\"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"mirage22_1000\": pd.read_parquet(\n\"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n\"utmobile19\": pd.read_parquet(\n\"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/runsinfo_flowpic_dim_32.parquet\"\n),\n}\n</code></pre> <pre><code>def prepare_data(df):\nres = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\nres.loc[:, \"id\"] = (\n\"split_index\"\n+ res.loc[:, \"split_index\"].astype(str)\n+ \"_seed\"\n+ res.loc[:, \"seed\"].astype(str)\n)\nres = res[[\"aug_name\", \"id\", \"f1\"]]\nreturn res.sort_values([\"aug_name\", \"id\"])\ndef get_ranks(df):\ndf1 = prepare_data(df)\ndf1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\ndf1.columns = df1.columns.get_level_values(1)\nnew_df = pd.DataFrame(\n{\n\"changertt\": df1[\"changertt\"].values,\n\"colorjitter\": df1[\"colorjitter\"].values,\n\"horizontalflip\": df1[\"horizontalflip\"].values,\n\"noaug\": df1[\"noaug\"].values,\n\"packetloss\": df1[\"packetloss\"].values,\n\"rotate\": df1[\"rotate\"].values,\n\"timeshift\": df1[\"timeshift\"].values,\n}\n)\nreplacement = {\n\"noaug\": \"No augmentation\",\n\"horizontalflip\": \"Horizontal flip\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n\"colorjitter\": \"Color jitter\",\n\"changertt\": \"Change RTT\",\n\"packetloss\": \"Packet Loss\",\n}\nnew_df = new_df.rename(columns=replacement).dropna()\nrankmat = new_df.rank(axis=\"columns\", ascending=False)\nreturn rankmat\n</code></pre> <pre><code>def prepare_ranks_data(dataset):\nres = get_ranks(dat[dataset])\nres[\"dataset\"] = dataset\nreturn res\ntogether = pd.concat(\n[\nprepare_ranks_data(\"mirage19\"),\nprepare_ranks_data(\"mirage22_10\"),\nprepare_ranks_data(\"mirage22_1000\"),\nprepare_ranks_data(\"utmobile19\"),\n]\n)\ndf_tmp = pd.melt(\ntogether, id_vars=[\"dataset\"], var_name=\"augmentation\", value_name=\"rank\"\n)\n</code></pre> <pre><code>/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_54500/2978918141.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n</code></pre> <pre><code>df_tmp2 = df_tmp.groupby([\"dataset\", \"augmentation\"])[\"rank\"].mean().reset_index()\n</code></pre> <pre><code>df_tmp2.head()\n</code></pre> dataset augmentation rank 0 mirage19 Change RTT 1.0 1 mirage19 Color jitter 5.2 2 mirage19 Horizontal flip 3.0 3 mirage19 No augmentation 4.0 4 mirage19 Packet Loss 5.3 <pre><code>sns.heatmap(\ndata=df_tmp2.pivot(index=\"augmentation\", columns=\"dataset\", values=\"rank\"),\nannot=True,\nfmt=\".1f\",\n)\n</code></pre> <pre><code>&lt;Axes: xlabel='dataset', ylabel='augmentation'&gt;\n</code></pre>"},{"location":"paper_tables_and_figures/figure6_ucdavis_kde_on_pkts_size/","title":"Figure 6: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import itertools\n\nimport numpy as np\nimport pandas as pd\n</pre> import itertools  import numpy as np import pandas as pd In\u00a0[2]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import LogNorm, Normalize  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[3]: Copied! <pre>import tcbench as tcb\nfrom tcbench import dataprep\n</pre> import tcbench as tcb from tcbench import dataprep In\u00a0[4]: Copied! <pre>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</pre> FLOWPIC_DIM = 32 FLOWPIC_BLOCK_DURATION = 15 In\u00a0[5]: Copied! <pre># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</pre> # load unfiltered dataset dset = dataprep.FlowpicDataset(     data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),     timetofirst_colname=\"timetofirst\",     pkts_size_colname=\"pkts_size\",     pkts_dir_colname=\"pkts_dir\",     target_colname=\"app\",     flowpic_dim=FLOWPIC_DIM,     flowpic_block_duration=FLOWPIC_BLOCK_DURATION, ) In\u00a0[8]: Copied! <pre>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\n\nTARGETS_LABEL, PARTITIONS_NAME\n</pre> TARGETS_LABEL = sorted(dset.df[\"app\"].unique()) PARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())  TARGETS_LABEL, PARTITIONS_NAME Out[8]: <pre>(['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'],\n ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'])</pre> In\u00a0[12]: Copied! <pre>all_pkts_size = dict()\n\nfor partition_name in PARTITIONS_NAME:\n    all_pkts_size[partition_name] = dict()\n\n    for app in TARGETS_LABEL:\n        df_tmp = dset.df[\n            (dset.df[\"partition\"] == partition_name) &amp; (dset.df[\"app\"] == app)\n        ]\n\n        l = []\n        for idx in df_tmp.index:\n            ser = df_tmp.loc[idx]\n            indexes = np.where(ser[\"timetofirst\"] &lt; FLOWPIC_BLOCK_DURATION)[0]\n            pkts_size = ser[\"pkts_size\"][indexes]\n            l.append(pkts_size)\n        all_pkts_size[partition_name][app] = np.concatenate(l)\n</pre> all_pkts_size = dict()  for partition_name in PARTITIONS_NAME:     all_pkts_size[partition_name] = dict()      for app in TARGETS_LABEL:         df_tmp = dset.df[             (dset.df[\"partition\"] == partition_name) &amp; (dset.df[\"app\"] == app)         ]          l = []         for idx in df_tmp.index:             ser = df_tmp.loc[idx]             indexes = np.where(ser[\"timetofirst\"] &lt; FLOWPIC_BLOCK_DURATION)[0]             pkts_size = ser[\"pkts_size\"][indexes]             l.append(pkts_size)         all_pkts_size[partition_name][app] = np.concatenate(l) In\u00a0[13]: Copied! <pre># WARNING: computing the KDE will take a few minutes\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\n\nline_props = {\n    \"pretraining\": dict(linestyle=\"-\"),\n    \"retraining-script-triggered\": dict(\n        linestyle=(0, (1, 1))\n    ), \n    \"retraining-human-triggered\": dict(linestyle=(0, (1, 1))),\n}\n\nfor ax, app in zip(axes, TARGETS_LABEL):\n    for partition_name in [\n        \"pretraining\",\n        \"retraining-script-triggered\",\n        \"retraining-human-triggered\",\n    ]:\n        props = line_props[partition_name]\n        sns.kdeplot(\n            ax=ax,\n            data=all_pkts_size[partition_name][app],\n            linewidth=2,\n            label=partition_name,\n            **props,\n            fill=True,\n            alpha=0.1\n        )\n    ax.legend(bbox_to_anchor=(0.5, 1.5), loc=\"upper center\")\n    ax.set_title(app, fontsize=10)\n    ax.set_xlim((-500, 1800))\n    ax.set_xlabel(\"packet size\")\n    ax.set_ylabel(\"kde\")\n\nplt.tight_layout()\n</pre> # WARNING: computing the KDE will take a few minutes  fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))  line_props = {     \"pretraining\": dict(linestyle=\"-\"),     \"retraining-script-triggered\": dict(         linestyle=(0, (1, 1))     ),      \"retraining-human-triggered\": dict(linestyle=(0, (1, 1))), }  for ax, app in zip(axes, TARGETS_LABEL):     for partition_name in [         \"pretraining\",         \"retraining-script-triggered\",         \"retraining-human-triggered\",     ]:         props = line_props[partition_name]         sns.kdeplot(             ax=ax,             data=all_pkts_size[partition_name][app],             linewidth=2,             label=partition_name,             **props,             fill=True,             alpha=0.1         )     ax.legend(bbox_to_anchor=(0.5, 1.5), loc=\"upper center\")     ax.set_title(app, fontsize=10)     ax.set_xlim((-500, 1800))     ax.set_xlabel(\"packet size\")     ax.set_ylabel(\"kde\")  plt.tight_layout()"},{"location":"paper_tables_and_figures/figure6_ucdavis_kde_on_pkts_size/#figure-6-investigating-root-cause-of-g1-discrepancies-kernel-density-estimation-of-the-per-class-packet-size-distributions","title":"Figure 6: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.\u00b6","text":""},{"location":"paper_tables_and_figures/figure6_ucdavis_kde_on_pkts_size/","title":"Figure 6: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.","text":"<pre><code>import itertools\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import tcbench as tcb\nfrom tcbench import dataprep\n</code></pre> <pre><code>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</code></pre> <pre><code># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\ndata=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\ntimetofirst_colname=\"timetofirst\",\npkts_size_colname=\"pkts_size\",\npkts_dir_colname=\"pkts_dir\",\ntarget_colname=\"app\",\nflowpic_dim=FLOWPIC_DIM,\nflowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\nTARGETS_LABEL, PARTITIONS_NAME\n</code></pre> <pre><code>(['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'],\n ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'])\n</code></pre> <pre><code>all_pkts_size = dict()\nfor partition_name in PARTITIONS_NAME:\nall_pkts_size[partition_name] = dict()\nfor app in TARGETS_LABEL:\ndf_tmp = dset.df[\n(dset.df[\"partition\"] == partition_name) &amp; (dset.df[\"app\"] == app)\n]\nl = []\nfor idx in df_tmp.index:\nser = df_tmp.loc[idx]\nindexes = np.where(ser[\"timetofirst\"] &lt; FLOWPIC_BLOCK_DURATION)[0]\npkts_size = ser[\"pkts_size\"][indexes]\nl.append(pkts_size)\nall_pkts_size[partition_name][app] = np.concatenate(l)\n</code></pre> <pre><code># WARNING: computing the KDE will take a few minutes\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\nline_props = {\n\"pretraining\": dict(linestyle=\"-\"),\n\"retraining-script-triggered\": dict(\nlinestyle=(0, (1, 1))\n), \n\"retraining-human-triggered\": dict(linestyle=(0, (1, 1))),\n}\nfor ax, app in zip(axes, TARGETS_LABEL):\nfor partition_name in [\n\"pretraining\",\n\"retraining-script-triggered\",\n\"retraining-human-triggered\",\n]:\nprops = line_props[partition_name]\nsns.kdeplot(\nax=ax,\ndata=all_pkts_size[partition_name][app],\nlinewidth=2,\nlabel=partition_name,\n**props,\nfill=True,\nalpha=0.1\n)\nax.legend(bbox_to_anchor=(0.5, 1.5), loc=\"upper center\")\nax.set_title(app, fontsize=10)\nax.set_xlim((-500, 1800))\nax.set_xlabel(\"packet size\")\nax.set_ylabel(\"kde\")\nplt.tight_layout()\n</code></pre>"},{"location":"paper_tables_and_figures/figure8b_icdm_finetuning_per_class_metrics_on_human/","title":"Figure 8(b): Classwise evaluation on human.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import pathlib  import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import statsmodels.stats.api as sms  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[2]: Copied! <pre>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</pre> def compute_confidence_intervals(array, alpha=0.05):     array = np.array(array)     low, high = sms.DescrStatsW(array).tconfint_mean(alpha)     mean = array.mean()     ci = high - mean     return ci In\u00a0[3]: Copied! <pre>path = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/IncrementalSampling_Retraining(human-triggered)_20/\"\n)\n\nclass_reps = list(path.glob(\"*class_rep.csv\"))\n\nper_cls = np.stack(\n    [\n        pd.read_csv(file)[:5][[\"Accuracy\", \"precision\", \"recall\", \"f1-score\"]].values\n        for file in class_reps\n    ],\n    axis=0,\n)\n\n\nmeans = np.mean(per_cls, axis=0)\n\ncis = np.zeros([per_cls.shape[1], per_cls.shape[2]])\nfor i in range(per_cls.shape[1]):\n    for j in range(per_cls.shape[2]):\n        cis[i, j] = compute_confidence_intervals(per_cls[:, i, j])\n</pre> path = pathlib.Path(     \"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/IncrementalSampling_Retraining(human-triggered)_20/\" )  class_reps = list(path.glob(\"*class_rep.csv\"))  per_cls = np.stack(     [         pd.read_csv(file)[:5][[\"Accuracy\", \"precision\", \"recall\", \"f1-score\"]].values         for file in class_reps     ],     axis=0, )   means = np.mean(per_cls, axis=0)  cis = np.zeros([per_cls.shape[1], per_cls.shape[2]]) for i in range(per_cls.shape[1]):     for j in range(per_cls.shape[2]):         cis[i, j] = compute_confidence_intervals(per_cls[:, i, j]) In\u00a0[5]: Copied! <pre>X = [\"G Drive\", \"Youtube\", \"G Doc\", \"G Search\", \"G Music\"]\nX_axis = np.arange(len(X))\n\n\nfig, ax = plt.subplots(figsize=(7, 6.5))\nax.bar(\n    X_axis - 0.3,\n    means[:, 0],\n    0.2,\n    label=\"Accuracy\",\n    yerr=cis[:, 0],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis - 0.1,\n    means[:, 1],\n    0.2,\n    label=\"Precision\",\n    yerr=cis[:, 1],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis + 0.1,\n    means[:, 2],\n    0.2,\n    label=\"Recall\",\n    yerr=cis[:, 2],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis + 0.3,\n    means[:, 3],\n    0.2,\n    label=\"F1\",\n    yerr=cis[:, 3],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\n\n\nplt.xticks(X_axis, X)\nax.set_xlabel(\"Class\")\nax.set_ylabel(\"Value\")\nax.set_ylim([0, 1])\nplt.legend()\nax.legend(bbox_to_anchor=(1, 1.02))\nplt.grid(axis=\"y\")\nplt.show()\n</pre> X = [\"G Drive\", \"Youtube\", \"G Doc\", \"G Search\", \"G Music\"] X_axis = np.arange(len(X))   fig, ax = plt.subplots(figsize=(7, 6.5)) ax.bar(     X_axis - 0.3,     means[:, 0],     0.2,     label=\"Accuracy\",     yerr=cis[:, 0],     ecolor=\"black\",     alpha=0.5,     capsize=10, ) ax.bar(     X_axis - 0.1,     means[:, 1],     0.2,     label=\"Precision\",     yerr=cis[:, 1],     ecolor=\"black\",     alpha=0.5,     capsize=10, ) ax.bar(     X_axis + 0.1,     means[:, 2],     0.2,     label=\"Recall\",     yerr=cis[:, 2],     ecolor=\"black\",     alpha=0.5,     capsize=10, ) ax.bar(     X_axis + 0.3,     means[:, 3],     0.2,     label=\"F1\",     yerr=cis[:, 3],     ecolor=\"black\",     alpha=0.5,     capsize=10, )   plt.xticks(X_axis, X) ax.set_xlabel(\"Class\") ax.set_ylabel(\"Value\") ax.set_ylim([0, 1]) plt.legend() ax.legend(bbox_to_anchor=(1, 1.02)) plt.grid(axis=\"y\") plt.show()"},{"location":"paper_tables_and_figures/figure8b_icdm_finetuning_per_class_metrics_on_human/#figure-8b-classwise-evaluation-on-human","title":"Figure 8(b): Classwise evaluation on human.\u00b6","text":""},{"location":"paper_tables_and_figures/figure8b_icdm_finetuning_per_class_metrics_on_human/","title":"Figure 8(b): Classwise evaluation on human.","text":"<pre><code>import pathlib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\narray = np.array(array)\nlow, high = sms.DescrStatsW(array).tconfint_mean(alpha)\nmean = array.mean()\nci = high - mean\nreturn ci\n</code></pre> <pre><code>path = pathlib.Path(\n\"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/IncrementalSampling_Retraining(human-triggered)_20/\"\n)\nclass_reps = list(path.glob(\"*class_rep.csv\"))\nper_cls = np.stack(\n[\npd.read_csv(file)[:5][[\"Accuracy\", \"precision\", \"recall\", \"f1-score\"]].values\nfor file in class_reps\n],\naxis=0,\n)\nmeans = np.mean(per_cls, axis=0)\ncis = np.zeros([per_cls.shape[1], per_cls.shape[2]])\nfor i in range(per_cls.shape[1]):\nfor j in range(per_cls.shape[2]):\ncis[i, j] = compute_confidence_intervals(per_cls[:, i, j])\n</code></pre> <pre><code>X = [\"G Drive\", \"Youtube\", \"G Doc\", \"G Search\", \"G Music\"]\nX_axis = np.arange(len(X))\nfig, ax = plt.subplots(figsize=(7, 6.5))\nax.bar(\nX_axis - 0.3,\nmeans[:, 0],\n0.2,\nlabel=\"Accuracy\",\nyerr=cis[:, 0],\necolor=\"black\",\nalpha=0.5,\ncapsize=10,\n)\nax.bar(\nX_axis - 0.1,\nmeans[:, 1],\n0.2,\nlabel=\"Precision\",\nyerr=cis[:, 1],\necolor=\"black\",\nalpha=0.5,\ncapsize=10,\n)\nax.bar(\nX_axis + 0.1,\nmeans[:, 2],\n0.2,\nlabel=\"Recall\",\nyerr=cis[:, 2],\necolor=\"black\",\nalpha=0.5,\ncapsize=10,\n)\nax.bar(\nX_axis + 0.3,\nmeans[:, 3],\n0.2,\nlabel=\"F1\",\nyerr=cis[:, 3],\necolor=\"black\",\nalpha=0.5,\ncapsize=10,\n)\nplt.xticks(X_axis, X)\nax.set_xlabel(\"Class\")\nax.set_ylabel(\"Value\")\nax.set_ylim([0, 1])\nplt.legend()\nax.legend(bbox_to_anchor=(1, 1.02))\nplt.grid(axis=\"y\")\nplt.show()\n</code></pre>"},{"location":"paper_tables_and_figures/figure9_dropout_impact_supervised_setting/","title":"Figure 9: Accuracy difference w/ and w/o Dropout in supervised learning.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import itertools\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import itertools  import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import statsmodels.stats.api as sms  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[2]: Copied! <pre>df_with_dropout = pd.concat(\n    [\n        pd.read_parquet(\n            \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\"\n        ),\n        pd.read_parquet(\n            \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_32.parquet\"\n        ),\n    ]\n)\n</pre> df_with_dropout = pd.concat(     [         pd.read_parquet(             \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\"         ),         pd.read_parquet(             \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_32.parquet\"         ),     ] ) In\u00a0[5]: Copied! <pre>df_no_dropout = pd.concat(\n    [\n        pd.read_parquet(\n            \"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_1500.parquet\"\n        ),\n        pd.read_parquet(\n            \"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_32.parquet\"\n        ),\n    ]\n)\n</pre> df_no_dropout = pd.concat(     [         pd.read_parquet(             \"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_1500.parquet\"         ),         pd.read_parquet(             \"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_32.parquet\"         ),     ] ) In\u00a0[7]: Copied! <pre>df_tmp1 = df_with_dropout[\n    [\n        \"flowpic_dim\",\n        \"test_split_name\",\n        \"aug_name\",\n        \"seed\",\n        \"split_index\",\n        \"acc\",\n    ]\n].rename(columns={\"acc\": \"withdropout_acc\"})\ndf_tmp2 = df_no_dropout[\n    [\n        \"flowpic_dim\",\n        \"test_split_name\",\n        \"aug_name\",\n        \"seed\",\n        \"split_index\",\n        \"acc\",\n    ]\n].rename(columns={\"acc\": \"nodropout_acc\"})\ndf = pd.merge(\n    df_tmp1,\n    df_tmp2,\n    on=[\n        \"flowpic_dim\",\n        \"test_split_name\",\n        \"aug_name\",\n        \"seed\",\n        \"split_index\",\n    ],\n    suffixes=[\"withdropout_\", \"nodropout_\"],\n)\n</pre> df_tmp1 = df_with_dropout[     [         \"flowpic_dim\",         \"test_split_name\",         \"aug_name\",         \"seed\",         \"split_index\",         \"acc\",     ] ].rename(columns={\"acc\": \"withdropout_acc\"}) df_tmp2 = df_no_dropout[     [         \"flowpic_dim\",         \"test_split_name\",         \"aug_name\",         \"seed\",         \"split_index\",         \"acc\",     ] ].rename(columns={\"acc\": \"nodropout_acc\"}) df = pd.merge(     df_tmp1,     df_tmp2,     on=[         \"flowpic_dim\",         \"test_split_name\",         \"aug_name\",         \"seed\",         \"split_index\",     ],     suffixes=[\"withdropout_\", \"nodropout_\"], ) In\u00a0[8]: Copied! <pre>df = df.iloc[df[\"nodropout_acc\"].dropna().index]\n</pre> df = df.iloc[df[\"nodropout_acc\"].dropna().index] In\u00a0[9]: Copied! <pre>df[\"acc_diff\"] = df[\"withdropout_acc\"] - df[\"nodropout_acc\"]\n</pre> df[\"acc_diff\"] = df[\"withdropout_acc\"] - df[\"nodropout_acc\"] In\u00a0[10]: Copied! <pre>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</pre> def compute_confidence_intervals(array, alpha=0.05):     array = np.array(array)     low, high = sms.DescrStatsW(array).tconfint_mean(alpha)     mean = array.mean()     ci = high - mean     return ci In\u00a0[11]: Copied! <pre>df_merged = df.groupby([\"flowpic_dim\", \"test_split_name\", \"aug_name\"]).agg(\n    {\"acc_diff\": [\"mean\", \"std\", \"count\", \"min\", \"max\", compute_confidence_intervals]}\n)\ndf_merged = df_merged.rename(\n    columns={\"compute_confidence_intervals\": \"confidence_interval\"}\n)\ndf_merged = df_merged.droplevel(0, axis=1)\n</pre> df_merged = df.groupby([\"flowpic_dim\", \"test_split_name\", \"aug_name\"]).agg(     {\"acc_diff\": [\"mean\", \"std\", \"count\", \"min\", \"max\", compute_confidence_intervals]} ) df_merged = df_merged.rename(     columns={\"compute_confidence_intervals\": \"confidence_interval\"} ) df_merged = df_merged.droplevel(0, axis=1) In\u00a0[12]: Copied! <pre>df_merged\n</pre> df_merged Out[12]: mean std count min max confidence_interval flowpic_dim test_split_name aug_name 32 test-human changertt 8.835341e-01 3.421913 15 -6.024096 6.024096 1.894992 colorjitter 1.285141e+00 5.750530 15 -8.433735 10.843373 3.184537 horizontalflip -9.473903e-16 2.694058 15 -4.819277 6.024096 1.491919 noaug -1.204819e+00 3.020642 15 -4.819277 3.614458 1.672776 packetloss -8.032129e-01 2.215345 15 -3.614458 3.614458 1.226817 rotate -1.285141e+00 4.998116 15 -12.048193 6.024096 2.767864 timeshift -1.606426e-01 3.806343 15 -4.819277 6.024096 2.107882 test-script changertt 3.555556e-01 0.791489 15 -0.666667 2.000000 0.438312 colorjitter 5.777778e-01 1.256517 15 -2.000000 3.333333 0.695836 horizontalflip -1.333333e-01 1.104105 15 -2.666667 1.333333 0.611433 noaug -6.222222e-01 0.990964 15 -2.000000 0.666667 0.548778 packetloss 2.666667e-01 1.176489 15 -2.000000 2.666667 0.651518 rotate -1.777778e-01 0.924676 15 -1.333333 1.333333 0.512069 timeshift 4.444444e-02 1.053088 15 -2.000000 2.000000 0.583181 1500 test-human changertt 7.228916e-01 2.762465 15 -4.819277 4.819277 1.529802 colorjitter 1.847390e+00 2.446646 15 -1.204819 6.024096 1.354908 horizontalflip 5.622490e-01 1.633450 15 -2.409639 3.614458 0.904575 noaug -1.204819e+00 3.727437 15 -6.024096 7.228916 2.064186 packetloss 1.847390e+00 3.779005 15 -3.614458 9.638554 2.092743 rotate -5.622490e-01 2.488664 15 -3.614458 4.819277 1.378176 timeshift -1.124498e+00 1.903171 15 -3.614458 2.409639 1.053941 test-script changertt 4.444444e-02 1.167460 15 -1.333333 2.666667 0.646518 colorjitter 8.000000e-01 2.645151 15 -3.333333 6.666667 1.464836 horizontalflip 7.555556e-01 1.668887 15 -1.333333 4.000000 0.924199 noaug -1.777778e-01 1.521625 15 -2.000000 3.333333 0.842648 packetloss 7.555556e-01 2.150920 15 -2.000000 4.000000 1.191140 rotate -8.888889e-02 0.903842 15 -2.000000 1.333333 0.500531 timeshift 0.000000e+00 0.666667 15 -0.666667 1.333333 0.369188 In\u00a0[13]: Copied! <pre>fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))\n\nfor ax, (flowpic_dim, test_split_name) in zip(\n    axes.flatten(), itertools.product((32, 1500), (\"test-human\", \"test-script\"))\n):\n    # df_merged.loc[(flowpic_dim, test_split_name)]['mean'].plot(kind='bar', ax=ax)\n\n    ax.bar(\n        list(df_merged.loc[(flowpic_dim, test_split_name)].index),\n        df_merged.loc[(flowpic_dim, test_split_name)][\"mean\"],\n        yerr=df_merged.loc[(flowpic_dim, test_split_name)][\"confidence_interval\"],\n        align=\"center\",\n        alpha=0.5,\n        ecolor=\"black\",\n        capsize=10,\n    )\n\n    ax.set_title(f\"{test_split_name} @ {flowpic_dim}x{flowpic_dim}\")\n\n    ax.set_xticklabels(\n        list(df_merged.loc[(flowpic_dim, test_split_name)].index), rotation=90\n    )\n\n\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))  for ax, (flowpic_dim, test_split_name) in zip(     axes.flatten(), itertools.product((32, 1500), (\"test-human\", \"test-script\")) ):     # df_merged.loc[(flowpic_dim, test_split_name)]['mean'].plot(kind='bar', ax=ax)      ax.bar(         list(df_merged.loc[(flowpic_dim, test_split_name)].index),         df_merged.loc[(flowpic_dim, test_split_name)][\"mean\"],         yerr=df_merged.loc[(flowpic_dim, test_split_name)][\"confidence_interval\"],         align=\"center\",         alpha=0.5,         ecolor=\"black\",         capsize=10,     )      ax.set_title(f\"{test_split_name} @ {flowpic_dim}x{flowpic_dim}\")      ax.set_xticklabels(         list(df_merged.loc[(flowpic_dim, test_split_name)].index), rotation=90     )   plt.tight_layout() <pre>/tmp/ipykernel_62018/2491681095.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(list(df_merged.loc[(flowpic_dim, test_split_name)].index), rotation=90)\n</pre>"},{"location":"paper_tables_and_figures/figure9_dropout_impact_supervised_setting/#figure-9-accuracy-difference-w-and-wo-dropout-in-supervised-learning","title":"Figure 9: Accuracy difference w/ and w/o Dropout in supervised learning.\u00b6","text":""},{"location":"paper_tables_and_figures/figure9_dropout_impact_supervised_setting/","title":"Figure 9: Accuracy difference w/ and w/o Dropout in supervised learning.","text":"<pre><code>import itertools\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>df_with_dropout = pd.concat(\n[\npd.read_parquet(\n\"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\"\n),\npd.read_parquet(\n\"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/runsinfo_flowpic_dim_32.parquet\"\n),\n]\n)\n</code></pre> <pre><code>df_no_dropout = pd.concat(\n[\npd.read_parquet(\n\"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_1500.parquet\"\n),\npd.read_parquet(\n\"campaigns/ucdavis-icdm19/augmentation-at-loading-suppress-dropout/campaign_summary/1684566558/runsinfo_flowpic_dim_32.parquet\"\n),\n]\n)\n</code></pre> <pre><code>df_tmp1 = df_with_dropout[\n[\n\"flowpic_dim\",\n\"test_split_name\",\n\"aug_name\",\n\"seed\",\n\"split_index\",\n\"acc\",\n]\n].rename(columns={\"acc\": \"withdropout_acc\"})\ndf_tmp2 = df_no_dropout[\n[\n\"flowpic_dim\",\n\"test_split_name\",\n\"aug_name\",\n\"seed\",\n\"split_index\",\n\"acc\",\n]\n].rename(columns={\"acc\": \"nodropout_acc\"})\ndf = pd.merge(\ndf_tmp1,\ndf_tmp2,\non=[\n\"flowpic_dim\",\n\"test_split_name\",\n\"aug_name\",\n\"seed\",\n\"split_index\",\n],\nsuffixes=[\"withdropout_\", \"nodropout_\"],\n)\n</code></pre> <pre><code>df = df.iloc[df[\"nodropout_acc\"].dropna().index]\n</code></pre> <pre><code>df[\"acc_diff\"] = df[\"withdropout_acc\"] - df[\"nodropout_acc\"]\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\narray = np.array(array)\nlow, high = sms.DescrStatsW(array).tconfint_mean(alpha)\nmean = array.mean()\nci = high - mean\nreturn ci\n</code></pre> <pre><code>df_merged = df.groupby([\"flowpic_dim\", \"test_split_name\", \"aug_name\"]).agg(\n{\"acc_diff\": [\"mean\", \"std\", \"count\", \"min\", \"max\", compute_confidence_intervals]}\n)\ndf_merged = df_merged.rename(\ncolumns={\"compute_confidence_intervals\": \"confidence_interval\"}\n)\ndf_merged = df_merged.droplevel(0, axis=1)\n</code></pre> <pre><code>df_merged\n</code></pre> mean std count min max confidence_interval flowpic_dim test_split_name aug_name 32 test-human changertt 8.835341e-01 3.421913 15 -6.024096 6.024096 1.894992 colorjitter 1.285141e+00 5.750530 15 -8.433735 10.843373 3.184537 horizontalflip -9.473903e-16 2.694058 15 -4.819277 6.024096 1.491919 noaug -1.204819e+00 3.020642 15 -4.819277 3.614458 1.672776 packetloss -8.032129e-01 2.215345 15 -3.614458 3.614458 1.226817 rotate -1.285141e+00 4.998116 15 -12.048193 6.024096 2.767864 timeshift -1.606426e-01 3.806343 15 -4.819277 6.024096 2.107882 test-script changertt 3.555556e-01 0.791489 15 -0.666667 2.000000 0.438312 colorjitter 5.777778e-01 1.256517 15 -2.000000 3.333333 0.695836 horizontalflip -1.333333e-01 1.104105 15 -2.666667 1.333333 0.611433 noaug -6.222222e-01 0.990964 15 -2.000000 0.666667 0.548778 packetloss 2.666667e-01 1.176489 15 -2.000000 2.666667 0.651518 rotate -1.777778e-01 0.924676 15 -1.333333 1.333333 0.512069 timeshift 4.444444e-02 1.053088 15 -2.000000 2.000000 0.583181 1500 test-human changertt 7.228916e-01 2.762465 15 -4.819277 4.819277 1.529802 colorjitter 1.847390e+00 2.446646 15 -1.204819 6.024096 1.354908 horizontalflip 5.622490e-01 1.633450 15 -2.409639 3.614458 0.904575 noaug -1.204819e+00 3.727437 15 -6.024096 7.228916 2.064186 packetloss 1.847390e+00 3.779005 15 -3.614458 9.638554 2.092743 rotate -5.622490e-01 2.488664 15 -3.614458 4.819277 1.378176 timeshift -1.124498e+00 1.903171 15 -3.614458 2.409639 1.053941 test-script changertt 4.444444e-02 1.167460 15 -1.333333 2.666667 0.646518 colorjitter 8.000000e-01 2.645151 15 -3.333333 6.666667 1.464836 horizontalflip 7.555556e-01 1.668887 15 -1.333333 4.000000 0.924199 noaug -1.777778e-01 1.521625 15 -2.000000 3.333333 0.842648 packetloss 7.555556e-01 2.150920 15 -2.000000 4.000000 1.191140 rotate -8.888889e-02 0.903842 15 -2.000000 1.333333 0.500531 timeshift 0.000000e+00 0.666667 15 -0.666667 1.333333 0.369188 <pre><code>fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))\nfor ax, (flowpic_dim, test_split_name) in zip(\naxes.flatten(), itertools.product((32, 1500), (\"test-human\", \"test-script\"))\n):\n# df_merged.loc[(flowpic_dim, test_split_name)]['mean'].plot(kind='bar', ax=ax)\nax.bar(\nlist(df_merged.loc[(flowpic_dim, test_split_name)].index),\ndf_merged.loc[(flowpic_dim, test_split_name)][\"mean\"],\nyerr=df_merged.loc[(flowpic_dim, test_split_name)][\"confidence_interval\"],\nalign=\"center\",\nalpha=0.5,\necolor=\"black\",\ncapsize=10,\n)\nax.set_title(f\"{test_split_name} @ {flowpic_dim}x{flowpic_dim}\")\nax.set_xticklabels(\nlist(df_merged.loc[(flowpic_dim, test_split_name)].index), rotation=90\n)\nplt.tight_layout()\n</code></pre> <pre><code>/tmp/ipykernel_62018/2491681095.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(list(df_merged.loc[(flowpic_dim, test_split_name)].index), rotation=90)\n</code></pre>"},{"location":"paper_tables_and_figures/reference/","title":"Tables and Figures from the submission","text":"<p>The tables and figures are created via a set of  Jupyter notebooks. Specifically, these notebooks are located at <code>code_artifacts_paper132/notebooks/submission_tables_and_figures</code>  in the code artifacts.</p> <p>However, notice that they require from modeling artifacts and datasets installation.</p> <ul> <li> <p>To install modeling artifacts, grab <code>ml_artifacts.tgz</code> and unpack it under the  folder mentioned above. The tarball contains a <code>/campaigns</code> folder so the final structure should be <pre><code>tree notebooks/ -d -L 2\nnotebooks/\n\u251c\u2500\u2500 submission_tables_and_figures\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 campaigns\n\u2514\u2500\u2500 tutorials\n</code></pre></p> </li> <li> <p>To install/import datasets refer to the <code>install</code> and <code>import</code> pages.</p> </li> </ul>"},{"location":"paper_tables_and_figures/reference/#tables","title":"Tables","text":"<ul> <li> <p>Table 1: Summary of Datasets Properties.   <code>table1_datasets_properties.ipynb</code></p> </li> <li> <p>Table 2:  (G0) Baseline ML performance without augmentation in a supervised setting.  <code>table2_xgboost_baseline.ipynb</code></p> </li> <li> <p>Table 3: Comparing data augmentation functions applied in supervised training.   <code>table3_ucdavis-icdm19_comparing_data_augmentations_functions.ipynb</code></p> </li> <li> <p>Table 4:  Impact of dropout and SimCLR projection layer dimension on fine-tuning.  <code>table4_simclr_dropout_and_projectionlayer.ipynb</code></p> </li> <li> <p>Table 5: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.  <code>table5_simclr_other_augmentation_pairs.ipynb</code></p> </li> <li> <p>Table 6: Accuracy on 32x32 flowpic when enlarging training set (w/o Dropout).  <code>table6_simclr_larger_trainset.ipynb</code></p> </li> <li> <p>Table 7: (G3) Data augmentation in supervised setting on other datasets.   <code>table7_augmentation-at-loading_on_other_datasets.ipynb</code></p> </li> <li> <p>Table 8 - appendix: Macro-average Accuracy with different retraining dataset and different sampling methods.  <code>table8_icdm_finetuning_per_class_metrics_on_human.ipynb</code></p> </li> <li> <p>Table 9 - appendix: Performance comparison across augmentations for different flowpic sizes.  <code>table9_ucdavis-icdm19_tukey.ipynb</code></p> </li> </ul>"},{"location":"paper_tables_and_figures/reference/#figures","title":"Figures","text":"<ul> <li> <p>Figure 1: Average confusion matrixes for the 32x32 resolution across all experiments in Table 3.   <code>figure1_confusion_matrix_supervised_setting.ipynb</code></p> </li> <li> <p>Figure 2: Average 32x32 flowpic for each class across multiple data splits.   <code>figure2_ucdavis_per_class_average_flowpic</code></p> </li> <li> <p>Figure 3: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.  <code>figure3_ucdavis_augmentations_comparison</code></p> </li> <li> <p>Figure 4: Critical distance plot of the accuracy obtained with each augmentation across the four tested datasets.  <code>figure4_augmentations_comparison_across_datasets_critical_distance</code></p> </li> <li> <p>Figure 5: Average rank obtained per augmentation and dataset. Ranks closer to 1 indicate a better performance.  <code>figure5_augmentations_comparison_across_datasets_average_rank</code></p> </li> <li> <p>Figure 6 - appendix: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.  <code>figure6_ucdavis_per_class_average_flowpic</code></p> </li> <li> <p>Figure 7 - appendix: Accuracy on script with different sampling methods (borrowed from Rezaei at al. ICM19 and added extra annotations).</p> </li> <li> <p>Figure 8(b) - appendix: Classwise evaluation on <code>human</code>.  <code>figure8b_icdm_finetuning_per_class_metrics_on_human</code></p> </li> <li> <p>Figure 9 - appendix: Accuracy difference w/ and w/o Dropout in supervised learning.  <code>figure9_dropout_impact_supervised_setting.ipynb</code></p> </li> </ul>"},{"location":"paper_tables_and_figures/reference/#errata-corrige","title":"Errata corrige","text":"<p>When compiling this documentation we noticed the following mistake/typos with respect to the results reported by the submission.</p> <ol> <li> <p>In Table 1, the row <code>mirage-22</code> @ 10pkts report wrong values. Please refer to the related notebook.</p> </li> <li> <p>In Table 4, the CI have (minor) differences with respect to the notebook.</p> </li> <li> <p>In Table 8, the CI were wrongly reported (the original computation was not right).</p> </li> </ol>"},{"location":"paper_tables_and_figures/table1_datasets_properties/","title":"Table1 : Datasets properties","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[9]: Copied! <pre>import pandas as pd\nimport tcbench as tcb\n</pre> import pandas as pd import tcbench as tcb In\u00a0[91]: Copied! <pre>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n\n# add number of packets\ndf = df.assign(packets=df[\"pkts_size\"].apply(len))\n\n# number of samples\ndf_tmp = pd.DataFrame(\n    df.groupby([\"partition\", \"app\"])[\"app\"].value_counts()\n).reset_index()\ndf_tmp = df_tmp.pivot(index=\"partition\", columns=\"app\", values=\"count\")\ndf_tmp = df_tmp.assign(\n    count=df_tmp.sum(axis=1),\n    flows_min=df_tmp.min(axis=1),\n    flows_max=df_tmp.max(axis=1),\n    rho=(df_tmp.max(axis=1) / df_tmp.min(axis=1)).round(1),\n    classes=len(df[\"app\"].cat.categories),\n)\n\n# mean pkts per flow\nmean_pkts = df.groupby(\"partition\")[\"packets\"].mean().round(0)\nmean_pkts.name = \"mean_pkts\"\nflows_all = df.groupby(\"partition\")[\"partition\"].count()\nflows_all.name = \"flows_all\"\n\n# combining everything together\ndf_tmp = pd.concat((df_tmp, mean_pkts, flows_all), axis=1)\ndf_tmp[[\"classes\", \"flows_all\", \"flows_min\", \"flows_max\", \"rho\", \"mean_pkts\"]]\n</pre> df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)  # add number of packets df = df.assign(packets=df[\"pkts_size\"].apply(len))  # number of samples df_tmp = pd.DataFrame(     df.groupby([\"partition\", \"app\"])[\"app\"].value_counts() ).reset_index() df_tmp = df_tmp.pivot(index=\"partition\", columns=\"app\", values=\"count\") df_tmp = df_tmp.assign(     count=df_tmp.sum(axis=1),     flows_min=df_tmp.min(axis=1),     flows_max=df_tmp.max(axis=1),     rho=(df_tmp.max(axis=1) / df_tmp.min(axis=1)).round(1),     classes=len(df[\"app\"].cat.categories), )  # mean pkts per flow mean_pkts = df.groupby(\"partition\")[\"packets\"].mean().round(0) mean_pkts.name = \"mean_pkts\" flows_all = df.groupby(\"partition\")[\"partition\"].count() flows_all.name = \"flows_all\"  # combining everything together df_tmp = pd.concat((df_tmp, mean_pkts, flows_all), axis=1) df_tmp[[\"classes\", \"flows_all\", \"flows_min\", \"flows_max\", \"rho\", \"mean_pkts\"]] Out[91]: classes flows_all flows_min flows_max rho mean_pkts partition pretraining 5 6439 592 1915 3.2 6653.0 retraining-human-triggered 5 83 15 20 1.3 7666.0 retraining-script-triggered 5 150 30 30 1.0 7131.0 <p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> In\u00a0[106]: Copied! <pre># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</pre> # unfiltered df = tcb.load_parquet(tcb.DATASETS.MIRAGE19)  ser = df[\"app\"].value_counts() df_unfiltered = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"unfiltered\"], ) In\u00a0[107]: Copied! <pre># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=10\"],\n)\n</pre> # min_pkts = 10 df = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)  ser = df[\"app\"].value_counts() df_minpkts10 = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"min_pkts=10\"], ) In\u00a0[108]: Copied! <pre>pd.concat((df_unfiltered, df_minpkts10), axis=0)\n</pre> pd.concat((df_unfiltered, df_minpkts10), axis=0) Out[108]: classes flows_all flows_min flows_max rho mean_pkts unfiltered 21 122007 1986 11737 5.9 23.0 min_pkts=10 20 64172 1013 7505 7.4 17.0 <p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> In\u00a0[102]: Copied! <pre># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</pre> # unfiltered df = tcb.load_parquet(tcb.DATASETS.MIRAGE22)  ser = df[\"app\"].value_counts() df_unfiltered = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"unfiltered\"], ) In\u00a0[103]: Copied! <pre># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=10\"],\n)\n</pre> # min_pkts = 10 df = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=10)  ser = df[\"app\"].value_counts() df_minpkts10 = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"min_pkts=10\"], ) In\u00a0[104]: Copied! <pre># min_pkts = 1000\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=1000)\n\nser = df[\"app\"].value_counts()\ndf_minpkts1000 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=1000\"],\n)\n</pre> # min_pkts = 1000 df = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=1000)  ser = df[\"app\"].value_counts() df_minpkts1000 = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"min_pkts=1000\"], ) In\u00a0[105]: Copied! <pre>pd.concat((df_unfiltered, df_minpkts10, df_minpkts1000), axis=0)\n</pre> pd.concat((df_unfiltered, df_minpkts10, df_minpkts1000), axis=0) Out[105]: classes flows_all flows_min flows_max rho mean_pkts unfiltered 10 59071 2252 18882 8.4 3068.0 min_pkts=10 9 26773 970 4437 4.6 6598.0 min_pkts=1000 9 4569 190 2220 11.7 38321.0 In\u00a0[109]: Copied! <pre># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</pre> # unfiltered df = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21)  ser = df[\"app\"].value_counts() df_unfiltered = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"unfiltered\"], ) In\u00a0[111]: Copied! <pre># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"minpkts=10\"],\n)\n</pre> # unfiltered df = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21, min_pkts=10)  ser = df[\"app\"].value_counts() df_minpkts10 = pd.DataFrame(     [         dict(             classes=len(ser),             flows_all=ser.sum(),             flows_min=ser.min(),             flows_max=ser.max(),             rho=(ser.max() / ser.min()).round(1),             mean_pkts=df[\"packets\"].mean().round(0),         )     ],     index=[\"minpkts=10\"], ) In\u00a0[112]: Copied! <pre>pd.concat((df_unfiltered, df_minpkts10), axis=0)\n</pre> pd.concat((df_unfiltered, df_minpkts10), axis=0) Out[112]: classes flows_all flows_min flows_max rho mean_pkts unfiltered 17 22429 57 4716 82.7 716.0 minpkts=10 10 5685 104 2153 20.7 2741.0"},{"location":"paper_tables_and_figures/table1_datasets_properties/#table1-datasets-properties","title":"Table1 : Datasets properties\u00b6","text":""},{"location":"paper_tables_and_figures/table1_datasets_properties/#ucdavis-icdm19","title":"ucdavis-icdm19\u00b6","text":""},{"location":"paper_tables_and_figures/table1_datasets_properties/#mirage19","title":"mirage19\u00b6","text":""},{"location":"paper_tables_and_figures/table1_datasets_properties/#mirage22","title":"mirage22\u00b6","text":""},{"location":"paper_tables_and_figures/table1_datasets_properties/#utmobilenet21","title":"utmobilenet21\u00b6","text":""},{"location":"paper_tables_and_figures/table1_datasets_properties/","title":"Table1 : Datasets properties","text":"<pre><code>import pandas as pd\nimport tcbench as tcb\n</code></pre>"},{"location":"paper_tables_and_figures/table1_datasets_properties/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n# add number of packets\ndf = df.assign(packets=df[\"pkts_size\"].apply(len))\n# number of samples\ndf_tmp = pd.DataFrame(\ndf.groupby([\"partition\", \"app\"])[\"app\"].value_counts()\n).reset_index()\ndf_tmp = df_tmp.pivot(index=\"partition\", columns=\"app\", values=\"count\")\ndf_tmp = df_tmp.assign(\ncount=df_tmp.sum(axis=1),\nflows_min=df_tmp.min(axis=1),\nflows_max=df_tmp.max(axis=1),\nrho=(df_tmp.max(axis=1) / df_tmp.min(axis=1)).round(1),\nclasses=len(df[\"app\"].cat.categories),\n)\n# mean pkts per flow\nmean_pkts = df.groupby(\"partition\")[\"packets\"].mean().round(0)\nmean_pkts.name = \"mean_pkts\"\nflows_all = df.groupby(\"partition\")[\"partition\"].count()\nflows_all.name = \"flows_all\"\n# combining everything together\ndf_tmp = pd.concat((df_tmp, mean_pkts, flows_all), axis=1)\ndf_tmp[[\"classes\", \"flows_all\", \"flows_min\", \"flows_max\", \"rho\", \"mean_pkts\"]]\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts partition pretraining 5 6439 592 1915 3.2 6653.0 retraining-human-triggered 5 83 15 20 1.3 7666.0 retraining-script-triggered 5 150 30 30 1.0 7131.0"},{"location":"paper_tables_and_figures/table1_datasets_properties/#mirage19","title":"mirage19","text":"<p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19)\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"unfiltered\"],\n)\n</code></pre> <pre><code># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"min_pkts=10\"],\n)\n</code></pre> <pre><code>pd.concat((df_unfiltered, df_minpkts10), axis=0)\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 21 122007 1986 11737 5.9 23.0 min_pkts=10 20 64172 1013 7505 7.4 17.0"},{"location":"paper_tables_and_figures/table1_datasets_properties/#mirage22","title":"mirage22","text":"<p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22)\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"unfiltered\"],\n)\n</code></pre> <pre><code># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=10)\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"min_pkts=10\"],\n)\n</code></pre> <pre><code># min_pkts = 1000\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=1000)\nser = df[\"app\"].value_counts()\ndf_minpkts1000 = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"min_pkts=1000\"],\n)\n</code></pre> <pre><code>pd.concat((df_unfiltered, df_minpkts10, df_minpkts1000), axis=0)\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 10 59071 2252 18882 8.4 3068.0 min_pkts=10 9 26773 970 4437 4.6 6598.0 min_pkts=1000 9 4569 190 2220 11.7 38321.0"},{"location":"paper_tables_and_figures/table1_datasets_properties/#utmobilenet21","title":"utmobilenet21","text":"<pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21)\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"unfiltered\"],\n)\n</code></pre> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21, min_pkts=10)\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n[\ndict(\nclasses=len(ser),\nflows_all=ser.sum(),\nflows_min=ser.min(),\nflows_max=ser.max(),\nrho=(ser.max() / ser.min()).round(1),\nmean_pkts=df[\"packets\"].mean().round(0),\n)\n],\nindex=[\"minpkts=10\"],\n)\n</code></pre> <pre><code>pd.concat((df_unfiltered, df_minpkts10), axis=0)\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 17 22429 57 4716 82.7 716.0 minpkts=10 10 5685 104 2153 20.7 2741.0"},{"location":"paper_tables_and_figures/table2_xgboost_baseline/","title":"Table 2: (G0) Baseline ML performance without augmentation in a supervised setting.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[43]: Copied! <pre>df = pd.read_csv(\n    \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic/campaign_summary/1684951896/summary_flowpic_dim_32.csv\"\n)\n# df\n</pre> df = pd.read_csv(     \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic/campaign_summary/1684951896/summary_flowpic_dim_32.csv\" ) # df In\u00a0[44]: Copied! <pre># this is just reformatting to extracting the right values\ndf.columns = [col.split(\".\")[0] for col in df.columns]\ndf = df.set_index([\"test_split_name\", \"aug_name\"], drop=True)\ndf.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values])\ndf = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\n\ndf[\"acc\"].T.loc[[\"mean\", \"ci95\"]]\n</pre> # this is just reformatting to extracting the right values df.columns = [col.split(\".\")[0] for col in df.columns] df = df.set_index([\"test_split_name\", \"aug_name\"], drop=True) df.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values]) df = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)  df[\"acc\"].T.loc[[\"mean\", \"ci95\"]] Out[44]: test_split_name test-script test-human mean 96.53 71.81 ci95 0.15 2.85 In\u00a0[45]: Copied! <pre>df = pd.read_csv(\n    \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries/campaign_summary/1685008005/summary_max_n_pkts_10.csv\"\n)\n# df\n</pre> df = pd.read_csv(     \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries/campaign_summary/1685008005/summary_max_n_pkts_10.csv\" ) # df In\u00a0[41]: Copied! <pre># this is just reformatting to extracting the right values\ndf.columns = [col.split(\".\")[0] for col in df.columns]\ndf = df.set_index([\"test_split_name\", \"aug_name\"], drop=True)\ndf.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values])\ndf = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\n\ndf[\"acc\"].T.loc[[\"mean\", \"ci95\"]]\n</pre> # this is just reformatting to extracting the right values df.columns = [col.split(\".\")[0] for col in df.columns] df = df.set_index([\"test_split_name\", \"aug_name\"], drop=True) df.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values]) df = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)  df[\"acc\"].T.loc[[\"mean\", \"ci95\"]] Out[41]: test_split_name test-script test-human mean 94.53 67.47 ci95 0.45 1.51"},{"location":"paper_tables_and_figures/table2_xgboost_baseline/#table-2-g0-baseline-ml-performance-without-augmentation-in-a-supervised-setting","title":"Table 2: (G0) Baseline ML performance without augmentation in a supervised setting.\u00b6","text":""},{"location":"paper_tables_and_figures/table2_xgboost_baseline/","title":"Table 2: (G0) Baseline ML performance without augmentation in a supervised setting.","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>df = pd.read_csv(\n\"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic/campaign_summary/1684951896/summary_flowpic_dim_32.csv\"\n)\n# df\n</code></pre> <pre><code># this is just reformatting to extracting the right values\ndf.columns = [col.split(\".\")[0] for col in df.columns]\ndf = df.set_index([\"test_split_name\", \"aug_name\"], drop=True)\ndf.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values])\ndf = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\ndf[\"acc\"].T.loc[[\"mean\", \"ci95\"]]\n</code></pre> test_split_name test-script test-human mean 96.53 71.81 ci95 0.15 2.85 <pre><code>df = pd.read_csv(\n\"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries/campaign_summary/1685008005/summary_max_n_pkts_10.csv\"\n)\n# df\n</code></pre> <pre><code># this is just reformatting to extracting the right values\ndf.columns = [col.split(\".\")[0] for col in df.columns]\ndf = df.set_index([\"test_split_name\", \"aug_name\"], drop=True)\ndf.columns = pd.MultiIndex.from_arrays([df.columns, df.iloc[0].values])\ndf = df.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\ndf[\"acc\"].T.loc[[\"mean\", \"ci95\"]]\n</code></pre> test_split_name test-script test-human mean 94.53 67.47 ci95 0.45 1.51"},{"location":"paper_tables_and_figures/table3_ucdavis-icdm19_comparing_data_augmentations_functions/","title":"Table 3: Comparing data augmentation functions applied in supervised training.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport statsmodels.stats.api as sms\n</pre> import numpy as np import pandas as pd import statsmodels.stats.api as sms In\u00a0[2]: Copied! <pre>import itertools\nimport pathlib\nimport tempfile\n</pre> import itertools import pathlib import tempfile In\u00a0[\u00a0]: Copied! <pre>def compute_ci95(ser):\n    low, high = sms.DescrStatsW(ser.values).tconfint_mean(alpha=0.05)\n    mean = ser.mean()\n    ci = high - mean\n    return ci\n</pre> def compute_ci95(ser):     low, high = sms.DescrStatsW(ser.values).tconfint_mean(alpha=0.05)     mean = ser.mean()     ci = high - mean     return ci In\u00a0[7]: Copied! <pre>folder_campaign_summary = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/\"\n)\n</pre> folder_campaign_summary = pathlib.Path(     \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/\" ) In\u00a0[8]: Copied! <pre># load results\ndf = pd.concat(\n    [\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_32.parquet\"),\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_64.parquet\"),\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_1500.parquet\"),\n    ]\n)\n</pre> # load results df = pd.concat(     [         pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_32.parquet\"),         pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_64.parquet\"),         pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_1500.parquet\"),     ] ) In\u00a0[15]: Copied! <pre>df_agg_dict = dict()\nfor flowpic_dim in (32, 64, 1500):\n    df_tmp = df[df[\"flowpic_dim\"] == flowpic_dim]\n    df_agg = df_tmp.groupby([\"test_split_name\", \"aug_name\"]).agg(\n        {\"acc\": [\"count\", \"mean\", \"std\", compute_ci95]}\n    )\n    df_agg = df_agg.droplevel(0, axis=1).rename({\"compute_ci95\": \"ci95\"}, axis=1)\n    fname = folder_campaign_summary / f\"summary_flowpic_dim_{flowpic_dim}.csv\"\n    df_agg_dict[flowpic_dim] = df_agg\n</pre> df_agg_dict = dict() for flowpic_dim in (32, 64, 1500):     df_tmp = df[df[\"flowpic_dim\"] == flowpic_dim]     df_agg = df_tmp.groupby([\"test_split_name\", \"aug_name\"]).agg(         {\"acc\": [\"count\", \"mean\", \"std\", compute_ci95]}     )     df_agg = df_agg.droplevel(0, axis=1).rename({\"compute_ci95\": \"ci95\"}, axis=1)     fname = folder_campaign_summary / f\"summary_flowpic_dim_{flowpic_dim}.csv\"     df_agg_dict[flowpic_dim] = df_agg In\u00a0[140]: Copied! <pre># loading imc22-paper results\n# (there are oviously copied)\n\nIMC22_TABLE_TEST_SCRIPT = \"\"\"\naug_name,32,64,1500\nNo augmentation,98.67,99.1,96.22\nRotate,98.6,98.87,94.89\nHorizontal flip,98.93,99.27,97.33\nColor jitter,96.73,96.4,94.0\nPacket loss,98.73,99.6,96.22\nTime shift,99.13,99.53,97.56\nChange rtt,99.4,100.0,98.44\n\"\"\"\n\nIMC22_TABLE_TEST_HUMAN = \"\"\"\naug_name,32,64,1500\nNo augmentation,92.4,85.6,73.3\nRotate,93.73,87.07,77.3\nHorizontal flip,94.67,79.33,87.9\nColor jitter,82.93,74.93,68.0\nPacket loss,90.93,85.6,84.0\nTime shift,92.8,87.33,77.3\nChange rtt,96.4,88.6,90.7\n\"\"\"\n\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\n    f_tmp.write(IMC22_TABLE_TEST_SCRIPT)\n    f_tmp.seek(0)\n    df_imc22_table_test_script = pd.read_csv(f_tmp.name)\n    df_imc22_table_test_script = df_imc22_table_test_script.set_index(\"aug_name\")\n    df_imc22_table_test_script.columns = pd.MultiIndex.from_product(\n        [[\"imc22-paper\"], df_imc22_table_test_script.columns, [\"mean\"]]\n    )\n\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\n    f_tmp.write(IMC22_TABLE_TEST_HUMAN)\n    f_tmp.seek(0)\n    df_imc22_table_test_human = pd.read_csv(f_tmp.name)\n    df_imc22_table_test_human = df_imc22_table_test_human.set_index(\"aug_name\")\n    df_imc22_table_test_human.columns = pd.MultiIndex.from_product(\n        [[\"imc22-paper\"], df_imc22_table_test_human.columns, [\"mean\"]]\n    )\n</pre> # loading imc22-paper results # (there are oviously copied)  IMC22_TABLE_TEST_SCRIPT = \"\"\" aug_name,32,64,1500 No augmentation,98.67,99.1,96.22 Rotate,98.6,98.87,94.89 Horizontal flip,98.93,99.27,97.33 Color jitter,96.73,96.4,94.0 Packet loss,98.73,99.6,96.22 Time shift,99.13,99.53,97.56 Change rtt,99.4,100.0,98.44 \"\"\"  IMC22_TABLE_TEST_HUMAN = \"\"\" aug_name,32,64,1500 No augmentation,92.4,85.6,73.3 Rotate,93.73,87.07,77.3 Horizontal flip,94.67,79.33,87.9 Color jitter,82.93,74.93,68.0 Packet loss,90.93,85.6,84.0 Time shift,92.8,87.33,77.3 Change rtt,96.4,88.6,90.7 \"\"\"  with tempfile.NamedTemporaryFile(\"w\") as f_tmp:     f_tmp.write(IMC22_TABLE_TEST_SCRIPT)     f_tmp.seek(0)     df_imc22_table_test_script = pd.read_csv(f_tmp.name)     df_imc22_table_test_script = df_imc22_table_test_script.set_index(\"aug_name\")     df_imc22_table_test_script.columns = pd.MultiIndex.from_product(         [[\"imc22-paper\"], df_imc22_table_test_script.columns, [\"mean\"]]     )  with tempfile.NamedTemporaryFile(\"w\") as f_tmp:     f_tmp.write(IMC22_TABLE_TEST_HUMAN)     f_tmp.seek(0)     df_imc22_table_test_human = pd.read_csv(f_tmp.name)     df_imc22_table_test_human = df_imc22_table_test_human.set_index(\"aug_name\")     df_imc22_table_test_human.columns = pd.MultiIndex.from_product(         [[\"imc22-paper\"], df_imc22_table_test_human.columns, [\"mean\"]]     ) In\u00a0[141]: Copied! <pre>RENAMING = {\n    \"test-human\": \"human\",\n    \"test-script\": \"script\",\n    \"test-train-val-leftover\": \"leftover\",\n    \"noaug\": \"No augmentation\",\n    \"changertt\": \"Change rtt\",\n    \"colorjitter\": \"Color jitter\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"packetloss\": \"Packet loss\",\n    \"rotate\": \"Rotate\",\n    \"timeshift\": \"Time shift\",\n}\n\nAUG_NAME_ORDER = [\n    \"No augmentation\",\n    \"Rotate\",\n    \"Horizontal flip\",\n    \"Color jitter\",\n    \"Packet loss\",\n    \"Time shift\",\n    \"Change rtt\",\n]\n\npartial_dfs = {\n    \"human\": dict(),\n    \"script\": dict(),\n    \"leftover\": dict(),\n}\nfor flowpic_dim in (32, 64, 1500):\n    df_tmp = df_agg_dict[flowpic_dim][[\"mean\", \"ci95\"]].round(2).reset_index()\n    df_tmp = df_tmp.assign(\n        test_split_name=df_tmp[\"test_split_name\"].replace(RENAMING),\n        aug_name=df_tmp[\"aug_name\"].replace(RENAMING),\n    )\n    df_tmp = df_tmp.set_index(\"test_split_name\", drop=True)\n    for split_name in (\"script\", \"human\", \"leftover\"):\n        df_partial = df_tmp.loc[split_name].copy()\n        df_partial = df_partial.set_index(\"aug_name\", drop=True)\n        df_partial = df_partial.loc[AUG_NAME_ORDER]\n        partial_dfs[split_name][flowpic_dim] = df_partial\n</pre> RENAMING = {     \"test-human\": \"human\",     \"test-script\": \"script\",     \"test-train-val-leftover\": \"leftover\",     \"noaug\": \"No augmentation\",     \"changertt\": \"Change rtt\",     \"colorjitter\": \"Color jitter\",     \"horizontalflip\": \"Horizontal flip\",     \"packetloss\": \"Packet loss\",     \"rotate\": \"Rotate\",     \"timeshift\": \"Time shift\", }  AUG_NAME_ORDER = [     \"No augmentation\",     \"Rotate\",     \"Horizontal flip\",     \"Color jitter\",     \"Packet loss\",     \"Time shift\",     \"Change rtt\", ]  partial_dfs = {     \"human\": dict(),     \"script\": dict(),     \"leftover\": dict(), } for flowpic_dim in (32, 64, 1500):     df_tmp = df_agg_dict[flowpic_dim][[\"mean\", \"ci95\"]].round(2).reset_index()     df_tmp = df_tmp.assign(         test_split_name=df_tmp[\"test_split_name\"].replace(RENAMING),         aug_name=df_tmp[\"aug_name\"].replace(RENAMING),     )     df_tmp = df_tmp.set_index(\"test_split_name\", drop=True)     for split_name in (\"script\", \"human\", \"leftover\"):         df_partial = df_tmp.loc[split_name].copy()         df_partial = df_partial.set_index(\"aug_name\", drop=True)         df_partial = df_partial.loc[AUG_NAME_ORDER]         partial_dfs[split_name][flowpic_dim] = df_partial In\u00a0[142]: Copied! <pre>df_ours_script = pd.concat(partial_dfs[\"script\"], axis=1)\ndf_ours_script.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_script.columns.levels]\n)\n\ndf_ours_human = pd.concat(partial_dfs[\"human\"], axis=1)\ndf_ours_human.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_human.columns.levels]\n)\n\ndf_ours_leftover = pd.concat(partial_dfs[\"leftover\"], axis=1)\ndf_ours_leftover.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_leftover.columns.levels]\n)\n</pre> df_ours_script = pd.concat(partial_dfs[\"script\"], axis=1) df_ours_script.columns = pd.MultiIndex.from_product(     [[\"ours\"], *df_ours_script.columns.levels] )  df_ours_human = pd.concat(partial_dfs[\"human\"], axis=1) df_ours_human.columns = pd.MultiIndex.from_product(     [[\"ours\"], *df_ours_human.columns.levels] )  df_ours_leftover = pd.concat(partial_dfs[\"leftover\"], axis=1) df_ours_leftover.columns = pd.MultiIndex.from_product(     [[\"ours\"], *df_ours_leftover.columns.levels] ) In\u00a0[143]: Copied! <pre>print(\"=== test on script ===\")\ndf_tmp = pd.concat((df_imc22_table_test_script, df_ours_script), axis=1)\n\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n    (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n    (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n    (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.fillna(\"\")\n</pre> print(\"=== test on script ===\") df_tmp = pd.concat((df_imc22_table_test_script, df_ours_script), axis=1)  df_tmp.loc[\"mean_diff\", :] = np.nan df_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (     (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])     .mean()     .round(2) ) df_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (     (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])     .mean()     .round(2) ) df_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (     (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])     .mean()     .round(2) ) df_tmp.fillna(\"\") <pre>=== test on script ===\n</pre> Out[143]: imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 98.67 99.1 96.22 95.73 0.27 95.96 0.29 94.44 0.9 Rotate 98.6 98.87 94.89 96.36 0.39 96.89 0.39 95.47 0.47 Horizontal flip 98.93 99.27 97.33 95.11 0.41 95.96 0.49 95.11 0.68 Color jitter 96.73 96.4 94.0 97.87 0.45 97.42 0.67 94.89 0.83 Packet loss 98.73 99.6 96.22 96.98 0.48 96.89 0.53 95.96 0.7 Time shift 99.13 99.53 97.56 96.71 0.51 97.11 0.36 96.80 0.32 Change rtt 99.4 100.0 98.44 97.33 0.39 97.29 0.35 96.80 0.35 mean_diff -2.01 -2.18 -0.74 In\u00a0[144]: Copied! <pre>print(\"=== test on human ===\")\ndf_tmp = pd.concat((df_imc22_table_test_human, df_ours_human), axis=1)\n\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n    (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n    (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n    (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.fillna(\"\")\n</pre> print(\"=== test on human ===\") df_tmp = pd.concat((df_imc22_table_test_human, df_ours_human), axis=1)  df_tmp.loc[\"mean_diff\", :] = np.nan df_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (     (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])     .mean()     .round(2) ) df_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (     (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])     .mean()     .round(2) ) df_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (     (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])     .mean()     .round(2) ) df_tmp.fillna(\"\") <pre>=== test on human ===\n</pre> Out[144]: imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 92.4 85.6 73.3 69.48 1.17 69.88 1.26 68.67 1.07 Rotate 93.73 87.07 77.3 71.57 1.95 71.00 1.35 67.87 0.86 Horizontal flip 94.67 79.33 87.9 69.80 1.39 70.92 1.83 73.82 0.82 Color jitter 82.93 74.93 68.0 68.84 2.59 71.33 1.86 68.59 1.76 Packet loss 90.93 85.6 84.0 71.00 1.02 73.17 0.89 72.13 1.04 Time shift 92.8 87.33 77.3 70.36 1.65 72.53 1.02 70.84 1.34 Change rtt 96.4 88.6 90.7 70.04 2.44 72.05 1.16 72.69 1.48 mean_diff -21.82 -12.51 -9.13 In\u00a0[145]: Copied! <pre>print(\"=== test on leftover ===\")\ndf_ours_leftover\n</pre> print(\"=== test on leftover ===\") df_ours_leftover <pre>=== test on leftover ===\n</pre> Out[145]: ours 32 64 1500 mean ci95 mean ci95 mean ci95 aug_name No augmentation 96.05 0.19 96.22 0.31 95.62 0.51 Rotate 97.01 0.24 97.28 0.34 95.93 0.41 Horizontal flip 95.88 0.25 96.38 0.50 96.47 0.57 Color jitter 97.46 0.33 96.82 0.41 95.79 0.50 Packet loss 97.47 0.35 97.48 0.28 97.29 0.27 Time shift 97.44 0.42 97.78 0.38 97.94 0.19 Change rtt 98.24 0.31 98.29 0.39 98.43 0.12"},{"location":"paper_tables_and_figures/table3_ucdavis-icdm19_comparing_data_augmentations_functions/#table-3-comparing-data-augmentation-functions-applied-in-supervised-training","title":"Table 3: Comparing data augmentation functions applied in supervised training.\u00b6","text":""},{"location":"paper_tables_and_figures/table3_ucdavis-icdm19_comparing_data_augmentations_functions/","title":"Table 3: Comparing data augmentation functions applied in supervised training.","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport statsmodels.stats.api as sms\n</code></pre> <pre><code>import itertools\nimport pathlib\nimport tempfile\n</code></pre> <pre><code>def compute_ci95(ser):\nlow, high = sms.DescrStatsW(ser.values).tconfint_mean(alpha=0.05)\nmean = ser.mean()\nci = high - mean\nreturn ci\n</code></pre> <pre><code>folder_campaign_summary = pathlib.Path(\n\"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/\"\n)\n</code></pre> <pre><code># load results\ndf = pd.concat(\n[\npd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_32.parquet\"),\npd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_64.parquet\"),\npd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_1500.parquet\"),\n]\n)\n</code></pre> <pre><code>df_agg_dict = dict()\nfor flowpic_dim in (32, 64, 1500):\ndf_tmp = df[df[\"flowpic_dim\"] == flowpic_dim]\ndf_agg = df_tmp.groupby([\"test_split_name\", \"aug_name\"]).agg(\n{\"acc\": [\"count\", \"mean\", \"std\", compute_ci95]}\n)\ndf_agg = df_agg.droplevel(0, axis=1).rename({\"compute_ci95\": \"ci95\"}, axis=1)\nfname = folder_campaign_summary / f\"summary_flowpic_dim_{flowpic_dim}.csv\"\ndf_agg_dict[flowpic_dim] = df_agg\n</code></pre> <pre><code># loading imc22-paper results\n# (there are oviously copied)\nIMC22_TABLE_TEST_SCRIPT = \"\"\"\naug_name,32,64,1500\nNo augmentation,98.67,99.1,96.22\nRotate,98.6,98.87,94.89\nHorizontal flip,98.93,99.27,97.33\nColor jitter,96.73,96.4,94.0\nPacket loss,98.73,99.6,96.22\nTime shift,99.13,99.53,97.56\nChange rtt,99.4,100.0,98.44\n\"\"\"\nIMC22_TABLE_TEST_HUMAN = \"\"\"\naug_name,32,64,1500\nNo augmentation,92.4,85.6,73.3\nRotate,93.73,87.07,77.3\nHorizontal flip,94.67,79.33,87.9\nColor jitter,82.93,74.93,68.0\nPacket loss,90.93,85.6,84.0\nTime shift,92.8,87.33,77.3\nChange rtt,96.4,88.6,90.7\n\"\"\"\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\nf_tmp.write(IMC22_TABLE_TEST_SCRIPT)\nf_tmp.seek(0)\ndf_imc22_table_test_script = pd.read_csv(f_tmp.name)\ndf_imc22_table_test_script = df_imc22_table_test_script.set_index(\"aug_name\")\ndf_imc22_table_test_script.columns = pd.MultiIndex.from_product(\n[[\"imc22-paper\"], df_imc22_table_test_script.columns, [\"mean\"]]\n)\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\nf_tmp.write(IMC22_TABLE_TEST_HUMAN)\nf_tmp.seek(0)\ndf_imc22_table_test_human = pd.read_csv(f_tmp.name)\ndf_imc22_table_test_human = df_imc22_table_test_human.set_index(\"aug_name\")\ndf_imc22_table_test_human.columns = pd.MultiIndex.from_product(\n[[\"imc22-paper\"], df_imc22_table_test_human.columns, [\"mean\"]]\n)\n</code></pre> <pre><code>RENAMING = {\n\"test-human\": \"human\",\n\"test-script\": \"script\",\n\"test-train-val-leftover\": \"leftover\",\n\"noaug\": \"No augmentation\",\n\"changertt\": \"Change rtt\",\n\"colorjitter\": \"Color jitter\",\n\"horizontalflip\": \"Horizontal flip\",\n\"packetloss\": \"Packet loss\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n}\nAUG_NAME_ORDER = [\n\"No augmentation\",\n\"Rotate\",\n\"Horizontal flip\",\n\"Color jitter\",\n\"Packet loss\",\n\"Time shift\",\n\"Change rtt\",\n]\npartial_dfs = {\n\"human\": dict(),\n\"script\": dict(),\n\"leftover\": dict(),\n}\nfor flowpic_dim in (32, 64, 1500):\ndf_tmp = df_agg_dict[flowpic_dim][[\"mean\", \"ci95\"]].round(2).reset_index()\ndf_tmp = df_tmp.assign(\ntest_split_name=df_tmp[\"test_split_name\"].replace(RENAMING),\naug_name=df_tmp[\"aug_name\"].replace(RENAMING),\n)\ndf_tmp = df_tmp.set_index(\"test_split_name\", drop=True)\nfor split_name in (\"script\", \"human\", \"leftover\"):\ndf_partial = df_tmp.loc[split_name].copy()\ndf_partial = df_partial.set_index(\"aug_name\", drop=True)\ndf_partial = df_partial.loc[AUG_NAME_ORDER]\npartial_dfs[split_name][flowpic_dim] = df_partial\n</code></pre> <pre><code>df_ours_script = pd.concat(partial_dfs[\"script\"], axis=1)\ndf_ours_script.columns = pd.MultiIndex.from_product(\n[[\"ours\"], *df_ours_script.columns.levels]\n)\ndf_ours_human = pd.concat(partial_dfs[\"human\"], axis=1)\ndf_ours_human.columns = pd.MultiIndex.from_product(\n[[\"ours\"], *df_ours_human.columns.levels]\n)\ndf_ours_leftover = pd.concat(partial_dfs[\"leftover\"], axis=1)\ndf_ours_leftover.columns = pd.MultiIndex.from_product(\n[[\"ours\"], *df_ours_leftover.columns.levels]\n)\n</code></pre> <pre><code>print(\"=== test on script ===\")\ndf_tmp = pd.concat((df_imc22_table_test_script, df_ours_script), axis=1)\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n(df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n(df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n(df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.fillna(\"\")\n</code></pre> <pre><code>=== test on script ===\n</code></pre> imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 98.67 99.1 96.22 95.73 0.27 95.96 0.29 94.44 0.9 Rotate 98.6 98.87 94.89 96.36 0.39 96.89 0.39 95.47 0.47 Horizontal flip 98.93 99.27 97.33 95.11 0.41 95.96 0.49 95.11 0.68 Color jitter 96.73 96.4 94.0 97.87 0.45 97.42 0.67 94.89 0.83 Packet loss 98.73 99.6 96.22 96.98 0.48 96.89 0.53 95.96 0.7 Time shift 99.13 99.53 97.56 96.71 0.51 97.11 0.36 96.80 0.32 Change rtt 99.4 100.0 98.44 97.33 0.39 97.29 0.35 96.80 0.35 mean_diff -2.01 -2.18 -0.74 <pre><code>print(\"=== test on human ===\")\ndf_tmp = pd.concat((df_imc22_table_test_human, df_ours_human), axis=1)\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n(df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n(df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n(df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n.mean()\n.round(2)\n)\ndf_tmp.fillna(\"\")\n</code></pre> <pre><code>=== test on human ===\n</code></pre> imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 92.4 85.6 73.3 69.48 1.17 69.88 1.26 68.67 1.07 Rotate 93.73 87.07 77.3 71.57 1.95 71.00 1.35 67.87 0.86 Horizontal flip 94.67 79.33 87.9 69.80 1.39 70.92 1.83 73.82 0.82 Color jitter 82.93 74.93 68.0 68.84 2.59 71.33 1.86 68.59 1.76 Packet loss 90.93 85.6 84.0 71.00 1.02 73.17 0.89 72.13 1.04 Time shift 92.8 87.33 77.3 70.36 1.65 72.53 1.02 70.84 1.34 Change rtt 96.4 88.6 90.7 70.04 2.44 72.05 1.16 72.69 1.48 mean_diff -21.82 -12.51 -9.13 <pre><code>print(\"=== test on leftover ===\")\ndf_ours_leftover\n</code></pre> <pre><code>=== test on leftover ===\n</code></pre> ours 32 64 1500 mean ci95 mean ci95 mean ci95 aug_name No augmentation 96.05 0.19 96.22 0.31 95.62 0.51 Rotate 97.01 0.24 97.28 0.34 95.93 0.41 Horizontal flip 95.88 0.25 96.38 0.50 96.47 0.57 Color jitter 97.46 0.33 96.82 0.41 95.79 0.50 Packet loss 97.47 0.35 97.48 0.28 97.29 0.27 Time shift 97.44 0.42 97.78 0.38 97.94 0.19 Change rtt 98.24 0.31 98.29 0.39 98.43 0.12"},{"location":"paper_tables_and_figures/table4_simclr_dropout_and_projectionlayer/","title":"Table 4: Impact of dropout and SimCLR projection layer dimension on fine-tuning.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[34]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[35]: Copied! <pre>import itertools\n</pre> import itertools In\u00a0[32]: Copied! <pre>df = pd.read_csv(\n    \"campaigns/ucdavis-icdm19/simclr-dropout-and-projection/campaign_summary/1684821411/summary_flowpic_dim_32.csv\"\n)\n\n# reformat the raw report\nRENAME = {\n    \"acc\": \"count\",\n    \"acc.1\": \"mean\",\n    \"acc.2\": \"std\",\n    \"acc.3\": \"ci95\"\n}\ndf = df.set_index(['test_split_name', 'with_dropout', 'projection_layer_dim'], drop=True)\ndf = df.rename(RENAME, axis=1)\ndf = df.drop('finetune_augmentation', axis=1)\ndf = df.iloc[1:].astype(float).round(2)\ndf = df[['mean', 'ci95']].T\ndf = df[list(itertools.product(['test-script', 'test-human'], [True, False], [30.0, 84.0]))]\ndf\n</pre> df = pd.read_csv(     \"campaigns/ucdavis-icdm19/simclr-dropout-and-projection/campaign_summary/1684821411/summary_flowpic_dim_32.csv\" )  # reformat the raw report RENAME = {     \"acc\": \"count\",     \"acc.1\": \"mean\",     \"acc.2\": \"std\",     \"acc.3\": \"ci95\" } df = df.set_index(['test_split_name', 'with_dropout', 'projection_layer_dim'], drop=True) df = df.rename(RENAME, axis=1) df = df.drop('finetune_augmentation', axis=1) df = df.iloc[1:].astype(float).round(2) df = df[['mean', 'ci95']].T df = df[list(itertools.product(['test-script', 'test-human'], [True, False], [30.0, 84.0]))] df Out[32]: test_split_name test-script test-human with_dropout True False True False projection_layer_dim 30.0 84.0 30.0 84.0 30.0 84.0 30.0 84.0 mean 91.80 92.19 92.26 92.49 71.88 73.55 75.22 74.04 ci95 0.38 0.37 0.32 0.31 1.33 1.10 1.22 1.38"},{"location":"paper_tables_and_figures/table4_simclr_dropout_and_projectionlayer/#table-4-impact-of-dropout-and-simclr-projection-layer-dimension-on-fine-tuning","title":"Table 4: Impact of dropout and SimCLR projection layer dimension on fine-tuning.\u00b6","text":""},{"location":"paper_tables_and_figures/table4_simclr_dropout_and_projectionlayer/","title":"Table 4: Impact of dropout and SimCLR projection layer dimension on fine-tuning.","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>import itertools\n</code></pre> <pre><code>df = pd.read_csv(\n\"campaigns/ucdavis-icdm19/simclr-dropout-and-projection/campaign_summary/1684821411/summary_flowpic_dim_32.csv\"\n)\n# reformat the raw report\nRENAME = {\n\"acc\": \"count\",\n\"acc.1\": \"mean\",\n\"acc.2\": \"std\",\n\"acc.3\": \"ci95\"\n}\ndf = df.set_index(['test_split_name', 'with_dropout', 'projection_layer_dim'], drop=True)\ndf = df.rename(RENAME, axis=1)\ndf = df.drop('finetune_augmentation', axis=1)\ndf = df.iloc[1:].astype(float).round(2)\ndf = df[['mean', 'ci95']].T\ndf = df[list(itertools.product(['test-script', 'test-human'], [True, False], [30.0, 84.0]))]\ndf\n</code></pre> test_split_name test-script test-human with_dropout True False True False projection_layer_dim 30.0 84.0 30.0 84.0 30.0 84.0 30.0 84.0 mean 91.80 92.19 92.26 92.49 71.88 73.55 75.22 74.04 ci95 0.38 0.37 0.32 0.31 1.33 1.10 1.22 1.38"},{"location":"paper_tables_and_figures/table5_simclr_other_augmentation_pairs/","title":"Table 5: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[13]: Copied! <pre>def load_summary_csv(fname, level1, level2):\n    df = (\n        pd.read_csv(fname)\n        .set_index(\"test_split_name\", drop=True)\n        .drop([\"with_dropout\", \"projection_layer_dim\", \"finetune_augmentation\"], axis=1)\n    )\n    df.columns = df.iloc[0].values\n    df = df.iloc[1:]\n    df = df[[\"mean\", \"ci95\"]].astype(float).round(2)\n    df.columns = pd.MultiIndex.from_arrays(\n        [[level1, level1], [level2, level2], df.columns.tolist()]\n    )\n    df = df.loc[[\"test-script\", \"test-human\"]]\n    df.index.name = None\n    return df\n</pre> def load_summary_csv(fname, level1, level2):     df = (         pd.read_csv(fname)         .set_index(\"test_split_name\", drop=True)         .drop([\"with_dropout\", \"projection_layer_dim\", \"finetune_augmentation\"], axis=1)     )     df.columns = df.iloc[0].values     df = df.iloc[1:]     df = df[[\"mean\", \"ci95\"]].astype(float).round(2)     df.columns = pd.MultiIndex.from_arrays(         [[level1, level1], [level2, level2], df.columns.tolist()]     )     df = df.loc[[\"test-script\", \"test-human\"]]     df.index.name = None     return df In\u00a0[18]: Copied! <pre>pd.concat(\n    (\n        load_summary_csv(\n            \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\n            level1=\"Packet loss\",\n            level2=\"Color jitter\",\n        ),\n        load_summary_csv(\n            \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\n            level1=\"Packet loss\",\n            level2=\"Rotate\",\n        ),\n        load_summary_csv(\n            \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\n            level1=\"Change rtt\",\n            level2=\"Color jitter\",\n        ),\n        load_summary_csv(\n            \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\n            level1=\"Change rtt\",\n            level2=\"Rotate\",\n        ),\n        load_summary_csv(\n            \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\n            level1=\"Color jitter\",\n            level2=\"Rotate\",\n        ),\n    ),\n    axis=1,\n)\n</pre> pd.concat(     (         load_summary_csv(             \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",             level1=\"Packet loss\",             level2=\"Color jitter\",         ),         load_summary_csv(             \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",             level1=\"Packet loss\",             level2=\"Rotate\",         ),         load_summary_csv(             \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",             level1=\"Change rtt\",             level2=\"Color jitter\",         ),         load_summary_csv(             \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",             level1=\"Change rtt\",             level2=\"Rotate\",         ),         load_summary_csv(             \"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",             level1=\"Color jitter\",             level2=\"Rotate\",         ),     ),     axis=1, ) Out[18]: Packet loss Change rtt Color jitter Color jitter Rotate Color jitter Rotate Rotate mean ci95 mean ci95 mean ci95 mean ci95 mean ci95 test-script 89.87 0.37 91.79 0.28 91.42 0.36 92.56 0.32 91.79 0.33 test-human 73.84 1.25 71.56 1.21 74.59 1.10 73.43 1.35 70.93 1.18 <p>The pair (\"Time shift\", \"Change rtt\") missing from the table is coming from Table4 results (see :simple-jupyter: <code>table4_simclr_dropout_and_projectionlayer.ipynb</code>)</p>"},{"location":"paper_tables_and_figures/table5_simclr_other_augmentation_pairs/#table-5-comparing-the-fine-tuning-performance-when-using-different-pairs-of-augmentation-for-pretraining","title":"Table 5: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.\u00b6","text":""},{"location":"paper_tables_and_figures/table5_simclr_other_augmentation_pairs/","title":"Table 5: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>def load_summary_csv(fname, level1, level2):\ndf = (\npd.read_csv(fname)\n.set_index(\"test_split_name\", drop=True)\n.drop([\"with_dropout\", \"projection_layer_dim\", \"finetune_augmentation\"], axis=1)\n)\ndf.columns = df.iloc[0].values\ndf = df.iloc[1:]\ndf = df[[\"mean\", \"ci95\"]].astype(float).round(2)\ndf.columns = pd.MultiIndex.from_arrays(\n[[level1, level1], [level2, level2], df.columns.tolist()]\n)\ndf = df.loc[[\"test-script\", \"test-human\"]]\ndf.index.name = None\nreturn df\n</code></pre> <pre><code>pd.concat(\n(\nload_summary_csv(\n\"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\nlevel1=\"Packet loss\",\nlevel2=\"Color jitter\",\n),\nload_summary_csv(\n\"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\nlevel1=\"Packet loss\",\nlevel2=\"Rotate\",\n),\nload_summary_csv(\n\"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\nlevel1=\"Change rtt\",\nlevel2=\"Color jitter\",\n),\nload_summary_csv(\n\"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\nlevel1=\"Change rtt\",\nlevel2=\"Rotate\",\n),\nload_summary_csv(\n\"campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate/campaign_summary/1684886215/summary_flowpic_dim_32.csv\",\nlevel1=\"Color jitter\",\nlevel2=\"Rotate\",\n),\n),\naxis=1,\n)\n</code></pre> Packet loss Change rtt Color jitter Color jitter Rotate Color jitter Rotate Rotate mean ci95 mean ci95 mean ci95 mean ci95 mean ci95 test-script 89.87 0.37 91.79 0.28 91.42 0.36 92.56 0.32 91.79 0.33 test-human 73.84 1.25 71.56 1.21 74.59 1.10 73.43 1.35 70.93 1.18 <p>The pair (\"Time shift\", \"Change rtt\") missing from the table is coming from Table4 results (see  <code>table4_simclr_dropout_and_projectionlayer.ipynb</code>)</p>"},{"location":"paper_tables_and_figures/table6_simclr_larger_trainset/","title":"Table 6: Accuracy on 32x32 flowpic when enlargin training set (w/o Dropout)","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[1]: Copied! <pre>import os\nimport pathlib\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.stats.api as sms\nimport yaml\nfrom aim import Repo\n</pre> import os import pathlib from pathlib import Path  import numpy as np import pandas as pd import statsmodels.stats.api as sms import yaml from aim import Repo In\u00a0[3]: Copied! <pre>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</pre> def compute_confidence_intervals(array, alpha=0.05):     array = np.array(array)     low, high = sms.DescrStatsW(array).tconfint_mean(alpha)     mean = array.mean()     ci = high - mean     return ci In\u00a0[\u00a0]: Copied! <pre>campaign_folder = Path(\n    \"./campaigns/ucdavis-icdm19/larger-trainset/augmentation-at-loading\"\n)\n\nrepo = Repo.from_path(str(campaign_folder))\n\nct_df = pd.DataFrame(\n    columns=[\n        \"hash_id\",\n        \"contrastive_learning_seed\",\n        \"finetune_seed\",\n        \"suppress_dropout\",\n        \"test_set\",\n        \"test_acc\",\n    ]\n)\nsup_df = pd.DataFrame(\n    columns=[\"hash_id\", \"aug_name\", \"seed\", \"suppress_dropout\", \"test_set\", \"test_acc\"]\n)\n\n\nct_nodropout_hashes = []\nsup_nodropout_hashes = []\nfor hash_id in list(repo._all_run_hashes()):\n    path = campaign_folder / \"artifacts\" / hash_id / \"params.yml\"\n    if os.path.isfile(path):\n        conf = yaml.safe_load(path.read_text())\n\n        suppress_dropout = conf[\"suppress_dropout\"]\n\n        human_acc = (\n            pd.read_csv(\n                campaign_folder / \"artifacts\" / hash_id / \"test-human_class_rep.csv\"\n            )\n            .iloc[5]\n            .values[1]\n        )\n\n        script_acc = (\n            pd.read_csv(\n                campaign_folder / \"artifacts\" / hash_id / \"test-script_class_rep.csv\"\n            )\n            .iloc[5]\n            .values[1]\n        )\n\n        if conf[\"aim_experiment_name\"] == \"contrastive-learning-and-finetune\":\n            if suppress_dropout:\n                ct_nodropout_hashes.append(hash_id)\n\n                contrastive_learning_seed = conf[\"contrastive_learning_seed\"]\n                finetune_seed = conf[\"finetune_seed\"]\n\n                new_row_human = {\n                    \"hash_id\": hash_id,\n                    \"contrastive_learning_seed\": contrastive_learning_seed,\n                    \"finetune_seed\": finetune_seed,\n                    \"suppress_dropout\": suppress_dropout,\n                    \"test_set\": \"human\",\n                    \"test_acc\": human_acc * 100,\n                }\n                new_row_script = {\n                    \"hash_id\": hash_id,\n                    \"contrastive_learning_seed\": contrastive_learning_seed,\n                    \"finetune_seed\": finetune_seed,\n                    \"suppress_dropout\": suppress_dropout,\n                    \"test_set\": \"script\",\n                    \"test_acc\": script_acc * 100,\n                }\n\n                ct_df.loc[len(ct_df)] = new_row_human\n                ct_df.loc[len(ct_df)] = new_row_script\n\n        elif conf[\"aim_experiment_name\"] == \"augmentation-at-loading\":\n            if suppress_dropout:\n                sup_nodropout_hashes.append(hash_id)\n\n                seed = conf[\"seed\"]\n\n                aug_name = conf[\"aug_name\"]\n\n                new_row_human = {\n                    \"hash_id\": hash_id,\n                    \"aug_name\": aug_name,\n                    \"seed\": seed,\n                    \"suppress_dropout\": suppress_dropout,\n                    \"test_set\": \"human\",\n                    \"test_acc\": human_acc * 100,\n                }\n                new_row_script = {\n                    \"hash_id\": hash_id,\n                    \"aug_name\": aug_name,\n                    \"seed\": seed,\n                    \"suppress_dropout\": suppress_dropout,\n                    \"test_set\": \"script\",\n                    \"test_acc\": script_acc * 100,\n                }\n\n                sup_df.loc[len(sup_df)] = new_row_human\n                sup_df.loc[len(sup_df)] = new_row_script\n\n        else:  # aim runs rm\n            print(1, hash_id)\n\n    else:  # aim runs rm\n        print(2, hash_id)\n</pre> campaign_folder = Path(     \"./campaigns/ucdavis-icdm19/larger-trainset/augmentation-at-loading\" )  repo = Repo.from_path(str(campaign_folder))  ct_df = pd.DataFrame(     columns=[         \"hash_id\",         \"contrastive_learning_seed\",         \"finetune_seed\",         \"suppress_dropout\",         \"test_set\",         \"test_acc\",     ] ) sup_df = pd.DataFrame(     columns=[\"hash_id\", \"aug_name\", \"seed\", \"suppress_dropout\", \"test_set\", \"test_acc\"] )   ct_nodropout_hashes = [] sup_nodropout_hashes = [] for hash_id in list(repo._all_run_hashes()):     path = campaign_folder / \"artifacts\" / hash_id / \"params.yml\"     if os.path.isfile(path):         conf = yaml.safe_load(path.read_text())          suppress_dropout = conf[\"suppress_dropout\"]          human_acc = (             pd.read_csv(                 campaign_folder / \"artifacts\" / hash_id / \"test-human_class_rep.csv\"             )             .iloc[5]             .values[1]         )          script_acc = (             pd.read_csv(                 campaign_folder / \"artifacts\" / hash_id / \"test-script_class_rep.csv\"             )             .iloc[5]             .values[1]         )          if conf[\"aim_experiment_name\"] == \"contrastive-learning-and-finetune\":             if suppress_dropout:                 ct_nodropout_hashes.append(hash_id)                  contrastive_learning_seed = conf[\"contrastive_learning_seed\"]                 finetune_seed = conf[\"finetune_seed\"]                  new_row_human = {                     \"hash_id\": hash_id,                     \"contrastive_learning_seed\": contrastive_learning_seed,                     \"finetune_seed\": finetune_seed,                     \"suppress_dropout\": suppress_dropout,                     \"test_set\": \"human\",                     \"test_acc\": human_acc * 100,                 }                 new_row_script = {                     \"hash_id\": hash_id,                     \"contrastive_learning_seed\": contrastive_learning_seed,                     \"finetune_seed\": finetune_seed,                     \"suppress_dropout\": suppress_dropout,                     \"test_set\": \"script\",                     \"test_acc\": script_acc * 100,                 }                  ct_df.loc[len(ct_df)] = new_row_human                 ct_df.loc[len(ct_df)] = new_row_script          elif conf[\"aim_experiment_name\"] == \"augmentation-at-loading\":             if suppress_dropout:                 sup_nodropout_hashes.append(hash_id)                  seed = conf[\"seed\"]                  aug_name = conf[\"aug_name\"]                  new_row_human = {                     \"hash_id\": hash_id,                     \"aug_name\": aug_name,                     \"seed\": seed,                     \"suppress_dropout\": suppress_dropout,                     \"test_set\": \"human\",                     \"test_acc\": human_acc * 100,                 }                 new_row_script = {                     \"hash_id\": hash_id,                     \"aug_name\": aug_name,                     \"seed\": seed,                     \"suppress_dropout\": suppress_dropout,                     \"test_set\": \"script\",                     \"test_acc\": script_acc * 100,                 }                  sup_df.loc[len(sup_df)] = new_row_human                 sup_df.loc[len(sup_df)] = new_row_script          else:  # aim runs rm             print(1, hash_id)      else:  # aim runs rm         print(2, hash_id) In\u00a0[51]: Copied! <pre>sup_df_merged = sup_df.groupby(\n    [\n        \"suppress_dropout\",\n        \"aug_name\",\n        \"test_set\",\n    ]\n).agg({\"test_acc\": [\"mean\", compute_confidence_intervals]})\nsup_df_merged = sup_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"})\nsup_df_merged = sup_df_merged.droplevel(0, axis=1)\n\nsup_df_merged = sup_df_merged.droplevel(0).reset_index()\n</pre> sup_df_merged = sup_df.groupby(     [         \"suppress_dropout\",         \"aug_name\",         \"test_set\",     ] ).agg({\"test_acc\": [\"mean\", compute_confidence_intervals]}) sup_df_merged = sup_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"}) sup_df_merged = sup_df_merged.droplevel(0, axis=1)  sup_df_merged = sup_df_merged.droplevel(0).reset_index() In\u00a0[52]: Copied! <pre>sup_df_merged = sup_df_merged.pivot(\n    index=\"aug_name\", columns=\"test_set\", values=[\"mean\", \"ci95\"]\n)\nsup_df_merged = sup_df_merged[\n    [(\"mean\", \"script\"), (\"ci95\", \"script\"), (\"mean\", \"human\"), (\"ci95\", \"human\")]\n]\nsup_df_merged.columns = pd.MultiIndex.from_tuples(\n    tpl[::-1] for tpl in sup_df_merged.columns\n)\n</pre> sup_df_merged = sup_df_merged.pivot(     index=\"aug_name\", columns=\"test_set\", values=[\"mean\", \"ci95\"] ) sup_df_merged = sup_df_merged[     [(\"mean\", \"script\"), (\"ci95\", \"script\"), (\"mean\", \"human\"), (\"ci95\", \"human\")] ] sup_df_merged.columns = pd.MultiIndex.from_tuples(     tpl[::-1] for tpl in sup_df_merged.columns ) In\u00a0[54]: Copied! <pre>AUGMENTATIONS_ORDER = [\n    \"noaug\",\n    \"rotate\",\n    \"horizontalflip\",\n    \"colorjitter\",\n    \"packetloss\",\n    \"timeshift\",\n    \"changertt\",\n]\n\nRENAME = {\n    \"noaug\": \"No augmentation\",\n    \"changertt\": \"Change rtt\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"colorjitter\": \"Color jitter\",\n    \"packetloss\": \"Packet loss\",\n    \"rotate\": \"Rotate\",\n    \"timeshift\": \"Time shift\",\n}\n\nprint(\"=== supervised augmentation (at loading) ===\")\nsup_df_merged.loc[AUGMENTATIONS_ORDER].rename(RENAME).round(2)\n</pre> AUGMENTATIONS_ORDER = [     \"noaug\",     \"rotate\",     \"horizontalflip\",     \"colorjitter\",     \"packetloss\",     \"timeshift\",     \"changertt\", ]  RENAME = {     \"noaug\": \"No augmentation\",     \"changertt\": \"Change rtt\",     \"horizontalflip\": \"Horizontal flip\",     \"colorjitter\": \"Color jitter\",     \"packetloss\": \"Packet loss\",     \"rotate\": \"Rotate\",     \"timeshift\": \"Time shift\", }  print(\"=== supervised augmentation (at loading) ===\") sup_df_merged.loc[AUGMENTATIONS_ORDER].rename(RENAME).round(2) <pre>=== supervised augmentation (at loading) ===\n</pre> Out[54]: script human mean ci95 mean ci95 aug_name No augmentation 98.40 0.21 73.31 1.01 Rotate 98.53 0.13 74.16 1.22 Horizontal flip 98.20 0.18 74.52 1.15 Color jitter 98.43 0.32 72.71 0.64 Packet loss 98.70 0.19 72.89 0.59 Time shift 98.70 0.26 72.71 0.97 Change rtt 98.53 0.13 71.45 1.04 In\u00a0[30]: Copied! <pre>ct_df_merged = ct_df.groupby([\"suppress_dropout\", \"test_set\"]).agg(\n    {\"test_acc\": [\"mean\", compute_confidence_intervals]}\n)\nct_df_merged = ct_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"})\nct_df_merged = ct_df_merged.droplevel(0, axis=1).droplevel(0)\n\nprint(\"=== using simclr and finetuning ===\")\nct_df_merged.loc[[\"script\", \"human\"]].round(2)\n</pre> ct_df_merged = ct_df.groupby([\"suppress_dropout\", \"test_set\"]).agg(     {\"test_acc\": [\"mean\", compute_confidence_intervals]} ) ct_df_merged = ct_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"}) ct_df_merged = ct_df_merged.droplevel(0, axis=1).droplevel(0)  print(\"=== using simclr and finetuning ===\") ct_df_merged.loc[[\"script\", \"human\"]].round(2) <pre>=== using simclr and finetuning ===\n</pre> Out[30]: mean ci95 test_set script 94.10 0.48 human 80.61 2.96"},{"location":"paper_tables_and_figures/table6_simclr_larger_trainset/#table-6-accuracy-on-32x32-flowpic-when-enlargin-training-set-wo-dropout","title":"Table 6: Accuracy on 32x32 flowpic when enlargin training set (w/o Dropout)\u00b6","text":""},{"location":"paper_tables_and_figures/table6_simclr_larger_trainset/","title":"Table 6: Accuracy on 32x32 flowpic when enlargin training set (w/o Dropout)","text":"<pre><code>import os\nimport pathlib\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport statsmodels.stats.api as sms\nimport yaml\nfrom aim import Repo\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\narray = np.array(array)\nlow, high = sms.DescrStatsW(array).tconfint_mean(alpha)\nmean = array.mean()\nci = high - mean\nreturn ci\n</code></pre> <pre><code>campaign_folder = Path(\n\"./campaigns/ucdavis-icdm19/larger-trainset/augmentation-at-loading\"\n)\nrepo = Repo.from_path(str(campaign_folder))\nct_df = pd.DataFrame(\ncolumns=[\n\"hash_id\",\n\"contrastive_learning_seed\",\n\"finetune_seed\",\n\"suppress_dropout\",\n\"test_set\",\n\"test_acc\",\n]\n)\nsup_df = pd.DataFrame(\ncolumns=[\"hash_id\", \"aug_name\", \"seed\", \"suppress_dropout\", \"test_set\", \"test_acc\"]\n)\nct_nodropout_hashes = []\nsup_nodropout_hashes = []\nfor hash_id in list(repo._all_run_hashes()):\npath = campaign_folder / \"artifacts\" / hash_id / \"params.yml\"\nif os.path.isfile(path):\nconf = yaml.safe_load(path.read_text())\nsuppress_dropout = conf[\"suppress_dropout\"]\nhuman_acc = (\npd.read_csv(\ncampaign_folder / \"artifacts\" / hash_id / \"test-human_class_rep.csv\"\n)\n.iloc[5]\n.values[1]\n)\nscript_acc = (\npd.read_csv(\ncampaign_folder / \"artifacts\" / hash_id / \"test-script_class_rep.csv\"\n)\n.iloc[5]\n.values[1]\n)\nif conf[\"aim_experiment_name\"] == \"contrastive-learning-and-finetune\":\nif suppress_dropout:\nct_nodropout_hashes.append(hash_id)\ncontrastive_learning_seed = conf[\"contrastive_learning_seed\"]\nfinetune_seed = conf[\"finetune_seed\"]\nnew_row_human = {\n\"hash_id\": hash_id,\n\"contrastive_learning_seed\": contrastive_learning_seed,\n\"finetune_seed\": finetune_seed,\n\"suppress_dropout\": suppress_dropout,\n\"test_set\": \"human\",\n\"test_acc\": human_acc * 100,\n}\nnew_row_script = {\n\"hash_id\": hash_id,\n\"contrastive_learning_seed\": contrastive_learning_seed,\n\"finetune_seed\": finetune_seed,\n\"suppress_dropout\": suppress_dropout,\n\"test_set\": \"script\",\n\"test_acc\": script_acc * 100,\n}\nct_df.loc[len(ct_df)] = new_row_human\nct_df.loc[len(ct_df)] = new_row_script\nelif conf[\"aim_experiment_name\"] == \"augmentation-at-loading\":\nif suppress_dropout:\nsup_nodropout_hashes.append(hash_id)\nseed = conf[\"seed\"]\naug_name = conf[\"aug_name\"]\nnew_row_human = {\n\"hash_id\": hash_id,\n\"aug_name\": aug_name,\n\"seed\": seed,\n\"suppress_dropout\": suppress_dropout,\n\"test_set\": \"human\",\n\"test_acc\": human_acc * 100,\n}\nnew_row_script = {\n\"hash_id\": hash_id,\n\"aug_name\": aug_name,\n\"seed\": seed,\n\"suppress_dropout\": suppress_dropout,\n\"test_set\": \"script\",\n\"test_acc\": script_acc * 100,\n}\nsup_df.loc[len(sup_df)] = new_row_human\nsup_df.loc[len(sup_df)] = new_row_script\nelse:  # aim runs rm\nprint(1, hash_id)\nelse:  # aim runs rm\nprint(2, hash_id)\n</code></pre> <pre><code>sup_df_merged = sup_df.groupby(\n[\n\"suppress_dropout\",\n\"aug_name\",\n\"test_set\",\n]\n).agg({\"test_acc\": [\"mean\", compute_confidence_intervals]})\nsup_df_merged = sup_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"})\nsup_df_merged = sup_df_merged.droplevel(0, axis=1)\nsup_df_merged = sup_df_merged.droplevel(0).reset_index()\n</code></pre> <pre><code>sup_df_merged = sup_df_merged.pivot(\nindex=\"aug_name\", columns=\"test_set\", values=[\"mean\", \"ci95\"]\n)\nsup_df_merged = sup_df_merged[\n[(\"mean\", \"script\"), (\"ci95\", \"script\"), (\"mean\", \"human\"), (\"ci95\", \"human\")]\n]\nsup_df_merged.columns = pd.MultiIndex.from_tuples(\ntpl[::-1] for tpl in sup_df_merged.columns\n)\n</code></pre> <pre><code>AUGMENTATIONS_ORDER = [\n\"noaug\",\n\"rotate\",\n\"horizontalflip\",\n\"colorjitter\",\n\"packetloss\",\n\"timeshift\",\n\"changertt\",\n]\nRENAME = {\n\"noaug\": \"No augmentation\",\n\"changertt\": \"Change rtt\",\n\"horizontalflip\": \"Horizontal flip\",\n\"colorjitter\": \"Color jitter\",\n\"packetloss\": \"Packet loss\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n}\nprint(\"=== supervised augmentation (at loading) ===\")\nsup_df_merged.loc[AUGMENTATIONS_ORDER].rename(RENAME).round(2)\n</code></pre> <pre><code>=== supervised augmentation (at loading) ===\n</code></pre> script human mean ci95 mean ci95 aug_name No augmentation 98.40 0.21 73.31 1.01 Rotate 98.53 0.13 74.16 1.22 Horizontal flip 98.20 0.18 74.52 1.15 Color jitter 98.43 0.32 72.71 0.64 Packet loss 98.70 0.19 72.89 0.59 Time shift 98.70 0.26 72.71 0.97 Change rtt 98.53 0.13 71.45 1.04 <pre><code>ct_df_merged = ct_df.groupby([\"suppress_dropout\", \"test_set\"]).agg(\n{\"test_acc\": [\"mean\", compute_confidence_intervals]}\n)\nct_df_merged = ct_df_merged.rename(columns={\"compute_confidence_intervals\": \"ci95\"})\nct_df_merged = ct_df_merged.droplevel(0, axis=1).droplevel(0)\nprint(\"=== using simclr and finetuning ===\")\nct_df_merged.loc[[\"script\", \"human\"]].round(2)\n</code></pre> <pre><code>=== using simclr and finetuning ===\n</code></pre> mean ci95 test_set script 94.10 0.48 human 80.61 2.96"},{"location":"paper_tables_and_figures/table7_augmentation-at-loading_on_other_datasets/","title":"Table 7: (G3) Data augmentation in supervised setting on other datasets.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[46]: Copied! <pre>import pathlib\n\nimport pandas as pd\n\nAUGMENTATIONS_ORDER = [\n    \"noaug\",\n    \"rotate\",\n    \"horizontalflip\",\n    \"colorjitter\",\n    \"packetloss\",\n    \"timeshift\",\n    \"changertt\",\n]\n\nRENAME = {\n    \"noaug\": \"No augmentation\",\n    \"changertt\": \"Change rtt\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"colorjitter\": \"Color jitter\",\n    \"packetloss\": \"Packet loss\",\n    \"rotate\": \"Rotate\",\n    \"timeshift\": \"Time shift\",\n}\n</pre> import pathlib  import pandas as pd  AUGMENTATIONS_ORDER = [     \"noaug\",     \"rotate\",     \"horizontalflip\",     \"colorjitter\",     \"packetloss\",     \"timeshift\",     \"changertt\", ]  RENAME = {     \"noaug\": \"No augmentation\",     \"changertt\": \"Change rtt\",     \"horizontalflip\": \"Horizontal flip\",     \"colorjitter\": \"Color jitter\",     \"packetloss\": \"Packet loss\",     \"rotate\": \"Rotate\",     \"timeshift\": \"Time shift\", } In\u00a0[44]: Copied! <pre>def load_summary_report(fname, level0):\n    df = (\n        pd.read_csv(fname)\n        .drop(\"test_split_name\", axis=1)\n        .set_index(\"aug_name\", drop=True)\n    )\n    df = df[[\"f1\", \"f1.1\", \"f1.2\", \"f1.3\"]]\n    df.columns = df.iloc[0].values\n    df = df.iloc[1:]\n    df = df[[\"mean\", \"ci95\"]].astype(float).round(4) * 100\n    df = df.loc[AUGMENTATIONS_ORDER].rename(RENAME)\n    df.columns = pd.MultiIndex.from_arrays([[level0, level0], df.columns])\n\n    return df\n</pre> def load_summary_report(fname, level0):     df = (         pd.read_csv(fname)         .drop(\"test_split_name\", axis=1)         .set_index(\"aug_name\", drop=True)     )     df = df[[\"f1\", \"f1.1\", \"f1.2\", \"f1.3\"]]     df.columns = df.iloc[0].values     df = df.iloc[1:]     df = df[[\"mean\", \"ci95\"]].astype(float).round(4) * 100     df = df.loc[AUGMENTATIONS_ORDER].rename(RENAME)     df.columns = pd.MultiIndex.from_arrays([[level0, level0], df.columns])      return df In\u00a0[45]: Copied! <pre>pd.concat(\n    (\n        load_summary_report(\n            \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n            \"mirage22 - minpkts10\",\n        ),\n        load_summary_report(\n            \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n            \"mirage22 - minpkts1000\",\n        ),\n        load_summary_report(\n            \"campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n            \"utmobilenet21 - minpkts10\",\n        ),\n        load_summary_report(\n            \"campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n            \"mirage19 - minpkts10\",\n        ),\n    ),\n    axis=1,\n)\n</pre> pd.concat(     (         load_summary_report(             \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",             \"mirage22 - minpkts10\",         ),         load_summary_report(             \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",             \"mirage22 - minpkts1000\",         ),         load_summary_report(             \"campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",             \"utmobilenet21 - minpkts10\",         ),         load_summary_report(             \"campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",             \"mirage19 - minpkts10\",         ),     ),     axis=1, ) Out[45]: mirage22 - minpkts10 mirage22 - minpkts1000 utmobilenet21 - minpkts10 mirage19 - minpkts10 mean ci95 mean ci95 mean ci95 mean ci95 aug_name No augmentation 90.31 1.08 79.85 2.80 84.23 1.82 69.61 1.62 Rotate 88.95 1.32 86.54 2.84 88.29 1.04 61.20 1.39 Horizontal flip 91.07 0.84 81.47 3.17 85.93 1.89 70.35 1.41 Color jitter 89.40 1.57 80.69 2.94 86.22 2.22 67.48 1.71 Packet loss 91.91 0.96 84.20 3.79 85.79 1.23 67.50 1.91 Time shift 92.53 0.84 84.67 3.71 88.64 1.03 70.68 1.64 Change rtt 94.11 0.75 90.96 1.77 88.55 1.63 74.07 1.48"},{"location":"paper_tables_and_figures/table7_augmentation-at-loading_on_other_datasets/#table-7-g3-data-augmentation-in-supervised-setting-on-other-datasets","title":"Table 7: (G3) Data augmentation in supervised setting on other datasets.\u00b6","text":""},{"location":"paper_tables_and_figures/table7_augmentation-at-loading_on_other_datasets/","title":"Table 7: (G3) Data augmentation in supervised setting on other datasets.","text":"<pre><code>import pathlib\nimport pandas as pd\nAUGMENTATIONS_ORDER = [\n\"noaug\",\n\"rotate\",\n\"horizontalflip\",\n\"colorjitter\",\n\"packetloss\",\n\"timeshift\",\n\"changertt\",\n]\nRENAME = {\n\"noaug\": \"No augmentation\",\n\"changertt\": \"Change rtt\",\n\"horizontalflip\": \"Horizontal flip\",\n\"colorjitter\": \"Color jitter\",\n\"packetloss\": \"Packet loss\",\n\"rotate\": \"Rotate\",\n\"timeshift\": \"Time shift\",\n}\n</code></pre> <pre><code>def load_summary_report(fname, level0):\ndf = (\npd.read_csv(fname)\n.drop(\"test_split_name\", axis=1)\n.set_index(\"aug_name\", drop=True)\n)\ndf = df[[\"f1\", \"f1.1\", \"f1.2\", \"f1.3\"]]\ndf.columns = df.iloc[0].values\ndf = df.iloc[1:]\ndf = df[[\"mean\", \"ci95\"]].astype(float).round(4) * 100\ndf = df.loc[AUGMENTATIONS_ORDER].rename(RENAME)\ndf.columns = pd.MultiIndex.from_arrays([[level0, level0], df.columns])\nreturn df\n</code></pre> <pre><code>pd.concat(\n(\nload_summary_report(\n\"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n\"mirage22 - minpkts10\",\n),\nload_summary_report(\n\"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n\"mirage22 - minpkts1000\",\n),\nload_summary_report(\n\"campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n\"utmobilenet21 - minpkts10\",\n),\nload_summary_report(\n\"campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/1684958367/summary_flowpic_dim_32.csv\",\n\"mirage19 - minpkts10\",\n),\n),\naxis=1,\n)\n</code></pre> mirage22 - minpkts10 mirage22 - minpkts1000 utmobilenet21 - minpkts10 mirage19 - minpkts10 mean ci95 mean ci95 mean ci95 mean ci95 aug_name No augmentation 90.31 1.08 79.85 2.80 84.23 1.82 69.61 1.62 Rotate 88.95 1.32 86.54 2.84 88.29 1.04 61.20 1.39 Horizontal flip 91.07 0.84 81.47 3.17 85.93 1.89 70.35 1.41 Color jitter 89.40 1.57 80.69 2.94 86.22 2.22 67.48 1.71 Packet loss 91.91 0.96 84.20 3.79 85.79 1.23 67.50 1.91 Time shift 92.53 0.84 84.67 3.71 88.64 1.03 70.68 1.64 Change rtt 94.11 0.75 90.96 1.77 88.55 1.63 74.07 1.48"},{"location":"paper_tables_and_figures/table8_icdm_finetuning_per_class_metrics_on_human/","title":"Table 8: Macro-average Accuracy with different retraining dataset and different sampling methods","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[81]: Copied! <pre>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</pre> import pathlib  import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import statsmodels.stats.api as sms  %matplotlib inline %config InlineBackend.figure_format='retina' In\u00a0[82]: Copied! <pre>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</pre> def compute_confidence_intervals(array, alpha=0.05):     array = np.array(array)     low, high = sms.DescrStatsW(array).tconfint_mean(alpha)     mean = array.mean()     ci = high - mean     return ci In\u00a0[83]: Copied! <pre>path = pathlib.Path(\"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/\")\n\nclass_repss = list(path.glob(\"*10/\"))\n</pre> path = pathlib.Path(\"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/\")  class_repss = list(path.glob(\"*10/\")) In\u00a0[84]: Copied! <pre>data = dict()\n\nfor path in class_repss:\n    if \"script\" in str(path):\n        class_reps = list(path.glob(\"*class_rep.csv\"))\n        accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\n\n        augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\n        data[augmentation_name] = (\n            np.mean(accs) * 100,\n            compute_confidence_intervals(accs),\n        )\n\ndf_script = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_script.columns = pd.MultiIndex.from_arrays([[\"script\", \"script\"], df_script.columns])\n# df_script\n</pre> data = dict()  for path in class_repss:     if \"script\" in str(path):         class_reps = list(path.glob(\"*class_rep.csv\"))         accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]          augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")         data[augmentation_name] = (             np.mean(accs) * 100,             compute_confidence_intervals(accs),         )  df_script = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2) df_script.columns = pd.MultiIndex.from_arrays([[\"script\", \"script\"], df_script.columns]) # df_script In\u00a0[85]: Copied! <pre>data = dict()\nfor path in class_repss:\n    if \"human\" in str(path):\n        class_reps = list(path.glob(\"*class_rep.csv\"))\n        accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\n\n        augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\n        data[augmentation_name] = (\n            np.mean(accs) * 100,\n            compute_confidence_intervals(accs),\n        )\n\ndf_human = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_human.columns = pd.MultiIndex.from_arrays([[\"human\", \"human\"], df_human.columns])\n</pre> data = dict() for path in class_repss:     if \"human\" in str(path):         class_reps = list(path.glob(\"*class_rep.csv\"))         accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]          augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")         data[augmentation_name] = (             np.mean(accs) * 100,             compute_confidence_intervals(accs),         )  df_human = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2) df_human.columns = pd.MultiIndex.from_arrays([[\"human\", \"human\"], df_human.columns]) In\u00a0[86]: Copied! <pre>pd.concat((df_script, df_human), axis=1).T\n</pre> pd.concat((df_script, df_human), axis=1).T Out[86]: FixedStep Random Incremental script mean 87.11 94.63 96.22 ci95 0.09 0.02 0.01 human mean 82.60 87.29 92.56 ci95 0.03 0.04 0.03"},{"location":"paper_tables_and_figures/table8_icdm_finetuning_per_class_metrics_on_human/#table-8-macro-average-accuracy-with-different-retraining-dataset-and-different-sampling-methods","title":"Table 8: Macro-average Accuracy with different retraining dataset and different sampling methods\u00b6","text":""},{"location":"paper_tables_and_figures/table8_icdm_finetuning_per_class_metrics_on_human/","title":"Table 8: Macro-average Accuracy with different retraining dataset and different sampling methods","text":"<pre><code>import pathlib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\narray = np.array(array)\nlow, high = sms.DescrStatsW(array).tconfint_mean(alpha)\nmean = array.mean()\nci = high - mean\nreturn ci\n</code></pre> <pre><code>path = pathlib.Path(\"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/\")\nclass_repss = list(path.glob(\"*10/\"))\n</code></pre> <pre><code>data = dict()\nfor path in class_repss:\nif \"script\" in str(path):\nclass_reps = list(path.glob(\"*class_rep.csv\"))\naccs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\naugmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\ndata[augmentation_name] = (\nnp.mean(accs) * 100,\ncompute_confidence_intervals(accs),\n)\ndf_script = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_script.columns = pd.MultiIndex.from_arrays([[\"script\", \"script\"], df_script.columns])\n# df_script\n</code></pre> <pre><code>data = dict()\nfor path in class_repss:\nif \"human\" in str(path):\nclass_reps = list(path.glob(\"*class_rep.csv\"))\naccs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\naugmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\ndata[augmentation_name] = (\nnp.mean(accs) * 100,\ncompute_confidence_intervals(accs),\n)\ndf_human = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_human.columns = pd.MultiIndex.from_arrays([[\"human\", \"human\"], df_human.columns])\n</code></pre> <pre><code>pd.concat((df_script, df_human), axis=1).T\n</code></pre> FixedStep Random Incremental script mean 87.11 94.63 96.22 ci95 0.09 0.02 0.01 human mean 82.60 87.29 92.56 ci95 0.03 0.04 0.03"},{"location":"paper_tables_and_figures/table9_ucdavis-icdm19_tukey/","title":"Table 9: Performance comparison across augmentations for different flowpic sizes.","text":"<p>:simple-jupyter: :material-download:</p> In\u00a0[25]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[13]: Copied! <pre>from scipy.stats import tukey_hsd\n</pre> from scipy.stats import tukey_hsd In\u00a0[3]: Copied! <pre>df = pd.read_parquet('campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet')\n</pre> df = pd.read_parquet('campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet') In\u00a0[17]: Copied! <pre>df_script = df[df['test_split_name'] == 'test-script']\n\nacc_32 = df_script[df_script['flowpic_dim'] == 32]['acc'].values.tolist()\nacc_64 = df_script[df_script['flowpic_dim'] == 64]['acc'].values.tolist()\nacc_1500 = df_script[df_script['flowpic_dim'] == 1500]['acc'].values.tolist()\n</pre> df_script = df[df['test_split_name'] == 'test-script']  acc_32 = df_script[df_script['flowpic_dim'] == 32]['acc'].values.tolist() acc_64 = df_script[df_script['flowpic_dim'] == 64]['acc'].values.tolist() acc_1500 = df_script[df_script['flowpic_dim'] == 1500]['acc'].values.tolist() In\u00a0[19]: Copied! <pre>res = tukey_hsd(acc_32, acc_64, acc_1500)\n</pre> res = tukey_hsd(acc_32, acc_64, acc_1500) In\u00a0[29]: Copied! <pre>df = pd.DataFrame(np.array([res.pvalue[0, 1], res.pvalue[0, 2], res.pvalue[1,2]]).reshape(-1, 1), columns=['pvalue'],\n            index=pd.MultiIndex.from_arrays([('32x32', '32x32', '64x64'), ('64x64', '1500x1500', '1500x1500')]))\ndf = df.assign(is_different=df['pvalue']&lt;0.05)\n</pre> df = pd.DataFrame(np.array([res.pvalue[0, 1], res.pvalue[0, 2], res.pvalue[1,2]]).reshape(-1, 1), columns=['pvalue'],             index=pd.MultiIndex.from_arrays([('32x32', '32x32', '64x64'), ('64x64', '1500x1500', '1500x1500')])) df = df.assign(is_different=df['pvalue']&lt;0.05) In\u00a0[30]: Copied! <pre>df\n</pre> df Out[30]: pvalue is_different 32x32 64x64 9.380580e-01 False 1500x1500 3.718318e-07 True 64x64 1500x1500 6.270783e-08 True"},{"location":"paper_tables_and_figures/table9_ucdavis-icdm19_tukey/#table-9-performance-comparison-across-augmentations-for-different-flowpic-sizes","title":"Table 9: Performance comparison across augmentations for different flowpic sizes.\u00b6","text":""},{"location":"paper_tables_and_figures/table9_ucdavis-icdm19_tukey/","title":"Table 9: Performance comparison across augmentations for different flowpic sizes.","text":"<pre><code>import pandas as pd\nimport numpy as np\n</code></pre> <pre><code>from scipy.stats import tukey_hsd\n</code></pre> <pre><code>df = pd.read_parquet('campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet')\n</code></pre> <pre><code>df_script = df[df['test_split_name'] == 'test-script']\nacc_32 = df_script[df_script['flowpic_dim'] == 32]['acc'].values.tolist()\nacc_64 = df_script[df_script['flowpic_dim'] == 64]['acc'].values.tolist()\nacc_1500 = df_script[df_script['flowpic_dim'] == 1500]['acc'].values.tolist()\n</code></pre> <pre><code>res = tukey_hsd(acc_32, acc_64, acc_1500)\n</code></pre> <pre><code>df = pd.DataFrame(np.array([res.pvalue[0, 1], res.pvalue[0, 2], res.pvalue[1,2]]).reshape(-1, 1), columns=['pvalue'],\nindex=pd.MultiIndex.from_arrays([('32x32', '32x32', '64x64'), ('64x64', '1500x1500', '1500x1500')]))\ndf = df.assign(is_different=df['pvalue']&lt;0.05)\n</code></pre> <pre><code>df\n</code></pre> pvalue is_different 32x32 64x64 9.380580e-01 False 1500x1500 3.718318e-07 True 64x64 1500x1500 6.270783e-08 True"},{"location":"tcbench/overview/","title":"Overview","text":""},{"location":"tcbench/overview/#install-tcbench","title":"Install <code>tcbench</code>","text":"<p>First prepare a python virtual environment, for example via  conda <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> <p>Grab the latest <code>code_artifacts_paper132.tgz</code>  from  figshare and unpack it. It contains a folder <code>/code_artifacts_paper132</code> from which you can trigger the installation.</p> <pre><code>cd code_artifacts_paper132\npython -m pip install .\n</code></pre> <p>All dependecies are automatically installed.</p> <pre><code>tcbench --version\n</code></pre> <p>Output</p> <pre><code>version: 0.0.16\n</code></pre>"},{"location":"tcbench/overview/#tcbench-internals","title":"<code>tcbench</code> internals","text":"<p><code>tcbench</code> is composed by 3 collections of modules:</p> <ul> <li> <p><code>cli</code> contains modules for composing the command line interfaces using click and  rich.</p> </li> <li> <p><code>libtcdatasets</code> contains modules for  datasets curation.</p> </li> <li> <p><code>modeling</code> contains modules for DL/ML modeling.</p> </li> </ul>"},{"location":"tcbench/overview/#cli","title":"<code>cli</code>","text":"<p>The entry point of <code>tcbench</code> is the script <code>tcbench/cli/main.py</code>: This is the root of the click prompt.</p> <p>The different sub-commands are organized in separate modules</p> <ul> <li><code>cli.command_aimrepo</code> for the <code>aimrepo</code> sub-command.</li> <li><code>cli.command_campaign</code> for the <code>campaign</code> sub-command.</li> <li><code>cli.command_datasets</code> for the <code>datasets</code> sub-command.</li> <li><code>cli.command_singlerun</code> for the <code>run</code> sub-command.</li> </ul> <p>while <code>cli.clickutils</code> and <code>cli.richutils</code> collects some utility functions used for formatting the CLI and its output.</p>"},{"location":"tcbench/overview/#libtcdatasets","title":"<code>libtcdatasets</code>","text":"<p>A module <code>datasets_utils</code> contains general utility function used across the other modules in this group.</p> <p>The remaining are pairs of modules each associated to  a different datasets.</p> <ul> <li> <p><code>XYZ_to_parquet</code> modules convert the original raw data into the curated format </p> </li> <li> <p><code>XYZ_generate_splits</code> modules split the data into train/val/test splits.</p> </li> </ul> <p>For instance, for <code>ucdavis-icdm19</code> the two modules are <code>ucdavis_icdm19_csv_to_parquet</code> and <code>ucdavis_icdm19_generate_splits</code>. Please refer to datasets install for more details about this pre-processing steps.</p> <p>These module pairs are designed to be also usable from the command line. As a matter of fact, the curation triggered from the <code>tcbench</code> CLI is just a wrapper around the lower level command line parser.</p> <p>For instance <pre><code>cd code_artifacts_paper132\npython tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py --help\n</code></pre></p> <p>Output</p> <pre><code>usage: ucdavis_icdm19_csv_to_parquet.py [-h] --input-folder INPUT_FOLDER\n                                        [--output-folder OUTPUT_FOLDER]\n                                        [--num-workers NUM_WORKERS]\n\noptions:\n  -h, --help            show this help message and exit\n  --input-folder INPUT_FOLDER, -i INPUT_FOLDER\n                        Root folder of UCDavis dataset\n  --output-folder OUTPUT_FOLDER, -o OUTPUT_FOLDER\n                        Folder where to save output parquet files (default:\n                        datasets/ucdavis-icdm19)\n  --num-workers NUM_WORKERS, -w NUM_WORKERS\n                        Number of workers for parallel loading (default: 4)\n</code></pre> <p>Important</p> <p>We discourage the direct use of these lower level modules in favor of the global <code>tcbench</code> CLI which automatically handles configurations so to have a uniform installation across datasets.</p>"},{"location":"tcbench/overview/#modeling","title":"<code>modeling</code>","text":"<p>These modules in this group handle DL/ML model training.</p> <ul> <li><code>modeling.aimutils</code> collects utility functions related to AIM repositories.</li> <li><code>modeling.augmentation</code> collects classes and functions related to data augmentation.</li> <li><code>modeling.backbone</code> collects classes and functions for DL architectures.</li> <li><code>modeling.dataprep</code> collects classes and functions related to data loading and preparation.</li> <li><code>modeling.losses</code> collectsion functions for SimCLR losses.</li> <li><code>modeling.methods</code> collects classes handling training.</li> <li><code>modeling.utils</code> collects complementary utility functions.</li> </ul> <p>These modules are \"glued\" together into two sub-group utilities</p> <ul> <li><code>run_&lt;XYZ&gt;</code> are modules triggering runs</li> <li><code>run_campaign_&lt;XYZ&gt;</code> are modules tringgering campaigns</li> </ul> <p>Both these module types work also as script and can be invoked on the command line.</p> <p>For instance with the following we can trigger individual run to investigate the role of different augmentations. This is equivalent to use <code>tcbench run augment-at-loading</code>.</p> <pre><code>python tcbench/modeling/run_augmentations_at_loading.py --help\n</code></pre> <p>Output</p> <pre><code>usage: run_augmentations_at_loading.py [-h] [--artifacts-folder ARTIFACTS_FOLDER]\n                                       [--workers WORKERS] [--gpu-index GPU_INDEX]\n                                       [--aim-repo AIM_REPO]\n                                       [--aim-experiment-name AIM_EXPERIMENT_NAME]\n                                       [--final] [--flowpic-dim {32,64,1500}]\n                                       [--flowpic-block-duration FLOWPIC_BLOCK_DURATION]\n                                       [--dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}]\n                                       [--dataset-minpkts {-1,10,100,1000}]\n                                       [--split-index SPLIT_INDEX]\n                                       [--max-samples-per-class MAX_SAMPLES_PER_CLASS]\n                                       [--train-val-split-ratio TRAIN_VAL_SPLIT_RATIO]\n                                       [--aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}]\n                                       [--suppress-val-augmentation] [--seed SEED]\n                                       [--batch-size BATCH_SIZE]\n                                       [--patience-steps PATIENCE_STEPS]\n                                       [--learning-rate LEARNING_RATE] [--epochs EPOCHS]\n                                       [--suppress-test-train-val-leftover]\n                                       [--suppress-dropout]\n\noptions:\n  -h, --help            show this help message and exit\n  --artifacts-folder ARTIFACTS_FOLDER\n                        Artifact folder (default: aim-repo/artifacts)\n  --workers WORKERS     Number of parallel worker for loading the data (default: 20)\n  --gpu-index GPU_INDEX\n                        The GPU id to use (default: 0)\n  --aim-repo AIM_REPO   Local aim folder or URL of AIM remote server (default: aim-repo)\n  --aim-experiment-name AIM_EXPERIMENT_NAME\n                        The name of the experiment for AIM tracking (default: augmentation-\n                        at-loading)\n  --final\n  --flowpic-dim {32,64,1500}\n                        Flowpic dimension (default: 32)\n  --flowpic-block-duration FLOWPIC_BLOCK_DURATION\n                        Flowpic block duration (in seconds) (default: 15)\n  --dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}\n                        Dataset to use for modeling (default: ucdavis-icdm19)\n  --dataset-minpkts {-1,10,100,1000}\n                        When used in combination with --dataset can refine the dataset and\n                        split to use for modeling (default: -1)\n  --split-index SPLIT_INDEX\n                        Datasplit index (default: 0)\n  --max-samples-per-class MAX_SAMPLES_PER_CLASS\n                        Activated when --split-index is -1 to define how many samples to\n                        select for train+val (with a 80/20 split between train and val\n                        (default: -1)\n  --train-val-split-ratio TRAIN_VAL_SPLIT_RATIO\n                        Training train/val split (default: 0.8)\n  --aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}\n                        Augmentation policy (default: noaug)\n  --suppress-val-augmentation\n                        Do not augment validation set (default: False)\n  --seed SEED           Random seed (default: 12345)\n  --batch-size BATCH_SIZE\n                        Training batch size (default: 64)\n  --patience-steps PATIENCE_STEPS\n  --learning-rate LEARNING_RATE\n                        Traning learning rate (default: 0.001)\n  --epochs EPOCHS       Number of epochs for training (default: 50)\n  --suppress-test-train-val-leftover\n                        Skip test on leftover split (default: False)\n  --suppress-dropout    Mask dropout layers with Identity (default: False)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/","title":"clickutils","text":""},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.compose_help_string_from_list","title":"<code>compose_help_string_from_list(items)</code>","text":"<p>Compose a string from a list</p> Source code in <code>tcbench/cli/clickutils.py</code> <pre><code>def compose_help_string_from_list(items:List[str]) -&gt; str:\n\"\"\"Compose a string from a list\"\"\"\nreturn \"\\[\" + f'{\"|\".join(items)}' + \"].\"\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.convert_params_dict_to_list","title":"<code>convert_params_dict_to_list(params, skip_params=None)</code>","text":"<p>Convert a dictionary of parameters (name,value) pairs into a list of \"-- Source code in <code>tcbench/cli/clickutils.py</code> <pre><code>def convert_params_dict_to_list(params:Dict[str,Any], skip_params:List[str]=None) -&gt; List[str]:\n\"\"\"Convert a dictionary of parameters (name,value) pairs into a list of \"--&lt;param-name&gt; &lt;param-value&gt;\"\"\"\nif skip_params is None:\nskip_params = set()\nl = []\nfor par_name, par_value in params.items():\nif par_name in skip_params or par_value == False or par_value is None:\ncontinue\npar_name = par_name.replace(\"_\", \"-\")\nif par_value == True:\nl.append(f\"--{par_name}\")\nelse:\nl.append(f\"--{par_name} {str(par_value)}\")\nreturn l\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.help_append_choices","title":"<code>help_append_choices(help_string, values)</code>","text":"<p>Append to an help string a styled version of a list of values</p> Source code in <code>tcbench/cli/clickutils.py</code> <pre><code>def help_append_choices(help_string:str, values:List[str]) -&gt; str:\n\"\"\"Append to an help string a styled version of a list of values\"\"\"\ntext = \"|\".join([f\"[bold]{text}[/bold]\" for text in values])\nreturn f\"{help_string} [yellow]Choices: [{text}][/yellow]\"\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/","title":"command_aimrepo","text":""},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.aimrepo","title":"<code>aimrepo(ctx)</code>","text":"<p>Investigate AIM repository content.</p> Source code in <code>tcbench/cli/command_aimrepo.py</code> <pre><code>@click.group(\"aimrepo\")\n@click.pass_context\ndef aimrepo(ctx):\n\"\"\"Investigate AIM repository content.\"\"\"\npass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.ls","title":"<code>ls(ctx, aim_repo_folder)</code>","text":"<p>List a subset of properties of each run.</p> Source code in <code>tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"ls\")\n@click.pass_context\n@click.option(\n\"--aim-repo\",\n\"aim_repo_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\ndef ls(ctx, aim_repo_folder):\n\"\"\"List a subset of properties of each run.\"\"\"\nfrom tcbench.modeling import aimutils\nif not aim_repo_folder.exists():\nraise RuntimeError(f'Not found {aim_repo_folder}')\nif not (aim_repo_folder / '.aim'):\nraise RuntimeError(f'The input {aim_repo_folder} is not an AIM repository')\nrepo = aim.Repo(str(aim_repo_folder))\nprop = aimutils.get_repo_properties(repo)\ndf = prop['df_run']\ncols = ['hash', 'creation_time', 'end_time']\nif 'campaign_id' in df.columns:\ncols.insert(1, 'campaign_id')\ndf = df[cols]\ndf = df.astype(str)\ntable = Table(box=None)\nfor col in df.columns:\ntable.add_column(col)\nfor idx in range(len(df)):\ntable.add_row(*df.iloc[idx].values)\nconsole.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.properties","title":"<code>properties(ctx, aim_repo_folder)</code>","text":"<p>List properties across all runs.</p> Source code in <code>tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"properties\")\n@click.pass_context\n@click.option(\n\"--aim-repo\",\n\"aim_repo_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\ndef properties(ctx, aim_repo_folder):\n\"\"\"List properties across all runs.\"\"\"\nfrom tcbench.modeling import aimutils\ndef format_duration(timedelta):\nparts = timedelta.components\ntext = ''\nif parts.days &gt; 0:\ntext += f'{parts.days}d '\nif parts.hours &gt; 0:\ntext += f'{parts.hours}h'\ntext += f'{parts.minutes}m{parts.seconds}s'\nreturn text\nrepo = aim.Repo(str(aim_repo_folder))\nprop = aimutils.get_repo_properties(repo)\ntable = Table(box=box.ROUNDED)\ntable.add_column('Name', overflow='fold')\ntable.add_column('No. unique', overflow='fold', justify='right')\ntable.add_column('Value', overflow='fold')\ntable.add_row('runs', '-', str(len(prop['df_run']))) \nduration_mean, duration_std = prop['duration']\nduration_mean = format_duration(duration_mean)\nduration_std = format_duration(duration_std)\ntable.add_row(f'duration (mean {PLUSMINUS} std)', '-', f'{duration_mean} {PLUSMINUS} {duration_std}')\ntable.add_row('metrics', str(len(prop['metrics'])), str(prop['metrics']))\ntable.add_row('contexts', str(len(prop['contexts'])), str(prop['contexts']))\ntable.add_section()\nfor hparam_name in sorted([name for name in prop if name.startswith('hparam')] + ['experiment']):\nvalues = prop[hparam_name]\ntable.add_row(\nhparam_name.replace('hparams.',''),\nf'{len(values)}',\nstr(values),\n)\nconsole.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.report","title":"<code>report(ctx, **kwargs)</code>","text":"<p>Summarize runs performance metrics.</p> Source code in <code>tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"report\")\n@click.pass_context\n@click.option(\n\"--aim-repo\",\n\"aim_repo_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n\"--campaign-id\",\n\"campaign_id\",\ndefault=None,\nshow_default=True,\nhelp=\"Campaign ID to select. By default consider the latest registered campaign.\",\n)\n@click.option(\n\"--metrics\",\n\"metrics\",\ndefault=\"acc\",\nshow_default=True,\nhelp=\"Coma separated list of metrics to consider.\",\n)\n@click.option(\n\"--contexts\",\n\"splits\",\ndefault=None,\nshow_default=True,\nhelp=\"Coma separated list of test split names to consider. By default consider only split which name starts with 'test'\",\n)\n@click.option(\n\"--groupby\",\n\"groupby_params\",\ndefault=None,\nshow_default=True,\nhelp=\"Coma separated list of parameters to aggregate campaign results. By default, try to guess the list from the properties with more than one value.\"\n)\n@click.option(\n\"--output-folder\",\n\"output_folder\",\ndefault=pathlib.Path('.'),\nshow_default=True,\ntype=pathlib.Path,\nhelp=\"Folder where to store output reports.\",\n)\n@click.option(\n\"--precision\",\n\"float_precision\",\ndefault=2,\nshow_default=2,\ntype=int,\nhelp='Number of floating point digits in console report.',\n)\ndef report(ctx, **kwargs):\n\"\"\"Summarize runs performance metrics.\"\"\"\nimport pandas as pd\nfrom tcbench.modeling import aimutils, utils\nrepo = aim.Repo(str(kwargs['aim_repo_folder']))\nprop = aimutils.get_repo_properties(repo)\nmetrics = kwargs['metrics'].split(',')\ncontexts = kwargs['splits']\nif contexts is None:\ncontexts = [\ncontext_name\nfor context_name in prop['contexts']\nif context_name.startswith('test')\n]\nelse:\ncontexts = contexts.split(',')\ngroupby_params = kwargs['groupby_params']\nif groupby_params is None:\ngroupby_params = []\nfor prop_name, prop_values in prop.items():\nif prop_name.startswith('hparams') and \\\n               prop_name not in {\n'hparams.campaign_exp_idx', \n'hparams.seed', \n'hparams.split_index'\n} and \\\n               len(prop_values) &gt; 1:\ngroupby_params.append(prop_name)\nif len(groupby_params) == 0:\ngroupby_params = prop['hparams.campaign_id']\nelse:\nl = []\nfor param_name in groupby_params.split(','):\nif param_name in prop:\nl.append(param_name)\nelif f'hparams.{param_name}' in prop:\nl.append(f'hparams.{param_name}')\nelse:\nconsole.print(f'[yellow]WARNING: parameter {param_name} unknown and will be skipped')\ngroupby_params = l\nif 'test_split_name' not in groupby_params:\ngroupby_params.insert(0, 'test_split_name')\nif any(metric_name not in prop['metrics'] for metric_name in metrics):\nraise RuntimeError(f'Metric {metrics_name} not available: possible values are {prop[\"metrics\"]}')\nif any(context_name not in prop['contexts'] for context_name in contexts):\nraise RuntimeError(f'Split {context_name} not available: possible values are {prop[\"contexts\"]}')\ndf = aimutils.metrics_to_pandas(repo, prop['df_run'], metrics, contexts)\ndf = df.assign(duration=df[\"end_time\"] - df[\"creation_time\"])\nfor campaign_id in prop['hparams.campaign_id']:\ndf_campaign = df[df['hparams.campaign_id'] == campaign_id]\nif len(df_campaign) == 0:\nconsole.print(f'WARNING: missing campaign_id:{campaign_id}')\ncontinue\nconsole.print(f'campaign_id: {campaign_id}')\nconsole.print(f'runs: {df_campaign[\"hash\"].nunique()}')\ndf_tmp = df_campaign[groupby_params + metrics + ['duration', 'hash']]\ndf_tmp = df_tmp.astype({mtr:float for mtr in metrics})\nagg_funcs = {\nmtr_name: [\"count\", \"mean\", \"std\", ('ci95', utils.compute_confidence_intervals)]\nfor mtr_name in metrics\n}\nagg_funcs[\"duration\"] = [\"mean\", \"std\", ('ci95', utils.compute_confidence_intervals)]\nagg_funcs['hash'] = [('runs', 'count')]\ndf_agg = (\ndf_tmp\n.groupby(by=groupby_params)\n.agg(agg_funcs)\n)\n## table to console\n_report_rich_table((df_agg\n.round(kwargs['float_precision'])\n.drop([tpl for tpl in df_agg.columns if tpl[1] == 'count'], axis=1)\n))\n## dump to file\ndf_agg.index.names = [name.replace('hparams.', '') for name in df_agg.index.names]\ndf_agg = df_agg.drop([('hash', 'runs')], axis=1)\nfolder = kwargs['output_folder'] / 'campaing_summary' / campaign_id\nif not folder.exists():\nfolder.mkdir(parents=True)\nis_xgboost_on_pktseries = (\n'hparams.flow_representation' in df.columns and\n(df['hparams.flow_representation'].unique() == ['pktseries']).all()\n)\nhparam_name = 'flowpic_dim'\nif is_xgboost_on_pktseries:\nhparam_name = 'max_n_pkts'\nfor hparam_value in df_campaign[f'hparams.{hparam_name}'].unique():\n# run info\ndf_tmp = df_campaign[df_campaign[f'hparams.{hparam_name}'] == hparam_value]\ndf_tmp = df_tmp.reset_index(drop=True)\ndf_tmp.columns = [col.replace('hparams.','') for col in df_tmp.columns]\nfname = folder / f'runsinfo_{hparam_name}_{hparam_value}.parquet'\nconsole.print(f'saving: {fname}')\ndf_tmp.to_parquet(fname)\n# metrics report\ndf_tmp = df_agg.reset_index()\nif (hparam_name, '') in df_tmp.columns:\ndf_tmp = df_tmp[df_tmp[hparam_name] == hparam_value]\ndf_tmp = df_tmp.drop([(hparam_name, '')], axis=1)\nfname = folder / f'summary_{hparam_name}_{hparam_value}.csv'\nconsole.print(f'saving: {fname}')\ndf_tmp.to_csv(fname, index=None)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/","title":"command_campaign","text":""},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.campaign","title":"<code>campaign(ctx)</code>","text":"<p>Triggers a modeling campaign.</p> Source code in <code>tcbench/cli/command_campaign.py</code> <pre><code>@click.group(\"campaign\")\n@click.pass_context\ndef campaign(ctx):\n\"\"\"Triggers a modeling campaign.\"\"\"\npass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.augment_at_loading","title":"<code>augment_at_loading(ctx, **kwargs)</code>","text":"<p>Modeling by applying data augmentation when loading the training set.</p> Source code in <code>tcbench/cli/command_campaign.py</code> <pre><code>@campaign.command(\"augment-at-loading\")\n@click.pass_context\n@click.option(\n\"--artifacts-folder\",\n\"artifacts_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nshow_default=True,\nhelp=\"Artifacts folder.\",\n)\n@click.option(\n\"--workers\",\n\"workers\",\ntype=int,\ndefault=20,\nshow_default=True,\nhelp=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n\"--gpu-index\",\n\"gpu_index\",\ntype=str,\ndefault=\"0\",\nshow_default=True,\nhelp=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n\"--aim-repo\",\n\"aim_repo\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n\"--aim-experiment-name\",\n\"aim_experiment_name\",\ndefault=\"augmentations-at-loading\",\nshow_default=True,\nhelp=\"The name of the experiment for AIM tracking.\",\n)\n###############\n# flowpic\n###############\n@click.option(\n\"--flowpic-dims\",\n\"flowpic_dims\",\ntype=str,\ndefault=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)),\nshow_default=True,\nhelp=\"Coma separated list of flowpic dimensions for experiments.\",\n)\n# @click.option(\n#    \"--flowpic-block-duration\",\n#    \"flowpic_block_duration\",\n#    type=int,\n#    default=15,\n#    show_default=True,\n#    help=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n# )\n###############\n# data\n###############\n@click.option(\n\"--dataset\",\n\"dataset\",\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=str(DATASETS.UCDAVISICDM19),\nshow_default=True,\nhelp=\"Dataset to use for modeling.\",\n)\n@click.option(\n\"--dataset-minpkts\",\ntype=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\ndefault=\"-1\",\ncallback=CLICK_CALLBACK_TOINT,\nshow_default=True,\nhelp=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n)\n@click.option(\n\"--split-indexes\",\n\"split_indexes\",\ntype=str,\ndefault=None,\nhelp=\"Coma separted list of split indexes (by default all splits are used).\",\n)\n@click.option(\n\"--max-samples-per-class\",\ntype=int,\ndefault=-1,\nhelp=\"Activated when --split-indexes is -1 to define how many samples to select for train+val (with a 80/20 split between train and val).\",\n)\n###############\n# training\n###############\n# @click.option(\n#    \"--train-val-split-ratio\",\n#    \"train_val_split_ratio\",\n#    type=float,\n#    default=0.8,\n#    show_default=True,\n#    help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n# )\n@click.option(\n\"--augmentations\",\ntype=str,\ndefault=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS)),\nshow_default=True,\nhelp=\"Coma separated list of augmentations for experiments. Choices: \"\n+ VALID_AUGMENTATIONS_FOR_AUGATLOAD,\n)\n@click.option(\n\"--seeds\",\n\"seeds\",\ntype=str,\ndefault=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_SEEDS)),\nshow_default=True,\nhelp=\"Coma separated list of seed for experiments.\",\n)\n@click.option(\n\"--batch-size\",\n\"batch_size\",\ntype=int,\ndefault=32,\nshow_default=True,\nhelp=\"Training batch size.\",\n)\n@click.option(\n\"--patience-steps\",\n\"patience_steps\",\ndefault=5,\ntype=int,\nshow_default=True,\nhelp=\"Max. number of epochs without improvement before stopping training.\",\n)\n@click.option(\n\"--learning-rate\",\n\"learning_rate\",\ntype=float,\ndefault=0.001,\nshow_default=True,\nhelp=\"Training learning rate.\",\n)\n@click.option(\n\"--epochs\",\n\"epochs\",\ntype=int,\ndefault=50,\nshow_default=True,\nhelp=\"Number of epochs for training.\",\n)\n@click.option(\n\"--no-test-leftover\",\n\"suppress_test_train_val_leftover\",\ndefault=False,\nis_flag=True,\nhelp=\"Skip test on leftover split (specific for ucdavis-icdm19, and default enabled for all other datasets).\",\n)\n@click.option(\n\"--no-dropout\",\n\"suppress_dropout\",\ndefault=False,\nis_flag=True,\nhelp=\"Mask dropout layers with Identity layers.\",\n)\n@click.option(\n\"--method\",\n\"method\",\ntype=click.Choice(\n(str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n),\ndefault=str(MODELING_METHOD_TYPE.MONOLITHIC),\nshow_default=True,\nhelp=\"Method to use for training.\",\n)\n@click.option(\n\"--input-repr\",\n\"flow_representation\",\ntype=CLICK_TYPE_INPUT_REPR,\ncallback=CLICK_CALLBACK_INPUT_REPR,\ndefault=str(MODELING_INPUT_REPR_TYPE.PKTSERIES),\nshow_default=True,\nmetavar=\"TEXT\",\nhelp=\"Input representation.\",\n)\n@click.option(\n\"--pktseries-len\",\n\"max_n_pkts\",\ndefault=\"10,30\",\nshow_default=True,\nmetavar=\"INTEGER\",\nhelp=\"Number of packets (when using time series as input).\",\n)\n@click.option(\n\"--campaign-id\",\n\"campaign_id\",\ntype=str,\ndefault=None,\nhelp=\"A campaign id to mark all experiments.\",\n)\n@click.option(\n\"--dry-run\",\n\"dry_run\",\ndefault=False,\nis_flag=True,\nhelp=\"Show the number of experiments and then quit.\",\n)\n@click.option(\n\"--max-train-splits\",\n\"max_train_splits\",\ntype=int,\ndefault=-1,\nshow_default=True,\nhelp=\"The maximum number of training splits to experiment with. If -1, use all available.\",\n)\ndef augment_at_loading(ctx, **kwargs):\n\"\"\"Modeling by applying data augmentation when loading the training set.\"\"\"\nmethod = kwargs[\"method\"]\nif method == str(MODELING_METHOD_TYPE.MONOLITHIC):\nfrom tcbench.modeling import (\nrun_campaign_augmentations_at_loading as entry_point,\n)\nif str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\nkwargs[\"suppress_test_train_val_leftover\"] = True\nparams = clickutils.convert_params_dict_to_list(\nkwargs, skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n)\nelse:\nfrom tcbench.modeling import (\nrun_campaign_augmentations_at_loading_xgboost as entry_point,\n)\nparams = clickutils.convert_params_dict_to_list(\nkwargs,\nskip_params=[\n\"method\",\n\"gpu_index\",\n\"augmentations\",\n\"batch_size\",\n\"learning_rate\",\n\"patience_steps\",\n\"epochs\",\n\"suppress_dropout\",\n\"dataset_minpkts\",\n# \"flowpic_dims\",\n\"dataset\",\n\"workers\",\n],\n)\nparser = entry_point.cli_parser()\nargs = parser.parse_args((\" \".join(params)).split())\nargs.method = method\nentry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.contrastivelearning_and_finetune","title":"<code>contrastivelearning_and_finetune(ctx, **kwargs)</code>","text":"<p>Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.</p> Source code in <code>tcbench/cli/command_campaign.py</code> <pre><code>@campaign.command(\"contralearn-and-finetune\")\n@click.pass_context\n@click.option(\n\"--campaign-id\",\n\"campaign_id\",\ntype=str,\ndefault=None,\nhelp=\"A campaign id to mark all experiments.\",\n)\n@click.option(\n\"--workers\",\n\"workers\",\ntype=int,\ndefault=50,\nshow_default=True,\nhelp=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n\"--gpu-index\",\n\"gpu_index\",\ntype=str,\ndefault=\"0\",\nshow_default=True,\nhelp=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n\"--aim-repo\",\n\"aim_repo\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n\"--aim-experiment-name\",\n\"aim_experiment_name\",\ndefault=\"contrastive-learning-and-finetune\",\nshow_default=True,\nhelp=\"The name of the experiment for AIM tracking.\",\n)\n@click.option(\n\"--artifacts-folder\",\n\"artifacts_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nshow_default=True,\nhelp=\"Artifacts folder.\",\n)\n@click.option(\n\"--dry-run\",\n\"dry_run\",\ndefault=False,\nis_flag=True,\nhelp=\"Show the number of experiments and then quit.\",\n)\n@click.option(\n\"--max-train-splits\",\n\"max_train_splits\",\ntype=int,\ndefault=-1,\nshow_default=True,\nhelp=\"The maximum number of training splits to experiment with. If -1, use all available.\",\n)\n@click.option(\n\"--augmentations\",\ntype=str,\ndefault=DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_AUGMENTATIONS,\nshow_default=True,\nhelp=\"Coma separated list of augmentations. Choices: \"\n+ VALID_AUGMENTATIONS_FOR_CONTRALEARN,\n)\n@click.option(\n\"--flowpic-dims\",\n\"flowpic_dim\",\ntype=str,\ndefault=\"32\",\nshow_default=True,\nhelp=\"Coma separated list of flowpic dimensions for experiments.\",\n)\n@click.option(\n\"--cl-seeds\",\n\"contrastive_learning_seeds\",\ndefault=\",\".join(\nmap(\nstr,\nDEFAULT_CAMPAING_CONTRALEARNANDFINETUNE_SEEDS_CONTRALEARN,\n)\n),\nshow_default=True,\nhelp=\"Coma separated list of seeds to use for contrastive learning pretraining.\",\n)\n@click.option(\n\"--ft-seeds\",\n\"finetune_seeds\",\ndefault=\",\".join(map(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_SEEDS_FINETUNE)),\nshow_default=True,\nhelp=\"Coma separated list of seeds to use for finetune training.\",\n)\n@click.option(\n\"--dropout\",\n\"dropout\",\ndefault=\"disable\",\nshow_default=True,\nhelp=\"Coma separated list. Choices:\"\n+ clickutils.compose_help_string_from_list((\"enable\", \"disable\")),\n)\n@click.option(\n\"--cl-projection-layer-dims\",\n\"projection_layer_dims\",\ntype=str,\ndefault=\"30\",\nhelp=\"Coma separate list of contrastive learning projection layer dimensions.\",\nshow_default=True,\n)\n#    parser.add_argument(\n#        \"--finetune-augmentation\", default=\"none\",\n#        choices=(\"none\", \"only-views\", \"views-and-original\"),\n#        help=utils.compose_cli_help_string(\"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\")\n#    )\n@click.option(\n\"--batch-size\",\n\"batch_size\",\ntype=int,\ndefault=32,\nshow_default=True,\nhelp=\"Training batch size.\",\n)\n@click.option(\n\"--split-indexes\",\n\"split_indexes\",\ntype=str,\ndefault=None,\nhelp=\"Coma separted list of split indexes (by default all splits are used).\",\n)\ndef contrastivelearning_and_finetune(ctx, **kwargs):\n\"\"\"Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\"\"\"\nfrom tcbench.modeling import (\nrun_campaign_contrastive_learning_and_finetune as entry_point,\n)\nkwargs[\"dataset\"] = DATASETS.UCDAVISICDM19\nif str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\nkwargs[\"suppress_test_train_val_leftover\"] = True\nparams = clickutils.convert_params_dict_to_list(\nkwargs,\nskip_params=[\"dataset\"],\n)\nparser = entry_point.cli_parser()\nargs = parser.parse_args((\" \".join(params)).split())\nentry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/","title":"command_datasets","text":""},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.datasets","title":"<code>datasets(ctx)</code>","text":"<p>Install/Remove traffic classification datasets.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@click.group()\n@click.pass_context\ndef datasets(ctx):\n\"\"\"Install/Remove traffic classification datasets.\"\"\"\npass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.info","title":"<code>info(ctx, dataset_name)</code>","text":"<p>Show the meta-data related to supported datasets.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"info\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=False,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\nhelp=\"Dataset to install.\",\ndefault=None,\n)\ndef info(ctx, dataset_name):\n\"\"\"Show the meta-data related to supported datasets.\"\"\"\nrich_obj = datasets_utils.get_rich_tree_datasets_properties(dataset_name)\ncli.console.print(rich_obj)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.install","title":"<code>install(ctx, dataset_name, input_folder)</code>","text":"<p>Install a dataset.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"install\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=True,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\nhelp=\"Dataset to install.\",\n)\n@click.option(\n\"--input-folder\",\n\"-i\",\n\"input_folder\",\nrequired=False,\ntype=pathlib.Path,\ndefault=None,\nhelp=\"Folder where to find pre-downloaded tarballs.\",\n)\ndef install(ctx, dataset_name, input_folder):\n\"\"\"Install a dataset.\"\"\"\nif (\ndataset_name\nin (\ndatasets_utils.DATASETS.UCDAVISICDM19,\ndatasets_utils.DATASETS.UTMOBILENET21,\n)\nand input_folder is None\n):\nraise RuntimeError(\nf\"Cannot automatically download {dataset_name}. Please download it separately and retry install using the --input-folder option\"\n)\ndatasets_utils.install(dataset_name, input_folder)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.lsparquet","title":"<code>lsparquet(ctx, dataset_name)</code>","text":"<p>Tree view of the datasets parquet files.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"lsparquet\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=False,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=None,\nhelp=\"Dataset to install.\",\n)\ndef lsparquet(ctx, dataset_name):\n\"\"\"Tree view of the datasets parquet files.\"\"\"\nrich_obj = datasets_utils.get_rich_tree_parquet_files(dataset_name)\ncli.console.print(rich_obj)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.schema","title":"<code>schema(ctx, dataset_name, schema_type)</code>","text":"<p>Show datasets schemas</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"schema\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=False,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=None,\nhelp=\"Dataset to install.\",\n)\n@click.option(\n\"--type\",\n\"-t\",\n\"schema_type\",\nrequired=False,\ntype=click.Choice((\"unfiltered\", \"filtered\", \"splits\")),\ndefault=\"unfiltered\",\nhelp=\"Schema type (unfiltered: original raw data; filtered: curated data; splits: train/val/test splits).\",\n)\ndef schema(ctx, dataset_name, schema_type):\n\"\"\"Show datasets schemas\"\"\"\nrich_obj = datasets_utils.get_rich_dataset_schema(dataset_name, schema_type)\ncli.console.print(rich_obj)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.report_samples_count","title":"<code>report_samples_count(ctx, dataset_name, min_pkts, split)</code>","text":"<p>Show report on number of samples per class.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"samples-count\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=False,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=None,\nhelp=\"Dataset to install.\",\n)\n@click.option(\n\"--min-pkts\",\n\"min_pkts\",\nrequired=False,\ntype=click.Choice((\"-1\", \"10\", \"1000\")),\ndefault=\"-1\",\nhelp=\"\",\n)\n@click.option(\n\"--split\",\n\"split\",\nrequired=False,\ntype=click.Choice((\"human\", \"script\", \"0\", \"1\", \"2\", \"3\", \"4\")),\ndefault=None,\nhelp=\"\",\n)\ndef report_samples_count(ctx, dataset_name, min_pkts, split):\n\"\"\"Show report on number of samples per class.\"\"\"\nwith cli.console.status(\"Computing...\", spinner=\"dots\"):\nmin_pkts = int(min_pkts)\nif min_pkts == -1 and split is not None:\nif dataset_name != datasets_utils.DATASETS.UCDAVISICDM19:\nmin_pkts = 10\ndf_split = None\nif dataset_name == datasets_utils.DATASETS.UCDAVISICDM19 or split is None:\ndf = datasets_utils.load_parquet(dataset_name, min_pkts, split)\nelse:\ndf = datasets_utils.load_parquet(dataset_name, min_pkts, split=None)\ndf_split = datasets_utils.load_parquet(dataset_name, min_pkts, split=split)\ntitle = \"unfiltered\"\nif dataset_name == datasets_utils.DATASETS.UCDAVISICDM19:\nif split is not None:\ntitle = f\"filtered, split: {split}\"\nelse:\ntitle = []\nif min_pkts != -1:\ntitle.append(f\"min_pkts: {min_pkts}\")\nif split:\ntitle.append(f\"split: {split}\")\nif title:\ntitle = \", \".join(title)\nelse:\ntitle = \"unfiltered\"\nif df_split is None:\nif (\ndataset_name == datasets_utils.DATASETS.UCDAVISICDM19\nand min_pkts == -1\nand split is None\n):\nser = df.groupby([\"partition\", \"app\"])[\"app\"].count()\nelse:\nser = df[\"app\"].value_counts()\nrichutils.rich_samples_count_report(ser, title=title)\nelse:\nrichutils.rich_splits_report(df, df_split, split_index=split, title=title)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.import_datasets","title":"<code>import_datasets(ctx, input_folder)</code>","text":"<p>Import datasets.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"import\")\n@click.pass_context\n@click.option(\n\"--input-folder\",\n\"-i\",\n\"input_folder\",\nrequired=False,\ntype=pathlib.Path,\ndefault=None,\nhelp=\"Folder where to find pre-computed curated datasets.\",\n)\ndef import_datasets(ctx, input_folder):\n\"\"\"Import datasets.\"\"\"\nsubfolders = list(input_folder.iterdir())\nif len(subfolders) == 1 and subfolders[0].name == \"datasets\":\ninput_folder /= \"datasets\"\ndatasets_root_folder = datasets_utils.get_datasets_root_folder()\nfor item in input_folder.iterdir():\nif not item.is_dir():\ncontinue\nsrc = item\ndst = datasets_root_folder / item.name\nwith cli.console.status(f\"Importing {item.name}...\", spinner=\"dots\"):\nshutil.copytree(str(src), str(dst))\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.delete_dataset","title":"<code>delete_dataset(ctx, dataset_name)</code>","text":"<p>Delete a dataset.</p> Source code in <code>tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"delete\")\n@click.pass_context\n@click.option(\n\"--name\",\n\"-n\",\n\"dataset_name\",\nrequired=False,\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=None,\nhelp=\"Dataset to delete.\",\n)\ndef delete_dataset(ctx, dataset_name):\n\"\"\"Delete a dataset.\"\"\"\nfolder = datasets_utils.get_dataset_folder(dataset_name)\nif not folder.exists():\ncli.console.print(f\"[red]Dataset {dataset_name} is not installed[/red]\")\nelse:\nwith cli.console.status(f\"Deleting {dataset_name}...\", spinner=\"dots\"):\nshutil.rmtree(str(folder))\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/","title":"command_singlerun","text":""},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.singlerun","title":"<code>singlerun(ctx)</code>","text":"<p>Triggers a modeling run.</p> Source code in <code>tcbench/cli/command_singlerun.py</code> <pre><code>@click.group(\"run\")\n@click.pass_context\ndef singlerun(ctx):\n\"\"\"Triggers a modeling run.\"\"\"\npass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.augment_at_loading","title":"<code>augment_at_loading(ctx, **kwargs)</code>","text":"<p>Modeling by applying data augmentation when loading the training set.</p> Source code in <code>tcbench/cli/command_singlerun.py</code> <pre><code>@singlerun.command(\"augment-at-loading\")\n@click.pass_context\n@click.option(\n\"--artifacts-folder\",\n\"artifacts_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nshow_default=True,\nhelp=\"Artifacts folder.\",\n)\n@click.option(\n\"--workers\",\n\"workers\",\ntype=int,\ndefault=20,\nshow_default=True,\nhelp=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n\"--gpu-index\",\n\"gpu_index\",\ntype=str,\ndefault=\"0\",\nshow_default=True,\nhelp=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n\"--aim-repo\",\n\"aim_repo\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n\"--aim-experiment-name\",\n\"aim_experiment_name\",\ndefault=\"augmentation-at-loading\",\nshow_default=True,\nhelp=\"The name of the experiment for AIM tracking.\",\n)\n###############\n# flowpic\n###############\n@click.option(\n\"--flowpic-dim\",\n\"flowpic_dim\",\ntype=click.Choice((\"32\", \"64\", \"1500\")),\ncallback=CLICK_CALLBACK_TOINT,\ndefault=\"32\",\nshow_default=True,\nhelp=\"Flowpic dimension.\",\n)\n@click.option(\n\"--flowpic-block-duration\",\n\"flowpic_block_duration\",\ntype=int,\ndefault=15,\nshow_default=True,\nhelp=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n)\n###############\n# data\n###############\n@click.option(\n\"--dataset\",\n\"dataset\",\ntype=CLICK_TYPE_DATASET_NAME,\ncallback=CLICK_CALLBACK_DATASET_NAME,\ndefault=str(DATASETS.UCDAVISICDM19),\nshow_default=True,\nhelp=\"Dataset to use for modeling.\",\n)\n@click.option(\n\"--dataset-minpkts\",\ntype=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\ndefault=\"-1\",\ncallback=CLICK_CALLBACK_TOINT,\nshow_default=True,\nhelp=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n)\n@click.option(\n\"--split-index\",\n\"split_index\",\ntype=int,\ndefault=0,\nshow_default=True,\nhelp=\"Data split index.\",\n)\n#    parser.add_argument(\n#        \"--max-samples-per-class\",\n#        type=int,\n#        default=-1,\n#        help=utils.compose_cli_help_string(\"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\")\n#    )\n###############\n# training\n###############\n@click.option(\n\"--train-val-split-ratio\",\n\"train_val_split_ratio\",\ntype=float,\ndefault=0.8,\nshow_default=True,\nhelp=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n)\n@click.option(\n\"--aug-name\",\n\"aug_name\",\ntype=click.Choice(\n(\n\"noaug\",\n\"rotate\",\n\"horizontalflip\",\n\"colorjitter\",\n\"packetloss\",\n\"timeshift\",\n\"changertt\",\n)\n),\ndefault=\"noaug\",\nshow_default=True,\nhelp=\"Name of the augmentation to use.\",\n)\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\n@click.option(\n\"--seed\",\n\"seed\",\ntype=int,\ndefault=12345,\nshow_default=True,\nhelp=\"Seed to initialize random generators.\",\n)\n@click.option(\n\"--batch-size\",\n\"batch_size\",\ntype=int,\ndefault=32,\nshow_default=True,\nhelp=\"Training batch size\",\n)\n@click.option(\n\"--patience-steps\",\n\"patience_steps\",\ndefault=5,\ntype=int,\nshow_default=True,\nhelp=\"Max. number of epochs without improvement before stopping training.\",\n)\n@click.option(\n\"--learning-rate\",\n\"learning_rate\",\ntype=float,\ndefault=0.001,\nshow_default=True,\nhelp=\"Training learning rate.\",\n)\n@click.option(\n\"--epochs\",\n\"epochs\",\ntype=int,\ndefault=50,\nshow_default=True,\nhelp=\"Number of epochs for training.\",\n)\n@click.option(\n\"--no-test-leftover\",\n\"suppress_test_train_val_leftover\",\ndefault=False,\nis_flag=True,\nhelp=\"Skip test on leftover split (specific for ucdavis-icdm19, and default enabled for all other datasets).\",\n)\n@click.option(\n\"--no-dropout\",\n\"suppress_dropout\",\ndefault=False,\nis_flag=True,\nhelp=\"Mask dropout layers with Identity layers.\",\n)\n@click.option(\n\"--method\",\n\"method\",\ntype=click.Choice(\n(str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n),\ndefault=str(MODELING_METHOD_TYPE.MONOLITHIC),\nshow_default=True,\nhelp=\"Method to use for training.\",\n)\n@click.option(\n\"--input-repr\",\n\"flow_representation\",\ntype=CLICK_TYPE_INPUT_REPR,\ncallback=CLICK_CALLBACK_INPUT_REPR,\ndefault=str(MODELING_INPUT_REPR_TYPE.PKTSERIES),\nshow_default=True,\nmetavar=\"TEXT\",\nhelp=\"Input representation.\",\n)\n@click.option(\n\"--pktseries-len\",\n\"max_n_pkts\",\ntype=click.Choice((\"10\", \"30\")),\ndefault=\"10\",\nshow_default=True,\nmetavar=\"INTEGER\",\nhelp=\"Number of packets (when using time series as input).\",\n)\ndef augment_at_loading(ctx, **kwargs):\n\"\"\"Modeling by applying data augmentation when loading the training set.\"\"\"\nmethod = kwargs[\"method\"]\nif method == str(MODELING_METHOD_TYPE.MONOLITHIC):\nfrom tcbench.modeling import run_augmentations_at_loading as entry_point\nif str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\nkwargs[\"suppress_test_train_val_leftover\"] = True\nparams = clickutils.convert_params_dict_to_list(\nkwargs, skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n)\nelse:\nfrom tcbench.modeling import run_augmentations_at_loading_xgboost as entry_point\nparams = clickutils.convert_params_dict_to_list(\nkwargs,\nskip_params=[\n\"method\",\n\"gpu_index\",\n\"aug_name\",\n\"batch_size\",\n\"learning_rate\",\n\"patience_steps\",\n\"epochs\",\n\"suppress_dropout\",\n\"dataset_minpkts\",\n],\n)\nparser = entry_point.cli_parser()\nargs = parser.parse_args((\" \".join(params)).split())\nargs.method = method\nentry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.contrastivelearning_and_finetune","title":"<code>contrastivelearning_and_finetune(ctx, **kwargs)</code>","text":"<p>Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.</p> Source code in <code>tcbench/cli/command_singlerun.py</code> <pre><code>@singlerun.command(\"contralearn-and-finetune\")\n@click.pass_context\n@click.option(\n\"--artifacts-folder\",\n\"artifacts_folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nshow_default=True,\nhelp=\"Artifacts folder.\",\n)\n@click.option(\n\"--workers\",\n\"workers\",\ntype=int,\ndefault=20,\nshow_default=True,\nhelp=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n\"--gpu-index\",\n\"gpu_index\",\ntype=str,\ndefault=\"0\",\nshow_default=True,\nhelp=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n\"--aim-repo\",\n\"aim_repo\",\ntype=pathlib.Path,\ndefault=DEFAULT_AIM_REPO,\nshow_default=True,\nhelp=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n\"--aim-experiment-name\",\n\"aim_experiment_name\",\ndefault=\"contrastive-learning-and-finetune\",\nshow_default=True,\nhelp=\"The name of the experiment for AIM tracking.\",\n)\n#    parser.add_argument(\"--final\", action=\"store_true\", default=False)\n###############\n# flowpic\n###############\n@click.option(\n\"--flowpic-dim\",\n\"flowpic_dim\",\ntype=click.Choice((\"32\",)),  # \"64\", \"1500\")),\ncallback=CLICK_CALLBACK_TOINT,\ndefault=\"32\",\nshow_default=True,\nhelp=\"Flowpic dimension.\",\n)\n@click.option(\n\"--flowpic-block-duration\",\n\"flowpic_block_duration\",\ntype=int,\ndefault=15,\nshow_default=True,\nhelp=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n)\n###############\n# data\n###############\n@click.option(\n\"--dataset\",\n\"dataset\",\ntype=click.Choice((str(DATASETS.UCDAVISICDM19),)),  # CLICK_TYPE_DATASET_NAME,\n# callback=CLICK_CALLBACK_DATASET_NAME,\ndefault=str(DATASETS.UCDAVISICDM19),\nshow_default=True,\nhelp=\"Dataset to use for modeling.\",\n)\n# @click.option(\n#    \"--dataset-minpkts\",\n#    type=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\n#    default=\"-1\",\n#    callback=CLICK_CALLBACK_TOINT,\n#    show_default=True,\n#    help=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n# )\n@click.option(\n\"--split-index\",\n\"split_index\",\ntype=int,\ndefault=0,\nshow_default=True,\nhelp=\"Data split index.\",\n)\n###############\n# training\n###############\n# @click.option(\n#    \"--train-val-split-ratio\",\n#    \"train_val_split_ratio\",\n#    type=float,\n#    default=0.8,\n#    show_default=True,\n#    help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n# )\n# @click.option(\n#    \"--aug-name\",\n#    \"aug_name\",\n#    type=click.Choice(\n#        (\n#            \"noaug\",\n#            \"rotate\",\n#            \"horizontalflip\",\n#            \"colorjitter\",\n#            \"packetloss\",\n#            \"timeshift\",\n#            \"changertt\",\n#        )\n#    ),\n#    default=\"noaug\",\n#    show_default=True,\n#    help=\"Name of the augmentation to use.\",\n# )\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\n@click.option(\n\"--batch-size\",\n\"batch_size\",\ntype=int,\ndefault=32,\nshow_default=True,\nhelp=\"Training batch size\",\n)\n@click.option(\n\"--no-dropout\",\n\"suppress_dropout\",\ndefault=False,\nis_flag=True,\nhelp=\"Mask dropout layers with Identity layers.\",\n)\n# @click.option(\n#    \"--method\",\n#    \"method\",\n#    type=click.Choice(\n#        (str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n#    ),\n#    default=str(MODELING_METHOD_TYPE.MONOLITHIC),\n#    show_default=True,\n#    help=\"Method to use for training.\",\n# )\n@click.option(\n\"--cl-aug-names\",\n\"augmentations\",\ndefault=\"changertt,timeshift\",\nshow_default=True,\nhelp=\"Coma separated list of augmentations pool for contrastive learning.\",\n)\n@click.option(\n\"--cl-projection-layer-dim\",\n\"projection_layer_dim\",\ntype=int,\ndefault=30,\nhelp=\"The number of units in the contrastive learning projection layer.\",\nshow_default=True,\n)\n@click.option(\n\"--cl-learning-rate\",\n\"contrastive_learning_lr\",\ntype=float,\ndefault=0.001,\nshow_default=True,\nhelp=\"Learning rate for pretraining.\",\n)\n@click.option(\n\"--cl-seed\",\n\"contrastive_learning_seed\",\ntype=int,\ndefault=12345,\nshow_default=True,\nhelp=\"Seed for contrastive learning pretraining.\",\n)\n@click.option(\n\"--cl-patience-steps\",\n\"contrastive_learning_patience_steps\",\ntype=int,\ndefault=3,\nhelp=\"Max steps to wait before stopping training if the top5 validation accuracy does not improve.\",\nshow_default=True,\n)\n@click.option(\n\"--cl-temperature\",\n\"contrastive_learning_temperature\",\ntype=float,\ndefault=0.07,\nhelp=\"Temperature for InfoNCE loss.\",\nshow_default=True,\n)\n@click.option(\n\"--cl-epochs\",\n\"contrastive_learning_epochs\",\ntype=int,\ndefault=50,\nshow_default=True,\nhelp=\"Epochs for contrastive learning pretraining.\",\n)\n####################################\n# finetune configs\n####################################\n@click.option(\n\"--ft-learning-rate\",\n\"finetune_lr\",\ntype=float,\ndefault=0.01,\nhelp=\"Learning rate for finetune.\",\nshow_default=True,\n)\n@click.option(\n\"--ft-patience-steps\",\n\"finetune_patience_steps\",\ntype=int,\ndefault=5,\nshow_default=True,\nhelp=\"Max steps to wait before stopping finetune training loss does not improve.\",\n)\n@click.option(\n\"--ft-patience-min-delta\",\n\"finetune_patience_min_delta\",\ntype=float,\ndefault=0.001,\nshow_default=True,\nhelp=\"Minimum decrease of training loss to be considered as improvement.\",\n)\n@click.option(\n\"--ft-train-samples\",\n\"finetune_train_samples\",\ntype=int,\ndefault=10,\nshow_default=True,\nhelp=\"Number of samples per-class for finetune training.\",\n)\n@click.option(\n\"--ft-epochs\",\n\"finetune_epochs\",\ntype=int,\ndefault=50,\nshow_default=True,\nhelp=\"Epochs for finetune training.\",\n)\n@click.option(\n\"--ft-seed\",\n\"finetune_seed\",\ntype=int,\ndefault=12345,\nshow_default=True,\nhelp=\"Seed for finetune training.\",\n)\ndef contrastivelearning_and_finetune(ctx, **kwargs):\n\"\"\"Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\"\"\"\nimport tcbench.modeling.run_contrastive_learning_and_finetune as entry_point\nparams = clickutils.convert_params_dict_to_list(\nkwargs,  # skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n)\nparser = entry_point.cli_parser()\nargs = parser.parse_args((\" \".join(params)).split())\nargs.method = \"simclr\"\nargs.augmentations = kwargs[\"augmentations\"].split(\",\")\nentry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/","title":"richutils","text":""},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_series","title":"<code>_rich_table_from_series(ser, columns, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas Series</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_series(ser:pd.Series, columns:List[str], with_total:bool=False) -&gt; rich.table.Table:\n\"\"\"Compose a rich Table from a pandas Series\"\"\"\ntable = Table()\ntable.add_column(columns[0])\ntable.add_column(columns[1], justify=\"right\")\nfor index, value in zip(ser.index, ser.values):\ntable.add_row(str(index), str(value))\nif with_total:\ntable.add_section()\ntable.add_row(\"__total__\", str(ser.values.sum()))\nreturn table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_series_multiindex","title":"<code>_rich_table_from_series_multiindex(ser, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas Series with MultiIndex</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_series_multiindex(ser:pd.Series, with_total:bool=False) -&gt; rich.table.Table:\n\"\"\"Compose a rich Table from a pandas Series with MultiIndex\"\"\"\ntable = Table()\nfor col in ser.index.names:\ntable.add_column(col)\ntable.add_column(\"samples\", justify=\"right\")\nfor level0 in ser.index.levels[0]:\ntmp = ser.loc[level0]\nfor level1, value in zip(tmp.index, tmp.values):\ntable.add_row(str(level0), str(level1), str(value))\nlevel0 = \"\"\nif with_total:\ntable.add_row(\"\", \"__total__\", str(tmp.values.sum()))\ntable.add_section()\nreturn table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_dataframe","title":"<code>_rich_table_from_dataframe(df, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas DataFrame</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_dataframe(df:pd.DataFrame, with_total:bool=False) -&gt; rich.table.Table:\n\"\"\"Compose a rich Table from a pandas DataFrame\"\"\"\ntable = Table()\ntable.add_column(df.index.name)\nfor col in df.columns:\ntable.add_column(col, justify=\"right\")\nfor idx in range(df.shape[0]):\ntable.add_row(df.index[idx], *list(df.iloc[idx].values.astype(str)))\nif with_total:\ntable.add_section()\ntable.add_row(\"__total__\", *list(map(str, df.sum().values)))\nreturn table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_samples_count_report","title":"<code>rich_samples_count_report(obj, columns=None, title=None, with_total=True)</code>","text":"<p>Compute and format into a table the per-class samples count</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def rich_samples_count_report(obj:pd.Series | pd.DataFrame, columns:List[str]=None, title:str=None, with_total:bool=True) -&gt; rich.table.Table:\n\"\"\"Compute and format into a table the per-class samples count\"\"\"\nif columns is None:\ncolumns = [\"app\", \"samples\"]\nif isinstance(obj, pd.Series):\nif isinstance(obj.index, pd.MultiIndex):\ntable = _rich_table_from_series_multiindex(obj, with_total=with_total)\nelse:\ntable = _rich_table_from_series(obj, columns, with_total=with_total)\nelse:\ntable = _rich_table_from_dataframe(obj, with_total=with_total)\nif title is not None:\nconsole.print(title)\nconsole.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_packets_report","title":"<code>rich_packets_report(df, packets_colname='packets', title=None)</code>","text":"<p>Compute and reports stats for number of packets per flow</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def rich_packets_report(df:pd.DataFrame, packets_colname:str=\"packets\", title:str=None) -&gt; rich.table.Table:\n\"\"\"Compute and reports stats for number of packets per flow\"\"\"\nser = df[packets_colname].describe().round(2)\nrich_samples_count_report(\nser, columns=[\"stat\", \"value\"], title=title, with_total=False\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_label","title":"<code>rich_label(text, extra_new_line=False)</code>","text":"<p>Output on the console a formatted label</p> Source code in <code>tcbench/cli/richutils.py</code> <pre><code>def rich_label(text:str, extra_new_line:bool=False) -&gt; None:\n\"\"\"Output on the console a formatted label\"\"\"\nif extra_new_line:\nconsole.print()\nconsole.print(Panel(text, box=box.ROUNDED, expand=False, padding=0))\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets/","title":"Tcbench libtcdatasets","text":""},{"location":"tcbench/api/tcbench_libtcdatasets/#generating-trainvaltest-splits","title":"Generating train/val/test splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/","title":"datasets_utils","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_yaml","title":"<code>load_yaml(fname)</code>","text":"<p>Load an input YAML file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the YAML filename to load</p> required Return <p>The YAML object loaded</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_yaml(fname: pathlib.Path) -&gt; Dict[Any, Any]:\n\"\"\"Load an input YAML file\n    Arguments:\n        fname: the YAML filename to load\n    Return:\n        The YAML object loaded\n    \"\"\"\nwith open(fname) as fin:\nreturn yaml.safe_load(fin)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_config","title":"<code>load_config(fname)</code>","text":"<p>Load the configuration file of the framework</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the YAML config file to load</p> required Return <p>The loaded config file</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_config(fname: pathlib.Path) -&gt; Dict:\n\"\"\"Load the configuration file of the framework\n    Arguments:\n        fname: the YAML config file to load\n    Return:\n        The loaded config file\n    \"\"\"\nreturn load_yaml(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.download_url","title":"<code>download_url(url, save_to)</code>","text":"<p>Download a dataset tarball.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>the object URL</p> required <code>save_to</code> <code>pathlib.Path</code> <p>an optional destination folder. If dst is None, the tarball is placed at the root_folder specified by the archive (or downloadutils.DEFAULT_DOWNLOAD_FOLDER when no root_folder is specified). Note that if dst != root_folder, the internal metadata will be adjusted using install()</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>the path of the downloaded file</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def download_url(url: str, save_to: pathlib.Path) -&gt; pathlib.Path:\n\"\"\"Download a dataset tarball.\n    Args:\n        url: the object URL\n        save_to: an optional destination folder. If dst is None, the tarball is placed at\n            the root_folder specified by the archive (or downloadutils.DEFAULT_DOWNLOAD_FOLDER\n            when no root_folder is specified). Note that if dst != root_folder,\n            the internal metadata will be adjusted using install()\n    Returns:\n        the path of the downloaded file\n    \"\"\"\nsave_to = pathlib.Path(save_to)\nfname = pathlib.Path(url).name\nsave_as = save_to / fname\nresp = requests.get(url, stream=True)\ntotalbytes = int(resp.headers.get(\"content-length\", 0))\nif not save_as.parent.exists():\nsave_as.parent.mkdir(parents=True)\nwith open(str(save_as), \"wb\") as fout, richprogress.Progress(\nrichprogress.TextColumn(\"[progress.description]{task.description}\"),\nrichprogress.BarColumn(),\nrichprogress.FileSizeColumn(),\nrichprogress.TextColumn(\"/\"),\nrichprogress.TotalFileSizeColumn(),\nrichprogress.TextColumn(\"eta\"),\nrichprogress.TimeRemainingColumn(),\nconsole=console,\n) as progressbar:\ntask_id = progressbar.add_task(\"Downloading...\", total=totalbytes)\nfor data in resp.iter_content(chunk_size=64 * 1024):\nsize = fout.write(data)\nprogressbar.advance(task_id, advance=size)\nreturn save_as\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_datasets_root_folder","title":"<code>get_datasets_root_folder()</code>","text":"<p>Returns the path where datasets all datasets are installed</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_datasets_root_folder() -&gt; pathlib.Path:\n\"\"\"Returns the path where datasets all datasets are installed\"\"\"\nreturn _get_module_folder() / FOLDER_DATASETS\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_dataset_folder","title":"<code>get_dataset_folder(dataset_name)</code>","text":"<p>Returns the path where a specific datasets in installed</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_dataset_folder(dataset_name: str | DATASETS) -&gt; pathlib.Path:\n\"\"\"Returns the path where a specific datasets in installed\"\"\"\nif dataset_name:\ndataset_name = str(dataset_name)\nreturn _get_module_folder() / FOLDER_DATASETS / dataset_name\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_dataset_parquet_filename","title":"<code>get_dataset_parquet_filename(dataset_name, min_pkts=-1, split=None, animation=False)</code>","text":"<p>Returns the path of a dataset parquet file</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str | DATASETS</code> <p>The name of the dataset</p> required <code>min_pkts</code> <code>int</code> <p>the filtering rule applied when curating the datasets. If -1, load the unfiltered dataset</p> <code>-1</code> <code>split</code> <code>str</code> <p>if min_pkts!=-1, is used to request the loading of the split file. For DATASETS.UCDAVISICDM19 values can be \"human\", \"script\" or a number between 0 and 4. For all other dataset split can be anything which is not None (e.g., True)</p> <code>None</code> <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The pathlib.Path of a dataset parquet file</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_dataset_parquet_filename(\ndataset_name: str | DATASETS, min_pkts: int = -1, split: str = None, animation=False\n) -&gt; pathlib.Path:\n\"\"\"Returns the path of a dataset parquet file\n    Arguments:\n        dataset_name: The name of the dataset\n        min_pkts: the filtering rule applied when curating the datasets.\n            If -1, load the unfiltered dataset\n        split: if min_pkts!=-1, is used to request the loading of\n            the split file. For DATASETS.UCDAVISICDM19\n            values can be \"human\", \"script\" or a number\n            between 0 and 4.\n            For all other dataset split can be anything\n            which is not None (e.g., True)\n    Returns:\n        The pathlib.Path of a dataset parquet file\n    \"\"\"\ndataset_folder = get_dataset_folder(dataset_name) / \"preprocessed\"\npath = dataset_folder / f\"{dataset_name}.parquet\"\nif isinstance(split, int) and split &lt; 0:\nsplit = None\n#    if isinstance(split, bool):\n#        split = 0\n#    elif isinstance(split, int):\n#        split = str(split)\n#\n#    if min_pkts == -1 and (split is None or int(split) &lt; 0):\n#        return path\n#\nif isinstance(dataset_name, str):\ndataset_name = DATASETS.from_str(dataset_name)\nif dataset_name != DATASETS.UCDAVISICDM19:\nif min_pkts != -1:\ndataset_folder /= \"imc23\"\nif split is None:\npath = (\ndataset_folder\n/ f\"{dataset_name}_filtered_minpkts{min_pkts}.parquet\"\n)\nelse:\npath = (\ndataset_folder\n/ f\"{dataset_name}_filtered_minpkts{min_pkts}_splits.parquet\"\n)\nelse:\n#        if split is None:\n#            raise RuntimeError('split cannot be None for ucdavis-icdm19')\n#        dataset_folder /= 'imc23'\n#        if split in ('human', 'script'):\n#            path = dataset_folder / f'test_split_{split}.parquet'\n#        else:\n#            if split == 'train':\n#                split = 0\n#            path = dataset_folder / f'train_split_{split}.parquet'\nif split is not None:\ndataset_folder /= \"imc23\"\nif split in (\"human\", \"script\"):\npath = dataset_folder / f\"test_split_{split}.parquet\"\nelse:\nif split == \"train\":\nsplit = 0\npath = dataset_folder / f\"train_split_{split}.parquet\"\nreturn path\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_parquet","title":"<code>load_parquet(dataset_name, min_pkts=-1, split=None, columns=None, animation=False)</code>","text":"<p>Load and returns a dataset parquet file</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str | DATASETS</code> <p>The name of the dataset</p> required <code>min_pkts</code> <code>int</code> <p>the filtering rule applied when curating the datasets. If -1, load the unfiltered dataset</p> <code>-1</code> <code>split</code> <code>str</code> <p>if min_pkts!=-1, is used to request the loading of the split file. For DATASETS.UCDAVISICDM19 values can be \"human\", \"script\" or a number between 0 and 4. For all other dataset split can be anything which is not None (e.g., True)</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>A list of columns to load (if None, load all columns)</p> <code>None</code> <code>animation</code> <code>bool</code> <p>if True, create a loading animation on the console</p> <code>False</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas dataframe and the related parquet file used to load the dataframe</p> Source code in <code>tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_parquet(\ndataset_name: str | DATASETS,\nmin_pkts: int = -1,\nsplit: str = None,\ncolumns: List[str] = None,\nanimation: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"Load and returns a dataset parquet file\n    Arguments:\n        dataset_name: The name of the dataset\n        min_pkts: the filtering rule applied when curating the datasets.\n            If -1, load the unfiltered dataset\n        split: if min_pkts!=-1, is used to request the loading of\n            the split file. For DATASETS.UCDAVISICDM19\n            values can be \"human\", \"script\" or a number\n            between 0 and 4.\n            For all other dataset split can be anything\n            which is not None (e.g., True)\n        columns: A list of columns to load (if None, load all columns)\n        animation: if True, create a loading animation on the console\n    Returns:\n        A pandas dataframe and the related parquet file used to load the dataframe\n    \"\"\"\npath = get_dataset_parquet_filename(dataset_name, min_pkts, split)\nimport pandas as pd\nfrom tcbench import cli\nif animation:\nwith cli.console.status(f\"loading: {path}...\", spinner=\"dots\"):\nreturn pd.read_parquet(path, columns=columns)\nreturn pd.read_parquet(path, columns=columns)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/","title":"mirage19_json_to_parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.scan_for_strings","title":"<code>scan_for_strings(raw_packets, n_packets, min_len=5)</code>","text":"<p>Extract ASCII strings by processing packets payload</p> <p>Parameters:</p> Name Type Description Default <code>raw_packets</code> <code>List</code> <p>a list of list of raw packets bytes. The outer list represent a packet, the inner list the individual bytes (as integer values) of the payload</p> required <code>n_packets</code> <code>int</code> <p>maximum number of packets to process</p> required <code>min_len</code> <code>int</code> <p>minimum lenght of the strings to return</p> <code>5</code> Return <p>An array containing the identified string</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def scan_for_strings(raw_packets: List, n_packets: int, min_len: int = 5) -&gt; List[str]:\n\"\"\"Extract ASCII strings by processing packets payload\n    Arguments:\n        raw_packets: a list of list of raw packets bytes. The outer list\n            represent a packet, the inner list the individual bytes (as integer values)\n            of the payload\n        n_packets: maximum number of packets to process\n        min_len: minimum lenght of the strings to return\n    Return:\n        An array containing the identified string\n    \"\"\"\nstrings = []\nfor pkt in raw_packets[:n_packets]:\nif not pkt:\ncontinue\ntext = \"\".join(char if char in VALIDCHARS else \"#\" for char in map(chr, pkt))\nfor string in re.findall(r\"[^#]+\", text):\nif \"http:\" in string or (\nlen(string) &gt;= min_len and (\".\" in string or \" \" in string)\n):\nstrings.append(string)\nreturn strings\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.flatten_dict","title":"<code>flatten_dict(data, parent_name=None)</code>","text":"<p>Helper function to flatten a nested dictionary. For example, the structure {\"a\":{\"b\":{\"c\":1, \"d\":2}}} is transformed into [(\"a_b_c\":1), (\"a_b_d\":2)]</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>the dictionary to explore</p> required <code>parent_name</code> <code>str</code> <p>the key associated the data currently processed</p> <code>None</code> Return <p>A list of (str, value) pairs where the str is the flattened key corresponding the the chaining of the nested key dictionary</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def flatten_dict(data: Dict, parent_name: str = None) -&gt; List:\n\"\"\"Helper function to flatten a nested dictionary.\n    For example, the structure {\"a\":{\"b\":{\"c\":1, \"d\":2}}}\n    is transformed into [(\"a_b_c\":1), (\"a_b_d\":2)]\n    Arguments:\n        data: the dictionary to explore\n        parent_name: the key associated the data\n            currently processed\n    Return:\n        A list of (str, value) pairs where\n        the str is the flattened key corresponding\n        the the chaining of the nested key dictionary\n    \"\"\"\nif not isinstance(data, dict):\nreturn [(parent_name, data)]\nnew_items = []\nfor key, value in data.items():\nnew_key = key if not parent_name else f\"{parent_name}_{key}\"\nnew_items.extend(flatten_dict(value, new_key))\nreturn new_items\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.convert_to_dataframe","title":"<code>convert_to_dataframe(data)</code>","text":"<p>Convert a nested dictionary into a pandas DataFrame</p> Argument <p>data: a (possibly) nested dictionary of key-value pairs</p> Return <p>A pandas DataFrame collecting the flattened keys of the nested dictionary and the related value</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def convert_to_dataframe(data: Dict) -&gt; pd.DataFrame:\n\"\"\"Convert a nested dictionary into a pandas DataFrame\n    Argument:\n        data: a (possibly) nested dictionary of key-value pairs\n    Return:\n        A pandas DataFrame collecting the flattened keys\n        of the nested dictionary and the related value\n    \"\"\"\nnew_data = dict()\nfor net_tuple, value in data.items():\nnew_data[net_tuple] = dict(flatten_dict(value))\ndf = pd.DataFrame(new_data).T\ndf.columns = [col.lower() for col in df.columns]\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.worker","title":"<code>worker(fname, save_to=None)</code>","text":"<p>A helper function to transform a JSON MIRAGE input file into a pandas DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the JSON file to process</p> required <code>save_to</code> <code>str</code> <p>an optional file name where to store the transformed data as parquet</p> <code>None</code> Return <p>A pandas DataFrame with the loaded JSON data</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def worker(fname: str, save_to: str = None) -&gt; pd.DataFrame:\n\"\"\"A helper function to transform a JSON MIRAGE input\n    file into a pandas DataFrame\n    Arguments:\n        fname: the JSON file to process\n        save_to: an optional file name where to store the\n            transformed data as parquet\n    Return:\n        A pandas DataFrame with the loaded JSON data\n    \"\"\"\nwith open(str(fname), \"r\") as fin:\ndata = json.load(fin)\nfor net_tuple in data:\npayload_bytes = data[net_tuple][\"packet_data\"][\"L4_raw_payload\"]\ndata[net_tuple][\"strings\"] = scan_for_strings(payload_bytes, 10, 5)\ndf = convert_to_dataframe(data)\n# extract name from filename\nandroid_name = \"None\"\nif \"None\" not in fname.name:\n_1, android_name, *_ = fname.name.split(\"_\")\ndf = df.assign(\nandroid_name=android_name,\ndevice_name=fname.parent.name,\n)\n#    if progress:\n#        print(f\".\", end=\"\", flush=True)\nif save_to:\nout_fname = save_to / fname.parent.name / fname.name\nif not out_fname.parent.exists():\nout_fname.parent.mkdir(parents=True)\nout_fname = out_fname.with_suffix(\".parquet\")\nif out_fname.exists():\nraise RuntimeError(f\"file {out_fname} already exists!\")\ndf.to_parquet(out_fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.postprocess","title":"<code>postprocess(df)</code>","text":"<p>Process a the data loaded from MIRAGE JSON to (1) identify a background class; (2) remove invalid IPs (e.g., 127.0.0.1, 255.255.255.255); (3) add a unique \"row_id\" row; (4) add an \"app\" column with the label represented as category</p> Argument <p>df: the pandas DataFrame to process</p> Return <p>The modified version of the input data</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def postprocess(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Process a the data loaded from MIRAGE JSON to\n    (1) identify a background class;\n    (2) remove invalid IPs (e.g., 127.0.0.1, 255.255.255.255);\n    (3) add a unique \"row_id\" row;\n    (4) add an \"app\" column with the label represented as category\n    Argument:\n        df: the pandas DataFrame to process\n    Return:\n        The modified version of the input data\n    \"\"\"\n# create a background class\ndf = df.assign(\napp=np.where(\ndf[\"android_name\"] == df[\"flow_metadata_bf_label\"],\ndf[\"android_name\"],\n\"background\",\n)\n)\n# split connection index to recover network tuple info\ndf = df.reset_index().rename({\"index\": \"conn_id\"}, axis=1)\ndf = df.assign(_tmp_col=df[\"conn_id\"].str.split(\",\"))\ndf = df.assign(\nsrc_ip=df[\"_tmp_col\"].str[0],\nsrc_port=df[\"_tmp_col\"].str[1],\ndst_ip=df[\"_tmp_col\"].str[2],\ndst_port=df[\"_tmp_col\"].str[3],\nproto=df[\"_tmp_col\"].str[4],\n).drop(\"_tmp_col\", axis=1)\n# drop invalid IPs\n# df = df[(~df[\"src_ip\"].isin(INVALID_IPS)) &amp; (~df[\"dst_ip\"].isin(INVALID_IPS))]\n# add a unique row_id\ndf = df.reset_index().rename({\"index\": \"row_id\"}, axis=1)\n# enforce app to be categorical\ndf = df.assign(\napp=df[\"app\"].astype(\"category\"),\npackets=df[\"packet_data_l4_payload_bytes\"].apply(len),\n)\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.main","title":"<code>main(input_folder, save_as=None, workers=30)</code>","text":"<p>The main processing loop</p> Argument <p>input_folder: the folder where the MIRAGE JSON files are contained save_as: the output filename where to store the parquet     after loading the data workers: number of parallel workers to use for processing</p> Source code in <code>tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def main(\ninput_folder: str, save_as: pathlib.Path = None, workers: int = 30\n) -&gt; pd.DataFrame:\n\"\"\"The main processing loop\n    Argument:\n        input_folder: the folder where the MIRAGE JSON files are contained\n        save_as: the output filename where to store the parquet\n            after loading the data\n        workers: number of parallel workers to use for processing\n    \"\"\"\ninput_folder = pathlib.Path(input_folder)\nif (input_folder / \"MIRAGE-2019_traffic_dataset_downloadable\").exists():\ninput_folder /= \"MIRAGE-2019_traffic_dataset_downloadable\"\n# creating a temporary folder\nwith tempfile.TemporaryDirectory() as tmp_folder:\ntmp_folder = pathlib.Path(tmp_folder)\nfiles = list(input_folder.glob(\"*/*.json\"))\nif len(files) == 0:\nraise RuntimeError(\nf\"Did not find any .json file for input folder {input_folder} ! Make sure the input folder support a */*.json glob search\"\n)\nprint(f\"found {len(files)} JSON files to load\")\nfunc_worker = functools.partial(worker, save_to=tmp_folder)\n# params = []\n# for path in files:\n#    params.append(\n#        (\n#            path,\n#            True,\n#            tmp_folder,\n#        )\n#    )\nwith richprogress.Progress(\nrichprogress.TextColumn(\"[progress.description]{task.description}\"),\nrichprogress.BarColumn(),\nrichprogress.MofNCompleteColumn(),\nrichprogress.TimeElapsedColumn(),\nconsole=console,\n) as progressbar:\ntask_id = progressbar.add_task(\"Converting JSONs...\", total=len(files))\nwith Pool(workers) as pool:\n# for _ in pool.imap(worker, params):\nfor _ in pool.imap_unordered(func_worker, files):\nprogressbar.advance(task_id)\nprint(\"merging files...\")\nl = list(map(pd.read_parquet, tmp_folder.glob(\"*/*.parquet\")))\ndf = pd.concat(l)\ndf = postprocess(df)\nif save_as is not None:\nif not save_as.parent.exists():\nsave_as.parent.mkdir(parents=True)\nprint(f\"saving: {save_as}\")\ndf.to_parquet(save_as)\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage22_json_to_parquet/","title":"mirage22_json_to_parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_mirage22_json_to_parquet/#tcbench.libtcdatasets.mirage22_json_to_parquet.postprocess","title":"<code>postprocess(df)</code>","text":"<p>Process the loaded MIRAGE JSON by (1) adding a background class; (2) adding an \"app\" column with label information, and encoding it as pandas category</p> Source code in <code>tcbench/libtcdatasets/mirage22_json_to_parquet.py</code> <pre><code>def postprocess(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Process the loaded MIRAGE JSON by\n    (1) adding a background class;\n    (2) adding an \"app\" column with label information, and encoding it as pandas category\n    \"\"\"\ndf = df.assign(\napp=np.where(\ndf[\"android_name\"] == df[\"flow_metadata_bf_label\"],\ndf[\"android_name\"],\n\"background\",\n)\n)\ndf = df.assign(\napp=np.where(\ndf[\"flow_metadata_bf_activity\"] == \"Unknown\", \"background\", df[\"app\"]\n)\n)\ndf = df.assign(\napp=df[\"app\"].astype(\"category\"),\npackets=df[\"packet_data_l4_payload_bytes\"].apply(len),\n)\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_mirage19_generate_splits/","title":"Tcbench libtcdatasets tcbench mirage19 generate splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_mirage22_generate_splits/","title":"mirage22_generate_splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/","title":"ucdavis_icdm19_generate_splits","text":"<p>This module is taking the monolithic parquet files generated using  ucdavis-icdm19_csv-to-parquet.py and create random \"splits\" for training.</p> <p>According to the logic of the paper those splits contains 100 samples from the /pretraining while two test splits are generated using the original /Retraining(human-triggered) and  /Retraining(script-triggered) partitions</p>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/#tcbench.libtcdatasets.ucdavis_icdm19_generate_splits.generate_train_splits","title":"<code>generate_train_splits(path, n_splits=5, seed=12345)</code>","text":"<p>Extract n_splits of 100 samples per classes</p> Source code in <code>tcbench/libtcdatasets/ucdavis_icdm19_generate_splits.py</code> <pre><code>def generate_train_splits(\npath: pathlib.Path, n_splits: int = 5, seed=12345\n) -&gt; List[pd.DataFrame]:\n\"\"\"Extract n_splits of 100 samples per classes\"\"\"\npath = pathlib.Path(path)\nsave_to = path.parent\ndf = pd.read_parquet(path).reset_index(drop=True)\n# Quote from Sec.3.1\n# \"for training set we use only 100 \"triggered by script\" flows per class\"\n#\n# We interpret this as the training data is selected from the\n# /pretraining folder of the original dataset\n# as the other two subfolders have &lt; 100 samples\npartition = \"pretraining\"\napps = df[\"app\"].unique()\nn_samples = 100\nrng = np.random.default_rng(seed)\ndf_tmp = df[df[\"partition\"] == partition]\ntrain_splits = [[] for _ in range(n_splits)]\nfor app in apps:\nindexes = df_tmp[df_tmp[\"app\"] == app].index.values\nrng.shuffle(indexes)\nindexes = indexes[: n_samples * n_splits]\nfor split_indexes, l_split in zip(np.split(indexes, n_splits), train_splits):\nl_split.append(df_tmp.loc[split_indexes])\nsamples_expected = len(apps) * n_samples\nfor idx, l_split in enumerate(train_splits):\nsplit = pd.concat(l_split)\nsamples_found = split.shape[0]\nassert (\nsamples_found == samples_expected\n), f\"generated a split with {samples_found} samples rather than {samples_expected}\"\nfname = path.parent / \"imc23\" / f\"train_split_{idx}.parquet\"\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\nprint(f\"saving: {fname}\")\nsplit.to_parquet(fname)\ntrain_splits[idx] = split\nsplit = pd.read_parquet(path.parent / \"imc23\" / f\"train_split_0.parquet\")\nser = split[\"app\"].value_counts()\nrich_samples_count_report(\nser, title=f\"samples count : train_split = 0 to {n_splits-1}\"\n)\nreturn train_splits\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/#tcbench.libtcdatasets.ucdavis_icdm19_generate_splits.generate_test_splits","title":"<code>generate_test_splits(path)</code>","text":"<p>Extract the predefined test splits from the monolithic parquet file</p> Source code in <code>tcbench/libtcdatasets/ucdavis_icdm19_generate_splits.py</code> <pre><code>def generate_test_splits(path: pathlib.Path) -&gt; List[pd.DataFrame]:\n\"\"\"Extract the predefined test splits from the monolithic parquet file\"\"\"\npath = pathlib.Path(path)\n# print(f\"loading: {path}\")\ndf = pd.read_parquet(path).reset_index(drop=True)\ndf_test_human = df[df[\"partition\"] == \"retraining-human-triggered\"]\ndf_test_script = df[df[\"partition\"] == \"retraining-script-triggered\"]\nfor df_tmp, name in zip((df_test_human, df_test_script), (\"human\", \"script\")):\nfname = path.parent / \"imc23\" / f\"test_split_{name}.parquet\"\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\nprint(f\"\\nsaving: {fname}\")\ndf_tmp.to_parquet(fname)\nser = df_tmp[\"app\"].value_counts()\nrich_samples_count_report(ser, title=f\"samples count : {fname.stem}\")\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/","title":"utmobilenet21_generate_splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/#tcbench.libtcdatasets.utmobilenet21_generate_splits.filter_dataset","title":"<code>filter_dataset(df, min_pkts=10, min_samples_per_class=100)</code>","text":"<p>Remove flows with less than 10 packets and classes with less than 100 samples</p> Source code in <code>tcbench/libtcdatasets/utmobilenet21_generate_splits.py</code> <pre><code>def filter_dataset(\ndf: pd.DataFrame, min_pkts: int = 10, min_samples_per_class: int = 100\n) -&gt; pd.DataFrame:\n\"\"\"Remove flows with less than 10 packets and classes with less than 100 samples\"\"\"\n# filtering out flows with less the 10 packets\ndf = df[df[\"packets\"] &gt; min_pkts]\nfiltered_samples_count = df[\"app\"].value_counts()\n# removing classes with less than 100 samples\nvalid_classes = filtered_samples_count[\nfiltered_samples_count &gt; min_samples_per_class\n].index.tolist()\ndf = df[df[\"app\"].isin(valid_classes)]\nfinal_samples_count = df[\"app\"].value_counts()\nfinal_samples_count = final_samples_count[final_samples_count &gt; 0]\nfinal_samples_count.name = \"expected_samples\"\ndf = df.drop(\"row_id\", axis=1)\ndf = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\ndf = df.set_index(\"row_id\", drop=False)\ndf.index.name = None\ndf = df.assign(app=df[\"app\"].astype(str).astype(\"category\"))\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/#tcbench.libtcdatasets.utmobilenet21_generate_splits._verify_splits","title":"<code>_verify_splits(df, df_splits)</code>","text":"<p>Double check that by pulling the samples based on train/val/test indexes we obtain the per-class samples count</p> Source code in <code>tcbench/libtcdatasets/utmobilenet21_generate_splits.py</code> <pre><code>def _verify_splits(df, df_splits):\n\"\"\"Double check that by pulling the samples based\n    on train/val/test indexes we obtain the per-class samples count\n    \"\"\"\nexpected_samples_count = df[\"app\"].value_counts()\nexpected_samples_count.name = \"expected_samples\"\nser = df_splits.iloc[0]\ntrain_indexes = ser[\"train_indexes\"]\nval_indexes = ser[\"val_indexes\"]\ntest_indexes = ser[\"test_indexes\"]\ndf_tmp = pd.DataFrame(\n(\ndf.iloc[train_indexes][\"app\"].value_counts(),\ndf.iloc[val_indexes][\"app\"].value_counts(),\ndf.iloc[test_indexes][\"app\"].value_counts(),\n),\nindex=[\"train_samples\", \"val_samples\", \"test_samples\"],\n).T\ndf_tmp = df_tmp.assign(total=df_tmp.sum(axis=1))\ndf_tmp = pd.concat((df_tmp, expected_samples_count), axis=1)\nassert (df_tmp[\"total\"] == df_tmp[\"expected_samples\"]).all()\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/","title":"ucdavis_icdm19_csv_to_parquet","text":"<p>This module is preprocessing the original CSV files of the ucdavis-icdm19 paper  (one csv file for each differen flow) to generate a single parquet file where each  row encodes the per-flow timeseries and  numpy arrays</p> <p>The input dataset is composed of three set  of files in 3 subfolders: /pretraining, /Retraining(human-triggered) and /Retraining(script-triggered) ---we call these partitions.</p> <p>Within each partition, there are 5 subfolders, one for each application.</p> <p>For each application, a different file represents a different flow (the flowid is the filename itself). Each flow reports 4 columns corresponding to \"absolute time\", \"relative time to first packet\", \"packet size\" and \"direction\" of each individual packet of a flow.</p> <p>The aim of this module is load all CSVs (across partitions and apps) into a single parquet file where each row corresponds to a different flow described by the following properties - row_id: a unique row index - app: one of ['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'] - flow_id: the original file name without extension - partition: one of ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'] - num_pkts: number of packets in the flow   - duration: duration of the flow - bytes: number of bytes of the flow - pkts_unixtime: np.array with the absolute time of each packet - timetofirst: np.array with relative time of each packet with respect to the first packet - pkts_size: np.array with each packet size - pkts_dir: np.array with each packet direction (0 or 1) - pkts_iat: np.array with inter packet time</p>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/#tcbench.libtcdatasets.ucdavis_icdm19_csv_to_parquet.worker","title":"<code>worker(fname)</code>","text":"<p>Helper function to load an individual CSV file into a pandas DataFrame</p> Source code in <code>tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py</code> <pre><code>def worker(fname: pathlib.Path) -&gt; pd.DataFrame:\n\"\"\"Helper function to load an individual CSV file\n    into a pandas DataFrame\n    \"\"\"\nfname = pathlib.Path(fname)\napp = fname.parts[-2]\npartition = fname.parts[-3]\ndf = pd.read_csv(\nfname,\nsep=\"\\t\",\nnames=[\"unixtime\", \"timetofirst\", \"pkts_size\", \"pkts_dir\"],\ndtype=dict(\nunixtime=np.float64,\ntimetofirst=np.float64,\npkts_size=np.int16,\npkts_dir=np.int8,\n),\n)\ndf_new = pd.DataFrame(\n[\n[\napp.lower().replace(\" \", \"-\"),  # app\nfname.stem,  # flowid\npartition.lower().replace(\"(\", \"-\").replace(\")\", \"\"),  # partition\ndf.shape[0],  # num_pkts\ndf[\"timetofirst\"].values[-1],  # duration\ndf[\"pkts_size\"].sum(),  # bytes\ndf[\"unixtime\"].values,  # unixtime\ndf[\"timetofirst\"].values,  # timetofirst\ndf[\"pkts_size\"].values,  # pkts_size\ndf[\"pkts_dir\"].values,  # pkts_dir\ndf[\"timetofirst\"].diff().fillna(0).values,  # pkts_iat\n]\n],\ncolumns=[\n\"app\",\n\"flow_id\",\n\"partition\",\n\"num_pkts\",\n\"duration\",\n\"bytes\",\n\"unixtime\",\n\"timetofirst\",\n\"pkts_size\",\n\"pkts_dir\",\n\"pkts_iat\",\n],\n)\nreturn df_new\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/#tcbench.libtcdatasets.ucdavis_icdm19_csv_to_parquet.load_files","title":"<code>load_files(folder, n_workers=20)</code>","text":"<p>Load all CSVs (across the 3 dataset partitions) using multiprocessing and concatenate them into a single DataFrame</p> Source code in <code>tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py</code> <pre><code>def load_files(folder: pathlib.Path, n_workers=20) -&gt; pd.DataFrame:\n\"\"\"Load all CSVs (across the 3 dataset partitions)\n    using multiprocessing and concatenate them into a single DataFrame\n    \"\"\"\npartitions = [\n\"pretraining\",\n\"Retraining(human-triggered)\",\n\"Retraining(script-triggered)\",\n]\napp_names = [\n\"Google Doc\",\n\"Google Drive\",\n\"Google Music\",\n\"Google Search\",\n\"Youtube\",\n]\n# check that we have all folder\nsubfolders = list(itertools.product(partitions, app_names))\nfiles = []\nfor partition, app in subfolders:\npath = folder / partition / app\nif not path.exists():\nraise RuntimeError(f\"missing {path}\")\nfiles.extend(list(path.glob(\"*.txt\")))\nfiles = sorted(files)\nprint(f\"found {len(files)} CSV files to load\")\nfrom tcbench.cli import get_rich_console\nwith Progress(console=get_rich_console()) as progress:\ntask_id = progress.add_task(\"Converting CSVs...\", total=len(files))\nl = []\nwith multiprocessing.Pool(n_workers) as pool:\nfor item in pool.imap(worker, files):\nl.append(item)\nprogress.advance(task_id)\nprint(f\"concatenating files\")\n# sorting to make sure that\n# multiprocessing does not (unintentionally)\n# breaks ordering (for replicability)\ndf = pd.concat(l, axis=0).sort_values(by=[\"partition\", \"flow_id\"])\n# adding a unique row\ndf = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\ndf = df.assign(app=df[\"app\"].astype(\"category\"))\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/","title":"utmobilenet21_csv_to_parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.create_connection_id","title":"<code>create_connection_id(ser)</code>","text":"<p>Associate a 5-tuple connection id to a flow</p> <p>Parameters:</p> Name Type Description Default <code>ser</code> <code>pd.Series</code> <p>a pandas Series with per-flow data</p> required Return <p>A string encoding the 5-tuple connection id</p> Source code in <code>tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def create_connection_id(ser: pd.Series) -&gt; str:\n\"\"\"Associate a 5-tuple connection id to a flow\n    Arguments:\n        ser: a pandas Series with per-flow data\n    Return:\n        A string encoding the 5-tuple connection id\n    \"\"\"\nproto = ser[\"ip_proto\"]\nsrc_ip, dst_ip = ser[[\"ip_src\", \"ip_dst\"]].values\nif proto == 6:\nsrc_port, dst_port = ser[[\"tcp_srcport\", \"tcp_dstport\"]].values\nelse:\nsrc_port, dst_port = ser[[\"udp_srcport\", \"udp_dstport\"]].values\nif src_ip &gt; dst_ip:\nsrc_ip, src_port, dst_ip, dst_port = dst_ip, dst_port, src_ip, src_port\nconn_id = f\"{src_ip}_{src_port}_{dst_ip}_{dst_port}_{proto}\"\nreturn conn_id\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.worker","title":"<code>worker(fname, tmp_folder)</code>","text":"<p>Helper function responsible for processing a utmobilenet21 CSV and save it into a temporary folder</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the input CSV to process</p> required <code>tmp_folder</code> <code>str</code> <p>the temporary folder where to store intermediate output</p> required Source code in <code>tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def worker(fname: str, tmp_folder: str) -&gt; None:\n\"\"\"Helper function responsible for processing a\n    utmobilenet21 CSV and save it into a temporary folder\n    Arguments:\n        fname: the input CSV to process\n        tmp_folder: the temporary folder where to store\n            intermediate output\n    \"\"\"\n# load everything as string (type casting moved later)\ndf = pd.read_csv(fname, dtype=str)\ndf = df.assign(\nfname=fname.name,\npartition=fname.parent.name,\n)\n# reformat columns name\ndf.columns = [col.replace(\".\", \"_\") for col in df.columns]\nif \"location\" not in df.columns:\ndf = df.assign(location=\"\")\n# drop columns not needed\ndf = df[COLUMNS_TO_KEEP]\n# keep only TCP and UDP\ndf = df[\n(df[\"ip_proto\"].isin({\"6\", \"17\"}))\n&amp; (~df[\"ip_src\"].isna())\n&amp; (~df[\"ip_dst\"].isna())\n&amp; (df[\"ip_src\"] != \"127.0.0.1\")\n&amp; (df[\"ip_dst\"] != \"127.0.0.1\")\n]\n# extract app label\nfunc_parse_timestamp = functools.partial(\ndateutil.parser.parse, tzinfos={\"CDT\": -5 * 3600}\n)\ndf = df.assign(\napp=df[\"fname\"].apply(lambda text: text.split(\" \", 1)[0]),\nframe_time=df[\"frame_time\"]\n.apply(lambda text: float(func_parse_timestamp(text).strftime(\"%s.%f\")))\n.astype(float),\nconn_id=df.apply(create_connection_id, axis=1).astype(str),\n).to_parquet(tmp_folder / fname.with_suffix(\".parquet\").name)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.create_time_series","title":"<code>create_time_series(df)</code>","text":"<p>Compose flow time series by grouping packets the same flow into numpy arrays</p> Argument <p>df: the input pandas DataFrame where each entry corresponds to a different packet</p> Return <p>The generate per-flow pandas DataFrame containing the following columns (\"src_ip\", \"src_port\", \"dst_ip\", \"dst_port\", \"ip_proto\", \"first\", \"last\", \"duration\", \"packets\", \"bytes\", \"timetofirst\", \"pkts_size\", \"pkts_dir\", \"partition\", \"location\", \"fname\", \"app\" )</p> Source code in <code>tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def create_time_series(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Compose flow time series by grouping packets the same flow into\n    numpy arrays\n    Argument:\n        df: the input pandas DataFrame where each entry corresponds to a different packet\n    Return:\n        The generate per-flow pandas DataFrame containing the following\n        columns (\"src_ip\", \"src_port\", \"dst_ip\", \"dst_port\", \"ip_proto\",\n        \"first\", \"last\", \"duration\", \"packets\", \"bytes\", \"timetofirst\",\n        \"pkts_size\", \"pkts_dir\", \"partition\", \"location\", \"fname\", \"app\"\n        )\n    \"\"\"\nif df.shape[0] == 0:\nreturn pd.DataFrame(\ncolumns=[\n\"src_ip\",\n\"src_port\",\n\"dst_ip\",\n\"dst_port\",\n\"ip_proto\",\n\"first\",\n\"last\",\n\"duration\",\n\"packets\",\n\"bytes\",\n\"timetofirst\",\n\"pkts_size\",\n\"pkts_dir\",\n\"partition\",\n\"location\",\n\"fname\",\n\"app\",\n]\n)\ndf_tmp = df.sort_values(by=\"frame_time\")\nfirst_pkt = df_tmp.iloc[0]\nsrc_ip, dst_ip = first_pkt[\"ip_src\"], first_pkt[\"ip_dst\"]\nfirst = first_pkt[\"frame_time\"]\nlast = df_tmp.iloc[-1][\"frame_time\"]\nduration = last - first\npackets = df_tmp.shape[0]\nip_proto = first_pkt[\"ip_proto\"]\npartition = first_pkt[\"partition\"]\nlocation = first_pkt[\"location\"]\nfname = first_pkt[\"fname\"]\napp = fname.split(\"_\", 1)[0]\nif df_tmp[\"ip_proto\"].values[0] == 6:\npkts_size = df_tmp[\"tcp_len\"].fillna(0).values\nsrc_port, dst_port = first_pkt[\"tcp_srcport\"], first_pkt[\"tcp_dstport\"]\nelse:\npkts_size = df_tmp[\"udp_length\"].fillna(0).values\nsrc_port, dst_port = first_pkt[\"udp_srcport\"], first_pkt[\"udp_dstport\"]\n_bytes = pkts_size.sum()\ntimetofirst = df_tmp[\"frame_time\"].diff().fillna(0).values\npkts_dir = (df_tmp[\"ip_src\"] == first_pkt[\"ip_src\"]).astype(int).values\ndf_res = pd.DataFrame(\n[\n[\nsrc_ip,\nsrc_port,\ndst_ip,\ndst_port,\nip_proto,\nfirst,\nlast,\nduration,\npackets,\n_bytes,\ntimetofirst,\npkts_size,\npkts_dir,\npartition,\nlocation,\nfname,\napp,\n]\n],\ncolumns=[\n\"src_ip\",\n\"src_port\",\n\"dst_ip\",\n\"dst_port\",\n\"ip_proto\",\n\"first\",\n\"last\",\n\"duration\",\n\"packets\",\n\"bytes\",\n\"timetofirst\",\n\"pkts_size\",\n\"pkts_dir\",\n\"partition\",\n\"location\",\n\"fname\",\n\"app\",\n],\n)\nreturn df_res\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.main","title":"<code>main(args)</code>","text":"<p>Main function loading CSVs and converting them into a monolithic parquet file</p> Source code in <code>tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def main(args: argparse.Namespace):\n\"\"\"Main function loading CSVs and converting them into\n    a monolithic parquet file\n    \"\"\"\nif (args.input_folder / \"csvs\").exists():\nargs.input_folder /= \"csvs\"\nwith Client(n_workers=args.num_workers) as client:\nstaging_folder = pathlib.Path(args.tmp_staging_folder)\noutput_folder = pathlib.Path(args.output_folder)\nextra_line = False\nfor partition in args.input_folder.iterdir():\nif extra_line:\nprint()\nprint(f\"processing: {partition}\")\nextra_line = True\nfiles = list(partition.glob(\"*.csv\"))\npartition_name = partition.name.lower().replace(\" \", \"_\")\ncurr_staging_folder = staging_folder / partition_name\nif curr_staging_folder.exists():\nshutil.rmtree(str(curr_staging_folder))\ncurr_staging_folder.mkdir(parents=True)\n##############\n# stage1: convert csv to parquet + some cleaning\n##############\n(curr_staging_folder / \"stage1\").mkdir(parents=True)\nfunc = functools.partial(worker, tmp_folder=curr_staging_folder / \"stage1\")\nprint(f\"found {len(files)} files\")\nwith richprogress.Progress(\nrichprogress.TextColumn(\"[progress.description]{task.description}\"),\nrichprogress.BarColumn(),\nrichprogress.MofNCompleteColumn(),\nrichprogress.TimeElapsedColumn(),\nconsole=console,\n) as progressbar:\ntask_id = progressbar.add_task(\"Converting CSVs...\", total=len(files))\nwith multiprocessing.Pool(min(len(files), 30)) as pool:\nfor item in pool.imap_unordered(func, files):\nprogressbar.advance(task_id)\nprint(\"stage1 completed\")\n#########################\n# stage2: repartition (if needed)\n#########################\nddf = dd.read_parquet(curr_staging_folder / \"stage1\")\nif len(files) &gt; 1000:\nddf = ddf.reset_index(drop=True).repartition(50)\nddf.reset_index(drop=True).persist()\nprogress(ddf)\nddf.to_parquet(curr_staging_folder / \"stage2\")\nprint(\"stage2 completed\")\n#########################\n# stage3: minor types conversion\n#########################\nddf = dd.read_parquet(curr_staging_folder / \"stage2\").reset_index(drop=True)\n# convert all to float (because some numbers are float?)\n# then convert to int64 (which supports nan)\nddf1 = ddf.astype(\n{\n\"frame_time\": float,\n\"ip_proto\": float,\n\"tcp_len\": float,\n\"tcp_srcport\": float,\n\"tcp_dstport\": float,\n\"udp_srcport\": float,\n\"udp_dstport\": float,\n\"udp_length\": float,\n}\n).persist()\nprogress(ddf1)\nddf1.to_parquet(curr_staging_folder / \"stage3\")  # , schema=pa_schema)\nprint(\"stage3 completed\")\n#########################\n# last: time series creation\n#########################\nddf = dd.read_parquet(curr_staging_folder / \"stage3\").reset_index(drop=True)\nmeta = pd.DataFrame(\ndtype=object,\ncolumns=[\n\"src_ip\",\n\"src_port\",\n\"dst_ip\",\n\"dst_port\",\n\"ip_proto\",\n\"first\",\n\"last\",\n\"duration\",\n\"packets\",\n\"bytes\",\n\"timetofirst\",\n\"pkts_size\",\n\"pkts_dir\",\n\"partition\",\n\"location\",\n\"fname\",\n\"app\",\n],\n)\nmeta = meta.astype(\n{\n\"src_ip\": str,\n\"dst_ip\": str,\n\"src_port\": int,\n\"dst_port\": int,\n\"ip_proto\": int,\n\"first\": float,\n\"last\": float,\n\"duration\": float,\n\"packets\": int,\n\"bytes\": int,\n\"partition\": str,\n\"location\": str,\n\"fname\": str,\n\"app\": str,\n}\n)\nschema = dict(meta.dtypes)\npa_schema = pa.schema(\n[\n(\nname.lower(),\npa.string()\nif dtype == np.dtype(object)\nelse pa.from_numpy_dtype(dtype),\n)\nfor name, dtype in schema.items()\nif name not in (\"timetofirst\", \"pkts_size\", \"pkts_dir\")\n]\n)\npa_schema = pa_schema.append(pa.field(\"pkts_size\", pa.list_(pa.int64())))\npa_schema = pa_schema.append(pa.field(\"pkts_dir\", pa.list_(pa.int64())))\npa_schema = pa_schema.append(\npa.field(\"timetofirst\", pa.list_(pa.float64()))\n)\nddf2 = ddf.groupby(\"conn_id\").apply(create_time_series, meta=meta).persist()\nprogress(ddf2)\nddf2.reset_index(drop=True).to_parquet(\ncurr_staging_folder / \"stage4\", schema=pa_schema\n)\nprint(\"stage4 completed\")\n### we can finally pack everything together\ndf = (\ndd.read_parquet(curr_staging_folder / \"stage4\")\n.reset_index(drop=True)\n.compute()\n)\nfname = (staging_folder / partition_name).with_suffix(\".parquet\")\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\nprint(f\"saving: {fname}\")\ndf.to_parquet(fname)\nprint(\"merging all partitions\")\nif not output_folder.exists():\noutput_folder.mkdir(parents=True)\ndf = dd.read_parquet(staging_folder / \"*.parquet\").compute()\ndf = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\ndf = df.assign(app=df[\"app\"].astype(\"category\"))\nfname = output_folder / \"utmobilenet21.parquet\"\nprint(f\"saving: {fname}\")\ndf.to_parquet(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/","title":"aimutils","text":"<p>This module contains a collection  of utility functions used to interact with a AIM repository</p>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.list_repo","title":"<code>list_repo(repo)</code>","text":"<p>List all runs in the repository as pandas DataFrame</p> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def list_repo(repo: aim.Repo) -&gt; pd.DataFrame:\n\"\"\"List all runs in the repository as pandas DataFrame\"\"\"\n#return pd.concat(run.dataframe() for run in repo.iter_runs())\nreturn pd.concat(\nitem.run.dataframe()\nfor item in repo.query_runs('', report_mode=0).iter_runs()\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.get_latest_campaign_id","title":"<code>get_latest_campaign_id(repo, experiment_name)</code>","text":"<p>Extract the latest campaign id (defined when launching a modeling campaign) from a AIM repo</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>aim.Repo</code> <p>the AIM repository to use</p> required <code>experiment_name</code> <code>str</code> <p>the experiment name from which extract the last campaign</p> required Return <p>The campaign id found</p> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def get_latest_campaign_id(repo: aim.Repo, experiment_name: str) -&gt; str:\n\"\"\"Extract the latest campaign id (defined when launching\n    a modeling campaign) from a AIM repo\n    Arguments:\n        repo: the AIM repository to use\n        experiment_name: the experiment name from which extract the last campaign\n    Return:\n        The campaign id found\n    \"\"\"\nquery = f\"run.experiment == '{experiment_name}'\"\ndf = pd.concat(\n[\nentry.run.dataframe()\nfor entry in repo.query_runs(query, report_mode=0).iter_runs()\n]\n)\ncampaign_ids = df[\"hparams.campaign_id\"].replace({np.nan: \"0\"})\nreturn campaign_ids.unique().max()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.load_campaign","title":"<code>load_campaign(repo, campaign_id=None, experiment_name=None)</code>","text":"<p>Load the latest campaign of experiment into a pandas DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>aim.Repo</code> <p>the AIM repository to query</p> required <code>campaign_id</code> <code>str</code> <p>the campaing_id of the runs to select (if None, it will search for the latest campaign_id)</p> <code>None</code> <code>experiment_name</code> <code>str</code> <p>the experiment_name for the runs to select</p> <code>None</code> Return <p>A tuple with two objects: a DataFrame collecting the informatio for all the runs associated to the campaign, and a list of run object</p> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def load_campaign(\nrepo: aim.Repo, \ncampaign_id: str = None,\nexperiment_name: str = None\n) -&gt; Tuple[pd.DataFrame, List[aim.Run]]:\n\"\"\"Load the latest campaign of experiment into a pandas DataFrame\n    Arguments:\n        repo: the AIM repository to query\n        campaign_id: the campaing_id of the runs to select (if None,\n            it will search for the latest campaign_id)\n        experiment_name: the experiment_name for the runs to select\n    Return:\n        A tuple with two objects: a DataFrame collecting the informatio\n        for all the runs associated to the campaign, and a list\n        of run object\n    \"\"\"\nif campaign_id is None:\ncampaign_id = get_latest_campaign_id(repo, args.experiment_name)\nprint(f\"latest campaign_id: {campaign_id}\")\nquery = f\"\"\"\n    run.hparams[\"campaign_id\"] == '{campaign_id}'\n    \"\"\"\nif experiment_name != '':\nquery += f\"and run.experiment == '{experiment_name}'\"\nruns = []\nl = []\nfor entry in repo.query_runs(query, report_mode=0).iter_runs():\nl.append(entry.run.dataframe())\nruns.append(entry.run)\ndf = pd.concat(l)\n## remove __system columns\ncols_to_drop = [col for col in df.columns if col.startswith(\"__system\")]\nif cols_to_drop:\ndf = df.drop(cols_to_drop, axis=1)\n## rename hparams.blablabla\nrename_cols = {col: col.replace(\"hparams.\", \"\") for col in df.columns}\nif rename_cols:\ndf = df.rename(rename_cols, axis=1)\nreturn df, runs\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.query_metric","title":"<code>query_metric(repo, run_hashes, metric, context)</code>","text":"<p>Collect all metrics associated to a list of runs</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>aim.Repo</code> <p>the AIM repo to query</p> required <code>run_hashes</code> <code>List[str]</code> <p>a list of run hash values to search</p> required <code>metric</code> <code>str</code> <p>the metric to extract (e.g., \"acc\")</p> required <code>context</code> <code>str</code> <p>the split on which the metric was computed (e.g., \"test\")</p> required Return <p>A dictionary where keys maps to run hashes, and values to the related metric found</p> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def query_metric(\nrepo: aim.Repo,\nrun_hashes: List[str],\nmetric: str,\ncontext: str,\n) -&gt; Dict[str, Any]:\n\"\"\"Collect all metrics associated to a list of runs\n    Arguments:\n        repo: the AIM repo to query\n        run_hashes: a list of run hash values to search\n        metric: the metric to extract (e.g., \"acc\")\n        context: the split on which the metric was computed (e.g., \"test\")\n    Return:\n        A dictionary where keys maps to run hashes, and values\n        to the related metric found\n    \"\"\"\nquery = \"\"\"\n    metric.name == '{metric}' and \n    metric.context['subset'] == '{context}'\n    \"\"\".format(\nmetric=metric, context=context\n)\nquery = f\"run.hash in {run_hashes} and {query}\"\nmetrics = {\nitem.run.hash: item.values.last()[1]\nfor item in repo.query_metrics(query, report_mode=0)\n}\nif len(metrics) != len(run_hashes):\nmissing_hashes = set(run_hashes) - set(metrics.keys())\nprint(\nf\"WARNING: found {len(metrics)} metrics for metric={metric} context={context}\"\n)\nfor run_hash in missing_hashes:\nif run_hash not in metrics:\nmetrics[run_hash] = np.nan\nreturn metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.metrics_to_pandas","title":"<code>metrics_to_pandas(repo, df_run, metrics, contexts)</code>","text":"<p>Loads a set of metrics across multiple test splits into a dataframe already containing campaign related information</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>aim.Repo</code> <p>the AIM repo to query</p> required <code>df_run</code> <code>pd.DataFrame</code> <p>a dataframe obtained invoking .load_latest_campaign()</p> required <code>metrics</code> <code>List[str]</code> <p>the list of metrics to load</p> required <code>contexts</code> <code>List[str]</code> <p>the set of split names to query metrics from</p> required Return <p>Expands df_campaign by adding columns related to the loaded metric values</p> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def metrics_to_pandas(\nrepo: aim.Repo,\ndf_run: pd.DataFrame,\nmetrics: List[str],\ncontexts: List[str],\n) -&gt; pd.DataFrame:\n\"\"\"Loads a set of metrics across multiple test splits into\n    a dataframe already containing campaign related information\n    Arguments:\n        repo: the AIM repo to query\n        df_run: a dataframe obtained invoking .load_latest_campaign()\n        metrics: the list of metrics to load\n        contexts: the set of split names to query metrics from\n    Return:\n        Expands df_campaign by adding columns related to the loaded metric values\n    \"\"\"\ndf_campaign = df_run.copy()\n## add (empty) metrics columns\nfor mtr in metrics:\ndf_campaign.loc[:, mtr] = np.nan\ndf_campaign = df_campaign.assign(test_split_name=None)\nl = []\nfor ctx in contexts: \nwith console.status(f\"collecting metrics {ctx}...\", spinner=\"dots\"):\ndf_new = df_campaign.copy()\ndf_new = df_new.assign(test_split_name=ctx)\nfor mtr in metrics:\ndf_new.loc[:, mtr] = df_new[\"hash\"].copy()\nmetric_dict = query_metric(\nrepo=repo,\nrun_hashes=set(df_campaign[\"hash\"].values),\nmetric=mtr,\ncontext=ctx,\n)\ndf_new.loc[:, mtr] = df_new[mtr].replace(metric_dict)\nl.append(df_new)\ndf_new = pd.concat(l)\nreturn df_new\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.track_metrics","title":"<code>track_metrics(tracker, metrics, context, epoch=None)</code>","text":"<p>Save into a run metrics information</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>aim.Run</code> <p>the AIM run where to save metrics</p> required <code>metrics</code> <code>Dict[str, Any]</code> <p>a dictionary of key-value pairs to save</p> required <code>context</code> <code>str</code> <p>the context related to the metrics (e.g., train/val/test)</p> required <code>epoch</code> <code>int</code> <p>the epoch when the metrics where collected</p> <code>None</code> Source code in <code>tcbench/modeling/aimutils.py</code> <pre><code>def track_metrics(\ntracker: aim.Run, metrics: Dict[str, Any], context: str, epoch: int = None\n) -&gt; None:\n\"\"\"Save into a run metrics information\n    Arguments:\n        tracker: the AIM run where to save metrics\n        metrics: a dictionary of key-value pairs to save\n        context: the context related to the metrics (e.g., train/val/test)\n        epoch: the epoch when the metrics where collected\n    \"\"\"\nfor name, value in metrics.items():\ntracker.track(value, name, epoch=epoch, context=dict(subset=context))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/","title":"augmentation","text":"<p>This module contains the logic to generate Flowpic data representations and apply augmentations on either flowpic or raw time series.</p> <p>Each augmentation is handled as a subclass of Augmentation which is a callable object.</p> <p>Moreover, each augmentation is designed to have its own  random generator and a set of hyperparameters which which  generate the parameters of an augmentation. Differently from pytorch APIs, this enables visibility on the set of params used for an augmentation (use .get_params())</p>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.Augmentation","title":"<code>Augmentation</code>","text":"<p>Base class for augmentation functions</p> <p>Attributes:</p> Name Type Description <code>rng</code> <p>the numpy random generator used for sampling parameters</p> <code>hyper_params</code> <p>a dictionary of hypter parameters to set up the sampling of the augmentation parameters</p> <code>paramgs</code> <p>a dictionary with the latest parameters generated for a transformation</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class Augmentation:\n\"\"\"\n    Base class for augmentation functions\n    Attributes:\n        rng: the numpy random generator used for sampling parameters\n        hyper_params: a dictionary of hypter parameters to set up the\n            sampling of the augmentation parameters\n        paramgs: a dictionary with the latest parameters generated\n            for a transformation\n    \"\"\"\ndef __init__(\nself,\nrng: np.random.Generator,\nrandomize_at_every_call: bool = True,\n**hyper_params: Dict[str, Any],\n):\n\"\"\"\n        Arguments:\n            rng: a numpy random number generator\n            randomize_at_every_call: if True, the parameters for the augmentation\n                are generated at each call\n        \"\"\"\nself.rng = rng\nself.hyper_params = hyper_params\nself.randomize_at_every_call = randomize_at_every_call\nself.is_first_call = True\nself.params = {}\ndef update_params(self) -&gt; None:\npass\ndef get_params(self) -&gt; Dict[str, Any]:\nreturn self.params\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.Augmentation.__init__","title":"<code>__init__(rng, randomize_at_every_call=True, **hyper_params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>rng</code> <code>np.random.Generator</code> <p>a numpy random number generator</p> required <code>randomize_at_every_call</code> <code>bool</code> <p>if True, the parameters for the augmentation are generated at each call</p> <code>True</code> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def __init__(\nself,\nrng: np.random.Generator,\nrandomize_at_every_call: bool = True,\n**hyper_params: Dict[str, Any],\n):\n\"\"\"\n    Arguments:\n        rng: a numpy random number generator\n        randomize_at_every_call: if True, the parameters for the augmentation\n            are generated at each call\n    \"\"\"\nself.rng = rng\nself.hyper_params = hyper_params\nself.randomize_at_every_call = randomize_at_every_call\nself.is_first_call = True\nself.params = {}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationRotate","title":"<code>AugmentationRotate</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for random rotation</p> <p>The rotation can be configured passing a \"min_degree\" and \"max_degree\" as hyper parameters (-10, 10) by default.</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationRotate(Augmentation):\n\"\"\"\n    An augmentation for random rotation\n    The rotation can be configured passing a \"min_degree\" and \"max_degree\"\n    as hyper parameters (-10, 10) by default.\n    \"\"\"\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.hyper_params.setdefault(\"min_degree\", -10)\nself.hyper_params.setdefault(\"max_degree\", 10)\nself.params = dict(angle=0)\ndef update_params(self) -&gt; None:\nself.params[\"angle\"] = self.rng.uniform(\nself.hyper_params[\"min_degree\"], self.hyper_params[\"max_degree\"]\n)\ndef __call__(self, mtx: np.array) -&gt; np.array:\nif self.is_first_call or self.randomize_at_every_call:\nself.update_params()\nself.is_first_call = False\ntensor = numpy_to_tensor(mtx)\n# .rotate() is counter-clockwise (which\n# is counter intuitive, so we change the sign\ntensor = T.functional.rotate(tensor, -self.params[\"angle\"])\nreturn tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationHorizontalFlip","title":"<code>AugmentationHorizontalFlip</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for static horizontal flip</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationHorizontalFlip(Augmentation):\n\"\"\"\n    An augmentation for static horizontal flip\n    \"\"\"\ndef __init__(self, *args, **kwargs):\nsuper().__init__(rng=None)\ndef __call__(self, mtx: np.array) -&gt; np.array:\ntensor = numpy_to_tensor(mtx)\ntensor = T.functional.hflip(tensor)\nreturn tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationColorJitter","title":"<code>AugmentationColorJitter</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for applying random modification of brightness, saturation, contrast and hue</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationColorJitter(Augmentation):\n\"\"\"\n    An augmentation for applying random modification of\n    brightness, saturation, contrast and hue\n    \"\"\"\n# Code inspired by the default logic of torchvision.transformations.ColorJitter\n# https://pytorch.org/vision/main/_modules/torchvision/transforms/transforms.html#ColorJitter\ndef __init__(\nself,\nrng,\nrandomize_at_every_call=True,\nbrightness=0.8,\nsaturation=0.8,\ncontrast=0.8,\nhue=0.2,\n):\nsuper().__init__(\nrng,\nrandomize_at_every_call,\nbrightness=brightness,\nsaturation=saturation,\ncontrast=contrast,\nhue=hue,\n)\n(\nself.hyper_params[\"min_brightness\"],\nself.hyper_params[\"max_brightness\"],\n) = self._check_input(self.hyper_params[\"brightness\"], \"brightness\")\n(\nself.hyper_params[\"min_saturation\"],\nself.hyper_params[\"max_saturation\"],\n) = self._check_input(self.hyper_params[\"saturation\"], \"saturation\")\n(\nself.hyper_params[\"min_contrast\"],\nself.hyper_params[\"max_contrast\"],\n) = self._check_input(self.hyper_params[\"contrast\"], \"contrast\")\nself.hyper_params[\"min_hue\"], self.hyper_params[\"max_hue\"] = self._check_input(\nself.hyper_params[\"hue\"],\n\"hue\",\ncenter=0,\nbound=(-0.5, 0.5),\nclip_first_on_zero=False,\n)\nself.params = dict()\ndef _check_input(\nself, value, name, center=1, bound=(0, float(\"inf\")), clip_first_on_zero=True\n):\nif isinstance(value, numbers.Number):\nvalue = [center - float(value), center + float(value)]\nif clip_first_on_zero:\nvalue[0] = max(value[0], 0.0)\nelif isinstance(value, (tuple, list)) and len(value) == 2:\nvalue = [float(value[0]), float(value[1])]\nif not bound[0] &lt;= value[0] &lt;= value[1] &lt;= bound[1]:\nraise ValueError(\nf\"{name} values should be between {bound}, but got {value}.\"\n)\n# if value is 0 or (1., 1.) for brightness/contrast/saturation\n# or (0., 0.) for hue, do nothing\nif value[0] == value[1] == center:\nreturn None\nelse:\nreturn tuple(value)\ndef update_params(self) -&gt; None:\nself._order = self.rng.permutation(4)\nself.params[\"brightness\"] = self.rng.uniform(\nself.hyper_params[\"min_brightness\"], self.hyper_params[\"max_brightness\"]\n)\nself.params[\"saturation\"] = self.rng.uniform(\nself.hyper_params[\"min_saturation\"], self.hyper_params[\"max_saturation\"]\n)\nself.params[\"contrast\"] = self.rng.uniform(\nself.hyper_params[\"min_contrast\"], self.hyper_params[\"max_contrast\"]\n)\nself.params[\"hue\"] = self.rng.uniform(\nself.hyper_params[\"min_hue\"], self.hyper_params[\"max_hue\"]\n)\ndef __call__(self, mtx: np.array) -&gt; np.array:\nif self.is_first_call or self.randomize_at_every_call:\nself.update_params()\ntensor = numpy_to_tensor(mtx)\nfor idx in self._order:\nif idx == 0:\ntensor = T.functional.adjust_brightness(\ntensor, self.params[\"brightness\"]\n)\nelif idx == 1:\ntensor = T.functional.adjust_saturation(\ntensor, self.params[\"saturation\"]\n)\nelif idx == 2:\ntensor = T.functional.adjust_contrast(tensor, self.params[\"contrast\"])\nelse:\ntensor = T.functional.adjust_hue(tensor, self.params[\"hue\"])\nself.is_first_call = False\nreturn tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationPacketLoss","title":"<code>AugmentationPacketLoss</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for applying packet loss (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationPacketLoss(Augmentation):\n\"\"\"\n    An augmentation for applying packet loss (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\ndef __init__(self, rng, randomize_at_every_call=True, delta_time=0.1):\nsuper().__init__(rng, randomize_at_every_call, delta_time=delta_time)\n# Note: differently from the other augmentations,\n# we intentionally sample a new t\n# at every call as to shape the augmentation\n# in function of the input. This is due to\n# possible padding in the flowpic: if the\n# traffic is occurring only at the beginning\n# of the flow, sampling uniformly t from a large\n# window (as from above quote) unlikely alter\n# the input data\ndef __call__(\nself, timetofirst: np.array, pkts_size: np.array\n) -&gt; Tuple[np.array, np.array, np.array]:\nsession_time = timetofirst[-1]\nrandom_t_in_session = self.rng.uniform(low=0.0, high=session_time)\nmin_ts = random_t_in_session - self.hyper_params[\"delta_time\"]\nmax_ts = random_t_in_session + self.hyper_params[\"delta_time\"]\nself.params[\"min_ts\"] = min_ts\nself.params[\"max_ts\"] = max_ts\nindexes_to_drop = np.where((timetofirst &gt;= min_ts) &amp; (timetofirst &lt;= max_ts))[0]\nnew_timetofirst = _copy_and_delete_from_numpy_array(\ntimetofirst, indexes_to_drop\n)\nnew_pkts_size = _copy_and_delete_from_numpy_array(pkts_size, indexes_to_drop)\nreturn new_timetofirst, new_pkts_size, indexes_to_drop\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationTimeShift","title":"<code>AugmentationTimeShift</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for applying time shift (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationTimeShift(Augmentation):\n\"\"\"\n    An augmentation for applying time shift (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\ndef __init__(self, rng, randomize_at_every_call=True, delta_time=1):\nsuper().__init__(rng, randomize_at_every_call, delta_time=delta_time)\ndef update_params(self):\nself.params[\"shift\"] = self.rng.uniform(\n-self.hyper_params[\"delta_time\"], self.hyper_params[\"delta_time\"]\n)\ndef __call__(\nself, timetofirst: np.array, pkts_size: np.array\n) -&gt; Tuple[np.array, np.array, np.array]:\nif self.is_first_call or self.randomize_at_every_call:\nself.update_params()\nnew_timetofirst = np.copy(timetofirst) + self.params[\"shift\"]\nindexes = np.where(new_timetofirst &lt; 0)[0]\nnew_pkts_size = pkts_size\nif len(indexes) &gt; 0:\nnew_timetofirst = new_timetofirst[len(indexes) :]\nnew_pkts_size = np.copy(new_pkts_size)[len(indexes) :]\nreturn new_timetofirst, new_pkts_size, indexes\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationChangeRTT","title":"<code>AugmentationChangeRTT</code>","text":"<p>         Bases: <code>Augmentation</code></p> <p>An augmentation for applying change rtt (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationChangeRTT(Augmentation):\n\"\"\"\n    An augmentation for applying change rtt (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\ndef __init__(self, rng, randomize_at_every_call=True, min_alpha=0.5, max_alpha=1.5):\nsuper().__init__(\nrng, randomize_at_every_call, min_alpha=min_alpha, max_alpha=max_alpha\n)\nself.params = dict()\ndef update_params(self):\nself.params[\"alpha\"] = self.rng.uniform(\nself.hyper_params[\"min_alpha\"], self.hyper_params[\"max_alpha\"]\n)\ndef __call__(\nself, timetofirst: np.array, pkts_size: np.array\n) -&gt; Tuple[np.array, np.array, np.array]:\nif self.is_first_call or self.randomize_at_every_call:\nself.update_params()\nnew_timetofirst = np.copy(timetofirst) * self.params[\"alpha\"]\nreturn new_timetofirst, pkts_size, None\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.get_flowpic","title":"<code>get_flowpic(timetofirst, pkts_size, dim=32, max_block_duration=15)</code>","text":"<p>Generate a Flowpic from time series</p> <p>Parameters:</p> Name Type Description Default <code>timetofirst</code> <code>NDArray</code> <p>time series (in seconds) of the intertime between a packet and the first packet of the flow</p> required <code>pkts_size</code> <code>NDArray</code> <p>time series of the packets size</p> required <code>dim</code> <code>int</code> <p>pixels size of the output representation</p> <code>32</code> <code>max_block_duration</code> <code>int</code> <p>how many seconds of the input time series to process</p> <code>15</code> Return <p>a 2d numpy array encoding a flowpic</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def get_flowpic(\ntimetofirst: NDArray,\npkts_size: NDArray,\ndim: int = 32,\nmax_block_duration: int = 15,\n) -&gt; NDArray:\n\"\"\"Generate a Flowpic from time series\n    Arguments:\n        timetofirst: time series (in seconds) of the intertime between a packet and the first packet of the flow\n        pkts_size: time series of the packets size\n        dim: pixels size of the output representation\n        max_block_duration: how many seconds of the input time series to process\n    Return:\n        a 2d numpy array encoding a flowpic\n    \"\"\"\nindexes = np.where(timetofirst &lt; max_block_duration)[0]\ntimetofirst = timetofirst[indexes]\npkts_size = np.clip(pkts_size[indexes], a_min=0, a_max=MAX_PACKET_SIZE)\ntimetofirst_norm = (timetofirst / max_block_duration) * dim\npkts_size_norm = (pkts_size / MAX_PACKET_SIZE) * dim\nbins = range(dim + 1)\nmtx, _, _ = np.histogram2d(x=pkts_size_norm, y=timetofirst_norm, bins=(bins, bins))\n# Quote from Sec.2.1 of the IMC22 paper\n# &gt; If more than max value (255) packets of\n# &gt; a certain size arrive in a time slot,\n# &gt; we set the pixel value to max value\nmtx = np.clip(mtx, a_min=0, a_max=255).astype(\"uint8\")\nreturn mtx\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.numpy_to_tensor","title":"<code>numpy_to_tensor(mtx)</code>","text":"<p>Transforms a 2d numpy array into a 3d Tensor adding an extra dimension</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def numpy_to_tensor(mtx: np.array) -&gt; torch.Tensor:\n\"\"\"Transforms a 2d numpy array into a 3d Tensor adding an extra dimension\"\"\"\nif len(mtx.shape) == 2:\nreturn torch.from_numpy(np.expand_dims(mtx, 0))\nreturn torch.from_numpy(mtx)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.tensor_to_numpy","title":"<code>tensor_to_numpy(tensor)</code>","text":"<p>Transforms a 3d tensor into a 2d numpy array removing the first dimension</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def tensor_to_numpy(tensor: torch.Tensor) -&gt; np.array:\n\"\"\"Transforms a 3d tensor into a 2d numpy array removing the first dimension\"\"\"\nmtx = tensor.numpy()\nif mtx.ndim &gt; 2 and mtx.shape[0] == 1:\nreturn mtx.squeeze()\nreturn mtx\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation._copy_and_delete_from_numpy_array","title":"<code>_copy_and_delete_from_numpy_array(array, indexes)</code>","text":"<p>Helper function to remove elements from a numpy array</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def _copy_and_delete_from_numpy_array(array: np.array, indexes: np.array) -&gt; np.array:\n\"\"\"Helper function to remove elements from a numpy array\"\"\"\nreturn np.delete(np.copy(array), indexes)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.augmentation_factory","title":"<code>augmentation_factory(aug_name, rng, hyper_params)</code>","text":"<p>A factory method to create instances of augmentation classes</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def augmentation_factory(\naug_name: str, rng: np.random.Generator, hyper_params: Dict[str, Any]\n) -&gt; Augmentation:\n\"\"\"A factory method to create instances of augmentation classes\"\"\"\nif aug_name not in AUGMENTATION_CLASSES:\nreturn None\nif hyper_params is None or len(hyper_params) == 0:\nhyper_params = AUGMENTATION_DEFAULT_HPARAMS.get(aug_name, {}).copy()\nreturn AUGMENTATION_CLASSES[aug_name](rng, **hyper_params)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.apply_augmentation","title":"<code>apply_augmentation(aug_name, aug, **kwargs)</code>","text":"<p>Applies a transformation to the input features</p> Source code in <code>tcbench/modeling/augmentation.py</code> <pre><code>def apply_augmentation(\naug_name: str, aug: Augmentation, **kwargs: Dict[str, Any]\n) -&gt; Dict[str, NDArray]:\n\"\"\"Applies a transformation to the input features\"\"\"\nif aug_name in {\"rotate\", \"horizontalflip\", \"colorjitter\"}:\nkwargs[\"flowpic\"] = aug(kwargs[\"flowpic\"])\nelif aug_name in {\"timeshift\", \"packetloss\", \"changertt\"}:\nnew_timetofirst, new_pkts_size, _ = aug(\nkwargs[\"timetofirst\"], kwargs[\"pkts_size\"]\n)\nkwargs[\"timetofirst\"] = new_timetofirst\nkwargs[\"pkts_size\"] = new_pkts_size\nkwargs[\"flowpic\"] = get_flowpic(new_timetofirst, new_pkts_size)\nreturn kwargs\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/","title":"backbone","text":"<p>This module collects all network architectures.</p> <p>A few convetion are used for networks</p> <ol> <li> <p>All networks are expected to inherity from  the archetype class called BaseNet</p> </li> <li> <p>BaseNet is composing layers by means of two attribytes: .features is used for feature extraction while .classifier  corresponds to the final model head</p> </li> </ol>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet","title":"<code>BaseNet</code>","text":"<p>         Bases: <code>nn.Module</code></p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>class BaseNet(nn.Module):\ndef __init__(self, *args, **kwargs):\nsuper().__init__()\nself._init_args = args\nself._init_kwargs = kwargs\nself.features = None\nself.classifier = None\ndef forward(self, x):\nif self.features is None:\nraise RuntimeError(\"self.features in None, i.e., no architecture defined\")\nx = self.features(x)\nif self.classifier:\nx = self.classifier(x)\nreturn x\ndef get_copy(self) -&gt; Self:\n\"\"\"Get weights from the model\"\"\"\nreturn deepcopy(self.state_dict())\ndef set_state_dict(self, state_dict) -&gt; Self:\n\"\"\"Set weights into the model\"\"\"\nself.load_state_dict(deepcopy(state_dict))\nreturn self\ndef load_weights(self, fname: pathlib.Path, drop_classifier: bool = False) -&gt; Self:\n\"\"\"Load into the network the weights stored into a file\n        Argument:\n            fname: the file storing the weights\n            drop_classifier: if the network needs need to remove the classifier\n        \"\"\"\nstate_dict = torch.load(fname)\nif drop_classifier:\nkeys_to_drop = [key for key in state_dict if key.startswith(\"classifier\")]\nfor key in keys_to_drop:\ndel state_dict[key]\nself.set_state_dict(state_dict)\nreturn self\ndef save_weights(self, fname: pathlib.Path) -&gt; None:\n\"\"\"Store to file the weights of the network\n        Arguments:\n            fname: the file where to store the weights\n        \"\"\"\nfname = pathlib.Path(fname)\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\ntorch.save(self.state_dict(), fname)\ndef _find_index_of_last_linear_layer(self) -&gt; int:\n\"\"\"Introspect the network architecture to identify\n        the index of the last linear layer\"\"\"\nlast_block = self.features[-1]\nfor idx in range(len(last_block) - 1, -1, -1):\nlayer = last_block[idx]\nif isinstance(layer, nn.Identity):\ncontinue\nelif isinstance(layer, nn.Linear):\nreturn idx\nraise RuntimeError(f\"Didn't find any linear layer\")\ndef latent_space_dim(self) -&gt; int:\n\"\"\"Returns the number of units in the latent space\n        of the model (i.e., the shape of the output generated\n        be .features)\n        \"\"\"\nidx = self._find_index_of_last_linear_layer()\nlayer = self.features[-1][idx]\nreturn list(layer.parameters())[-1].shape[0]\ndef reset_classifier(self, num_classes) -&gt; Self:\n\"\"\"Recreate the classifier of the network\n        Attributes:\n            num_classes: the number of units for the new classifier layer\n        \"\"\"\nself.classifier = nn.Linear(self.latent_space_dim(), num_classes)\nself.initialize_weights(self.classifier)\nreturn self\n@property\ndef num_classes(self) -&gt; int:\n\"\"\"The number of units for the classifier of the network\"\"\"\nif self.classifier is None or isinstance(self.classifier, nn.Identity):\nreturn None\nreturn list(self.classifier.parameters())[0].shape[0]\ndef is_equal_to(self, other_net: BaseNet) -&gt; bool:\n\"\"\"Returns True if other_net is identical\n        (architecture and weights) to the current\n        network\"\"\"\nreturn are_equal(self, other_net)\ndef initialize_weights(self, m: nn.Module) -&gt; None:\n\"\"\"Initialize the weights of the network using Kaiming He\"\"\"\nif isinstance(m, nn.Conv2d):\nnn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\nif m.bias is not None:\nnn.init.constant_(m.bias.data, 0)\nelif isinstance(m, nn.Linear):\nnn.init.kaiming_uniform_(m.weight.data)\nnn.init.constant_(m.bias.data, 0)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>The number of units for the classifier of the network</p>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.get_copy","title":"<code>get_copy()</code>","text":"<p>Get weights from the model</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def get_copy(self) -&gt; Self:\n\"\"\"Get weights from the model\"\"\"\nreturn deepcopy(self.state_dict())\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.set_state_dict","title":"<code>set_state_dict(state_dict)</code>","text":"<p>Set weights into the model</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def set_state_dict(self, state_dict) -&gt; Self:\n\"\"\"Set weights into the model\"\"\"\nself.load_state_dict(deepcopy(state_dict))\nreturn self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.load_weights","title":"<code>load_weights(fname, drop_classifier=False)</code>","text":"<p>Load into the network the weights stored into a file</p> Argument <p>fname: the file storing the weights drop_classifier: if the network needs need to remove the classifier</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def load_weights(self, fname: pathlib.Path, drop_classifier: bool = False) -&gt; Self:\n\"\"\"Load into the network the weights stored into a file\n    Argument:\n        fname: the file storing the weights\n        drop_classifier: if the network needs need to remove the classifier\n    \"\"\"\nstate_dict = torch.load(fname)\nif drop_classifier:\nkeys_to_drop = [key for key in state_dict if key.startswith(\"classifier\")]\nfor key in keys_to_drop:\ndel state_dict[key]\nself.set_state_dict(state_dict)\nreturn self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.save_weights","title":"<code>save_weights(fname)</code>","text":"<p>Store to file the weights of the network</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the file where to store the weights</p> required Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def save_weights(self, fname: pathlib.Path) -&gt; None:\n\"\"\"Store to file the weights of the network\n    Arguments:\n        fname: the file where to store the weights\n    \"\"\"\nfname = pathlib.Path(fname)\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\ntorch.save(self.state_dict(), fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet._find_index_of_last_linear_layer","title":"<code>_find_index_of_last_linear_layer()</code>","text":"<p>Introspect the network architecture to identify the index of the last linear layer</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def _find_index_of_last_linear_layer(self) -&gt; int:\n\"\"\"Introspect the network architecture to identify\n    the index of the last linear layer\"\"\"\nlast_block = self.features[-1]\nfor idx in range(len(last_block) - 1, -1, -1):\nlayer = last_block[idx]\nif isinstance(layer, nn.Identity):\ncontinue\nelif isinstance(layer, nn.Linear):\nreturn idx\nraise RuntimeError(f\"Didn't find any linear layer\")\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.latent_space_dim","title":"<code>latent_space_dim()</code>","text":"<p>Returns the number of units in the latent space of the model (i.e., the shape of the output generated be .features)</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def latent_space_dim(self) -&gt; int:\n\"\"\"Returns the number of units in the latent space\n    of the model (i.e., the shape of the output generated\n    be .features)\n    \"\"\"\nidx = self._find_index_of_last_linear_layer()\nlayer = self.features[-1][idx]\nreturn list(layer.parameters())[-1].shape[0]\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.reset_classifier","title":"<code>reset_classifier(num_classes)</code>","text":"<p>Recreate the classifier of the network</p> <p>Attributes:</p> Name Type Description <code>num_classes</code> <p>the number of units for the new classifier layer</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def reset_classifier(self, num_classes) -&gt; Self:\n\"\"\"Recreate the classifier of the network\n    Attributes:\n        num_classes: the number of units for the new classifier layer\n    \"\"\"\nself.classifier = nn.Linear(self.latent_space_dim(), num_classes)\nself.initialize_weights(self.classifier)\nreturn self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.is_equal_to","title":"<code>is_equal_to(other_net)</code>","text":"<p>Returns True if other_net is identical (architecture and weights) to the current network</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def is_equal_to(self, other_net: BaseNet) -&gt; bool:\n\"\"\"Returns True if other_net is identical\n    (architecture and weights) to the current\n    network\"\"\"\nreturn are_equal(self, other_net)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.initialize_weights","title":"<code>initialize_weights(m)</code>","text":"<p>Initialize the weights of the network using Kaiming He</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def initialize_weights(self, m: nn.Module) -&gt; None:\n\"\"\"Initialize the weights of the network using Kaiming He\"\"\"\nif isinstance(m, nn.Conv2d):\nnn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\nif m.bias is not None:\nnn.init.constant_(m.bias.data, 0)\nelif isinstance(m, nn.Linear):\nnn.init.kaiming_uniform_(m.weight.data)\nnn.init.constant_(m.bias.data, 0)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.have_same_layers_and_types","title":"<code>have_same_layers_and_types(net1, net2)</code>","text":"<p>Compares to networks based on architecture</p> <p>Parameters:</p> Name Type Description Default <code>net1</code> <code>BaseNet</code> <p>a network</p> required <code>net2</code> <code>BaseNet</code> <p>another netwokr</p> required Return <p>True if the two network have the same number of layers each with the same type</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def have_same_layers_and_types(net1: BaseNet, net2: BaseNet) -&gt; bool:\n\"\"\"Compares to networks based on architecture\n    Arguments:\n        net1: a network\n        net2: another netwokr\n    Return:\n        True if the two network have the same\n        number of layers each with the same type\n    \"\"\"\nif len(net1.features) != len(net2.features):\nreturn False\nfor block1, block2 in zip(net1.features, net2.features):\nif len(block1) != len(block2):\nreturn False\nfor layer1, layer2 in zip(block1, block2):\nif type(layer1) != type(layer2):\nreturn False\nhas_classifier1 = net1.classifier is None\nhas_classifier2 = net2.classifier is None\nhas_classifier = has_classifier1 + has_classifier2\nif has_classifier == 1:\nreturn False\nelif has_classifier == 0:\nreturn True\nreturn type(net1.classifier) == type(net2.classifier)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.are_equal","title":"<code>are_equal(net1, net2)</code>","text":"<p>Compare two networks considering both architecture and weights</p> <p>Parameters:</p> Name Type Description Default <code>net1</code> <code>BaseNet</code> <p>a network</p> required <code>net2</code> <code>BaseNet</code> <p>another netwokr</p> required Return <p>True if the two network have the same number of layers each with the same type and they have the same weights</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def are_equal(net1: BaseNet, net2: BaseNet) -&gt; bool:\n\"\"\"Compare two networks considering both architecture and weights\n    Arguments:\n        net1: a network\n        net2: another netwokr\n    Return:\n        True if the two network have the same\n        number of layers each with the same type\n        and they have the same weights\n    \"\"\"\nif not have_same_layers_and_types(net1, net2):\nreturn False\nfor (name1, weights1), (name2, weights2) in zip(\nnet1.to(\"cpu\").state_dict().items(), net2.to(\"cpu\").state_dict().items()\n):\nif not (weights1 == weights2).all():\nreturn False\nreturn True\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.compute_features_size","title":"<code>compute_features_size(input_size, modules)</code>","text":"<p>Compute the number of units at the end of a chain of modules</p> <p>Attributes:</p> Name Type Description <code>input_size</code> <p>the shape of the input tensor</p> <code>modules</code> <p>a list of modules processing the input in sequence</p> Return <p>The number of units in the ouput generated processing a input of the specified shape through the list of modules</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def compute_features_size(input_size: Tuple[int], modules: List[nn.Module]) -&gt; int:\n\"\"\"Compute the number of units at the end of a chain of modules\n    Attributes:\n        input_size: the shape of the input tensor\n        modules: a list of modules processing the input in sequence\n    Return:\n        The number of units in the ouput generated processing\n        a input of the specified shape through the list of modules\n    \"\"\"\nx_dummy = torch.autograd.Variable(torch.ones(1, *input_size))\nfor m in modules:\nx_dummy = m(x_dummy)\nreturn int(np.prod(x_dummy.shape))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.has_dropout_layer","title":"<code>has_dropout_layer(module)</code>","text":"<p>Detect if the input module is a dropout layer or is a network containing a dropout layer</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def has_dropout_layer(module: nn.Module) -&gt; bool:\n\"\"\"Detect if the input module is a dropout layer\n    or is a network containing a dropout layer\n    \"\"\"\nif isinstance(module, (nn.Dropout, nn.Dropout1d, nn.Dropout2d)):\nreturn True\nreturn any(map(has_dropout_layer, module.children()))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.clone_net","title":"<code>clone_net(net)</code>","text":"<p>An utility function to clone a network</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to clone</p> required Return <p>A new instance of the same network passed as input initialized with the same weights</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def clone_net(net: BaseNet) -&gt; BaseNet:\n\"\"\"An utility function to clone a network\n    Arguments:\n        net: the network to clone\n    Return:\n        A new instance of the same network passed\n        as input initialized with the same weights\n    \"\"\"\ncurr_module = sys.modules[__name__]\nnet_class = getattr(curr_module, net.__class__.__name__)\nnew_net = net_class(*net._init_args, **net._init_kwargs)\n# the classifier might have been added after instanciation\nif net.classifier:\nnew_net.reset_classifier(num_classes=net.num_classes)\nfor idx in range(len(net.features)):\nnew_block = new_net.features[idx]\nold_block = net.features[idx]\nfor idx2 in range(len(new_block)):\nnew_layer = new_block[idx2]\nold_layer = old_block[idx2]\nif isinstance(old_layer, nn.Identity) and not isinstance(\nnew_layer, nn.Identity\n):\nnew_block[idx2] = nn.Identity()\nif isinstance(net.classifier, nn.Identity):\nnew_net.classifier = nn.Identity()\nnew_net.set_state_dict(net.get_copy())\nreturn new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.net_factory","title":"<code>net_factory(num_classes=5, flowpic_dim=32, with_dropout=True, projection_layer_dim=None)</code>","text":"<p>An utilify function to create Flowpic-related networks</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>the number of classes for the classifier</p> <code>5</code> <code>flowpic_dim</code> <code>int</code> <p>the resolution of the flowpic representation</p> <code>32</code> <code>with_dropout</code> <code>bool</code> <p>if False, the network use nn.Identity to mask out dropout layers</p> <code>True</code> <code>projection_layer_dim</code> <code>int</code> <p>the number of units for the SimCLR projection layer</p> <code>None</code> Return <p>the instanciated network</p> Source code in <code>tcbench/modeling/backbone.py</code> <pre><code>def net_factory(\nnum_classes: int = 5,\nflowpic_dim: int = 32,\nwith_dropout: bool = True,\nprojection_layer_dim: int = None,\n) -&gt; BaseNet:\n\"\"\"An utilify function to create Flowpic-related networks\n    Arguments:\n        num_classes: the number of classes for the classifier\n        flowpic_dim: the resolution of the flowpic representation\n        with_dropout: if False, the network use nn.Identity to mask out dropout layers\n        projection_layer_dim: the number of units for the SimCLR projection layer\n    Return:\n        the instanciated network\n    \"\"\"\nkwargs = dict(\nnum_classes=num_classes,\nflowpic_dim=flowpic_dim,\nwith_dropout=with_dropout,\nprojection_layer_dim=projection_layer_dim,\n)\nif flowpic_dim in (32, 64):\nreturn LeNet5FlowpicIMC22_Mini(**kwargs)\nreturn LeNet5FlowpicIMC22_Full(**kwargs)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/","title":"dataprep","text":"<p>This modules contains the a hierarchy of classes for composing Datasets and a variety of  function to load those objects from file</p> <p>All datasets are inherited from an archetype class named FlowpicDataset. This is a wrapper around a pandas DataFrame and offer functionality to create flowpic representation based on raw time series.</p> <p>Two other classes are then created to apply transformation functions</p> <p>Specifically: - AugmentWhenLoadingDataset: this class applies      transformations when instanciated.     This is useful when performing supervised training</p> <ul> <li>MultiViewDataset: this class applies     multi-view transformation on-the-fly.     This is useful when performing contrastive learning training</li> </ul>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset","title":"<code>FlowpicDataset</code>","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>class FlowpicDataset(torch.utils.data.Dataset):\ndef __init__(\nself,\ndata: str | pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\npkts_dir_colname: str,\ntarget_colname: str,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nquiet: bool = False,\nlogger: logging.Logger = None,\nn_workers: int = 10,\nflow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,  # str='flowpic',\nmax_n_pkts: int = 10,\n):\n\"\"\"\n        Arguments:\n            data: if a string, it corresponds to a parquet file from\n                where to load the raw data; if a pandas DataFrame,\n                the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            pkts_dir_colname: the column name mapping to the\n                packet direction time series\n            target_colname: the column name where the labels are stored\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            quiet: if False, no output on the console is generated when loading\n            logger: the logger to use\n            n_workers: how many processes to spawn when processing the data\n            flow_representation: flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction\n            max_n_pkts: timeseries length in case of flow_representation==\"pktseries\"\n        \"\"\"\nself.scaler = None\nself.flow_representation = flow_representation\nself.max_n_pkts = max_n_pkts\nself.logger = logger\nself.timetofirst_colname = timetofirst_colname\nself.pkts_size_colname = pkts_size_colname\nself.pkts_dir_colname = pkts_dir_colname\nself.transform = torchvision.transforms.Compose(\n[\ntorchvision.transforms.ToTensor(),\n]\n)\nself.n_workers = n_workers\nself.df = data\nif isinstance(data, (str, pathlib.Path)):\nif not quiet:\nself.log_msg(f\"loading: {data}\")\nself.df = pd.read_parquet(data)\nif \"flowpic\" not in self.df:\nself.df = self.add_flowpic(\nself.df,\ntimetofirst_colname,\npkts_size_colname,\nflowpic_dim,\nflowpic_block_duration,\nn_workers,\n)\nself.data = self.df[\"flowpic\"].values\nself.target = self.df[target_colname].cat.codes.astype(\"int64\").values\nself.target_colname = target_colname\nself.num_classes = self.df[target_colname].nunique()\ndef log_msg(self, msg: str) -&gt; None:\n\"\"\"An utility function to log messages\"\"\"\nutils.log_msg(msg, self.logger)\ndef set_scaler(self, scaler):\nself.scaler = scaler\n@classmethod\ndef add_flowpic(\ncls,\ndf: pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nn_workers: int = 1,\n) -&gt; pd.DataFrame:\n\"\"\"\n        Process a raw dataframe to create flowpic representation\n        Arguments:\n            df: a pandas DataFrame, the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            n_workers: how many processes to spawn when processing the data\n        \"\"\"\nfunc = functools.partial(\naugmentation.get_flowpic,\ndim=flowpic_dim,\nmax_block_duration=flowpic_block_duration,\n)\nparams = []\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\nparams.append((ser[timetofirst_colname], ser[pkts_size_colname]))\nif n_workers &gt; 1:\nwith multiprocessing.Pool(n_workers) as pool:\nflowpic_l = pool.starmap(func, params)\nelse:\nflowpic_l = [func(*pars) for pars in params]\nreturn df.assign(flowpic=flowpic_l)\ndef __len__(self) -&gt; int:\n\"\"\"Returns how many samples are in the dataset\"\"\"\nreturn len(self.target)\ndef __getitem__(self, index:int) -&gt; Any:\n\"\"\"\n        Arguments:\n            index: the index of the sample\n        Return:\n            A tuple with the flowpic representation (as tensor)\n            and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")\n        \"\"\"\nif self.flow_representation == MODELING_INPUT_REPR_TYPE.FLOWPIC:  #'flowpic':\nreturn (\nself.transform(np.expand_dims(self.data[index], 2)).double(),\nself.target[index],\n)\nser = self.df.iloc[[index]]\nnormalize_df = _create_df_to_normalize_pkt_series(\nser,\nself.timetofirst_colname,\nself.pkts_size_colname,\nself.pkts_dir_colname,\nself.max_n_pkts,\n)\nnormalized = self.scaler.transform(normalize_df)\nreturn (normalized, self.target[index])\n#        df = _create_df_to_normalize_pkt_series(ser,\n#                                        self.timetofirst_colname,\n#                                        self.pkts_size_colname,\n#                                        self.pkts_dir_colname,\n#                                        self.max_n_pkts)\n#\n#        if self.scaler:\n#            df = self.scaler.transform(df)\n#        return (df, self.target[index])\ndef num_classes(self):\n\"\"\"Returns the number of classes in the dataset\"\"\"\nreturn self.df[self.target_colname].nunique()\ndef samples_count(self) -&gt; pd.Series:\n\"\"\"\n        Return:\n            a pd.Series with the frequency count of\n            number of samples per class in the dataset\n        \"\"\"\nreturn self.df[self.target_colname].value_counts()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__init__","title":"<code>__init__(data, timetofirst_colname, pkts_size_colname, pkts_dir_colname, target_colname, flowpic_dim=32, flowpic_block_duration=15, quiet=False, logger=None, n_workers=10, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | pd.DataFrame</code> <p>if a string, it corresponds to a parquet file from where to load the raw data; if a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>pkts_dir_colname</code> <code>str</code> <p>the column name mapping to the packet direction time series</p> required <code>target_colname</code> <code>str</code> <p>the column name where the labels are stored</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>quiet</code> <code>bool</code> <p>if False, no output on the console is generated when loading</p> <code>False</code> <code>logger</code> <code>logging.Logger</code> <p>the logger to use</p> <code>None</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>10</code> <code>flow_representation</code> <code>MODELING_INPUT_REPR_TYPE</code> <p>flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction</p> <code>MODELING_INPUT_REPR_TYPE.FLOWPIC</code> <code>max_n_pkts</code> <code>int</code> <p>timeseries length in case of flow_representation==\"pktseries\"</p> <code>10</code> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def __init__(\nself,\ndata: str | pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\npkts_dir_colname: str,\ntarget_colname: str,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nquiet: bool = False,\nlogger: logging.Logger = None,\nn_workers: int = 10,\nflow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,  # str='flowpic',\nmax_n_pkts: int = 10,\n):\n\"\"\"\n    Arguments:\n        data: if a string, it corresponds to a parquet file from\n            where to load the raw data; if a pandas DataFrame,\n            the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        pkts_dir_colname: the column name mapping to the\n            packet direction time series\n        target_colname: the column name where the labels are stored\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        quiet: if False, no output on the console is generated when loading\n        logger: the logger to use\n        n_workers: how many processes to spawn when processing the data\n        flow_representation: flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction\n        max_n_pkts: timeseries length in case of flow_representation==\"pktseries\"\n    \"\"\"\nself.scaler = None\nself.flow_representation = flow_representation\nself.max_n_pkts = max_n_pkts\nself.logger = logger\nself.timetofirst_colname = timetofirst_colname\nself.pkts_size_colname = pkts_size_colname\nself.pkts_dir_colname = pkts_dir_colname\nself.transform = torchvision.transforms.Compose(\n[\ntorchvision.transforms.ToTensor(),\n]\n)\nself.n_workers = n_workers\nself.df = data\nif isinstance(data, (str, pathlib.Path)):\nif not quiet:\nself.log_msg(f\"loading: {data}\")\nself.df = pd.read_parquet(data)\nif \"flowpic\" not in self.df:\nself.df = self.add_flowpic(\nself.df,\ntimetofirst_colname,\npkts_size_colname,\nflowpic_dim,\nflowpic_block_duration,\nn_workers,\n)\nself.data = self.df[\"flowpic\"].values\nself.target = self.df[target_colname].cat.codes.astype(\"int64\").values\nself.target_colname = target_colname\nself.num_classes = self.df[target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>An utility function to log messages</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n\"\"\"An utility function to log messages\"\"\"\nutils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.add_flowpic","title":"<code>add_flowpic(df, timetofirst_colname, pkts_size_colname, flowpic_dim=32, flowpic_block_duration=15, n_workers=1)</code>  <code>classmethod</code>","text":"<p>Process a raw dataframe to create flowpic representation</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>1</code> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>@classmethod\ndef add_flowpic(\ncls,\ndf: pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nn_workers: int = 1,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Process a raw dataframe to create flowpic representation\n    Arguments:\n        df: a pandas DataFrame, the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        n_workers: how many processes to spawn when processing the data\n    \"\"\"\nfunc = functools.partial(\naugmentation.get_flowpic,\ndim=flowpic_dim,\nmax_block_duration=flowpic_block_duration,\n)\nparams = []\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\nparams.append((ser[timetofirst_colname], ser[pkts_size_colname]))\nif n_workers &gt; 1:\nwith multiprocessing.Pool(n_workers) as pool:\nflowpic_l = pool.starmap(func, params)\nelse:\nflowpic_l = [func(*pars) for pars in params]\nreturn df.assign(flowpic=flowpic_l)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns how many samples are in the dataset</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Returns how many samples are in the dataset\"\"\"\nreturn len(self.target)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>the index of the sample</p> required Return <p>A tuple with the flowpic representation (as tensor) and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def __getitem__(self, index:int) -&gt; Any:\n\"\"\"\n    Arguments:\n        index: the index of the sample\n    Return:\n        A tuple with the flowpic representation (as tensor)\n        and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")\n    \"\"\"\nif self.flow_representation == MODELING_INPUT_REPR_TYPE.FLOWPIC:  #'flowpic':\nreturn (\nself.transform(np.expand_dims(self.data[index], 2)).double(),\nself.target[index],\n)\nser = self.df.iloc[[index]]\nnormalize_df = _create_df_to_normalize_pkt_series(\nser,\nself.timetofirst_colname,\nself.pkts_size_colname,\nself.pkts_dir_colname,\nself.max_n_pkts,\n)\nnormalized = self.scaler.transform(normalize_df)\nreturn (normalized, self.target[index])\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.num_classes","title":"<code>num_classes()</code>","text":"<p>Returns the number of classes in the dataset</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def num_classes(self):\n\"\"\"Returns the number of classes in the dataset\"\"\"\nreturn self.df[self.target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.samples_count","title":"<code>samples_count()</code>","text":"Return <p>a pd.Series with the frequency count of number of samples per class in the dataset</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def samples_count(self) -&gt; pd.Series:\n\"\"\"\n    Return:\n        a pd.Series with the frequency count of\n        number of samples per class in the dataset\n    \"\"\"\nreturn self.df[self.target_colname].value_counts()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset","title":"<code>AugmentWhenLoadingDataset</code>","text":"<p>         Bases: <code>FlowpicDataset</code></p> <p>Wrapper around FlowpicDataset to enable creation of augmented samples when instanciating the class</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>class AugmentWhenLoadingDataset(FlowpicDataset):\n\"\"\"Wrapper around FlowpicDataset to enable creation\n    of augmented samples when instanciating the class\n    \"\"\"\ndef __init__(\nself,\ndata: str | pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\npkts_dir_colname: str,\ntarget_colname: str,\nflowpic_dim: int=32,\nflowpic_block_duration: int=15,\naug_name:str =\"noaug\",\naug_hparams: Dict[str, Any]=None,\naug_samples: int=10,\nquiet: bool=False,\nlogger: logging.Logger =None,\nn_workers: int=10,\nseed: int=12345,\nflow_representation:MODELING_INPUT_REPR_TYPE =MODELING_INPUT_REPR_TYPE.FLOWPIC, \nmax_n_pkts:int=10,\n):\n\"\"\"\n        Arguments:\n            data: if a string, it corresponds to a parquet file from\n                where to load the raw data; if a pandas DataFrame,\n                the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            target_colname: the column name where the labels are stored\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n                \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            aug_samples: how many samples to create for each\n                already existing sample\n            quiet: if False, no output on the console is generated when loading\n            logger: the logger to use\n            n_workers: how many processes to spawn when processing the data\n            seed: random seed\n            flow_representation: a MODELING_INPUT_REPR_TYPE value\n            max_n_pkts: packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)\n        \"\"\"\nsuper().__init__(\ndata,\ntimetofirst_colname=timetofirst_colname,\npkts_size_colname=pkts_size_colname,\npkts_dir_colname=pkts_dir_colname,\ntarget_colname=target_colname,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nquiet=quiet,\nlogger=logger,\nn_workers=n_workers,\nflow_representation=flow_representation,\nmax_n_pkts=max_n_pkts,\n)\nself.aug_samples = aug_samples\nself.seed = seed\nself.df = self.samples_augmentation(\naug_name=aug_name,\naug_hparams=aug_hparams,\nsamples=aug_samples,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nseed=seed,\n)\nself.data = self.df[\"flowpic\"].values\nself.target = self.df[target_colname].cat.codes.astype(\"int64\").values\nself.target_colname = target_colname\nself.num_classes = self.df[target_colname].nunique()\ndef regenerate_flowpic(\nself, dim: int = 32, block_duration: int = 15\n) -&gt; pd.DataFrame:\n\"\"\"\n        Overwrite the existing flowpic by creating new ones\n        Arguments:\n            dim: the flowpic resolution\n            block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n        Return:\n            A new dataframe with a \"flowpic\" column reporting the flowpic\n            for each available sample\n        \"\"\"\nself.df = self.add_flowpic(\nself.df,\nself.timetofirst_colname,\nself.pkts_size_colname,\ndim,\nblock_duration,\nself.n_workers,\n)\nreturn self.df\ndef _worker_aug_torch(\nself,\ndf: pd.DataFrame,\naug_class: augmentation.Augmentation,\nsamples: int = 9,\n*args: List[Any],\n**kwargs: Dict[str, Any],\n) -&gt; pd.DataFrame:\n\"\"\"\n        Helper function for a multi-processing worker handling\n        augmentations related to flowpic (i.e., working on Tensor data)\n        Arguments:\n            df: a batch of samples to process\n            aug_class: an instance of an augmentation class (see augmentation module)\n            samples: how many samples to create for each existings sample\n        Return:\n            An expanded version of the input dataframe containing\n            the new samples. IMPORTANT: the new DataFrame is performing\n            a shallow copy of the original and act only of a subset of\n            columns. Hence, the returned version might have semantical\n            incosistencies\n        \"\"\"\nnew_flowpic = []\nnew_aug_params = []\ndf = pd.concat([df.copy() for _ in range(samples)]).assign(\nis_augmented=True, aug_params={}\n)\ndtypes = dict(self.df.dtypes)\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\nnew_sample = ser.copy()\nnew_flowpic.append(aug_class(new_sample[\"flowpic\"]))\nnew_aug_params.append(aug_class.get_params().copy())\nprint(\".\", end=\"\", flush=True)\ndf = df.assign(flowpic=new_flowpic, aug_params=new_aug_params)\nreturn df\ndef _worker_aug_numpy(\nself,\ndf: pd.DataFrame,\naug_class: augmentation.Augmentation,\nsamples: int = 9,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\n):\n\"\"\"\n        Helper function for a multi-processing worker handling\n        augmentations related to time series (i.e., working with numpy arrays)\n        Arguments:\n            df: a batch of samples to process\n            aug_class: an instance of an augmentation class (see augmentation module)\n            samples: how many samples to create for each existings sample\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n        Return:\n            An expanded version of the input dataframe containing\n            the new samples. IMPORTANT: the new DataFrame is performing\n            a shallow copy of the original and act only of a subset of\n            columns. Hence, the returned version might have semantical\n            incosistencies\n        \"\"\"\nnew_timetofirst = []\nnew_pkts_size = []\nnew_flowpic = []\nnew_aug_params = []\ndf = pd.concat([df.copy() for _ in range(samples)]).assign(\nis_augmented=True, aug_params={}\n)\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\n_timetofirst, _pkts_size, indexes = aug_class(\nser[self.timetofirst_colname], ser[self.pkts_size_colname]\n)\nnew_aug_params.append(aug_class.get_params().copy())\nnew_timetofirst.append(_timetofirst)\nnew_pkts_size.append(_pkts_size)\nnew_flowpic.append(\naugmentation.get_flowpic(\n_timetofirst, _pkts_size, flowpic_dim, flowpic_block_duration\n)\n)\ndf = df.assign(\ntimetofirst=new_timetofirst,\npkts_size=new_pkts_size,\nflowpic=new_flowpic,\naug_params=new_aug_params,\n)\nprint(\".\", end=\"\", flush=True)\nreturn df\ndef _samples_augmentation_loop(\nself,\naug_name: str,\naug_hparams: Dict[str, Any],\nsamples: int,\nworker_func: Callable,\nseed: int = 12345,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\n) -&gt; pd.DataFrame:\n\"\"\"\n        Helper function handling the main loop for samples augmentation\n        by means of multiprocessing\n        Arguments:\n            aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n                \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            samples: how many samples to create for each\n                already existing sample\n            worker_func: the callback to use for augmentation\n            seed: the seed to use for augmentation\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n        Return:\n            A pandas DataFrame with all the original samples plus\n            the augmented ones\n        \"\"\"\nif aug_hparams is None:\naug_hparams = dict()\nparams = []\nindexes = self.df.index.values\npartition_size = indexes.shape[0] // self.n_workers\nfor idx in range(0, len(indexes), partition_size):\nrng = np.random.default_rng(seed + idx)\n# aug_class = augmentation.augmentation_factory(aug_name, rng, **aug_hparams)\naug_class = augmentation.augmentation_factory(aug_name, rng, aug_hparams)\npartition_indexes = indexes[idx : idx + partition_size]\nparams.append(\n(\nself.df.loc[partition_indexes],\naug_class,\nsamples,\nflowpic_dim,\nflowpic_block_duration,\n)\n)\n# Note: this is a very dirty trick\n#\n# We experienced deadlocks similar to what reported here\n# https://github.com/pytorch/pytorch/issues/3492\n# when using torchvision.transforms (with both functional APIs\n# and instanciating classes). But the logic in the\n# .augmentation module works fine in single process\n#\n# Relying on torch.multiprocessing\n# https://github.com/pytorch/pytorch/issues/3492\n# and setting torch.multiprogressing.set_start_method('spawn')\n# (in if __name__ == '__main__') fixed the issue\n#\n# But this requires invoking the .Pool() differently\n# depending on the underlining augmentation (pytorch or numpy)\nif worker_func.__name__ == \"_worker_aug_torch\":\nwith torch.multiprocessing.Pool(self.n_workers) as pool:\naugmented_l = pool.starmap(worker_func, params)\nelse:\nwith multiprocessing.Pool(self.n_workers) as pool:\naugmented_l = pool.starmap(worker_func, params)\nself.df = pd.concat([self.df] + augmented_l).reset_index()\nreturn self.df\ndef samples_augmentation(\nself,\naug_name:str=\"noaug\",\naug_hparams:Dict[str, Any]=None,\nsamples:int=None,\nseed:int=12345,\nflowpic_dim:int=32,\nflowpic_block_duration:int=15,\n):\n\"\"\"Applies samples augmentation\n        Arguments:\n            aug_name: one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            samples: final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)\n            seed: random number generator seed\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n        Return:\n            A pandas DataFrame with all original samples and the augmented ones\n        \"\"\"\nif samples is None:\nsamples = self.aug_samples\nif aug_name not in augmentation.AUGMENTATION_CLASSES:\nself.log_msg(\"no augmentation\")\nreturn self.df.assign(is_augmented=False)\nself.df = self.df.assign(is_augmented=False)\nsamples -= 1\nif aug_name == \"horizontalflip\":\nsamples = 1\nworker_func = self._worker_aug_numpy\nif aug_name in (\"rotate\", \"horizontalflip\", \"colorjitter\"):\nworker_func = self._worker_aug_torch\nself.log_msg(f\"data augmentation ({aug_name})\")\nself._samples_augmentation_loop(\naug_name=aug_name,\naug_hparams=aug_hparams,\nsamples=samples,\nworker_func=worker_func,\nseed=seed,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\n)\nprint()\nreturn self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.__init__","title":"<code>__init__(data, timetofirst_colname, pkts_size_colname, pkts_dir_colname, target_colname, flowpic_dim=32, flowpic_block_duration=15, aug_name='noaug', aug_hparams=None, aug_samples=10, quiet=False, logger=None, n_workers=10, seed=12345, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | pd.DataFrame</code> <p>if a string, it corresponds to a parquet file from where to load the raw data; if a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>target_colname</code> <code>str</code> <p>the column name where the labels are stored</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>aug_name</code> <code>str</code> <p>one of {\"noaug\", \"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"</p> <code>'noaug'</code> <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> <code>None</code> <code>aug_samples</code> <code>int</code> <p>how many samples to create for each already existing sample</p> <code>10</code> <code>quiet</code> <code>bool</code> <p>if False, no output on the console is generated when loading</p> <code>False</code> <code>logger</code> <code>logging.Logger</code> <p>the logger to use</p> <code>None</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>10</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>12345</code> <code>flow_representation</code> <code>MODELING_INPUT_REPR_TYPE</code> <p>a MODELING_INPUT_REPR_TYPE value</p> <code>MODELING_INPUT_REPR_TYPE.FLOWPIC</code> <code>max_n_pkts</code> <code>int</code> <p>packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)</p> <code>10</code> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def __init__(\nself,\ndata: str | pd.DataFrame,\ntimetofirst_colname: str,\npkts_size_colname: str,\npkts_dir_colname: str,\ntarget_colname: str,\nflowpic_dim: int=32,\nflowpic_block_duration: int=15,\naug_name:str =\"noaug\",\naug_hparams: Dict[str, Any]=None,\naug_samples: int=10,\nquiet: bool=False,\nlogger: logging.Logger =None,\nn_workers: int=10,\nseed: int=12345,\nflow_representation:MODELING_INPUT_REPR_TYPE =MODELING_INPUT_REPR_TYPE.FLOWPIC, \nmax_n_pkts:int=10,\n):\n\"\"\"\n    Arguments:\n        data: if a string, it corresponds to a parquet file from\n            where to load the raw data; if a pandas DataFrame,\n            the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        target_colname: the column name where the labels are stored\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n            \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        aug_samples: how many samples to create for each\n            already existing sample\n        quiet: if False, no output on the console is generated when loading\n        logger: the logger to use\n        n_workers: how many processes to spawn when processing the data\n        seed: random seed\n        flow_representation: a MODELING_INPUT_REPR_TYPE value\n        max_n_pkts: packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)\n    \"\"\"\nsuper().__init__(\ndata,\ntimetofirst_colname=timetofirst_colname,\npkts_size_colname=pkts_size_colname,\npkts_dir_colname=pkts_dir_colname,\ntarget_colname=target_colname,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nquiet=quiet,\nlogger=logger,\nn_workers=n_workers,\nflow_representation=flow_representation,\nmax_n_pkts=max_n_pkts,\n)\nself.aug_samples = aug_samples\nself.seed = seed\nself.df = self.samples_augmentation(\naug_name=aug_name,\naug_hparams=aug_hparams,\nsamples=aug_samples,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nseed=seed,\n)\nself.data = self.df[\"flowpic\"].values\nself.target = self.df[target_colname].cat.codes.astype(\"int64\").values\nself.target_colname = target_colname\nself.num_classes = self.df[target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.regenerate_flowpic","title":"<code>regenerate_flowpic(dim=32, block_duration=15)</code>","text":"<p>Overwrite the existing flowpic by creating new ones</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A new dataframe with a \"flowpic\" column reporting the flowpic for each available sample</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def regenerate_flowpic(\nself, dim: int = 32, block_duration: int = 15\n) -&gt; pd.DataFrame:\n\"\"\"\n    Overwrite the existing flowpic by creating new ones\n    Arguments:\n        dim: the flowpic resolution\n        block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n    Return:\n        A new dataframe with a \"flowpic\" column reporting the flowpic\n        for each available sample\n    \"\"\"\nself.df = self.add_flowpic(\nself.df,\nself.timetofirst_colname,\nself.pkts_size_colname,\ndim,\nblock_duration,\nself.n_workers,\n)\nreturn self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._worker_aug_torch","title":"<code>_worker_aug_torch(df, aug_class, samples=9, *args, **kwargs)</code>","text":"<p>Helper function for a multi-processing worker handling augmentations related to flowpic (i.e., working on Tensor data)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>a batch of samples to process</p> required <code>aug_class</code> <code>augmentation.Augmentation</code> <p>an instance of an augmentation class (see augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each existings sample</p> <code>9</code> Return <p>An expanded version of the input dataframe containing the new samples. IMPORTANT: the new DataFrame is performing a shallow copy of the original and act only of a subset of columns. Hence, the returned version might have semantical incosistencies</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def _worker_aug_torch(\nself,\ndf: pd.DataFrame,\naug_class: augmentation.Augmentation,\nsamples: int = 9,\n*args: List[Any],\n**kwargs: Dict[str, Any],\n) -&gt; pd.DataFrame:\n\"\"\"\n    Helper function for a multi-processing worker handling\n    augmentations related to flowpic (i.e., working on Tensor data)\n    Arguments:\n        df: a batch of samples to process\n        aug_class: an instance of an augmentation class (see augmentation module)\n        samples: how many samples to create for each existings sample\n    Return:\n        An expanded version of the input dataframe containing\n        the new samples. IMPORTANT: the new DataFrame is performing\n        a shallow copy of the original and act only of a subset of\n        columns. Hence, the returned version might have semantical\n        incosistencies\n    \"\"\"\nnew_flowpic = []\nnew_aug_params = []\ndf = pd.concat([df.copy() for _ in range(samples)]).assign(\nis_augmented=True, aug_params={}\n)\ndtypes = dict(self.df.dtypes)\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\nnew_sample = ser.copy()\nnew_flowpic.append(aug_class(new_sample[\"flowpic\"]))\nnew_aug_params.append(aug_class.get_params().copy())\nprint(\".\", end=\"\", flush=True)\ndf = df.assign(flowpic=new_flowpic, aug_params=new_aug_params)\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._worker_aug_numpy","title":"<code>_worker_aug_numpy(df, aug_class, samples=9, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Helper function for a multi-processing worker handling augmentations related to time series (i.e., working with numpy arrays)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>a batch of samples to process</p> required <code>aug_class</code> <code>augmentation.Augmentation</code> <p>an instance of an augmentation class (see augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each existings sample</p> <code>9</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>An expanded version of the input dataframe containing the new samples. IMPORTANT: the new DataFrame is performing a shallow copy of the original and act only of a subset of columns. Hence, the returned version might have semantical incosistencies</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def _worker_aug_numpy(\nself,\ndf: pd.DataFrame,\naug_class: augmentation.Augmentation,\nsamples: int = 9,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\n):\n\"\"\"\n    Helper function for a multi-processing worker handling\n    augmentations related to time series (i.e., working with numpy arrays)\n    Arguments:\n        df: a batch of samples to process\n        aug_class: an instance of an augmentation class (see augmentation module)\n        samples: how many samples to create for each existings sample\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n    Return:\n        An expanded version of the input dataframe containing\n        the new samples. IMPORTANT: the new DataFrame is performing\n        a shallow copy of the original and act only of a subset of\n        columns. Hence, the returned version might have semantical\n        incosistencies\n    \"\"\"\nnew_timetofirst = []\nnew_pkts_size = []\nnew_flowpic = []\nnew_aug_params = []\ndf = pd.concat([df.copy() for _ in range(samples)]).assign(\nis_augmented=True, aug_params={}\n)\nfor idx in range(df.shape[0]):\nser = df.iloc[idx]\n_timetofirst, _pkts_size, indexes = aug_class(\nser[self.timetofirst_colname], ser[self.pkts_size_colname]\n)\nnew_aug_params.append(aug_class.get_params().copy())\nnew_timetofirst.append(_timetofirst)\nnew_pkts_size.append(_pkts_size)\nnew_flowpic.append(\naugmentation.get_flowpic(\n_timetofirst, _pkts_size, flowpic_dim, flowpic_block_duration\n)\n)\ndf = df.assign(\ntimetofirst=new_timetofirst,\npkts_size=new_pkts_size,\nflowpic=new_flowpic,\naug_params=new_aug_params,\n)\nprint(\".\", end=\"\", flush=True)\nreturn df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._samples_augmentation_loop","title":"<code>_samples_augmentation_loop(aug_name, aug_hparams, samples, worker_func, seed=12345, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Helper function handling the main loop for samples augmentation by means of multiprocessing</p> <p>Parameters:</p> Name Type Description Default <code>aug_name</code> <code>str</code> <p>one of {\"noaug\", \"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"</p> required <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each already existing sample</p> required <code>worker_func</code> <code>Callable</code> <p>the callback to use for augmentation</p> required <code>seed</code> <code>int</code> <p>the seed to use for augmentation</p> <code>12345</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A pandas DataFrame with all the original samples plus the augmented ones</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def _samples_augmentation_loop(\nself,\naug_name: str,\naug_hparams: Dict[str, Any],\nsamples: int,\nworker_func: Callable,\nseed: int = 12345,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Helper function handling the main loop for samples augmentation\n    by means of multiprocessing\n    Arguments:\n        aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n            \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        samples: how many samples to create for each\n            already existing sample\n        worker_func: the callback to use for augmentation\n        seed: the seed to use for augmentation\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n    Return:\n        A pandas DataFrame with all the original samples plus\n        the augmented ones\n    \"\"\"\nif aug_hparams is None:\naug_hparams = dict()\nparams = []\nindexes = self.df.index.values\npartition_size = indexes.shape[0] // self.n_workers\nfor idx in range(0, len(indexes), partition_size):\nrng = np.random.default_rng(seed + idx)\n# aug_class = augmentation.augmentation_factory(aug_name, rng, **aug_hparams)\naug_class = augmentation.augmentation_factory(aug_name, rng, aug_hparams)\npartition_indexes = indexes[idx : idx + partition_size]\nparams.append(\n(\nself.df.loc[partition_indexes],\naug_class,\nsamples,\nflowpic_dim,\nflowpic_block_duration,\n)\n)\n# Note: this is a very dirty trick\n#\n# We experienced deadlocks similar to what reported here\n# https://github.com/pytorch/pytorch/issues/3492\n# when using torchvision.transforms (with both functional APIs\n# and instanciating classes). But the logic in the\n# .augmentation module works fine in single process\n#\n# Relying on torch.multiprocessing\n# https://github.com/pytorch/pytorch/issues/3492\n# and setting torch.multiprogressing.set_start_method('spawn')\n# (in if __name__ == '__main__') fixed the issue\n#\n# But this requires invoking the .Pool() differently\n# depending on the underlining augmentation (pytorch or numpy)\nif worker_func.__name__ == \"_worker_aug_torch\":\nwith torch.multiprocessing.Pool(self.n_workers) as pool:\naugmented_l = pool.starmap(worker_func, params)\nelse:\nwith multiprocessing.Pool(self.n_workers) as pool:\naugmented_l = pool.starmap(worker_func, params)\nself.df = pd.concat([self.df] + augmented_l).reset_index()\nreturn self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.samples_augmentation","title":"<code>samples_augmentation(aug_name='noaug', aug_hparams=None, samples=None, seed=12345, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Applies samples augmentation</p> <p>Parameters:</p> Name Type Description Default <code>aug_name</code> <code>str</code> <p>one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}</p> <code>'noaug'</code> <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> <code>None</code> <code>samples</code> <code>int</code> <p>final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)</p> <code>None</code> <code>seed</code> <code>int</code> <p>random number generator seed</p> <code>12345</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A pandas DataFrame with all original samples and the augmented ones</p> Source code in <code>tcbench/modeling/dataprep.py</code> <pre><code>def samples_augmentation(\nself,\naug_name:str=\"noaug\",\naug_hparams:Dict[str, Any]=None,\nsamples:int=None,\nseed:int=12345,\nflowpic_dim:int=32,\nflowpic_block_duration:int=15,\n):\n\"\"\"Applies samples augmentation\n    Arguments:\n        aug_name: one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        samples: final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)\n        seed: random number generator seed\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n    Return:\n        A pandas DataFrame with all original samples and the augmented ones\n    \"\"\"\nif samples is None:\nsamples = self.aug_samples\nif aug_name not in augmentation.AUGMENTATION_CLASSES:\nself.log_msg(\"no augmentation\")\nreturn self.df.assign(is_augmented=False)\nself.df = self.df.assign(is_augmented=False)\nsamples -= 1\nif aug_name == \"horizontalflip\":\nsamples = 1\nworker_func = self._worker_aug_numpy\nif aug_name in (\"rotate\", \"horizontalflip\", \"colorjitter\"):\nworker_func = self._worker_aug_torch\nself.log_msg(f\"data augmentation ({aug_name})\")\nself._samples_augmentation_loop(\naug_name=aug_name,\naug_hparams=aug_hparams,\nsamples=samples,\nworker_func=worker_func,\nseed=seed,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\n)\nprint()\nreturn self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_losses/","title":"losses","text":"<p>Code borrowed from https://github.com/HobbitLong/SupContrast</p>"},{"location":"tcbench/api/tcbench_modeling_losses/#tcbench.modeling.losses.SupConLoss","title":"<code>SupConLoss</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf. It also supports the unsupervised contrastive loss in SimCLR</p> Source code in <code>tcbench/modeling/losses.py</code> <pre><code>class SupConLoss(nn.Module):\n\"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\ndef __init__(\nself,\ntemperature=0.07,\ncontrast_mode=\"all\",\nbase_temperature=0.07,\ntopn_acc_rank=5,\n):\nsuper(SupConLoss, self).__init__()\nself.temperature = temperature\nself.contrast_mode = contrast_mode\nself.base_temperature = base_temperature\nself.topn_acc_rank = topn_acc_rank\ndef forward(\nself,\nfeatures: torch.Tensor,\nlabels: torch.Tensor = None,\nmask: torch.Tensor = None,\n) -&gt; Dict[str, Any]:\n\"\"\"Compute loss for model. If both `labels` and `mask` are None,\n        it degenerates to SimCLR unsupervised loss:\n        https://arxiv.org/pdf/2002.05709.pdf\n        Arguments:\n            features: hidden vector of shape [bsz, n_views, ...].\n            labels: ground truth of shape [bsz].\n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n                has the same class as sample i. Can be asymmetric.\n        Return:\n            A dictionary with key \"loss\" corresponding to the computed\n            loss; \"acc_top_1\" with the top-1 accuracy computed;\n            \"acc_top_n\" with the top-n accuracy computed\n        \"\"\"\ndevice = torch.device(\"cuda\") if features.is_cuda else torch.device(\"cpu\")\nif len(features.shape) &lt; 3:\nraise ValueError(\n\"`features` needs to be [bsz, n_views, ...],\"\n\"at least 3 dimensions are required\"\n)\nif len(features.shape) &gt; 3:\nfeatures = features.view(features.shape[0], features.shape[1], -1)\nbatch_size = features.shape[0]\nif labels is not None and mask is not None:\nraise ValueError(\"Cannot define both `labels` and `mask`\")\nelif labels is None and mask is None:\nmask = torch.eye(batch_size, dtype=torch.float32).to(device)\nelif labels is not None:\nlabels = labels.contiguous().view(-1, 1)\nif labels.shape[0] != batch_size:\nraise ValueError(\"Num of labels does not match num of features\")\nmask = torch.eq(labels, labels.T).float().to(device)\nelse:\nmask = mask.float().to(device)\ncontrast_count = features.shape[1]\ncontrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\nif self.contrast_mode == \"one\":\nanchor_feature = features[:, 0]\nanchor_count = 1\nelif self.contrast_mode == \"all\":\nanchor_feature = contrast_feature\nanchor_count = contrast_count\nelse:\nraise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n# compute logits\nanchor_dot_contrast = torch.div(\ntorch.matmul(anchor_feature, contrast_feature.T), self.temperature\n)\n# for numerical stability\nlogits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\nlogits = anchor_dot_contrast - logits_max.detach()\n# tile mask\n# meaning, repeating the mask horizontally and vertically\n# this is likely becoming a diagonal matrix\n# with also the position of the positives\nmask = mask.repeat(anchor_count, contrast_count)\n# mask-out self-contrast cases\n# meaning, a matrix of 1s without the main diagonal\nlogits_mask = torch.scatter(\ntorch.ones_like(mask),\n1,\ntorch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n0,\n)\n# now the mask is selecting only the positives\nmask = mask * logits_mask\n# compute log_prob\nexp_logits = torch.exp(logits) * logits_mask\nlog_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n# compute mean of log-likelihood over positive\nmean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n# loss\nloss = -(self.temperature / self.base_temperature) * mean_log_prob_pos\nloss = loss.view(anchor_count, batch_size).mean()\n#########################\n# compute top n accuracy\n#########################\ndiagonal = torch.eye(exp_logits.shape[0], dtype=bool).to(device)\npos_mask = mask.bool()\nexp_logits_noself = exp_logits.masked_fill(diagonal, -9e15)\ncomb_exp_logits_noself = torch.cat(\n[\nexp_logits_noself[pos_mask][:, None],\nexp_logits_noself.masked_fill(pos_mask, -9e15),\n],\ndim=-1,\n)\nsim_argsort = comb_exp_logits_noself.argsort(dim=-1, descending=True).argmin(\ndim=-1\n)\nacc_top_1 = (sim_argsort == 0).float().mean()\nacc_top_n = (sim_argsort &lt; self.topn_acc_rank).float().mean()\nreturn {\n\"loss\": loss,\n\"acc_top_1\": acc_top_1,\nf\"acc_top_{self.topn_acc_rank}\": acc_top_n,\n}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_losses/#tcbench.modeling.losses.SupConLoss.forward","title":"<code>forward(features, labels=None, mask=None)</code>","text":"<p>Compute loss for model. If both <code>labels</code> and <code>mask</code> are None, it degenerates to SimCLR unsupervised loss: https://arxiv.org/pdf/2002.05709.pdf</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>torch.Tensor</code> <p>hidden vector of shape [bsz, n_views, ...].</p> required <code>labels</code> <code>torch.Tensor</code> <p>ground truth of shape [bsz].</p> <code>None</code> <code>mask</code> <code>torch.Tensor</code> <p>contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j has the same class as sample i. Can be asymmetric.</p> <code>None</code> Return <p>A dictionary with key \"loss\" corresponding to the computed loss; \"acc_top_1\" with the top-1 accuracy computed; \"acc_top_n\" with the top-n accuracy computed</p> Source code in <code>tcbench/modeling/losses.py</code> <pre><code>def forward(\nself,\nfeatures: torch.Tensor,\nlabels: torch.Tensor = None,\nmask: torch.Tensor = None,\n) -&gt; Dict[str, Any]:\n\"\"\"Compute loss for model. If both `labels` and `mask` are None,\n    it degenerates to SimCLR unsupervised loss:\n    https://arxiv.org/pdf/2002.05709.pdf\n    Arguments:\n        features: hidden vector of shape [bsz, n_views, ...].\n        labels: ground truth of shape [bsz].\n        mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n            has the same class as sample i. Can be asymmetric.\n    Return:\n        A dictionary with key \"loss\" corresponding to the computed\n        loss; \"acc_top_1\" with the top-1 accuracy computed;\n        \"acc_top_n\" with the top-n accuracy computed\n    \"\"\"\ndevice = torch.device(\"cuda\") if features.is_cuda else torch.device(\"cpu\")\nif len(features.shape) &lt; 3:\nraise ValueError(\n\"`features` needs to be [bsz, n_views, ...],\"\n\"at least 3 dimensions are required\"\n)\nif len(features.shape) &gt; 3:\nfeatures = features.view(features.shape[0], features.shape[1], -1)\nbatch_size = features.shape[0]\nif labels is not None and mask is not None:\nraise ValueError(\"Cannot define both `labels` and `mask`\")\nelif labels is None and mask is None:\nmask = torch.eye(batch_size, dtype=torch.float32).to(device)\nelif labels is not None:\nlabels = labels.contiguous().view(-1, 1)\nif labels.shape[0] != batch_size:\nraise ValueError(\"Num of labels does not match num of features\")\nmask = torch.eq(labels, labels.T).float().to(device)\nelse:\nmask = mask.float().to(device)\ncontrast_count = features.shape[1]\ncontrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\nif self.contrast_mode == \"one\":\nanchor_feature = features[:, 0]\nanchor_count = 1\nelif self.contrast_mode == \"all\":\nanchor_feature = contrast_feature\nanchor_count = contrast_count\nelse:\nraise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n# compute logits\nanchor_dot_contrast = torch.div(\ntorch.matmul(anchor_feature, contrast_feature.T), self.temperature\n)\n# for numerical stability\nlogits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\nlogits = anchor_dot_contrast - logits_max.detach()\n# tile mask\n# meaning, repeating the mask horizontally and vertically\n# this is likely becoming a diagonal matrix\n# with also the position of the positives\nmask = mask.repeat(anchor_count, contrast_count)\n# mask-out self-contrast cases\n# meaning, a matrix of 1s without the main diagonal\nlogits_mask = torch.scatter(\ntorch.ones_like(mask),\n1,\ntorch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n0,\n)\n# now the mask is selecting only the positives\nmask = mask * logits_mask\n# compute log_prob\nexp_logits = torch.exp(logits) * logits_mask\nlog_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n# compute mean of log-likelihood over positive\nmean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n# loss\nloss = -(self.temperature / self.base_temperature) * mean_log_prob_pos\nloss = loss.view(anchor_count, batch_size).mean()\n#########################\n# compute top n accuracy\n#########################\ndiagonal = torch.eye(exp_logits.shape[0], dtype=bool).to(device)\npos_mask = mask.bool()\nexp_logits_noself = exp_logits.masked_fill(diagonal, -9e15)\ncomb_exp_logits_noself = torch.cat(\n[\nexp_logits_noself[pos_mask][:, None],\nexp_logits_noself.masked_fill(pos_mask, -9e15),\n],\ndim=-1,\n)\nsim_argsort = comb_exp_logits_noself.argsort(dim=-1, descending=True).argmin(\ndim=-1\n)\nacc_top_1 = (sim_argsort == 0).float().mean()\nacc_top_n = (sim_argsort &lt; self.topn_acc_rank).float().mean()\nreturn {\n\"loss\": loss,\n\"acc_top_1\": acc_top_1,\nf\"acc_top_{self.topn_acc_rank}\": acc_top_n,\n}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/","title":"methods","text":"<p>This modules contains a set of classes  used to handle training and inference, namely Trainer objects</p>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss","title":"<code>PatienceMonitorLoss</code>","text":"<p>A callable class implementing monitoring of a loss metric</p> <p>Attributes:</p> Name Type Description <code>steps</code> <p>the maximum patience</p> <code>steps_left</code> <p>steps left before patience expires</p> <code>min_delta</code> <p>the minimum difference against the best loss observed so far to be considered as an improvement</p> <code>best_loss</code> <p>the best loss observed so far</p> <code>best_epoch</code> <p>teh epoch when the best loss was observed</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class PatienceMonitorLoss:\n\"\"\"A callable class implementing monitoring of\n    a loss metric\n    Attributes:\n        steps: the maximum patience\n        steps_left: steps left before patience expires\n        min_delta: the minimum difference against\n            the best loss observed so far to\n            be considered as an improvement\n        best_loss: the best loss observed so far\n        best_epoch: teh epoch when the best loss was observed\n    \"\"\"\ndef __init__(self, steps: int = 5, min_delta: float = 0.001):\n\"\"\"\n        Arguments:\n            steps: the maximum patience\n            min_delta: the minimum difference against\n        \"\"\"\nself.steps = steps\nself.steps_left = steps\nself.min_delta = min_delta\nself.best_loss = np.inf\nself.best_epoch = -1\ndef is_improved(self, loss: float) -&gt; bool:\n\"\"\"Returns true if input loss differ\n        from the best observed loss by at least\n        min_delta\"\"\"\ndiff = self.best_loss - loss\nreturn diff &gt; self.min_delta\ndef is_expired(self) -&gt; bool:\n\"\"\"Returns True if steps_left == 0\"\"\"\nreturn self.steps_left == 0\ndef get_best_metrics(self) -&gt; Dict[str, float]:\n\"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\nreturn dict(best_loss=self.best_loss, best_epoch=self.best_epoch)\ndef __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n\"\"\"The input metrics is a dictionary expected to have\n        a \"loss\" key, and the related value is compared against\n        the best loss observed so far. Returns True if the\n        input loss is improved wrt the best loss observed\"\"\"\nloss = metrics[\"loss\"]\nif self.is_improved(loss):\nself.best_loss = loss\nself.steps_left = self.steps\nself.best_epoch = idx_epoch\nreturn True\nself.steps_left -= 1\nreturn False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.__init__","title":"<code>__init__(steps=5, min_delta=0.001)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>the maximum patience</p> <code>5</code> <code>min_delta</code> <code>float</code> <p>the minimum difference against</p> <code>0.001</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(self, steps: int = 5, min_delta: float = 0.001):\n\"\"\"\n    Arguments:\n        steps: the maximum patience\n        min_delta: the minimum difference against\n    \"\"\"\nself.steps = steps\nself.steps_left = steps\nself.min_delta = min_delta\nself.best_loss = np.inf\nself.best_epoch = -1\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.is_improved","title":"<code>is_improved(loss)</code>","text":"<p>Returns true if input loss differ from the best observed loss by at least min_delta</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def is_improved(self, loss: float) -&gt; bool:\n\"\"\"Returns true if input loss differ\n    from the best observed loss by at least\n    min_delta\"\"\"\ndiff = self.best_loss - loss\nreturn diff &gt; self.min_delta\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.is_expired","title":"<code>is_expired()</code>","text":"<p>Returns True if steps_left == 0</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def is_expired(self) -&gt; bool:\n\"\"\"Returns True if steps_left == 0\"\"\"\nreturn self.steps_left == 0\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.get_best_metrics","title":"<code>get_best_metrics()</code>","text":"<p>Returns a dictionary with best loss and epoch observed</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def get_best_metrics(self) -&gt; Dict[str, float]:\n\"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\nreturn dict(best_loss=self.best_loss, best_epoch=self.best_epoch)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.__call__","title":"<code>__call__(metrics, idx_epoch)</code>","text":"<p>The input metrics is a dictionary expected to have a \"loss\" key, and the related value is compared against the best loss observed so far. Returns True if the input loss is improved wrt the best loss observed</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n\"\"\"The input metrics is a dictionary expected to have\n    a \"loss\" key, and the related value is compared against\n    the best loss observed so far. Returns True if the\n    input loss is improved wrt the best loss observed\"\"\"\nloss = metrics[\"loss\"]\nif self.is_improved(loss):\nself.best_loss = loss\nself.steps_left = self.steps\nself.best_epoch = idx_epoch\nreturn True\nself.steps_left -= 1\nreturn False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy","title":"<code>PatienceMonitorAccuracy</code>","text":"<p>A callable class implementing monitoring of a performance metric</p> <p>Attributes     steps: the maximum patience     steps_left: steps left before patience expires     best_acc: the best loss observed so far     best_epoch: the epoch when the best loss was observed     acc_name: the name of the performance metric</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class PatienceMonitorAccuracy:\n\"\"\"A callable class implementing monitoring of\n    a performance metric\n    Attributes\n        steps: the maximum patience\n        steps_left: steps left before patience expires\n        best_acc: the best loss observed so far\n        best_epoch: the epoch when the best loss was observed\n        acc_name: the name of the performance metric\n    \"\"\"\ndef __init__(self, name: str = \"acc\", steps: int = 3):\n\"\"\"\n        Arguments:\n            name: the name of the performance metric\n            steps: the maximum patience\n        \"\"\"\nself.steps = steps\nself.steps_left = steps\nself.best_acc = -np.inf\nself.best_epoch = -1\nself.acc_name = name\ndef is_improved(self, acc: float) -&gt; bool:\n\"\"\"Return True if the input accuracy is\n        higher than the best observed so far\"\"\"\nreturn acc &gt; self.best_acc\ndef is_expired(self) -&gt; bool:\n\"\"\"Returns True if steps_left == 0\"\"\"\nreturn self.steps_left == 0\ndef get_best_metrics(self) -&gt; Dict[str, float]:\n\"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\nname = f\"best_{self.acc_name}\"\nreturn {name: self.best_acc, \"best_epoch\": self.best_epoch}\ndef __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n\"\"\"The input metrics is a dictionary expected to have\n        a key with the same name provided when instanciating\n        the class. The related value is compared against\n        the best loss observed so far. Returns True if the\n        performance metric is improved wrt the best performance observed\"\"\"\nacc = metrics[self.acc_name]\nif self.is_improved(acc):\nself.best_acc = acc\nself.steps_left = self.steps\nself.best_epoch = idx_epoch\nreturn True\nself.steps_left -= 1\nreturn False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.__init__","title":"<code>__init__(name='acc', steps=3)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the performance metric</p> <code>'acc'</code> <code>steps</code> <code>int</code> <p>the maximum patience</p> <code>3</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(self, name: str = \"acc\", steps: int = 3):\n\"\"\"\n    Arguments:\n        name: the name of the performance metric\n        steps: the maximum patience\n    \"\"\"\nself.steps = steps\nself.steps_left = steps\nself.best_acc = -np.inf\nself.best_epoch = -1\nself.acc_name = name\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.is_improved","title":"<code>is_improved(acc)</code>","text":"<p>Return True if the input accuracy is higher than the best observed so far</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def is_improved(self, acc: float) -&gt; bool:\n\"\"\"Return True if the input accuracy is\n    higher than the best observed so far\"\"\"\nreturn acc &gt; self.best_acc\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.is_expired","title":"<code>is_expired()</code>","text":"<p>Returns True if steps_left == 0</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def is_expired(self) -&gt; bool:\n\"\"\"Returns True if steps_left == 0\"\"\"\nreturn self.steps_left == 0\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.get_best_metrics","title":"<code>get_best_metrics()</code>","text":"<p>Returns a dictionary with best loss and epoch observed</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def get_best_metrics(self) -&gt; Dict[str, float]:\n\"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\nname = f\"best_{self.acc_name}\"\nreturn {name: self.best_acc, \"best_epoch\": self.best_epoch}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.__call__","title":"<code>__call__(metrics, idx_epoch)</code>","text":"<p>The input metrics is a dictionary expected to have a key with the same name provided when instanciating the class. The related value is compared against the best loss observed so far. Returns True if the performance metric is improved wrt the best performance observed</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n\"\"\"The input metrics is a dictionary expected to have\n    a key with the same name provided when instanciating\n    the class. The related value is compared against\n    the best loss observed so far. Returns True if the\n    performance metric is improved wrt the best performance observed\"\"\"\nacc = metrics[self.acc_name]\nif self.is_improved(acc):\nself.best_acc = acc\nself.steps_left = self.steps\nself.best_epoch = idx_epoch\nreturn True\nself.steps_left -= 1\nreturn False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer","title":"<code>SimpleTrainer</code>","text":"<p>A base class offering functionality for training and testing a supervised model</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class SimpleTrainer:\n\"\"\"A base class offering functionality for\n    training and testing a supervised model\n    \"\"\"\ndef __init__(\nself,\nnet: backbone.BaseNet,\noptimizer: pytorch.optim.Optimizer,\ncriterion: torch.nn.Module,\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\n):\n\"\"\"\n        Arguments:\n            net: the architecture to use\n            optimizer: the optimizer to use\n            criterion: the instance of the loss to use\n            device: the device to use\n            deterministic: see _make_deterministic()\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n        \"\"\"\nif deterministic:\n_make_deterministic()\nself.device = device\nself.optimizer = optimizer\nself.logger = logger\nself.net = net\nif self.net:\nself.net = net.double().to(device)\nself.criterion = criterion\nself.tracker = tracker\nself._is_training = False\nself._reset_metrics()\ndef log_msg(self, msg: str) -&gt; None:\n\"\"\"Register a message to file and echoes it\n        to the console\"\"\"\nutils.log_msg(msg, self.logger)\ndef _reset_metrics(self) -&gt; None:\n\"\"\"Helper method to clean (before training)\n        internal objects used for tracking metrics\n        \"\"\"\nself.best_model = None\nself.metrics = defaultdict(list)\ndef _track_metrics(\nself, metrics: Dict[str, float], context: str, epoch: int = None\n) -&gt; None:\n\"\"\"Helper method invoked during training, validation\n        and testing to track loss and performance metrics\"\"\"\nfor name, value in metrics.items():\nself.metrics[f\"{context}_{name}\"].append(value)\nif self.tracker:\nself.tracker.track(\nvalue, name, epoch=epoch, context=dict(subset=context)\n)\ndef _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int,\ncontext: str,\ntrack_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n            track_preds_targets: if True, return predicted and target labels\n        Returns:\n            A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n            If track_preds_targets is True, the dictionary contains also the keys\n           \"preds\" and \"targets\", each mapped to a list of integer labels\n        \"\"\"\ncum_loss = 0\ncorrect = 0\nsamples = 0\npreds = []\ntargets = []\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x can be a list if the data loader\n# is associated to a multi-view dataset\n# but in this context we are not using contrastive\n# learning, hence we concat the views\nif isinstance(x, list):\ny = y.repeat(len(x))\nx = torch.cat(x, dim=0)\nx = x.to(self.device)\ny = y.to(self.device)\nscores = self.net.forward(x)\nloss = self.criterion(scores, y)\ncum_loss += loss.item()\nif self._is_training:\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\ny_pred = scores.argmax(dim=1)\ncorrect += (y_pred == y).sum().item()\nsamples += x.shape[0]\nif track_preds_targets:\npreds.extend(y_pred.cpu().numpy().tolist())\ntargets.extend(y.cpu().numpy().tolist())\nprint(\".\", end=\"\", flush=True)\nprint(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = dict(\n# loss=cum_loss / (batch_idx+1),\nloss=cum_loss / batch_idx,\nacc=100 * correct / samples,\n)\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nif track_preds_targets:\nmetrics[\"preds\"] = preds\nmetrics[\"targets\"] = targets\nreturn metrics\ndef train_one_epoch(\nself, train_loader: torch.utils.data.DataLoader, idx_epoch: int, context: str\n) -&gt; Dict[str, Any]:\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself.net.train()\nself._is_training = True\nout = self._do_epoch(train_loader, idx_epoch, context)\nself._is_training = False\nreturn out\ndef train_loop(\nself,\nepochs: int,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n        The entry point for triggering training\n        Arguments:\n            epochs: number of epochs to run\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Returns:\n            The best model obtained during training\n        \"\"\"\nif context is None:\ncontext = \"\"\nelse:\ncontext += \"_\"\nif self.optimizer is None:\nraise RuntimeError(\"optimizer cannot be None when training\")\nself.net = self.net.to(self.device)\nself._reset_metrics()\nif backbone.has_dropout_layer(self.net):\nutils.log_msg(\n\"---\\nWARNING: Detected Dropout layer!\\nWARNING: During supervised training, the monitored train_acc will be inaccurate\\n---\",\nself.logger,\n)\nfor idx_epoch in range(epochs):\nif patience_monitor and patience_monitor.is_expired():\nbreak\ntrain_metrics = self.train_one_epoch(\ntrain_loader, idx_epoch, context=f\"{context}train\"\n)\nmsg = f\"epoch: {idx_epoch:3d} | \"\nmsg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\nfor metric_name, metric_value in train_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"train_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif val_loader:\nval_metrics, _ = self.test_loop(\nval_loader, idx_epoch, with_reports=False, context=f\"{context}val\"\n)\nmsg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\nfor metric_name, metric_value in val_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"val_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif patience_monitor and patience_monitor(val_metrics, idx_epoch):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}val\")\nmsg += \" | *\"\nelse:\nif patience_monitor and patience_monitor(train_metrics, idx_epoch):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}train\")\nmsg += \" | *\"\nelse:\nself.best_model = self.net.get_copy()\nif not quiet:\nself.log_msg(msg)\nif not quiet:\nif patience_monitor and patience_monitor.is_expired():\nself.log_msg(\"run out of patience\")\nelse:\nself.log_msg(\"reached max epochs\")\nself.net.set_state_dict(self.best_model)\nreturn self.net\ndef test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n        Run inference on a model (for testing or validation)\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\nself.net.eval()\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nwith torch.no_grad():\nmetrics = self._do_epoch(\ndata_loader, idx_epoch, track_preds_targets=True, context=context\n)\npreds = metrics[\"preds\"]\ntargets = metrics[\"targets\"]\ndel metrics[\"preds\"]\ndel metrics[\"targets\"]\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nreports = {}\nif with_reports:\nreports = dict(\nclass_rep=pd.DataFrame(\nclassification_report(targets, preds, output_dict=True)\n).T,\nconf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\npreds=preds,\n)\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.__init__","title":"<code>__init__(net, optimizer, criterion, device='cuda:0', deterministic=True, tracker=None, logger=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the architecture to use</p> required <code>optimizer</code> <code>pytorch.optim.Optimizer</code> <p>the optimizer to use</p> required <code>criterion</code> <code>torch.nn.Module</code> <p>the instance of the loss to use</p> required <code>device</code> <code>str</code> <p>the device to use</p> <code>'cuda:0'</code> <code>deterministic</code> <code>bool</code> <p>see _make_deterministic()</p> <code>True</code> <code>tracker</code> <code>aim.Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>logging.Logger</code> <p>the logging reference</p> <code>None</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(\nself,\nnet: backbone.BaseNet,\noptimizer: pytorch.optim.Optimizer,\ncriterion: torch.nn.Module,\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\n):\n\"\"\"\n    Arguments:\n        net: the architecture to use\n        optimizer: the optimizer to use\n        criterion: the instance of the loss to use\n        device: the device to use\n        deterministic: see _make_deterministic()\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n    \"\"\"\nif deterministic:\n_make_deterministic()\nself.device = device\nself.optimizer = optimizer\nself.logger = logger\nself.net = net\nif self.net:\nself.net = net.double().to(device)\nself.criterion = criterion\nself.tracker = tracker\nself._is_training = False\nself._reset_metrics()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>Register a message to file and echoes it to the console</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n\"\"\"Register a message to file and echoes it\n    to the console\"\"\"\nutils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._reset_metrics","title":"<code>_reset_metrics()</code>","text":"<p>Helper method to clean (before training) internal objects used for tracking metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _reset_metrics(self) -&gt; None:\n\"\"\"Helper method to clean (before training)\n    internal objects used for tracking metrics\n    \"\"\"\nself.best_model = None\nself.metrics = defaultdict(list)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._track_metrics","title":"<code>_track_metrics(metrics, context, epoch=None)</code>","text":"<p>Helper method invoked during training, validation and testing to track loss and performance metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _track_metrics(\nself, metrics: Dict[str, float], context: str, epoch: int = None\n) -&gt; None:\n\"\"\"Helper method invoked during training, validation\n    and testing to track loss and performance metrics\"\"\"\nfor name, value in metrics.items():\nself.metrics[f\"{context}_{name}\"].append(value)\nif self.tracker:\nself.tracker.track(\nvalue, name, epoch=epoch, context=dict(subset=context)\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._do_epoch","title":"<code>_do_epoch(data_loader, idx_epoch, context, track_preds_targets=False)</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> required <code>track_preds_targets</code> <code>bool</code> <p>if True, return predicted and target labels</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of metrics containing the measured average \"loss\" and \"acc\".</p> <code>Dict[str, Any]</code> <p>If track_preds_targets is True, the dictionary contains also the keys</p> <p>\"preds\" and \"targets\", each mapped to a list of integer labels</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int,\ncontext: str,\ntrack_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n        track_preds_targets: if True, return predicted and target labels\n    Returns:\n        A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n        If track_preds_targets is True, the dictionary contains also the keys\n       \"preds\" and \"targets\", each mapped to a list of integer labels\n    \"\"\"\ncum_loss = 0\ncorrect = 0\nsamples = 0\npreds = []\ntargets = []\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x can be a list if the data loader\n# is associated to a multi-view dataset\n# but in this context we are not using contrastive\n# learning, hence we concat the views\nif isinstance(x, list):\ny = y.repeat(len(x))\nx = torch.cat(x, dim=0)\nx = x.to(self.device)\ny = y.to(self.device)\nscores = self.net.forward(x)\nloss = self.criterion(scores, y)\ncum_loss += loss.item()\nif self._is_training:\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\ny_pred = scores.argmax(dim=1)\ncorrect += (y_pred == y).sum().item()\nsamples += x.shape[0]\nif track_preds_targets:\npreds.extend(y_pred.cpu().numpy().tolist())\ntargets.extend(y.cpu().numpy().tolist())\nprint(\".\", end=\"\", flush=True)\nprint(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = dict(\n# loss=cum_loss / (batch_idx+1),\nloss=cum_loss / batch_idx,\nacc=100 * correct / samples,\n)\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nif track_preds_targets:\nmetrics[\"preds\"] = preds\nmetrics[\"targets\"] = targets\nreturn metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, idx_epoch, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(\nself, train_loader: torch.utils.data.DataLoader, idx_epoch: int, context: str\n) -&gt; Dict[str, Any]:\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself.net.train()\nself._is_training = True\nout = self._do_epoch(train_loader, idx_epoch, context)\nself._is_training = False\nreturn out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.train_loop","title":"<code>train_loop(epochs, train_loader, val_loader=None, patience_monitor=None, quiet=False, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>number of epochs to run</p> required <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>backbone.BaseNet</code> <p>The best model obtained during training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_loop(\nself,\nepochs: int,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n    The entry point for triggering training\n    Arguments:\n        epochs: number of epochs to run\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Returns:\n        The best model obtained during training\n    \"\"\"\nif context is None:\ncontext = \"\"\nelse:\ncontext += \"_\"\nif self.optimizer is None:\nraise RuntimeError(\"optimizer cannot be None when training\")\nself.net = self.net.to(self.device)\nself._reset_metrics()\nif backbone.has_dropout_layer(self.net):\nutils.log_msg(\n\"---\\nWARNING: Detected Dropout layer!\\nWARNING: During supervised training, the monitored train_acc will be inaccurate\\n---\",\nself.logger,\n)\nfor idx_epoch in range(epochs):\nif patience_monitor and patience_monitor.is_expired():\nbreak\ntrain_metrics = self.train_one_epoch(\ntrain_loader, idx_epoch, context=f\"{context}train\"\n)\nmsg = f\"epoch: {idx_epoch:3d} | \"\nmsg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\nfor metric_name, metric_value in train_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"train_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif val_loader:\nval_metrics, _ = self.test_loop(\nval_loader, idx_epoch, with_reports=False, context=f\"{context}val\"\n)\nmsg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\nfor metric_name, metric_value in val_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"val_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif patience_monitor and patience_monitor(val_metrics, idx_epoch):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}val\")\nmsg += \" | *\"\nelse:\nif patience_monitor and patience_monitor(train_metrics, idx_epoch):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}train\")\nmsg += \" | *\"\nelse:\nself.best_model = self.net.get_copy()\nif not quiet:\nself.log_msg(msg)\nif not quiet:\nif patience_monitor and patience_monitor.is_expired():\nself.log_msg(\"run out of patience\")\nelse:\nself.log_msg(\"reached max epochs\")\nself.net.set_state_dict(self.best_model)\nreturn self.net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.test_loop","title":"<code>test_loop(data_loader, idx_epoch=None, with_reports=False, context=None)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n    Run inference on a model (for testing or validation)\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\nself.net.eval()\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nwith torch.no_grad():\nmetrics = self._do_epoch(\ndata_loader, idx_epoch, track_preds_targets=True, context=context\n)\npreds = metrics[\"preds\"]\ntargets = metrics[\"targets\"]\ndel metrics[\"preds\"]\ndel metrics[\"targets\"]\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nreports = {}\nif with_reports:\nreports = dict(\nclass_rep=pd.DataFrame(\nclassification_report(targets, preds, output_dict=True)\n).T,\nconf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\npreds=preds,\n)\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer","title":"<code>XGboostTrainer</code>","text":"<p>A base class offering functionality for training and testing a supervised model</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class XGboostTrainer:\n\"\"\"A base class offering functionality for\n    training and testing a supervised model\n    \"\"\"\ndef __init__(\nself,\nxgboost_model:Any,\nnet:Any=None,\ndevice:Any=None,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\n):\n\"\"\"\n        Arguments:\n            xgboost_model: XGboost model\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n        \"\"\"\nself.xgboost_model = xgboost_model\nself.logger = logger\nself.tracker = tracker\nself._is_training = False\nself._reset_metrics()\ndef log_msg(self, msg: str) -&gt; None:\n\"\"\"Register a message to file and echoes it\n        to the console\"\"\"\nutils.log_msg(msg, self.logger)\ndef _reset_metrics(self) -&gt; None:\n\"\"\"Helper method to clean (before training)\n        internal objects used for tracking metrics\n        \"\"\"\nself.best_model = None\nself.metrics = defaultdict(list)\ndef _track_metrics(self, metrics: Dict[str, float], context: str) -&gt; None:\n\"\"\"Helper method invoked during training, validation\n        and testing to track loss and performance metrics\"\"\"\nfor name, value in metrics.items():\nself.metrics[f\"{context}_{name}\"].append(value)\nif self.tracker:\nself.tracker.track(value, name, context=dict(subset=context))\ndef _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\ncontext: str,\ntrack_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n            track_preds_targets: if True, return predicted and target labels\n        Returns:\n            A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n            If track_preds_targets is True, the dictionary contains also the keys\n           \"preds\" and \"targets\", each mapped to a list of integer labels\n        \"\"\"\ncum_loss = 0\ncorrect = 0\nsamples = 0\npreds = []\ntargets = []\nx_all = []\ny_all = []\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x can be a list if the data loader\n# is associated to a multi-view dataset\n# but in this context we are not using contrastive\n# learning, hence we concat the views\n# if isinstance(x, list):\n#    y = y.repeat(len(x))\n#    x = torch.cat(x, dim=0)\n# scores = self.net.forward(x)\n# loss = self.criterion(scores, y)\n# cum_loss += loss.item()\n# print(x.reshape(x.shape[0], -1).shape)\n# x = x.cpu().numpy()\n# y = y.cpu().numpy()\nx_all.append(x.reshape(x.shape[0], -1))\n# y_all.append(y.reshape(y.shape[0], -1))\ny_all.append(y)\nx_all = np.concatenate(x_all, axis=0)\n# x_all = xgb.DMatrix(x_all)\ny_all = np.concatenate(y_all, axis=0)\nif self._is_training:\nself.xgboost_model.fit(x_all, y_all)\ny_pred = self.xgboost_model.predict(x_all)\ncorrect += (y_pred == y_all).sum().item()\nsamples += x_all.shape[0]\nif track_preds_targets:\npreds.extend(y_pred.tolist())\ntargets.extend(y_all.tolist())\nprint(\".\", end=\"\", flush=True)\nprint(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = dict(\n# loss=cum_loss / batch_idx,\nacc=100\n* correct\n/ samples,\n)\nself._track_metrics(metrics, context=context)\nif track_preds_targets:\nmetrics[\"preds\"] = preds\nmetrics[\"targets\"] = targets\nreturn metrics\ndef train_one_epoch(\nself, train_loader: torch.utils.data.DataLoader, context: str\n) -&gt; Dict[str, Any]:\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself._is_training = True\nout = self._do_epoch(train_loader, context)\nself._is_training = False\nreturn out\ndef train_loop(\nself,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\ncontext: str = None,\n) -&gt; Any:\n\"\"\"\n        The entry point for triggering training\n        Arguments:\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Returns:\n            The best model obtained during training\n        \"\"\"\nif context is None:\ncontext = \"\"\nelse:\ncontext += \"_\"\nself._reset_metrics()\n# for idx_epoch in range(epochs):\ntrain_metrics = self.train_one_epoch(train_loader, context=f\"{context}train\")\n# msg = f\"epoch: {idx_epoch:3d} | \"\n# msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\nmsg = \"\"\nfor metric_name, metric_value in train_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"train_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif val_loader:\nval_metrics, _ = self.test_loop(\nval_loader, with_reports=False, context=f\"{context}val\"\n)\n# msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\nfor metric_name, metric_value in val_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"val_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif patience_monitor and patience_monitor(val_metrics):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}val\")\nmsg += \" | *\"\nelse:\nif patience_monitor and patience_monitor(train_metrics):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}train\")\nmsg += \" | *\"\nelse:\npass\nif not quiet:\nself.log_msg(msg)\nif not quiet:\nself.log_msg(\"done\")\n# self.net.set_state_dict(self.best_model)\nreturn self.xgboost_model\ndef test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n        Run inference on a model (for testing or validation)\n        Arguments:\n            data_loader: the data to use\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nmetrics = self._do_epoch(data_loader, track_preds_targets=True, context=context)\npreds = metrics[\"preds\"]\ntargets = metrics[\"targets\"]\ndel metrics[\"preds\"]\ndel metrics[\"targets\"]\nself._track_metrics(metrics, context=context)\nreports = {}\nif with_reports:\nreports = dict(\nclass_rep=pd.DataFrame(\nclassification_report(targets, preds, output_dict=True)\n).T,\nconf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\npreds=preds,\n)\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.__init__","title":"<code>__init__(xgboost_model, net=None, device=None, tracker=None, logger=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgboost_model</code> <code>Any</code> <p>XGboost model</p> required <code>tracker</code> <code>aim.Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>logging.Logger</code> <p>the logging reference</p> <code>None</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(\nself,\nxgboost_model:Any,\nnet:Any=None,\ndevice:Any=None,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\n):\n\"\"\"\n    Arguments:\n        xgboost_model: XGboost model\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n    \"\"\"\nself.xgboost_model = xgboost_model\nself.logger = logger\nself.tracker = tracker\nself._is_training = False\nself._reset_metrics()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>Register a message to file and echoes it to the console</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n\"\"\"Register a message to file and echoes it\n    to the console\"\"\"\nutils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._reset_metrics","title":"<code>_reset_metrics()</code>","text":"<p>Helper method to clean (before training) internal objects used for tracking metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _reset_metrics(self) -&gt; None:\n\"\"\"Helper method to clean (before training)\n    internal objects used for tracking metrics\n    \"\"\"\nself.best_model = None\nself.metrics = defaultdict(list)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._track_metrics","title":"<code>_track_metrics(metrics, context)</code>","text":"<p>Helper method invoked during training, validation and testing to track loss and performance metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _track_metrics(self, metrics: Dict[str, float], context: str) -&gt; None:\n\"\"\"Helper method invoked during training, validation\n    and testing to track loss and performance metrics\"\"\"\nfor name, value in metrics.items():\nself.metrics[f\"{context}_{name}\"].append(value)\nif self.tracker:\nself.tracker.track(value, name, context=dict(subset=context))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._do_epoch","title":"<code>_do_epoch(data_loader, context, track_preds_targets=False)</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> required <code>track_preds_targets</code> <code>bool</code> <p>if True, return predicted and target labels</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of metrics containing the measured average \"loss\" and \"acc\".</p> <code>Dict[str, Any]</code> <p>If track_preds_targets is True, the dictionary contains also the keys</p> <p>\"preds\" and \"targets\", each mapped to a list of integer labels</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\ncontext: str,\ntrack_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n        track_preds_targets: if True, return predicted and target labels\n    Returns:\n        A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n        If track_preds_targets is True, the dictionary contains also the keys\n       \"preds\" and \"targets\", each mapped to a list of integer labels\n    \"\"\"\ncum_loss = 0\ncorrect = 0\nsamples = 0\npreds = []\ntargets = []\nx_all = []\ny_all = []\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x can be a list if the data loader\n# is associated to a multi-view dataset\n# but in this context we are not using contrastive\n# learning, hence we concat the views\n# if isinstance(x, list):\n#    y = y.repeat(len(x))\n#    x = torch.cat(x, dim=0)\n# scores = self.net.forward(x)\n# loss = self.criterion(scores, y)\n# cum_loss += loss.item()\n# print(x.reshape(x.shape[0], -1).shape)\n# x = x.cpu().numpy()\n# y = y.cpu().numpy()\nx_all.append(x.reshape(x.shape[0], -1))\n# y_all.append(y.reshape(y.shape[0], -1))\ny_all.append(y)\nx_all = np.concatenate(x_all, axis=0)\n# x_all = xgb.DMatrix(x_all)\ny_all = np.concatenate(y_all, axis=0)\nif self._is_training:\nself.xgboost_model.fit(x_all, y_all)\ny_pred = self.xgboost_model.predict(x_all)\ncorrect += (y_pred == y_all).sum().item()\nsamples += x_all.shape[0]\nif track_preds_targets:\npreds.extend(y_pred.tolist())\ntargets.extend(y_all.tolist())\nprint(\".\", end=\"\", flush=True)\nprint(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = dict(\n# loss=cum_loss / batch_idx,\nacc=100\n* correct\n/ samples,\n)\nself._track_metrics(metrics, context=context)\nif track_preds_targets:\nmetrics[\"preds\"] = preds\nmetrics[\"targets\"] = targets\nreturn metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(\nself, train_loader: torch.utils.data.DataLoader, context: str\n) -&gt; Dict[str, Any]:\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself._is_training = True\nout = self._do_epoch(train_loader, context)\nself._is_training = False\nreturn out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.train_loop","title":"<code>train_loop(train_loader, val_loader=None, patience_monitor=None, quiet=False, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The best model obtained during training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_loop(\nself,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\ncontext: str = None,\n) -&gt; Any:\n\"\"\"\n    The entry point for triggering training\n    Arguments:\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Returns:\n        The best model obtained during training\n    \"\"\"\nif context is None:\ncontext = \"\"\nelse:\ncontext += \"_\"\nself._reset_metrics()\n# for idx_epoch in range(epochs):\ntrain_metrics = self.train_one_epoch(train_loader, context=f\"{context}train\")\n# msg = f\"epoch: {idx_epoch:3d} | \"\n# msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\nmsg = \"\"\nfor metric_name, metric_value in train_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"train_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif val_loader:\nval_metrics, _ = self.test_loop(\nval_loader, with_reports=False, context=f\"{context}val\"\n)\n# msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\nfor metric_name, metric_value in val_metrics.items():\nif \"acc\" not in metric_name:\ncontinue\nmetric_name = f\"val_{metric_name}\"\nmsg += f\" | {metric_name}: {metric_value:5.1f}%\"\nif patience_monitor and patience_monitor(val_metrics):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}val\")\nmsg += \" | *\"\nelse:\nif patience_monitor and patience_monitor(train_metrics):\nself.best_model = self.net.get_copy()\nmetrics = patience_monitor.get_best_metrics()\nself._track_metrics(metrics, context=f\"{context}train\")\nmsg += \" | *\"\nelse:\npass\nif not quiet:\nself.log_msg(msg)\nif not quiet:\nself.log_msg(\"done\")\n# self.net.set_state_dict(self.best_model)\nreturn self.xgboost_model\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.test_loop","title":"<code>test_loop(data_loader, with_reports=False, context=None)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data to use</p> required <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n    Run inference on a model (for testing or validation)\n    Arguments:\n        data_loader: the data to use\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nmetrics = self._do_epoch(data_loader, track_preds_targets=True, context=context)\npreds = metrics[\"preds\"]\ntargets = metrics[\"targets\"]\ndel metrics[\"preds\"]\ndel metrics[\"targets\"]\nself._track_metrics(metrics, context=context)\nreports = {}\nif with_reports:\nreports = dict(\nclass_rep=pd.DataFrame(\nclassification_report(targets, preds, output_dict=True)\n).T,\nconf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\npreds=preds,\n)\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer","title":"<code>ContrastiveLearningTrainer</code>","text":"<p>         Bases: <code>SimpleTrainer</code></p> <p>A trainer designed for contrastive learning</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class ContrastiveLearningTrainer(SimpleTrainer):\n\"\"\"A trainer designed for contrastive learning\"\"\"\ndef __init__(\nself,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer,\ncriterion: torch.nn.Module,\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\n):\nsuper().__init__(\nnet=net,\noptimizer=optimizer,\ncriterion=criterion,\ndevice=device,\ndeterministic=deterministic,\ntracker=tracker,\nlogger=logger,\n)\n@classmethod\ndef prepare_net_for_train(\ncls, net: backbone.BaseNet, fname_weights: pathlib.Path = None\n) -&gt; backbone.BaseNet:\n\"\"\"\n        Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity)\n        its .classifier and the last activation function of .features\n        Arguments:\n            net: the network to modify\n            fname_weights: if provided, the weights are loaded into\n                the network after the modification\n        Return:\n            A new instance of the input network with architecture\n            modification required to run training contrastive learning\n            training\n        \"\"\"\nnew_net = backbone.clone_net(net)\nif not hasattr(net, \"prepare_for_contrastivelearning\"):\nraise RuntimeError(\n\"Did not find a .prepare_for_contrativelearning() method in the network. Cannot adapt the network for training\"\n)\nnew_net.prepare_for_contrastivelearning(fname_weights)\nnew_net = new_net.double()\nreturn new_net\n@classmethod\ndef init_train(\ncls,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\nfname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n\"\"\"\n        Clones the input network and prepares it for contrastive learning,\n        and instanciate a new optimized bounding it to the new network weights\n        Arguments:\n            net: the network to use\n            optimizer: the optimizer to tuse\n            fname_weights: if provided, the weights are loaded\n                into the new network before returning it\n        Return:\n            A tuple with the new updated network and the related optimizer\n        \"\"\"\nnew_net = cls.prepare_net_for_train(net, fname_weights)\nnew_optimizer = None\nif optimizer:\nnew_optimizer = _reset_optimizer(optimizer, new_net.parameters())\nreturn new_net, new_optimizer\ndef _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int,\ncontext: str = \"train\",\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A dictionary of metrics collected\n        \"\"\"\ncum_metrics = defaultdict(float)\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x is a list with the multiple views\nx = torch.cat(x, dim=0).to(self.device)\ny = y.to(self.device)\nbsz = y.shape[0]\n# forward pass\nfeatures = self.net(x)\n# apply L2 normalization\nfeatures = torch.nn.functional.normalize(features, dim=1)\n# compute loss\nf1, f2 = torch.split(features, [bsz, bsz], dim=0)\nfeatures = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\nmetrics = self.criterion(features)\nloss = metrics[\"loss\"]\nif self._is_training:\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\nprint(\".\", end=\"\", flush=True)\nfor name, value in metrics.items():\ncum_metrics[name] += value.item()\nprint(\"\\r\" + \" \" * batch_idx + \" \", end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = {}\nfor name, value in cum_metrics.items():\nvalue = value / (batch_idx + 1)\nif name.startswith(\"acc\"):\nvalue *= 100\nmetrics[name] = value\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nreturn metrics\ndef train_one_epoch(self, train_loader, idx_epoch, context):\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself.net.train()\nself._is_training = True\nout = self._do_epoch(train_loader, idx_epoch, context)\nself._is_training = False\nreturn out\ndef train_loop(\nself,\nepochs: int,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoaer = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\nrun_init_train: bool = True,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n        The entry point for triggering training\n        Arguments:\n            epochs: number of epochs to run\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            The best model obtained during training\n        \"\"\"\nassert context is not None\nif run_init_train:\nself.net, self.optimizer = self.init_train(self.net, self.optimizer)\nself.net = self.net.to(self.device)\nx, y = next(iter(train_loader))\nself.log_msg(\n\"\\n======= net adapted for contrastive learning training =========\"\n)\ntorchsummary.summary(self.net.float(), tuple(x[0][0].shape))\nself.net.double()\nreturn super().train_loop(\nepochs=epochs,\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nquiet=quiet,\ncontext=context,\n)\ndef test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: ind = None,\ncontext: str = None,\n*args,\n**kwargs,\n) -&gt; Tuple[Dict[str, Any], Any]:\n\"\"\"\n        Run inference on a model (for testing or validation)\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second is an empty dictionary\n        \"\"\"\nself.net.eval()\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nwith torch.no_grad():\nmetrics = self._do_epoch(data_loader, idx_epoch, context=context)\n# we return an empty report just to have\n# consistency with the return types\n# of SimpleTrainer\nreports = {}\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.prepare_net_for_train","title":"<code>prepare_net_for_train(net, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity) its .classifier and the last activation function of .features</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to modify</p> required <code>fname_weights</code> <code>pathlib.Path</code> <p>if provided, the weights are loaded into the network after the modification</p> <code>None</code> Return <p>A new instance of the input network with architecture modification required to run training contrastive learning training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef prepare_net_for_train(\ncls, net: backbone.BaseNet, fname_weights: pathlib.Path = None\n) -&gt; backbone.BaseNet:\n\"\"\"\n    Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity)\n    its .classifier and the last activation function of .features\n    Arguments:\n        net: the network to modify\n        fname_weights: if provided, the weights are loaded into\n            the network after the modification\n    Return:\n        A new instance of the input network with architecture\n        modification required to run training contrastive learning\n        training\n    \"\"\"\nnew_net = backbone.clone_net(net)\nif not hasattr(net, \"prepare_for_contrastivelearning\"):\nraise RuntimeError(\n\"Did not find a .prepare_for_contrativelearning() method in the network. Cannot adapt the network for training\"\n)\nnew_net.prepare_for_contrastivelearning(fname_weights)\nnew_net = new_net.double()\nreturn new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.init_train","title":"<code>init_train(net, optimizer=None, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clones the input network and prepares it for contrastive learning, and instanciate a new optimized bounding it to the new network weights</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to use</p> required <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>the optimizer to tuse</p> <code>None</code> <code>fname_weights</code> <code>pathlib.Path</code> <p>if provided, the weights are loaded into the new network before returning it</p> <code>None</code> Return <p>A tuple with the new updated network and the related optimizer</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_train(\ncls,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\nfname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n\"\"\"\n    Clones the input network and prepares it for contrastive learning,\n    and instanciate a new optimized bounding it to the new network weights\n    Arguments:\n        net: the network to use\n        optimizer: the optimizer to tuse\n        fname_weights: if provided, the weights are loaded\n            into the new network before returning it\n    Return:\n        A tuple with the new updated network and the related optimizer\n    \"\"\"\nnew_net = cls.prepare_net_for_train(net, fname_weights)\nnew_optimizer = None\nif optimizer:\nnew_optimizer = _reset_optimizer(optimizer, new_net.parameters())\nreturn new_net, new_optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer._do_epoch","title":"<code>_do_epoch(data_loader, idx_epoch, context='train')</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>'train'</code> Return <p>A dictionary of metrics collected</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int,\ncontext: str = \"train\",\n) -&gt; Dict[str, Any]:\n\"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A dictionary of metrics collected\n    \"\"\"\ncum_metrics = defaultdict(float)\nfor batch_idx, (x, y) in enumerate(data_loader):\n# x is a list with the multiple views\nx = torch.cat(x, dim=0).to(self.device)\ny = y.to(self.device)\nbsz = y.shape[0]\n# forward pass\nfeatures = self.net(x)\n# apply L2 normalization\nfeatures = torch.nn.functional.normalize(features, dim=1)\n# compute loss\nf1, f2 = torch.split(features, [bsz, bsz], dim=0)\nfeatures = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\nmetrics = self.criterion(features)\nloss = metrics[\"loss\"]\nif self._is_training:\nself.optimizer.zero_grad()\nloss.backward()\nself.optimizer.step()\nprint(\".\", end=\"\", flush=True)\nfor name, value in metrics.items():\ncum_metrics[name] += value.item()\nprint(\"\\r\" + \" \" * batch_idx + \" \", end=\"\", flush=True)\nprint(\"\\r\", end=\"\", flush=True)\nmetrics = {}\nfor name, value in cum_metrics.items():\nvalue = value / (batch_idx + 1)\nif name.startswith(\"acc\"):\nvalue *= 100\nmetrics[name] = value\nself._track_metrics(metrics, epoch=idx_epoch, context=context)\nreturn metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, idx_epoch, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(self, train_loader, idx_epoch, context):\n\"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\nself.net.train()\nself._is_training = True\nout = self._do_epoch(train_loader, idx_epoch, context)\nself._is_training = False\nreturn out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.train_loop","title":"<code>train_loop(epochs, train_loader, val_loader=None, patience_monitor=None, quiet=False, run_init_train=True, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>number of epochs to run</p> required <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>torch.utils.data.DataLoaer</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def train_loop(\nself,\nepochs: int,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoaer = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nquiet: bool = False,\nrun_init_train: bool = True,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n    The entry point for triggering training\n    Arguments:\n        epochs: number of epochs to run\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        The best model obtained during training\n    \"\"\"\nassert context is not None\nif run_init_train:\nself.net, self.optimizer = self.init_train(self.net, self.optimizer)\nself.net = self.net.to(self.device)\nx, y = next(iter(train_loader))\nself.log_msg(\n\"\\n======= net adapted for contrastive learning training =========\"\n)\ntorchsummary.summary(self.net.float(), tuple(x[0][0].shape))\nself.net.double()\nreturn super().train_loop(\nepochs=epochs,\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nquiet=quiet,\ncontext=context,\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.test_loop","title":"<code>test_loop(data_loader, idx_epoch=None, context=None, *args, **kwargs)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>ind</code> <p>the current epoch</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second is an empty dictionary</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: ind = None,\ncontext: str = None,\n*args,\n**kwargs,\n) -&gt; Tuple[Dict[str, Any], Any]:\n\"\"\"\n    Run inference on a model (for testing or validation)\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second is an empty dictionary\n    \"\"\"\nself.net.eval()\nif context is None:\ncontext = \"val\" if self._is_training else \"test\"\nwith torch.no_grad():\nmetrics = self._do_epoch(data_loader, idx_epoch, context=context)\n# we return an empty report just to have\n# consistency with the return types\n# of SimpleTrainer\nreports = {}\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.MonolithicTrainer","title":"<code>MonolithicTrainer</code>","text":"<p>         Bases: <code>SimpleTrainer</code></p> <p>A wrapper around SimpleTrainer and designed to be used in supervised classification scenarios</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class MonolithicTrainer(SimpleTrainer):\n\"\"\"A wrapper around SimpleTrainer and designed\n    to be used in supervised classification scenarios\n    \"\"\"\ndef __init__(\nself,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\ncriterion: torch.nn.Module = nn.CrossEntropyLoss(),\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\nreset_classifier: bool = False,\nnum_classes: int = None,\nxgboost_model=None,\n):\n\"\"\"\n        Arguments:\n            net: the architecture to use\n            optimizer: the optimizer to use\n            criterion: the instance of the loss to use\n            device: the device to use\n            deterministic: see _make_deterministic()\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n            reset_classifier: if True, the network is modified\n                to have a new layer\n            num_classes: number of units to use for the new\n                classifier head\n        \"\"\"\nif reset_classifier:\nif num_classes is None:\nraise RuntimeError(\nf\"num_classes cannot be None when resetting the model head\"\n)\nnet = net.reset_classifier(num_classes)\nsuper().__init__(\nnet=net,\noptimizer=optimizer,\ncriterion=criterion,\ndevice=device,\ndeterministic=deterministic,\ntracker=tracker,\nlogger=logger,\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.MonolithicTrainer.__init__","title":"<code>__init__(net, optimizer=None, criterion=nn.CrossEntropyLoss(), device='cuda:0', deterministic=True, tracker=None, logger=None, reset_classifier=False, num_classes=None, xgboost_model=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the architecture to use</p> required <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>the optimizer to use</p> <code>None</code> <code>criterion</code> <code>torch.nn.Module</code> <p>the instance of the loss to use</p> <code>nn.CrossEntropyLoss()</code> <code>device</code> <code>str</code> <p>the device to use</p> <code>'cuda:0'</code> <code>deterministic</code> <code>bool</code> <p>see _make_deterministic()</p> <code>True</code> <code>tracker</code> <code>aim.Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>logging.Logger</code> <p>the logging reference</p> <code>None</code> <code>reset_classifier</code> <code>bool</code> <p>if True, the network is modified to have a new layer</p> <code>False</code> <code>num_classes</code> <code>int</code> <p>number of units to use for the new classifier head</p> <code>None</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(\nself,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\ncriterion: torch.nn.Module = nn.CrossEntropyLoss(),\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\nreset_classifier: bool = False,\nnum_classes: int = None,\nxgboost_model=None,\n):\n\"\"\"\n    Arguments:\n        net: the architecture to use\n        optimizer: the optimizer to use\n        criterion: the instance of the loss to use\n        device: the device to use\n        deterministic: see _make_deterministic()\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n        reset_classifier: if True, the network is modified\n            to have a new layer\n        num_classes: number of units to use for the new\n            classifier head\n    \"\"\"\nif reset_classifier:\nif num_classes is None:\nraise RuntimeError(\nf\"num_classes cannot be None when resetting the model head\"\n)\nnet = net.reset_classifier(num_classes)\nsuper().__init__(\nnet=net,\noptimizer=optimizer,\ncriterion=criterion,\ndevice=device,\ndeterministic=deterministic,\ntracker=tracker,\nlogger=logger,\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer","title":"<code>SimCLRTrainer</code>","text":"<p>A trainer designed for SimCLR (https://arxiv.org/abs/2002.05709). Differently from the other trainers which are based on inheritancs, it is based on nesting a ContrastiveLearningTrainer object (for contrastive learning) and a MonolithicTrainer object (for finetune)</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>class SimCLRTrainer:\n\"\"\"A trainer designed for SimCLR (https://arxiv.org/abs/2002.05709).\n    Differently from the other trainers which are based on\n    inheritancs, it is based on nesting a ContrastiveLearningTrainer\n    object (for contrastive learning) and a MonolithicTrainer object (for finetune)\n    \"\"\"\ndef __init__(\nself,\npretrain_config: Dict[str, Any] = None,\nfinetune_config: Dict[str, Any] = None,\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\nxgboost_model=None,\n):\n\"\"\"\n        Arguments:\n            pretrain_config: a set of configuration required for pretraining.\n                The dictionary should contain an \"optimizer\" and the related instance\n                (None if missing) and a \"loss_temperature\" (0.07 if missing)\n            finetune_config: a set of configuration required for finetune.\n                The dictionary should contain an \"optimizer\" and the related instance\n                (None if missing).\n            device: the device for training and inference\n            tracker: the AIM run for metric tracking\n            logger: a logging object for console and file logging\n        \"\"\"\nself.tracker = tracker\nself.logger = logger\nself.device = device\nself.pretrain_config = pretrain_config\nself.pretrain_criterion = None\nself.pretrain_trainer = None\nself.pretrain_best_net = None\nself.finetune_config = finetune_config\nself.finetune_criterion = None\nself.finetune_trainer = None\nself.finetune_best_net = None\ntrainer_params = dict(\nnet=None,\ntracker=tracker,\nlogger=logger,\ndeterministic=False,\ndevice=device,\n)\nif pretrain_config is not None:\nself.pretrain_criterion = losses.SimCLRLoss(\ntemperature=pretrain_config.get(\"loss_temperature\", 0.07),\nbase_temperature=pretrain_config.get(\"loss_base_temperature\", 0.07),\ncontrast_mode=\"all\",\n)\nself.pretrain_trainer = ContrastiveLearningTrainer(\noptimizer=pretrain_config.get(\"optimizer\", None),\ncriterion=self.pretrain_criterion,\n**trainer_params,\n)\nif finetune_config is not None:\nself.finetune_criterion = nn.CrossEntropyLoss()\nself.finetune_trainer = MonolithicTrainer(\noptimizer=finetune_config.get(\"optimizer\", None),\ncriterion=self.finetune_criterion,\n**trainer_params,\n)\nif deterministic:\n_make_deterministic()\n@classmethod\ndef init_pretrain(\ncls,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\nfname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, Any]:\n\"\"\"\n        Clones the input network and prepares it for contrastive learning,\n        and instanciate a new optimized bounding it to the new network weights\n        Arguments:\n            net: the network to use\n            optimizer: the optimizer to tuse\n            fname_weights: if provided, the weights are loaded\n                into the new network before returning it\n        Return:\n            A tuple with the new updated network and the related optimizer\n        \"\"\"\nreturn ContrastiveLearningTrainer.init_train(net, optimizer, fname_weights)\ndef pretrain_loop(\nself,\nnet: backbone.BaseNet,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.Data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nepochs: int = 50,\nrun_init_pretrain: bool = True,\nfname_weights: pathlib.Path = None,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n        The entry point for triggering training\n        Arguments:\n            net: the network to use\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            epochs: number of epochs to run\n            run_init_pretrain: if True, invokes .init_pretrain()\n            fname_weights: a file with the weights to load after\n                preparing the model for training\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            The best model obtained during training\n        \"\"\"\nif run_init_pretrain:\nnet, optimizer = self.init_pretrain(\nnet, self.pretrain_trainer.optimizer, fname_weights\n)\nparams = list(net.parameters())\nexpected_dtype = params[0].dtype\nself.pretrain_trainer.net = net.to(self.device)\nself.pretrain_trainer.optimizer = optimizer\nx, y = next(iter(train_loader))\nutils.log_msg(\"\\n==== network adapted for pretrain ====\", self.logger)\ntorchsummary.summary(net.float(), tuple(x[0][0].shape))\nnet.double()\nself.pretrain_trainer.net = net\nself.pretrain_best_model = self.pretrain_trainer.train_loop(\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nepochs=epochs,\nrun_init_train=False,\ncontext=context,\n)\nreturn self.pretrain_best_model\n@classmethod\ndef prepare_net_for_finetune(\ncls,\nnet: backbone.BaseNet,\nnum_classes: int = 5,\nfname_pretrain_weights: pathlib.Path = None,\nfname_finetune_weights: pathlib.Path = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n        Clone a backbone.BaseNet related to contrastive learning an\n        prepare it for finetune. Specifically, the last linear layer\n        of the newtwork (the projection layer) is masked (via a nn.Identity)\n        and a classifier is added to the network\n        Arguments:\n            net: the network to modify\n            num_classes: the number of units for the classifier\n        Return:\n            A new instance of the input network with architecture\n            modification required to run training contrastive learning\n            training\n        \"\"\"\nif not hasattr(net, \"prepare_for_finetune\"):\nraise RuntimeError(\n\"Did not find a .prepare_for_finetune() method in the network. Cannot adapt the network for training\"\n)\nnew_net = backbone.clone_net(net)\nnew_net.prepare_for_finetune(\nnum_classes=num_classes,\nfname_pretrain_weights=fname_pretrain_weights,\nfname_finetune_weights=fname_finetune_weights,\n)\nreturn new_net\n@classmethod\ndef init_finetune(\ncls,\nnet: backbone.BaseNet,\nnum_classes: int,\nfname_pretrain_weights: pathlib.Path = None,\nfname_finetune_weights: pathlib.Path = None,\noptimizer=None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n\"\"\"\n        Initialize the network for finetuning adapting\n        it from contrastive-learning. Specifically, the\n        input network is the first modified for contrastive-learning,\n        and then further adjusted for finetune.\n        Arguments:\n            net: the network to use\n            num_classes: the number of classes for the classifier\n            fname_pretrain_weights: if specified, the weights\n                are loaded before adapting the network from\n                pretraining\n            fname_finetune_weights: if specified, the weights\n                are loaded after adapting the network for\n                finetune\n        \"\"\"\nnew_net = cls.prepare_net_for_finetune(\nnet, num_classes, fname_pretrain_weights, fname_finetune_weights\n)\nnew_optimizer = None\nif optimizer:\nnew_optimizer = _reset_optimizer(optimizer, new_net.classifier.parameters())\nreturn new_net, new_optimizer\ndef finetune_loop(\nself,\nnet: backbone.BaseNet,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nepochs: int = 50,\nnum_classes: int = 5,\nrun_init_finetune: bool = True,\nfname_pretrain_weights: pathlib.PAth = None,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n        The entry point for triggering training\n        Arguments:\n            net: the network to use\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            epochs: number of epochs to run\n            num_classes: the number of units for the classifier\n            run_init_finetune: if True, invokes .init_finetune()\n            fname_pretrain_weights: a file with the weights to load after\n                preparing the model for training\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            The best model obtained during training\n        \"\"\"\nif run_init_finetune:\nnet, optimizer = self.init_finetune(\nnet=net,\nnum_classes=num_classes,\nfname_pretrain_weights=fname_pretrain_weights,\noptimizer=self.finetune_trainer.optimizer,\n)\nnet = net.to(self.device)\nself.finetune_trainer.net = net\nself.finetune_trainer.optimizer = optimizer\nself.finetune_net = net\nx, y = next(iter(train_loader))\n# x can be a list if the underlining\n# dataset is multi-view\nif isinstance(x, list):\nx = x[0]\nutils.log_msg(\"\\n==== network adapted for fine-tuning ====\", self.logger)\ntorchsummary.summary(net.float(), tuple(x[0].shape))\nnet.double()\nself.finetune_trainer.net = net\nself.finetune_best_net = self.finetune_trainer.train_loop(\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nepochs=epochs,\ncontext=context,\n)\nreturn self.finetune_best_net\ndef finetune_test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n        Run inference on a (supervised) model (for testing or validation)\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\nreturn self.finetune_trainer.test_loop(\ndata_loader, idx_epoch, with_reports, context\n)\ndef pretrain_test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\ncontext: str = None,\n*args,\n**kwargs,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n        Run inference on a (unsupervised) model (for testing or validation)\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\nreturn self.pretrain_trainer.test_loop(data_loader, idx_epoch, context)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.__init__","title":"<code>__init__(pretrain_config=None, finetune_config=None, device='cuda:0', deterministic=True, tracker=None, logger=None, xgboost_model=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrain_config</code> <code>Dict[str, Any]</code> <p>a set of configuration required for pretraining. The dictionary should contain an \"optimizer\" and the related instance (None if missing) and a \"loss_temperature\" (0.07 if missing)</p> <code>None</code> <code>finetune_config</code> <code>Dict[str, Any]</code> <p>a set of configuration required for finetune. The dictionary should contain an \"optimizer\" and the related instance (None if missing).</p> <code>None</code> <code>device</code> <code>str</code> <p>the device for training and inference</p> <code>'cuda:0'</code> <code>tracker</code> <code>aim.Run</code> <p>the AIM run for metric tracking</p> <code>None</code> <code>logger</code> <code>logging.Logger</code> <p>a logging object for console and file logging</p> <code>None</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def __init__(\nself,\npretrain_config: Dict[str, Any] = None,\nfinetune_config: Dict[str, Any] = None,\ndevice: str = \"cuda:0\",\ndeterministic: bool = True,\ntracker: aim.Run = None,\nlogger: logging.Logger = None,\nxgboost_model=None,\n):\n\"\"\"\n    Arguments:\n        pretrain_config: a set of configuration required for pretraining.\n            The dictionary should contain an \"optimizer\" and the related instance\n            (None if missing) and a \"loss_temperature\" (0.07 if missing)\n        finetune_config: a set of configuration required for finetune.\n            The dictionary should contain an \"optimizer\" and the related instance\n            (None if missing).\n        device: the device for training and inference\n        tracker: the AIM run for metric tracking\n        logger: a logging object for console and file logging\n    \"\"\"\nself.tracker = tracker\nself.logger = logger\nself.device = device\nself.pretrain_config = pretrain_config\nself.pretrain_criterion = None\nself.pretrain_trainer = None\nself.pretrain_best_net = None\nself.finetune_config = finetune_config\nself.finetune_criterion = None\nself.finetune_trainer = None\nself.finetune_best_net = None\ntrainer_params = dict(\nnet=None,\ntracker=tracker,\nlogger=logger,\ndeterministic=False,\ndevice=device,\n)\nif pretrain_config is not None:\nself.pretrain_criterion = losses.SimCLRLoss(\ntemperature=pretrain_config.get(\"loss_temperature\", 0.07),\nbase_temperature=pretrain_config.get(\"loss_base_temperature\", 0.07),\ncontrast_mode=\"all\",\n)\nself.pretrain_trainer = ContrastiveLearningTrainer(\noptimizer=pretrain_config.get(\"optimizer\", None),\ncriterion=self.pretrain_criterion,\n**trainer_params,\n)\nif finetune_config is not None:\nself.finetune_criterion = nn.CrossEntropyLoss()\nself.finetune_trainer = MonolithicTrainer(\noptimizer=finetune_config.get(\"optimizer\", None),\ncriterion=self.finetune_criterion,\n**trainer_params,\n)\nif deterministic:\n_make_deterministic()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.init_pretrain","title":"<code>init_pretrain(net, optimizer=None, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clones the input network and prepares it for contrastive learning, and instanciate a new optimized bounding it to the new network weights</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to use</p> required <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>the optimizer to tuse</p> <code>None</code> <code>fname_weights</code> <code>pathlib.Path</code> <p>if provided, the weights are loaded into the new network before returning it</p> <code>None</code> Return <p>A tuple with the new updated network and the related optimizer</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_pretrain(\ncls,\nnet: backbone.BaseNet,\noptimizer: torch.optim.Optimizer = None,\nfname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, Any]:\n\"\"\"\n    Clones the input network and prepares it for contrastive learning,\n    and instanciate a new optimized bounding it to the new network weights\n    Arguments:\n        net: the network to use\n        optimizer: the optimizer to tuse\n        fname_weights: if provided, the weights are loaded\n            into the new network before returning it\n    Return:\n        A tuple with the new updated network and the related optimizer\n    \"\"\"\nreturn ContrastiveLearningTrainer.init_train(net, optimizer, fname_weights)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.pretrain_loop","title":"<code>pretrain_loop(net, train_loader, val_loader=None, patience_monitor=None, epochs=50, run_init_pretrain=True, fname_weights=None, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to use</p> required <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>torch.utils.Data.DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>epochs</code> <code>int</code> <p>number of epochs to run</p> <code>50</code> <code>run_init_pretrain</code> <code>bool</code> <p>if True, invokes .init_pretrain()</p> <code>True</code> <code>fname_weights</code> <code>pathlib.Path</code> <p>a file with the weights to load after preparing the model for training</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def pretrain_loop(\nself,\nnet: backbone.BaseNet,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.Data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nepochs: int = 50,\nrun_init_pretrain: bool = True,\nfname_weights: pathlib.Path = None,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n    The entry point for triggering training\n    Arguments:\n        net: the network to use\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        epochs: number of epochs to run\n        run_init_pretrain: if True, invokes .init_pretrain()\n        fname_weights: a file with the weights to load after\n            preparing the model for training\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        The best model obtained during training\n    \"\"\"\nif run_init_pretrain:\nnet, optimizer = self.init_pretrain(\nnet, self.pretrain_trainer.optimizer, fname_weights\n)\nparams = list(net.parameters())\nexpected_dtype = params[0].dtype\nself.pretrain_trainer.net = net.to(self.device)\nself.pretrain_trainer.optimizer = optimizer\nx, y = next(iter(train_loader))\nutils.log_msg(\"\\n==== network adapted for pretrain ====\", self.logger)\ntorchsummary.summary(net.float(), tuple(x[0][0].shape))\nnet.double()\nself.pretrain_trainer.net = net\nself.pretrain_best_model = self.pretrain_trainer.train_loop(\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nepochs=epochs,\nrun_init_train=False,\ncontext=context,\n)\nreturn self.pretrain_best_model\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.prepare_net_for_finetune","title":"<code>prepare_net_for_finetune(net, num_classes=5, fname_pretrain_weights=None, fname_finetune_weights=None)</code>  <code>classmethod</code>","text":"<p>Clone a backbone.BaseNet related to contrastive learning an prepare it for finetune. Specifically, the last linear layer of the newtwork (the projection layer) is masked (via a nn.Identity) and a classifier is added to the network</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to modify</p> required <code>num_classes</code> <code>int</code> <p>the number of units for the classifier</p> <code>5</code> Return <p>A new instance of the input network with architecture modification required to run training contrastive learning training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef prepare_net_for_finetune(\ncls,\nnet: backbone.BaseNet,\nnum_classes: int = 5,\nfname_pretrain_weights: pathlib.Path = None,\nfname_finetune_weights: pathlib.Path = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n    Clone a backbone.BaseNet related to contrastive learning an\n    prepare it for finetune. Specifically, the last linear layer\n    of the newtwork (the projection layer) is masked (via a nn.Identity)\n    and a classifier is added to the network\n    Arguments:\n        net: the network to modify\n        num_classes: the number of units for the classifier\n    Return:\n        A new instance of the input network with architecture\n        modification required to run training contrastive learning\n        training\n    \"\"\"\nif not hasattr(net, \"prepare_for_finetune\"):\nraise RuntimeError(\n\"Did not find a .prepare_for_finetune() method in the network. Cannot adapt the network for training\"\n)\nnew_net = backbone.clone_net(net)\nnew_net.prepare_for_finetune(\nnum_classes=num_classes,\nfname_pretrain_weights=fname_pretrain_weights,\nfname_finetune_weights=fname_finetune_weights,\n)\nreturn new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.init_finetune","title":"<code>init_finetune(net, num_classes, fname_pretrain_weights=None, fname_finetune_weights=None, optimizer=None)</code>  <code>classmethod</code>","text":"<p>Initialize the network for finetuning adapting it from contrastive-learning. Specifically, the input network is the first modified for contrastive-learning, and then further adjusted for finetune.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to use</p> required <code>num_classes</code> <code>int</code> <p>the number of classes for the classifier</p> required <code>fname_pretrain_weights</code> <code>pathlib.Path</code> <p>if specified, the weights are loaded before adapting the network from pretraining</p> <code>None</code> <code>fname_finetune_weights</code> <code>pathlib.Path</code> <p>if specified, the weights are loaded after adapting the network for finetune</p> <code>None</code> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_finetune(\ncls,\nnet: backbone.BaseNet,\nnum_classes: int,\nfname_pretrain_weights: pathlib.Path = None,\nfname_finetune_weights: pathlib.Path = None,\noptimizer=None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n\"\"\"\n    Initialize the network for finetuning adapting\n    it from contrastive-learning. Specifically, the\n    input network is the first modified for contrastive-learning,\n    and then further adjusted for finetune.\n    Arguments:\n        net: the network to use\n        num_classes: the number of classes for the classifier\n        fname_pretrain_weights: if specified, the weights\n            are loaded before adapting the network from\n            pretraining\n        fname_finetune_weights: if specified, the weights\n            are loaded after adapting the network for\n            finetune\n    \"\"\"\nnew_net = cls.prepare_net_for_finetune(\nnet, num_classes, fname_pretrain_weights, fname_finetune_weights\n)\nnew_optimizer = None\nif optimizer:\nnew_optimizer = _reset_optimizer(optimizer, new_net.classifier.parameters())\nreturn new_net, new_optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.finetune_loop","title":"<code>finetune_loop(net, train_loader, val_loader=None, patience_monitor=None, epochs=50, num_classes=5, run_init_finetune=True, fname_pretrain_weights=None, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the network to use</p> required <code>train_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>epochs</code> <code>int</code> <p>number of epochs to run</p> <code>50</code> <code>num_classes</code> <code>int</code> <p>the number of units for the classifier</p> <code>5</code> <code>run_init_finetune</code> <code>bool</code> <p>if True, invokes .init_finetune()</p> <code>True</code> <code>fname_pretrain_weights</code> <code>pathlib.PAth</code> <p>a file with the weights to load after preparing the model for training</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def finetune_loop(\nself,\nnet: backbone.BaseNet,\ntrain_loader: torch.utils.data.DataLoader,\nval_loader: torch.utils.data.DataLoader = None,\npatience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\nepochs: int = 50,\nnum_classes: int = 5,\nrun_init_finetune: bool = True,\nfname_pretrain_weights: pathlib.PAth = None,\ncontext: str = None,\n) -&gt; backbone.BaseNet:\n\"\"\"\n    The entry point for triggering training\n    Arguments:\n        net: the network to use\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        epochs: number of epochs to run\n        num_classes: the number of units for the classifier\n        run_init_finetune: if True, invokes .init_finetune()\n        fname_pretrain_weights: a file with the weights to load after\n            preparing the model for training\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        The best model obtained during training\n    \"\"\"\nif run_init_finetune:\nnet, optimizer = self.init_finetune(\nnet=net,\nnum_classes=num_classes,\nfname_pretrain_weights=fname_pretrain_weights,\noptimizer=self.finetune_trainer.optimizer,\n)\nnet = net.to(self.device)\nself.finetune_trainer.net = net\nself.finetune_trainer.optimizer = optimizer\nself.finetune_net = net\nx, y = next(iter(train_loader))\n# x can be a list if the underlining\n# dataset is multi-view\nif isinstance(x, list):\nx = x[0]\nutils.log_msg(\"\\n==== network adapted for fine-tuning ====\", self.logger)\ntorchsummary.summary(net.float(), tuple(x[0].shape))\nnet.double()\nself.finetune_trainer.net = net\nself.finetune_best_net = self.finetune_trainer.train_loop(\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=patience_monitor,\nepochs=epochs,\ncontext=context,\n)\nreturn self.finetune_best_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.finetune_test_loop","title":"<code>finetune_test_loop(data_loader, idx_epoch=None, with_reports=False, context=None)</code>","text":"<p>Run inference on a (supervised) model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def finetune_test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\nwith_reports: bool = False,\ncontext: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n    Run inference on a (supervised) model (for testing or validation)\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\nreturn self.finetune_trainer.test_loop(\ndata_loader, idx_epoch, with_reports, context\n)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.pretrain_test_loop","title":"<code>pretrain_test_loop(data_loader, idx_epoch=None, context=None, *args, **kwargs)</code>","text":"<p>Run inference on a (unsupervised) model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>torch.utils.data.DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def pretrain_test_loop(\nself,\ndata_loader: torch.utils.data.DataLoader,\nidx_epoch: int = None,\ncontext: str = None,\n*args,\n**kwargs,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n\"\"\"\n    Run inference on a (unsupervised) model (for testing or validation)\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\nreturn self.pretrain_trainer.test_loop(data_loader, idx_epoch, context)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods._make_deterministic","title":"<code>_make_deterministic()</code>","text":"<p>Helper method to force pytorch to be deterministic</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _make_deterministic():\n\"\"\"Helper method to force pytorch to be deterministic\"\"\"\ntorch.use_deterministic_algorithms(True)\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods._reset_optimizer","title":"<code>_reset_optimizer(optimizer, net_parameters)</code>","text":"<p>Helper method to recreate a new instance of the optimizer passed as input (via introspection) with the same configuration and bound to the network parameters of a model</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>the optimizer to clone</p> required <code>net_parameters</code> <code>Generator</code> <p>iterator obtained calling .parameters() from a pytorch module</p> required Return <p>a new instance of an optimizer</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def _reset_optimizer(\noptimizer: torch.optim.Optimizer, net_parameters: Generator\n) -&gt; torch.optim.Optimizer:\n\"\"\"Helper method to recreate a new instance of\n    the optimizer passed as input (via introspection)\n    with the same configuration and bound to the\n    network parameters of a model\n    Arguments:\n        optimizer: the optimizer to clone\n        net_parameters: iterator obtained calling .parameters() from a pytorch module\n    Return:\n        a new instance of an optimizer\n    \"\"\"\noptimizer_class = getattr(torch.optim, optimizer.__class__.__name__)\noptimizer_params = {\nname: value\nfor name, value in optimizer.param_groups[0].items()\nif name != \"params\"\n}\noptimizer = optimizer_class(net_parameters, **optimizer_params)\nreturn optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.trainer_factory","title":"<code>trainer_factory(method, *args, **kwargs)</code>","text":"<p>Helper function to instanciate trainer objects</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>either \"monolithic\" or \"simclr\" or \"xgboost\"</p> required <code>args</code> <code>Any</code> <p>positional arguments to use when instanciating the class (if any)</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>key/value arguments to use when instanciating the class (if any)</p> <code>{}</code> Return <p>a trainer object</p> Source code in <code>tcbench/modeling/methods.py</code> <pre><code>def trainer_factory(method: str, *args: Any, **kwargs: Any) -&gt; Any:\n\"\"\"Helper function to instanciate trainer objects\n    Arguments:\n        method: either \"monolithic\" or \"simclr\" or \"xgboost\"\n        args: positional arguments to use when instanciating the class (if any)\n        kwargs: key/value arguments to use when instanciating the class (if any)\n    Return:\n        a trainer object\n    \"\"\"\nreturn METHOD_CLASSES[method](*args, **kwargs)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/","title":"run_augmentations_at_loading","text":""},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.train","title":"<code>train(dataset_name=tcbench.DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, learning_rate=0.01, flowpic_dim=32, flowpic_block_duration=15, split_index=0, max_samples_per_class=-1, logger=None, aug_name='noaug', aug_samples=10, device='cuda:0', tracker=None, workers=50, artifacts_folder=None, seed=12345, epochs=50, patience_steps=5, patience_min_delta=0.001, train_val_split_ratio=0.8, suppress_val_augmentation=False, with_dropout=True, state=None)</code>","text":"<p>Model training</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def train(\ndataset_name: DATASETS = tcbench.DATASETS.UCDAVISICDM19,\ndataset_minpkts: int = -1,\nbatch_size: int = 32,\nlearning_rate: float = 0.01,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nsplit_index: int = 0,\nmax_samples_per_class: int = -1,\nlogger=None,\naug_name: str = \"noaug\",\naug_samples: int = 10,\ndevice: str = \"cuda:0\",\ntracker: aim.Run = None,\nworkers: int = 50,\nartifacts_folder=None,\nseed: int = 12345,\nepochs: int = 50,\npatience_steps: int = 5,\npatience_min_delta: float = 0.001,\ntrain_val_split_ratio: float = 0.8,\nsuppress_val_augmentation: bool = False,\nwith_dropout: bool = True,\nstate: dict = None,\n) -&gt; Dict[str, Any]:\n\"\"\"Model training\"\"\"\nif state is None:\nstate = dict()\naug_config = {aug_name: dict()}\ndset_train, dset_val = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\nsplit_idx=split_index,\nmax_samples_per_class=max_samples_per_class,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\naug_config=aug_config,\naug_samples=aug_samples,\naug_when_loading=True,\nn_workers=workers,\nsuppress_val_augmentation=suppress_val_augmentation,\nlogger=logger,\nseed=seed,\ndataset_minpkts=dataset_minpkts,\n)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dset_val, batch_size, shuffle=False)\nnet = backbone.net_factory(\nnum_classes=dset_train.num_classes,\nflowpic_dim=flowpic_dim,\nwith_dropout=with_dropout,\n)\ntorchsummary.summary(net.to(device), (1, flowpic_dim, flowpic_dim))\noptimizer = torch.optim.Adam(net.parameters(), learning_rate)\ntrainer_kwargs = dict(\nnet=net, optimizer=optimizer, tracker=tracker, device=device, logger=logger\n)\ntrainer = methods.trainer_factory(\"monolithic\", **trainer_kwargs)\nbest_net = trainer.train_loop(\nepochs=epochs,\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=methods.PatienceMonitorLoss(\nsteps=patience_steps, min_delta=patience_min_delta\n),\n)\nstate = dict(\nbest_net=best_net,\ndset_train=dset_train,\ndset_val=dset_val,\n)\nif artifacts_folder is not None:\nbest_net.save_weights(\nartifacts_folder / f\"best_model_weights_split_{split_index}.pt\"\n)\nreports = utils.classification_reports(\nbest_net,\ndset_train,\nbatch_size,\ndevice,\ncontext=\"train\",\nsave_to=artifacts_folder,\nlogger=logger,\n)\nstate[\"train_class_rep\"] = reports[\"class_rep\"]\nstate[\"train_conf_mtx\"] = reports[\"conf_mtx\"]\nreport = utils.classification_reports(\nbest_net,\ndset_val,\nbatch_size,\ndevice,\ncontext=\"val\",\nsave_to=artifacts_folder,\nlogger=logger,\n)\nstate[\"val_class_rep\"] = report[\"class_rep\"]\nstate[\"val_conf_mtx\"] = report[\"conf_mtx\"]\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.test","title":"<code>test(dataset_name, dataset_minpkts=-1, split_idx=0, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, device='cuda:0', tracker=None, artifacts_folder=None, with_dropout=True, state=None)</code>","text":"<p>Model testing</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def test(\ndataset_name: tcbench.DATASETS.UCDAVISICDM19,\ndataset_minpkts: int = -1,\nsplit_idx: int = 0,\nbatch_size: int = 32,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nlogger=None,\ndevice: str = \"cuda:0\",\ntracker: aim.Run = None,\nartifacts_folder: pathlib.Path = None,\nwith_dropout: bool = True,\nstate: dict = None,\n):\n\"\"\"Model testing\"\"\"\nif state is None:\nstate = dict()\ndset_dict = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TEST,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nlogger=logger,\ndataset_minpkts=dataset_minpkts,\n)\n# pick the first dataset name\n# just to identify the number of classes\nname = next(iter(dset_dict.keys()))\nnum_classes = dset_dict[name].num_classes\nnet = backbone.net_factory(\nnum_classes=num_classes, flowpic_dim=flowpic_dim, with_dropout=with_dropout\n)\nfname = artifacts_folder / f\"./best_model_weights_split_{split_idx}.pt\"\nnet.load_weights(fname)\nnet = net.to(device)\ntrainer = methods.trainer_factory(\nmethod=\"monolithic\", net=net, device=device, tracker=tracker, logger=logger\n)\nfor name, dset in dset_dict.items():\nloader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False)\ncontext = \"test\"\nif name != \"test\":\ncontext = f\"test-{name}\"\nmetrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\nutils.log_msg(\nf'Test dataset {name} | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\nlogger,\n)\nreports = utils.classification_reports(\nnet,\ndset,\nbatch_size,\ndevice=device,\ncontext=context,\nsave_to=artifacts_folder,\nlogger=logger,\n)\nstate[f\"{name}_class_rep\"] = reports[\"class_rep\"]\nstate[f\"{name}_conf_mtx\"] = reports[\"conf_mtx\"]\nclass_rep = reports[\"class_rep\"]\nprecision, recall, f1 = class_rep.loc[\n\"weighted avg\", [\"precision\", \"recall\", \"f1-score\"]\n].values\naimutils.track_metrics(\ntracker, dict(precision=precision, recall=recall, f1=f1), context=context\n)\nstate.update(dset_dict)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.test_with_train_val_leftover","title":"<code>test_with_train_val_leftover(dataset_name, dset_train, dset_val, split_idx, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, device='cuda:0', tracker=None, artifacts_folder=None, with_dropout=True, state=None, dataset_minpkts=-1)</code>","text":"<p>Model testing on leftover split</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def test_with_train_val_leftover(\ndataset_name: tcbench.DATASETS.UCDAVISICDM19,\ndset_train: dataprep.FlowpicDataset,\ndset_val: dataprep.FlowpicDataset,\nsplit_idx: int,\nbatch_size: int = 32,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nlogger=None,\ndevice: str = \"cuda:0\",\ntracker: aim.Run = None,\nartifacts_folder: pathlib.Path = None,\nwith_dropout: bool = True,\nstate: dict = None,\ndataset_minpkts: int = -1,\n):\n\"\"\"Model testing on leftover split\"\"\"\nif state is None:\nstate = dict()\ndset_leftover = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TRAIN_VAL_LEFTOVER,\ndset_train=dset_train,\ndset_val=dset_val,\nflowpic_dim=flowpic_dim,\nlogger=logger,\ndataset_minpkts=dataset_minpkts,\n)\nnum_classes = dset_train.num_classes\nnet = backbone.net_factory(\nnum_classes=num_classes, flowpic_dim=flowpic_dim, with_dropout=with_dropout\n)\nfname = artifacts_folder / f\"./best_model_weights_split_{split_idx}.pt\"\nnet.load_weights(fname)\nnet = net.to(device)\ntrainer = methods.trainer_factory(\nmethod=\"monolithic\", net=net, device=device, tracker=tracker, logger=logger\n)\nloader = torch.utils.data.DataLoader(\ndset_leftover, batch_size=batch_size, shuffle=False\n)\ncontext = f\"test-train-val-leftover\"\nmetrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\nutils.log_msg(\nf'Test dataset train-val-leftover | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\nlogger,\n)\nreports = utils.classification_reports(\nnet,\ndset_leftover,\nbatch_size,\ndevice=device,\ncontext=context,\nsave_to=artifacts_folder,\nlogger=logger,\n)\nstate[\"dset_leftover\"] = dset_leftover\nstate[\"leftover_class_rep\"] = reports[\"class_rep\"]\nstate[\"leftover_conf_mtx\"] = reports[\"conf_mtx\"]\nclass_rep = reports[\"class_rep\"]\nprecision, recall, f1 = class_rep.loc[\n\"weighted avg\", [\"precision\", \"recall\", \"f1-score\"]\n].values\naimutils.track_metrics(\ntracker, dict(precision=precision, recall=recall, f1=f1), context=context\n)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def main(args, extra_aim_hparams=None) -&gt; Dict[str, Any]:\n\"\"\"Entry point\"\"\"\n# bounding to a specific gpu\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_index\nargs.method = \"monolithic\"\nif extra_aim_hparams is None:\nextra_aim_hparams = {}\nutils.seed_everything(args.seed)\nif str(args.aim_repo).startswith(\"aim://\"):\nutils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\naim_repo_path = args.aim_repo\nelse:\naim_repo_path = pathlib.Path(args.aim_repo)\nargs.artifacts_folder = pathlib.Path(args.artifacts_folder)\naimutils.init_repository(aim_repo_path)\naim_run = aim.Run(\nrepo=aim_repo_path,\nexperiment=args.aim_experiment_name,\nlog_system_params=True,\ncapture_terminal_logs=True,\n)\naim_run_hash = utils.get_aim_run_hash(aim_run)\nartifacts_folder = args.artifacts_folder / aim_run_hash\nlogger = utils.get_logger(artifacts_folder / \"log.txt\")\nutils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\nutils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\nutils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\nif artifacts_folder.parent != aim_repo_path:\nutils.log_msg(\nf\"WARNING: the artifact folder is not a subfolder of the AIM repo\"\n)\nwith_dropout = not args.suppress_dropout\nrun_hparams = dict(\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\nsplit_index=args.split_index,\nmax_samples_per_class=args.max_samples_per_class,\naug_name=args.aug_name,\npatience_steps=args.patience_steps,\nsuppress_val_augmentation=args.suppress_val_augmentation,\ndataset=args.dataset,\ndataset_minpkts=args.dataset_minpkts,\nseed=args.seed,\nwith_dropout=with_dropout,\n**extra_aim_hparams,\n)\naim_run[\"hparams\"] = run_hparams\nutils.log_msg(\"--- run hparams ---\")\nfor param_name, param_value in run_hparams.items():\nutils.log_msg(f\"{param_name}: {param_value}\")\nutils.log_msg(\"-------------------\")\nstate = dict()\nstate = train(\ndataset_name=args.dataset,\ndataset_minpkts=args.dataset_minpkts,\nbatch_size=args.batch_size,\nlearning_rate=args.learning_rate,\npatience_steps=args.patience_steps,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\nsplit_index=args.split_index,\nmax_samples_per_class=args.max_samples_per_class,\naug_name=args.aug_name,\ntracker=aim_run,\nworkers=args.workers,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nseed=args.seed,\nepochs=args.epochs,\ntrain_val_split_ratio=args.train_val_split_ratio,\nsuppress_val_augmentation=args.suppress_val_augmentation,\nwith_dropout=with_dropout,\nstate=state,\n)\nstate = test(\ndataset_name=args.dataset,\ndataset_minpkts=args.dataset_minpkts,\nsplit_idx=args.split_index,\nbatch_size=args.batch_size,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\ntracker=aim_run,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nwith_dropout=with_dropout,\nstate=state,\n)\nif not args.suppress_test_train_val_leftover and args.dataset == \"ucdavis-icdm19\":\nstate = test_with_train_val_leftover(\ndataset_name=args.dataset,\ndset_train=state[\"dset_train\"],\ndset_val=state[\"dset_val\"],\nsplit_idx=args.split_index,\nbatch_size=args.batch_size,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\ntracker=aim_run,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nwith_dropout=with_dropout,\nstate=state,\n)\naim_run.close()\nutils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\n##################\n# general config\n##################\n#    parser.add_argument(\n#        \"--config\",\n#        \"-c\",\n#        type=pathlib.Path,\n#        required=True,\n#        default=\"./config.yml\",\n#        help=utils.compose_cli_help_string(\"General configuration file\"),\n#    )\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--workers\",\ntype=int,\ndefault=20,\nhelp=utils.compose_cli_help_string(\n\"Number of parallel worker for loading the data\"\n),\n)\nparser.add_argument(\n\"--gpu-index\",\ndefault=\"0\",\nhelp=utils.compose_cli_help_string(\"The GPU id to use\"),\n)\nparser.add_argument(\n\"--aim-repo\",\ndefault=DEFAULT_AIM_REPO,\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"augmentation-at-loading\",\nhelp=utils.compose_cli_help_string(\n\"The name of the experiment for AIM tracking\"\n),\n)\nparser.add_argument(\"--final\", action=\"store_true\", default=False)\n###############\n# flowpic\n###############\nparser.add_argument(\n\"--flowpic-dim\",\ntype=int,\nchoices=(32, 64, 1500),\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Flowpic dimension\"),\n)\nparser.add_argument(\n\"--flowpic-block-duration\",\ntype=int,\ndefault=15,\nhelp=utils.compose_cli_help_string(\"Flowpic block duration (in seconds)\"),\n)\n###############\n# data\n###############\nparser.add_argument(\n\"--dataset\",\nchoices=tuple(map(str, tcbench.DATASETS.__members__.values())),\ndefault=str(tcbench.DATASETS.UCDAVISICDM19),\nhelp=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n)\nparser.add_argument(\n\"--dataset-minpkts\",\nchoices=(-1, 10, 100, 1000),\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n),\n)\nparser.add_argument(\n\"--split-index\",\ntype=int,\ndefault=0,\nhelp=utils.compose_cli_help_string(\"Datasplit index\"),\n)\nparser.add_argument(\n\"--max-samples-per-class\",\ntype=int,\ndefault=-1,\nhelp=utils.compose_cli_help_string(\n\"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\"\n),\n)\n###############\n# training\n###############\nparser.add_argument(\n\"--train-val-split-ratio\",\ntype=float,\ndefault=0.8,\nhelp=utils.compose_cli_help_string(\"Training train/val split\"),\n)\nparser.add_argument(\n\"--aug-name\",\ntype=str,\nchoices=(\n\"noaug\",\n\"rotate\",\n\"horizontalflip\",\n\"colorjitter\",\n\"packetloss\",\n\"timeshift\",\n\"changertt\",\n),\ndefault=\"noaug\",\nhelp=utils.compose_cli_help_string(\"Augmentation policy\"),\n)\nparser.add_argument(\n\"--suppress-val-augmentation\",\naction=\"store_true\",\ndefault=False,\nhelp=utils.compose_cli_help_string(\"Do not augment validation set\"),\n)\nparser.add_argument(\n\"--seed\",\ntype=int,\ndefault=12345,\nhelp=utils.compose_cli_help_string(\"Random seed\"),\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=64,\nhelp=utils.compose_cli_help_string(\"Training batch size\"),\n)\nparser.add_argument(\"--patience-steps\", default=5, type=int)\nparser.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=0.001,\nhelp=utils.compose_cli_help_string(\"Traning learning rate\"),\n)\nparser.add_argument(\n\"--epochs\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\"Number of epochs for training\"),\n)\nparser.add_argument(\n\"--suppress-test-train-val-leftover\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n)\nparser.add_argument(\n\"--suppress-dropout\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n)\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/","title":"run_augmentations_at_loading_xgboost","text":""},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.train","title":"<code>train(dataset_name=DATASETS.UCDAVISICDM19, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, split_index=0, logger=None, tracker=None, workers=50, artifacts_folder=None, seed=12345, train_val_split_ratio=0.8, state=None)</code>","text":"<p>Train an XGBoost model</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def train(\ndataset_name: DATASETS = DATASETS.UCDAVISICDM19,\nflow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,\nmax_n_pkts: int = 10,\nbatch_size: int = 32,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nsplit_index: int = 0,\nlogger=None,\ntracker: aim.Run = None,\nworkers: int = 50,\nartifacts_folder=None,\nseed: int = 12345,\ntrain_val_split_ratio: float = 0.8,\nstate: dict = None,\n) -&gt; Dict[str, Any]:\n\"\"\"Train an XGBoost model\"\"\"\nif state is None:\nstate = dict()\ndset_train, dset_val = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\nflow_representation=flow_representation,\nmax_n_pkts=max_n_pkts,\nsplit_idx=split_index,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nn_workers=workers,\nlogger=logger,\nseed=seed,\n)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dset_val, batch_size, shuffle=False)\nxgboost_model = backbone.xgboost_factory(random_state=seed)\ntrainer_kwargs = dict(xgboost_model=xgboost_model, tracker=tracker, logger=logger)\ntrainer = methods.trainer_factory(\"xgboost\", **trainer_kwargs)\ntrained_model = trainer.train_loop(train_loader=train_loader, val_loader=val_loader)\nxgboost_model.save_model(artifacts_folder / f\"xgb_model_split_{split_index}.json\")\nutils.classification_reports(\nNone,\ndset_train,\nbatch_size,\nNone,\ncontext=\"train\",\nsave_to=artifacts_folder,\nlogger=logger,\nmethod=\"xgboost\",\nxgboost_model=xgboost_model,\n)\nutils.classification_reports(\nNone,\ndset_val,\nbatch_size,\nNone,\ncontext=\"val\",\nsave_to=artifacts_folder,\nlogger=logger,\nmethod=\"xgboost\",\nxgboost_model=xgboost_model,\n)\nstate = dict(\nbest_net=xgboost_model,\ndset_train=dset_train,\ndset_val=dset_val,\nscaler=dset_train.scaler,\n)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.test","title":"<code>test(dataset_name=DATASETS.UCDAVISICDM19, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10, split_idx=0, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, tracker=None, artifacts_folder=None, state=None)</code>","text":"<p>Test an XGBoost model</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def test(\ndataset_name: DATASETS = DATASETS.UCDAVISICDM19,\nflow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,\nmax_n_pkts: int = 10,\nsplit_idx: int = 0,\nbatch_size: int = 32,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nlogger=None,\ntracker: aim.Run = None,\nartifacts_folder: pathlib.Path = None,\nstate: dict = None,\n):\n\"\"\"Test an XGBoost model\"\"\"\nif state is None:\nstate = dict()\ndset_dict = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TEST,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nlogger=logger,\nflow_representation=flow_representation,\nmax_n_pkts=max_n_pkts,\n)\n# pick the first dataset name\n# just to identify the number of classes\nname = next(iter(dset_dict.keys()))\nnum_classes = dset_dict[name].num_classes\nfname = artifacts_folder / f\"./xgb_model_split_{split_idx}.json\"\nmodel_xgb_2 = xgb.XGBClassifier()\nmodel_xgb_2.load_model(fname)\ntrainer = methods.trainer_factory(\nmethod=\"xgboost\", xgboost_model=model_xgb_2, tracker=tracker, logger=logger\n)\nfor name, dset in dset_dict.items():\nloader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False)\ncontext = \"test\"\ndset.set_scaler(state[\"scaler\"])\nif name != \"test\":\ncontext = f\"test-{name}\"\nmetrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\nutils.log_msg(\nf'Test dataset {name} |  acc: {metrics[\"acc\"]:.1f}',\nlogger,\n)\nutils.classification_reports(\nNone,\ndset,\nbatch_size,\ndevice=\"-1\",\ncontext=context,\nsave_to=artifacts_folder,\nlogger=logger,\nmethod=\"xgboost\",\nxgboost_model=model_xgb_2,\n)\nstate.update(dset_dict)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.test_with_train_val_leftover","title":"<code>test_with_train_val_leftover(dataset_name, flow_representation, max_n_pkts, dset_train, dset_val, split_idx, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, tracker=None, artifacts_folder=None, state=None)</code>","text":"<p>Test and XGBoost model on a leftover split</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def test_with_train_val_leftover(\ndataset_name: DATASETS,\nflow_representation: MODELING_INPUT_REPR_TYPE,\nmax_n_pkts: int,\ndset_train: dataprep.FlowpicDataset,\ndset_val: dataprep.FlowpicDataset,\nsplit_idx: int,\nbatch_size: int = 32,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nlogger=None,\ntracker: aim.Run = None,\nartifacts_folder: pathlib.Path = None,\nstate: dict = None,\n):\n\"\"\"Test and XGBoost model on a leftover split\"\"\"\nif state is None:\nstate = dict()\ndset_leftover = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TRAIN_VAL_LEFTOVER,\ndset_train=dset_train,\ndset_val=dset_val,\nflowpic_dim=flowpic_dim,\nlogger=logger,\nflow_representation=flow_representation,\nmax_n_pkts=max_n_pkts,\n)\nnum_classes = dset_train.num_classes\nfname = artifacts_folder / f\"./xgb_model_split_{split_idx}.json\"\nmodel_xgb_2 = xgb.XGBClassifier()\nmodel_xgb_2.load_model(fname)\ntrainer = methods.trainer_factory(\nmethod=\"xgboost\",\nnet=None,\ndevice=\"-1\",\ntracker=tracker,\nlogger=logger,\nxgboost_model=model_xgb_2,\n)\ndset_leftover.set_scaler(state[\"scaler\"])\nloader = torch.utils.data.DataLoader(\ndset_leftover, batch_size=batch_size, shuffle=False\n)\ncontext = f\"test-train-val-leftover\"\nmetrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\nutils.log_msg(\nf'Test dataset train-val-leftover |  acc: {metrics[\"acc\"]:.1f}',\nlogger,\n)\nutils.classification_reports(\nNone,\ndset_leftover,\nbatch_size,\ndevice=\"-1\",\ncontext=context,\nsave_to=artifacts_folder,\nlogger=logger,\nmethod=\"xgboost\",\nxgboost_model=model_xgb_2,\n)\nstate[\"dset_leftover\"] = dset_leftover\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def main(args, extra_aim_hparams=None) -&gt; Dict[str, Any]:\n\"\"\"Entry point\"\"\"\nif extra_aim_hparams is None:\nextra_aim_hparams = {}\nargs.method = \"xgboost\"\nargs.dataset = DATASETS.from_str(str(args.dataset))\nargs.flow_representation = MODELING_INPUT_REPR_TYPE.from_str(\nstr(args.flow_representation)\n)\nargs.aug_name = \"noaug\"\nif str(args.aim_repo).startswith(\"aim://\"):\nutils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\naim_repo_path = args.aim_repo\nelse:\naim_repo_path = pathlib.Path(args.aim_repo)\nargs.artifacts_folder = pathlib.Path(args.artifacts_folder)\naimutils.init_repository(aim_repo_path)\naim_run = aim.Run(\nrepo=aim_repo_path,\nexperiment=args.aim_experiment_name,\nlog_system_params=True,\ncapture_terminal_logs=True,\n)\naim_run_hash = utils.get_aim_run_hash(aim_run)\nartifacts_folder = args.artifacts_folder / aim_run_hash\nlogger = utils.get_logger(artifacts_folder / \"log.txt\")\nutils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\nutils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\nutils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\nif artifacts_folder.parent != aim_repo_path:\nutils.log_msg(\nf\"WARNING: the artifact folder is not a subfolder of the AIM repo\"\n)\nrun_hparams = dict(\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\nsplit_index=args.split_index,\ndataset=str(args.dataset),\nseed=args.seed,\nflow_representation=str(args.flow_representation),\nmax_n_pkts=args.max_n_pkts,\n**extra_aim_hparams,\n)\naim_run[\"hparams\"] = run_hparams\nutils.log_msg(\"--- run hparams ---\")\nfor param_name, param_value in run_hparams.items():\nutils.log_msg(f\"{param_name}: {param_value}\")\nutils.log_msg(\"-------------------\")\nstate = dict()\nstate = train(\ndataset_name=args.dataset,\nflow_representation=args.flow_representation,\nmax_n_pkts=args.max_n_pkts,\nbatch_size=args.batch_size,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\nsplit_index=args.split_index,\ntracker=aim_run,\nworkers=args.workers,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nseed=args.seed,\ntrain_val_split_ratio=args.train_val_split_ratio,\nstate=state,\n)\nstate = test(\ndataset_name=args.dataset,\nflow_representation=args.flow_representation,\nmax_n_pkts=args.max_n_pkts,\nsplit_idx=args.split_index,\nbatch_size=args.batch_size,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\ntracker=aim_run,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nstate=state,\n)\nif (\nnot args.suppress_test_train_val_leftover\nand args.dataset == DATASETS.UCDAVISICDM19\n):\nstate = test_with_train_val_leftover(\ndataset_name=args.dataset,\nflow_representation=args.flow_representation,\nmax_n_pkts=args.max_n_pkts,\ndset_train=state[\"dset_train\"],\ndset_val=state[\"dset_val\"],\nsplit_idx=args.split_index,\nbatch_size=args.batch_size,\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\ntracker=aim_run,\nartifacts_folder=artifacts_folder,\nlogger=logger,\nstate=state,\n)\naim_run.close()\nutils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\n##################\n# general config\n##################\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--workers\",\ntype=int,\ndefault=20,\nhelp=utils.compose_cli_help_string(\n\"Number of parallel worker for loading the data\"\n),\n)\nparser.add_argument(\n\"--aim-repo\",\ndefault=DEFAULT_AIM_REPO,\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"xgb_pktseries\",\nhelp=utils.compose_cli_help_string(\n\"The name of the experiment for AIM tracking\"\n),\n)\nparser.add_argument(\"--final\", action=\"store_true\", default=False)\n###############\n# flowpic\n###############\nparser.add_argument(\n\"--flowpic-dim\",\ntype=int,\nchoices=(32, 64, 1500),\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Flowpic dimension\"),\n)\nparser.add_argument(\n\"--flowpic-block-duration\",\ntype=int,\ndefault=15,\nhelp=utils.compose_cli_help_string(\"Flowpic block duration (in seconds)\"),\n)\n###############\n# data\n###############\nparser.add_argument(\n\"--dataset\",\nchoices=(str(DATASETS.UCDAVISICDM19),),\ndefault=str(DATASETS.UCDAVISICDM19),\nhelp=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n)\nparser.add_argument(\n\"--split-index\",\ntype=int,\ndefault=0,\nhelp=utils.compose_cli_help_string(\"Datasplit index\"),\n)\n#    parser.add_argument(\n#        \"--max-samples-per-class\",\n#        type=int,\n#        default=-1,\n#        help=utils.compose_cli_help_string(\"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\")\n#    )\n###############\n# training\n###############\nparser.add_argument(\n\"--train-val-split-ratio\",\ntype=float,\ndefault=0.8,\nhelp=utils.compose_cli_help_string(\"Training train/val split\"),\n)\n#    parser.add_argument(\n#        \"--aug-name\",\n#        type=str,\n#        choices=(\n#            \"noaug\",\n#            \"rotate\",\n#            \"horizontalflip\",\n#            \"colorjitter\",\n#            \"packetloss\",\n#            \"timeshift\",\n#            \"changertt\",\n#        ),\n#        default=\"noaug\",\n#        help=utils.compose_cli_help_string(\"Augmentation policy\")\n#    )\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\nparser.add_argument(\n\"--seed\",\ntype=int,\ndefault=12345,\nhelp=utils.compose_cli_help_string(\"Random seed\"),\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Training batch size\"),\n)\nparser.add_argument(\n\"--suppress-test-train-val-leftover\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n)\nparser.add_argument(\n\"--flow-representation\",\nchoices=tuple(\nmap(str, MODELING_INPUT_REPR_TYPE.__members__.values())\n),  # ('flowpic', 'pktseries'),\ndefault=str(MODELING_INPUT_REPR_TYPE.FLOWPIC),  # \"flowpic\",\nhelp=utils.compose_cli_help_string(\n\"The string representing the flow representation\"\n),\n)\nparser.add_argument(\n\"--max-n-pkts\",\ntype=int,\ndefault=10,\nhelp=utils.compose_cli_help_string(\n\"The number of packets in case of xgboost on packet series\"\n),\n)\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/","title":"run_campaign_augmentations_at_loading","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/#tcbench.modeling.run_campaign_augmentations_at_loading.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_campaign_augmentations_at_loading.py</code> <pre><code>def main(args):\n\"\"\"Entry point\"\"\"\nargs.seeds = list(map(int, args.seeds.split(\",\")))\nargs.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\nargs.augmentations = list(args.augmentations.split(\",\"))\nif args.split_indexes is not None:\nargs.split_indexes = list(map(int, args.split_indexes.split(\",\")))\nfor dim in args.flowpic_dims:\nif dim not in DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS: \nraise RuntimeError(\nf\"Flowpic can only be of size {DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS}\"\n)\nfor aug_name in args.augmentations:\nif aug_name not in DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS:\nraise RuntimeError(\nf\"Invalid augmentation {arg_name}. Possible choices: {DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS}\"\n)\ndataset_name = args.dataset\nif args.split_indexes is None:\nsplit_indexes = datasets_utils.get_split_indexes(\ndataset_name, args.dataset_minpkts\n)\nelse:\nsplit_indexes = args.split_indexes\nif args.max_train_splits == -1:\nargs.max_train_splits = len(split_indexes)\nsplit_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\ncampaign_id = args.campaign_id\nif campaign_id is None:\ncampaign_id = datetime.now().strftime(\"%s\")\nextra_aim_hparams = dict(\ncampaign_id=campaign_id,\n)\nexperiments_grid = list(\nitertools.product(\nsplit_indexes, args.augmentations, args.flowpic_dims, args.seeds\n)\n)\ncum_run_completion_time = 0\navg_run_completion_time = 0\nfor exp_idx, (split_index, aug_name, flowpic_dim, seed) in enumerate(\nexperiments_grid\n):\ntime_run_start = time.time()\ntime_to_completion = timedelta(\nseconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n)\nprint()\nprint(\"#\" * 10)\nprint(\nf\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n)\nprint(\"#\" * 10)\nprint()\nif args.dry_run:\nprint(f\"split_indexes ({len(split_indexes)}): {split_indexes}\")\nprint(f\"augmentations ({len(args.augmentations)}): {args.augmentations}\")\nprint(f\"flowpic_dims  ({len(args.flowpic_dims)}): {args.flowpic_dims}\")\nprint(f\"seeds         ({len(args.seeds)}): {args.seeds}\")\nsys.exit(0)\nnew_params = dict(\nsplit_index=split_index,\naug_name=aug_name,\nflowpic_dim=flowpic_dim,\nseed=seed,\n)\nextra_aim_hparams[\"campaign_exp_idx\"] = exp_idx + 1\n# creating a \"dummy\" Namespace with all\n# default values which will be overwritten\n# based on the campain parameters\n# cmd = f'--config {args.config_fname}'.split()\ncmd = \"\"\nrun_args = run_augmentations_at_loading.cli_parser().parse_args(cmd)\nfor attr_name, _ in vars(run_args).items():\nif attr_name in new_params:\nsetattr(run_args, attr_name, new_params[attr_name])\n# directly passing parameters\nrun_args.final = False\nrun_args.method = \"monolithic\"\nrun_args.experiment_name = args.aim_experiment_name\nrun_args.gpu_index = args.gpu_index\nrun_args.aim_repo = args.aim_repo\nrun_args.artifacts_folder = args.artifacts_folder\nrun_args.patience_steps = args.patience_steps\nrun_args.suppress_test_train_val_leftover = (\nargs.suppress_test_train_val_leftover\n)\nrun_args.max_samples_per_class = args.max_samples_per_class\n# run_args.suppress_val_augmentation = args.suppress_val_augmentation\nrun_args.suppress_dropout = args.suppress_dropout\nrun_args.dataset = args.dataset\nrun_args.dataset_minpkts = args.dataset_minpkts\nrun_args.epochs = args.epochs\nrun_args.batch_size = args.batch_size\nrun_augmentations_at_loading.main(run_args, extra_aim_hparams)\ntime_run_end = time.time()\ncum_run_completion_time += time_run_end - time_run_start\navg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/#tcbench.modeling.run_campaign_augmentations_at_loading.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_campaign_augmentations_at_loading.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\nparser.add_argument(\n\"--aim-repo\",\ndefault=\"./debug\",\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=\"./debug/artifacts\",\nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--campaign-id\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"augmentations-at-loading\",\nhelp=utils.compose_cli_help_string(\n\"The experiment name to use for all Aim run in the campaign\"\n),\n)\nparser.add_argument(\n\"--workers\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\n\"Number of parallel worker for loading the data\"\n),\n)\nparser.add_argument(\n\"--gpu-index\",\ndefault=\"0\",\nhelp=utils.compose_cli_help_string(\"GPU where to operate\"),\n)\nparser.add_argument(\n\"--dry-run\",\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\n\"Show the number of experiments and then quit\"\n),\n)\n#############################\n# data options\n#############################\nparser.add_argument(\n\"--split-indexes\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\n\"Coma separted list of split indexes. Use -1 to disable predefined split\"\n),\n)\nparser.add_argument(\n\"--max-samples-per-class\",\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"Used in conjuction with --split-indexes -1 to dynamically generate a train/val split. The number of samples specified corresponds to train+val (which will be separated with 80/20 for train and val)\"\n),\n)\nparser.add_argument(\n\"--augmentations\",\ndefault=\",\".join(\nmap(str, DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS)\n),  # AUGMENTATIONS)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of augmentations for experiments\"\n),\n)\n#    parser.add_argument(\n#        \"--dataset\",\n#        choices=('ucdavis-icdm19', 'utmobilenet21', 'mirage19', 'mirage22'),\n#        default='ucdavis-icdm19',\n#        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n#    )\nparser.add_argument(\n\"--dataset\",\nchoices=tuple(map(str, DATASETS.__members__.values())),\ndefault=str(tcbench.DATASETS.UCDAVISICDM19),\nhelp=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n)\nparser.add_argument(\n\"--dataset-minpkts\",\nchoices=(-1, 10, 100, 1000),\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n),\n)\n#############################\n# train options\n#############################\nparser.add_argument(\n\"--max-train-splits\",\ntype=int,\ndefault=-1,\nhelp=utils.compose_cli_help_string(\n\"The maximum number of training splits to experiment with. If -1, use all available\"\n),\n)\nparser.add_argument(\n\"--seeds\",\ndefault=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_SEEDS)),  # SEEDS)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of seed for experiments\"\n),\n)\nparser.add_argument(\n\"--flowpic-dims\",\ndefault=\",\".join(\nmap(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)\n),  # FLOWPIC_DIMS)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of flowpic dimensions for experiments\"\n),\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Batch size\"),\n)\nparser.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=0.001,\nhelp=utils.compose_cli_help_string(\"Learning rate\"),\n)\nparser.add_argument(\"--patience-steps\", default=5, type=int)\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\nparser.add_argument(\n\"--suppress-test-train-val-leftover\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n)\nparser.add_argument(\n\"--suppress-dropout\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n)\nparser.add_argument(\n\"--epochs\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\"Number of epochs for training\"),\n)\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/","title":"run_campaign_augmentations_at_loading_xgboost","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/#tcbench.modeling.run_campaign_augmentations_at_loading_xgboost.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_campaign_augmentations_at_loading_xgboost.py</code> <pre><code>def main(args):\n\"\"\"Entry point\"\"\"\nargs.seeds = list(map(int, args.seeds.split(\",\")))\nargs.max_n_pkts = list(map(int, args.max_n_pkts.split(\",\")))\nargs.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\n# forcing specific parameters\nargs.dataset = DATASETS.UCDAVISICDM19\nargs.augmentations = AUGMENTATIONS\nargs.dataset_minpkts = -1\nif args.split_indexes is not None:\nargs.split_indexes = list(map(int, args.split_indexes.split(\",\")))\nfor dim in args.flowpic_dims:\nif (\ndim not in DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS\n):  # FLOWPIC_DIMS: #(32, 64, 1500):\nraise RuntimeError(\nf\"Invalid value {dim}: Flowpic can only be of size {DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS}\"\n)\nif args.split_indexes is None:\nsplit_indexes = datasets_utils.get_split_indexes(\nargs.dataset, args.dataset_minpkts\n)\nelse:\nsplit_indexes = args.split_indexes\nif args.max_train_splits == -1:\nargs.max_train_splits = len(split_indexes)\nsplit_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\ncampaign_id = args.campaign_id\nif campaign_id is None:\ncampaign_id = datetime.now().strftime(\"%s\")\nextra_aim_hparams = dict(\ncampaign_id=campaign_id,\n)\nl = [split_indexes, args.augmentations, args.seeds]\n# force a dummy value which will be ignored\n# in the downstream task\nif args.flow_representation == \"flowpic\":\nl.append(args.flowpic_dims)\nl.append([30])\nelse:\nl.append([32])\nl.append(args.max_n_pkts)\nexperiments_grid = list(itertools.product(*l))\ncum_run_completion_time = 0\navg_run_completion_time = 0\nfor exp_idx, (split_index, aug_name, seed, flowpic_dim, mnp) in enumerate(\nexperiments_grid\n):\ntime_run_start = time.time()\ntime_to_completion = timedelta(\nseconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n)\nprint()\nprint(\"#\" * 10)\nprint(\nf\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n)\nprint(\"#\" * 10)\nprint()\nif args.dry_run:\nprint(f\"split_indexes ({len(split_indexes)}): {split_indexes}\")\nprint(f\"augmentations ({len(args.augmentations)}): {args.augmentations}\")\nprint(f\"seeds         ({len(args.seeds)}): {args.seeds}\")\nif args.flow_representation == \"flowpic\":\nprint(f\"flowpic_dims  ({len(args.flowpic_dims)}): {args.flowpic_dims}\")\nelse:\nprint(f\"max_n_pkts    ({len(args.max_n_pkts)}): {args.max_n_pkts}\")\nsys.exit(0)\nnew_params = dict(\nsplit_index=split_index,\naug_name=aug_name,\nflowpic_dim=flowpic_dim,\nmax_n_pkts=mnp,\nseed=seed,\n)\nextra_aim_hparams[\"campaign_exp_idx\"] = exp_idx + 1\n# creating a \"dummy\" Namespace with all\n# default values which will be overwritten\n# based on the campain parameters\n# cmd = f'--config {args.config_fname}'.split()\ncmd = \"\"\nrun_args = run_augmentations_at_loading_xgboost.cli_parser().parse_args(cmd)\nfor attr_name, _ in vars(run_args).items():\nif attr_name in new_params:\nsetattr(run_args, attr_name, new_params[attr_name])\n# directly passing parameters\n# run_args.final = False\nrun_args.method = \"xgboost\"\nrun_args.experiment_name = args.aim_experiment_name\n# run_args.gpu_index = args.gpu_index\nrun_args.aim_repo = args.aim_repo\nrun_args.artifacts_folder = args.artifacts_folder\n# run_args.patience_steps = args.patience_steps\nrun_args.suppress_test_train_val_leftover = (\nargs.suppress_test_train_val_leftover\n)\nrun_args.max_samples_per_class = args.max_samples_per_class\n# run_args.suppress_val_augmentation = args.suppress_val_augmentation\n# run_args.suppress_dropout = args.suppress_dropout\nrun_args.dataset = args.dataset\nrun_args.flow_representation = args.flow_representation\nrun_args.batch_size = 32\nrun_augmentations_at_loading_xgboost.main(run_args, extra_aim_hparams)\ntime_run_end = time.time()\ncum_run_completion_time += time_run_end - time_run_start\navg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/#tcbench.modeling.run_campaign_augmentations_at_loading_xgboost.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_campaign_augmentations_at_loading_xgboost.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\nparser.add_argument(\n\"--aim-repo\",\ndefault=DEFAULT_AIM_REPO,\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER,\nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--campaign-id\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n)\nparser.add_argument(\n\"--split-indexes\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\n\"Comma separted list of split indexes. Use -1 to disable predefined split\"\n),\n)\nparser.add_argument(\n\"--max-train-splits\",\ntype=int,\ndefault=-1,\nhelp=utils.compose_cli_help_string(\n\"The maximum number of training splits to experiment with. If -1, use all available\"\n),\n)\nparser.add_argument(\n\"--max-samples-per-class\",\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"Used in conjuction with --split-indexes -1 to dynamically generate a train/val split. The number of samples specified corresponds to train+val (which will be separated with 80/20 for train and val)\"\n),\n)\nparser.add_argument(\n\"--seeds\",\ndefault=\",\".join(\nmap(\nstr,\nDEFAULT_CAMPAIGN_AUGATLOAD_SEEDS,\n)\n),  # SEEDS)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of seed for experiments\"\n),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"xgb_pkts\",\nhelp=utils.compose_cli_help_string(\n\"The experiment name to use for all Aim run in the campaign\"\n),\n)\nparser.add_argument(\n\"--flow-representation\",\nchoices=(\"flowpic\", \"pktseries\"),\ndefault=\"flowpic\",\nhelp=utils.compose_cli_help_string(\n\"The string representing the flow representation (flowpic or pktseries)\"\n),\n)\nparser.add_argument(\n\"--max-n-pkts\",\ndefault=\",\".join(\nmap(\nstr,\nDEFAULT_CAMPAIGN_AUGATLOAD_PKTSERIESLEN,\n)\n), \nhelp=utils.compose_cli_help_string(\n\"The number of packets in case of xgboost on packet series\"\n),\n)\nparser.add_argument(\n\"--suppress-test-train-val-leftover\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n)\nparser.add_argument(\n\"--flowpic-dims\",\ndefault=\",\".join(\nmap(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)\n),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of flowpic dimensions for experiments\"\n),\n)\n#    parser.add_argument(\n#        \"--dataset\",\n#        choices=(str(DATASETS.UCDAVISICDM19),),#('ucdavis-icdm19', #)'utmobilenet21'),\n#        default=str(DATASETS.UCDAVISICDM19), #'ucdavis-icdm19',\n#        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n#    )\nparser.add_argument(\n\"--dry-run\",\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\n\"Show the number of experiments and then quit\"\n),\n)\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/","title":"run_campaign_contrastive_learning_and_finetune","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/#tcbench.modeling.run_campaign_contrastive_learning_and_finetune.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_campaign_contrastive_learning_and_finetune.py</code> <pre><code>def main(args):\n\"\"\"Entry point\"\"\"\nargs.contrastive_learning_seeds = list(\nmap(int, args.contrastive_learning_seeds.split(\",\"))\n)\nargs.finetune_seeds = list(map(int, args.finetune_seeds.split(\",\")))\nargs.augmentations = args.augmentations.split(\",\")\nargs.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\nargs.projection_layer_dims = list(map(int, args.projection_layer_dims.split(\",\")))\nargs.dataset = DATASETS.UCDAVISICDM19\nargs.dropout = args.dropout.split(\",\")\nargs.suppress_dropout = list(item == \"disable\" for item in args.dropout)\nif args.split_indexes is not None:\nargs.split_indexes = list(map(int, args.split_indexes.split(\",\")))\nfor aug_name in args.augmentations:\nif aug_name not in DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_VALID_AUGMENTATIONS:\nraise RuntimeError(\nf\"Invalid augmentation {arg_name}. Possible choices: {DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_VALID_AUGMENTATIONS}\"\n)\nfor dim in args.flowpic_dims:\nif dim not in DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS:\nraise RuntimeError(\nf\"Flowpic can only be of size {DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS}\"\n)\nif args.split_indexes is None:\n# NOTE: min_pkts=-1 ...because as is it enforces ucdavis-icdm19\nsplit_indexes = datasets_utils.get_split_indexes(args.dataset, min_pkts=-1)\nelse:\nsplit_indexes = args.split_indexes\nif args.max_train_splits == -1:\nargs.max_train_splits = len(split_indexes)\nsplit_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\ncampaign_id = args.campaign_id\nif campaign_id is None:\ncampaign_id = datetime.now().strftime(\"%s\")\nextra_aim_hparams = dict(\ncampaign_id=campaign_id,\n)\nexperiments_grid = list(\nitertools.product(\nsplit_indexes,\nargs.contrastive_learning_seeds,\nargs.finetune_seeds,\nargs.flowpic_dims,\nargs.suppress_dropout,\nargs.projection_layer_dims,\n)\n)\nif len(experiments_grid) == 0:\nraise RuntimeError(f\"Something wrong: The experiments grid is empty\")\ncum_run_completion_time = 0\navg_run_completion_time = 0\nfor exp_idx, (\nsplit_index,\ncontrastive_learning_seed,\nfinetune_seed,\nflowpic_dim,\nsuppress_dropout,\nprojection_layer_dim,\n) in enumerate(experiments_grid):\ntime_run_start = time.time()\ntime_to_completion = timedelta(\nseconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n)\nprint()\nprint(\"#\" * 10)\nprint(\nf\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n)\nprint(\"#\" * 10)\nprint()\nif args.dry_run:\nprint(f\"split_indexes              ({len(split_indexes)}): {split_indexes}\")\nprint(\nf\"contrastive learning seeds ({len(args.contrastive_learning_seeds)}): {args.contrastive_learning_seeds}\"\n)\nprint(\nf\"finetune seeds             ({len(args.finetune_seeds)}): {args.finetune_seeds}\"\n)\nprint(\nf\"projection layer dims      ({len(args.projection_layer_dims)}): {args.projection_layer_dims}\"\n)\nprint(f\"dropout                    ({len(args.dropout)}): {args.dropout}\")\nprint(\nf\"flowpic dims               ({len(args.flowpic_dims)}): {args.flowpic_dims}\"\n)\nsys.exit(0)\nnew_params = dict(\nsplit_index=split_index,\ncontrastive_learning_seed=contrastive_learning_seed,\nfinetune_seed=finetune_seed,\n)\n# creating a \"dummy\" Namespace with all\n# default values which will be overwritten\n# based on the campain parameters\n# cmd = f'--config {args.config_fname}'.split()\ncmd = \"\"\nrun_args = run_contrastive_learning_and_finetune.cli_parser().parse_args(cmd)\nextra_aim_hparams[\"campaign_exp_idx\"] = exp_idx\nfor attr_name, _ in vars(run_args).items():\nif attr_name in new_params:\nsetattr(run_args, attr_name, new_params[attr_name])\nrun_args.final = False\nrun_args.method = \"simclr\"\n# run_args.config = args.config\nrun_args.aim_experiment_name = args.aim_experiment_name\nrun_args.aim_repo = args.aim_repo\nrun_args.artifacts_folder = args.artifacts_folder\nrun_args.gpu_index = args.gpu_index\nrun_args.suppress_dropout = suppress_dropout\nrun_args.flowpic_dim = flowpic_dim\nrun_args.projection_layer_dim = projection_layer_dim\n# run_args.finetune_augmentation = args.finetune_augmentation\nrun_args.augmentations = args.augmentations\nrun_args.batch_size = args.batch_size\nrun_contrastive_learning_and_finetune.main(run_args, extra_aim_hparams)\ntime_run_end = time.time()\ncum_run_completion_time += time_run_end - time_run_start\navg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/#tcbench.modeling.run_campaign_contrastive_learning_and_finetune.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_campaign_contrastive_learning_and_finetune.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\n###################\n## general options\n###################\nparser.add_argument(\n\"--campaign-id\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n)\nparser.add_argument(\n\"--aim-repo\",\ndefault=DEFAULT_AIM_REPO, \ntype=pathlib.Path,\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"contrastive-learning-and-finetune\",\nhelp=utils.compose_cli_help_string(\n\"The experiment name to use for all Aim run in the campaign\"\n),\n)\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=DEFAULT_ARTIFACTS_FOLDER, \nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--workers\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\n\"Number of parallel worker for loading the data\"\n),\n)\nparser.add_argument(\n\"--gpu-index\",\ndefault=\"0\",\nhelp=utils.compose_cli_help_string(\"GPU where to operate\"),\n)\nparser.add_argument(\n\"--dry-run\",\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\n\"Show the number of experiments and then quit\"\n),\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Training batch size\"),\n)\nparser.add_argument(\n\"--split-indexes\",\ndefault=None,\nhelp=utils.compose_cli_help_string(\n\"Coma separted list of split indexes. Use -1 to disable predefined split\"\n),\n)\n###################\n## data options\n###################\nparser.add_argument(\n\"--max-train-splits\",\ntype=int,\ndefault=-1,\nhelp=utils.compose_cli_help_string(\n\"The maximum number of training splits to experiment with. If -1, use all available\"\n),\n)\nparser.add_argument(\n\"--augmentations\",\ndefault=\"changertt,timeshift\",\nhelp=utils.compose_cli_help_string(\n\"A pair of augmentations to use for contrastive learning\"\n),\n)\n###################\n## flowpic options\n###################\nparser.add_argument(\n\"--flowpic-dims\",\ndefault=\",\".join(\nmap(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS)\n),  # FLOWPIC_DIMS)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of flowpic dimensions for experiments\"\n),\n)\n###################\n## training options\n###################\nparser.add_argument(\n\"--contrastive-learning-seeds\",\ndefault=\",\".join(\nmap(\nstr,\nDEFAULT_CAMPAING_CONTRALEARNANDFINETUNE_SEEDS_CONTRALEARN,\n)\n),  # SEEDS_CONTRASTIVELEARNING)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of seeds to use for contrastive learning pretraining\"\n),\n)\nparser.add_argument(\n\"--finetune-seeds\",\ndefault=\",\".join(\nmap(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_SEEDS_FINETUNE)\n),  ##SEEDS_FINETUNING)),\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of seeds to use for finetune training\"\n),\n)\nparser.add_argument(\n\"--dropout\",\ntype=str,\ndefault=\"disable\",\nhelp=utils.compose_cli_help_string(\n\"Coma separated list. Choices: (enable, disable)\"\n),\n)\nparser.add_argument(\n\"--projection-layer-dims\",\ndefault=\"30\",\nhelp=utils.compose_cli_help_string(\n\"Coma separated list of contrastive learning projection layer dimensions\"\n),\n)\n#    parser.add_argument(\n#        \"--finetune-augmentation\", default=\"none\",\n#        choices=(\"none\", \"only-views\", \"views-and-original\"),\n#        help=utils.compose_cli_help_string(\"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\")\n#    )\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/","title":"run_contrastive_learning_and_finetune","text":""},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.pretrain","title":"<code>pretrain(dataset_name=DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, learning_rate=0.001, flowpic_dim=32, flowpic_block_duration=15, split_idx=0, aug_samples=2, aug_config=None, logger=None, device='cuda:0', tracker=None, workers=50, artifacts_folder=None, seed=12345, epochs=50, patience_steps=3, loss_temperature=0.07, with_dropout=True, projection_layer_dim=30, max_samples_per_class=-1, state=None)</code>","text":"<p>Pretrain a model</p> Source code in <code>tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def pretrain(\ndataset_name: str = DATASETS.UCDAVISICDM19,\ndataset_minpkts: int = -1,\nbatch_size: int = 32,\nlearning_rate: float = 0.001,\nflowpic_dim: int = 32,\nflowpic_block_duration: int = 15,\nsplit_idx: int = 0,\naug_samples: int = 2,\naug_config: dict = None,\nlogger=None,\ndevice: str = \"cuda:0\",\ntracker: aim.Run = None,\nworkers: int = 50,\nartifacts_folder=None,\nseed: int = 12345,\nepochs: int = 50,\npatience_steps: int = 3,\nloss_temperature: float = 0.07,\nwith_dropout: bool = True,\nprojection_layer_dim: int = 30,\nmax_samples_per_class: int = -1,\nstate: dict = None,\n):\n\"\"\"Pretrain a model\"\"\"\nassert aug_samples &gt;= 2, \"aug_samples cannot be smaller than 2\"\nif state is None:\nstate = dict()\nutils.seed_everything(seed)\nif aug_config is None:\naug_config = dict(changertt={}, timeshift={})\ndset_train, dset_val = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\nsplit_idx=split_idx,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\naug_config=aug_config,\naug_samples=aug_samples,\naug_when_loading=False,\nn_workers=workers,\nlogger=logger,\ndataset_minpkts=dataset_minpkts,\nmax_samples_per_class=max_samples_per_class,\n)\ntrain_loader = torch.utils.data.DataLoader(\ndset_train,\nbatch_size=batch_size,\nshuffle=True,\n)\nval_loader = torch.utils.data.DataLoader(dset_val, batch_size=batch_size)\nnet = backbone.net_factory(\nnum_classes=None,\nflowpic_dim=32,\nwith_dropout=with_dropout,\nprojection_layer_dim=projection_layer_dim,\n)\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\ntrainer = methods.SimCLRTrainer(\npretrain_config=dict(\noptimizer=optimizer,\nloss_temperature=loss_temperature,\n),\ndeterministic=True,\ndevice=device,\ntracker=tracker,\nlogger=logger,\n)\nbest_net = trainer.pretrain_loop(\nnet=net,\ntrain_loader=train_loader,\nval_loader=val_loader,\npatience_monitor=methods.PatienceMonitorAccuracy(\n\"acc_top_5\", steps=patience_steps\n),\nepochs=epochs,\ncontext=\"contrastivelearning\",\n)\nif artifacts_folder:\nfname = artifacts_folder / f\"best_model_weights_pretrain_split_{split_idx}.pt\"\nutils.log_msg(f\"saving: {fname}\")\nbest_net.save_weights(fname)\nstate[\"best_net\"] = best_net\nstate[\"dset_train\"] = dset_train\nstate[\"dset_val\"] = dset_val\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.finetune_test","title":"<code>finetune_test(net, dset, dset_name, trainer, logger=None, batch_size=32, device='cuda:0', tracker=None, artifacts_folder=None)</code>","text":"<p>Test after finetune</p> Source code in <code>tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def finetune_test(\nnet: backbone.BaseNet,\ndset: dataprep.FlowpicDataset,\ndset_name: str,\ntrainer: methods.SimCLRTrainer,\nlogger: logging.Logger = None,\nbatch_size=32,\ndevice: str = \"cuda:0\",\ntracker: aim.Run = None,\nartifacts_folder=None,\n):\n\"\"\"Test after finetune\"\"\"\nutils.log_msg(f\"\\n--- finetune (test) on {dset_name} ---\", logger)\nutils.log_msg(dset.samples_count(), logger)\nloader = torch.utils.data.DataLoader(dset, batch_size, shuffle=False)\ncontext = f\"test-{dset_name}\"\nmetrics, reports = trainer.finetune_test_loop(\nloader, with_reports=True, context=context\n)\nutils.log_msg(\nf'Test dataset {dset_name} | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\nlogger,\n)\nutils.classification_reports(\nnet,\ndset,\nbatch_size,\ndevice=device,\ncontext=context,\nsave_to=artifacts_folder,\nlogger=logger,\n)\nreturn metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.finetune","title":"<code>finetune(dataset_name=tcbench.DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, learning_rate=0.01, tracker=None, logger=None, seed=12345, epochs=50, device='cuda:0', split_idx=None, artifacts_folder=None, fname_pretrain_weights=None, train_samples=10, patience_steps=5, patience_min_delta=0.001, with_dropout=True, projection_layer_dim=30, aug_config=None, aug_samples=2, aug_yield_also_original=False, state=None)</code>","text":"<p>Finetune a model</p> Source code in <code>tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def finetune(\n# config,\ndataset_name: str = tcbench.DATASETS.UCDAVISICDM19,\ndataset_minpkts: int = -1,\nbatch_size:int=32,\nflowpic_dim:int=32,\nflowpic_block_duration:int=15,\nlearning_rate:float=0.01,\ntracker:aim.Run=None,\nlogger:logging.Logger=None,\nseed:int=12345,\nepochs:int=50,\ndevice:str=\"cuda:0\",\nsplit_idx:int=None,\nartifacts_folder:pathlib.Path=None,\nfname_pretrain_weights:pathlib.Path=None,\ntrain_samples:int=10,\npatience_steps:int=5,\npatience_min_delta:float=0.001,\nwith_dropout: bool = True,\nprojection_layer_dim: int = 30,\naug_config: dict = None,\naug_samples: int = 2,\naug_yield_also_original: bool = False,\nstate: dict = None,\n) -&gt; Dict[str, Any]:\n\"\"\"Finetune a model\"\"\"\nutils.seed_everything(seed)\nif (artifacts_folder is None and fname_pretrain_weights is None) or (\nartifacts_folder is not None and fname_pretrain_weights is not None\n):\nraise RuntimeError(\"Provide either artifact_folder or fname_weights\")\nif artifacts_folder and split_idx is None:\nraise RuntimeError(\"When using artifact_folder, split_idx cannot be None\")\nfname_pretrain_weights = (\nartifacts_folder / f\"best_model_weights_pretrain_split_{split_idx}.pt\"\n)\ndset_dict = dataprep.load_dataset(\ndataset_name=dataset_name,\ndataset_type=MODELING_DATASET_TYPE.FINETUNING,\nflowpic_dim=flowpic_dim,\nflowpic_block_duration=flowpic_block_duration,\nlogger=logger,\ntrain_samples=train_samples,\nseed=seed,\naug_config=aug_config,\naug_samples=aug_samples,\naug_yield_also_original=aug_yield_also_original,\ndataset_minpkts=dataset_minpkts,\n)\ndataset_names = [name.split(\"_\")[0] for name in dset_dict if name.endswith(\"train\")]\nif state is None:\nstate = dict()\nfor dset_name in dataset_names:\ndset_train = dset_dict[f\"{dset_name}_train\"]\ndset_test = dset_dict[f\"{dset_name}_test\"]\nstate[f\"{dset_name}_dset_train\"] = dset_train\nstate[f\"{dset_name}_dset_test\"] = dset_test\nnum_classes = dset_train.num_classes\ntrain_loader = torch.utils.data.DataLoader(\ndset_train, batch_size=batch_size, shuffle=True\n)\nutils.log_msg(f\"\\n--- finetune (train) on {dset_name} ---\", logger)\nutils.log_msg(dset_train.samples_count(), logger)\n# the network here is just a dummy object\n# the actual network is created by the trainer\nnet = backbone.net_factory(\nnum_classes=None,\nflowpic_dim=flowpic_dim,\nwith_dropout=with_dropout,\nprojection_layer_dim=projection_layer_dim,\n)\n# the optimizer here is just a dummy\n# reference object. The final optimizer\n# is recreated when triggering the training\n# to adapt to the network and loaded weights\nfinetune_config = dict(\noptimizer=torch.optim.Adam(net.parameters(), learning_rate),\n)\ntrainer = methods.SimCLRTrainer(\nfinetune_config=finetune_config,\ndeterministic=True,\ndevice=device,\ntracker=tracker,\nlogger=logger,\n)\nbest_net = trainer.finetune_loop(\nnet=net,\ntrain_loader=train_loader,\nval_loader=None,\nepochs=epochs,\nnum_classes=dset_train.num_classes,\nfname_pretrain_weights=fname_pretrain_weights,\npatience_monitor=methods.PatienceMonitorLoss(\nsteps=patience_steps, min_delta=patience_min_delta\n),\ncontext=f\"finetune_{dset_name}\",\n)\nif artifacts_folder:\nfname = (\nartifacts_folder\n/ f\"best_model_weights_finetune_{dset_name}_from_split_{split_idx}.pt\"\n)\nutils.log_msg(f\"saving: {fname}\")\nbest_net.save_weights(fname)\nstate[f\"{dset_name}_best_net\"] = best_net\nmetrics, reports = finetune_test(\nnet=best_net,\ndset=dset_test,\ndset_name=dset_name,\ntrainer=trainer,\nlogger=logger,\nbatch_size=batch_size,\ndevice=device,\ntracker=tracker,\nartifacts_folder=artifacts_folder,\n)\nstate[f\"{dset_name}_class_rep\"] = reports[\"class_rep\"]\nstate[f\"{dset_name}_conf_mtx\"] = reports[\"conf_mtx\"]\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def main(args, extra_aim_hparams=None):\n\"\"\"Entry point\"\"\"\n# bounding to a specific gpu\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_index\nargs.method = \"simclr\"\nif extra_aim_hparams is None:\nextra_aim_hparams = {}\nif str(args.aim_repo).startswith(\"aim://\"):\nutils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\naim_repo_path = args.aim_repo\nelse:\naim_repo_path = pathlib.Path(args.aim_repo)\nargs.artifacts_folder = pathlib.Path(args.artifacts_folder)\naimutils.init_repository(aim_repo_path)\naim_run = aim.Run(\nrepo=aim_repo_path,\nexperiment=args.aim_experiment_name,\nlog_system_params=True,\ncapture_terminal_logs=True,\n)\naim_run_hash = utils.get_aim_run_hash(aim_run)\n# artifacts_folder = args.artifacts_folder / args.dataset / aim_run_hash\nartifacts_folder = args.artifacts_folder / aim_run_hash\nlogger = utils.get_logger(artifacts_folder / \"log.txt\")\nutils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\nutils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\nutils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\nif artifacts_folder.parent != aim_repo_path:\nutils.log_msg(\nf\"WARNING: the artifact folder is not a subfolder of the AIM repo\"\n)\nwith_dropout = not args.suppress_dropout\nrun_hparams = dict(\nflowpic_dim=args.flowpic_dim,\nsplit_index=args.split_index,\ndataset=args.dataset,\ndataset_minpkts=args.dataset_minpkts,\ncontrastive_learning_seed=args.contrastive_learning_seed,\nfinetune_seed=args.finetune_seed,\nfinetune_train_samples=args.finetune_train_samples,\nwith_dropout=with_dropout,\nprojection_layer_dim=args.projection_layer_dim,\nfinetune_augmentation=args.finetune_augmentation,\naugmentations=args.augmentations,\n**extra_aim_hparams,\n)\naim_run[\"hparams\"] = run_hparams\nutils.log_msg(\"--- run hparams ---\")\nfor param_name, param_value in run_hparams.items():\nutils.log_msg(f\"{param_name}: {param_value}\")\nutils.log_msg(\"-------------------\")\ngeneric_params = dict(\ndataset_name=args.dataset,\ndataset_minpkts=args.dataset_minpkts,\ntracker=aim_run,\nlogger=logger,\nartifacts_folder=artifacts_folder,\nwith_dropout=with_dropout,\nprojection_layer_dim=args.projection_layer_dim,\n)\nflowpic_params = dict(\nflowpic_dim=args.flowpic_dim,\nflowpic_block_duration=args.flowpic_block_duration,\n)\naug_config = dict(changertt={}, timeshift={})\nif args.augmentations:\naug_config = {aug_name: {} for aug_name in args.augmentations}\ncontrastive_learning_params = dict(\nsplit_idx=args.split_index,\nbatch_size=args.batch_size,\nlearning_rate=args.contrastive_learning_lr,\nseed=args.contrastive_learning_seed,\naug_config=aug_config,\naug_samples=2,\nepochs=args.contrastive_learning_epochs,\npatience_steps=args.contrastive_learning_patience_steps,\nloss_temperature=args.contrastive_learning_temperature,\nmax_samples_per_class=args.max_samples_per_class,\n)\nstate_pretrain = pretrain(\n**generic_params, **flowpic_params, **contrastive_learning_params\n)\nfinetune_params = dict(\nbatch_size=args.batch_size,\nlearning_rate=args.finetune_lr,\nseed=args.finetune_seed,\nepochs=args.finetune_epochs,\nsplit_idx=args.split_index,\ntrain_samples=args.finetune_train_samples,\npatience_steps=args.finetune_patience_steps,\npatience_min_delta=args.finetune_patience_min_delta,\n)\nif args.finetune_augmentation != \"none\":\nfinetune_params[\"aug_config\"] = contrastive_learning_params[\"aug_config\"]\nfinetune_params[\"aug_samples\"] = contrastive_learning_params[\"aug_samples\"]\nfinetune_params[\"aug_yield_also_original\"] = (\nargs.finetune_augmentation == \"views-and-original\"\n)\nstate_finetune = finetune(**generic_params, **flowpic_params, **finetune_params)\nstate = dict()\nfor key, value in state_pretrain.items():\nstate[f\"pretrain_{key}\"] = value\nfor key, valu in state_finetune.items():\nstate[f\"finetune_{key}\"] = value\naim_run.close()\nutils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\nreturn state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def cli_parser():\n\"\"\"Create an ArgumentParser\"\"\"\nparser = argparse.ArgumentParser()\n####################################\n# general configs\n####################################\n# parser.add_argument(\n#    \"--config\", \"-c\", type=pathlib.Path, required=True, default=\"./config.yml\",\n#    help=utils.compose_cli_help_string(\"General configuration file\"),\n# )\nparser.add_argument(\n\"--artifacts-folder\",\ntype=pathlib.Path,\ndefault=\"./debug/artifacts\",\nhelp=utils.compose_cli_help_string(\"Artifact folder\"),\n)\nparser.add_argument(\n\"--device\",\ndefault=\"cuda:0\",\nhelp=utils.compose_cli_help_string(\"Device where to run experiments\"),\n)\nparser.add_argument(\n\"--aim-repo\",\ndefault=\"./debug\",\nhelp=utils.compose_cli_help_string(\n\"Local aim folder or URL of AIM remote server\"\n),\n)\nparser.add_argument(\n\"--aim-experiment-name\",\ndefault=\"contrastive-learning-and-finetune\",\nhelp=utils.compose_cli_help_string(\n\"The experiment name to use for the Aim run\"\n),\n)\nparser.add_argument(\n\"--gpu-index\",\ndefault=\"0\",\nhelp=utils.compose_cli_help_string(\"The GPU id to use\"),\n)\nparser.add_argument(\"--final\", action=\"store_true\", default=False)\n####################################\n# data and dataset configs\n####################################\nparser.add_argument(\n\"--dataset\",\nchoices=tuple(map(str, tcbench.DATASETS.__members__.values())),\ndefault=str(tcbench.DATASETS.UCDAVISICDM19),\nhelp=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n)\nparser.add_argument(\n\"--dataset-minpkts\",\nchoices=(-1, 10, 100, 1000),\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n),\n)\nparser.add_argument(\n\"--workers\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\n\"Number of parallel worker for loading the data\"\n),\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=utils.compose_cli_help_string(\"Training batch size\"),\n)\nparser.add_argument(\n\"--split-index\",\ntype=int,\ndefault=0,\nhelp=utils.compose_cli_help_string(\"Datasplit index\"),\n)\nparser.add_argument(\n\"--augmentations\",\ndefault=\"changertt,timeshift\",\nhelp=utils.compose_cli_help_string(\n\"A pair of augmentations to use for contrastive learning\"\n),\n)\nparser.add_argument(\n\"--max-samples-per-class\",\ndefault=-1,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"Balance the dataset with the specified number of samples per class\"\n),\n)\n####################################\n# flowpic configs\n####################################\nparser.add_argument(\n\"--flowpic-dim\",\ntype=int,\ndefault=32,\nchoices=(32,),\nhelp=utils.compose_cli_help_string(\"Flowpic dimension\"),\n)\nparser.add_argument(\n\"--flowpic-block-duration\",\ntype=int,\ndefault=15,\nhelp=utils.compose_cli_help_string(\"Time window from which extract a flowpic\"),\n)\n####################################\n# model architecture\n####################################\nparser.add_argument(\n\"--suppress-dropout\",\ndefault=False,\naction=\"store_true\",\nhelp=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n)\nparser.add_argument(\n\"--finetune-augmentation\",\ndefault=\"none\",\nchoices=(\"none\", \"only-views\", \"views-and-original\"),\nhelp=utils.compose_cli_help_string(\n\"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\"\n),\n)\nparser.add_argument(\n\"--projection-layer-dim\",\ndefault=30,\ntype=int,\nhelp=utils.compose_cli_help_string(\n\"The number of units in the contrastive learning projection layer\"\n),\n)\n####################################\n# contrastive learning configs\n####################################\nparser.add_argument(\n\"--contrastive-learning-lr\",\ntype=float,\ndefault=0.001,\nhelp=utils.compose_cli_help_string(\"Learning rate for pretraining\"),\n)\nparser.add_argument(\n\"--contrastive-learning-seed\",\ntype=int,\ndefault=12345,\nhelp=utils.compose_cli_help_string(\"Seed for contrastive learning pretraining\"),\n)\nparser.add_argument(\n\"--contrastive-learning-patience-steps\",\ntype=int,\ndefault=3,\nhelp=utils.compose_cli_help_string(\n\"Max steps to wait before stopping training if the top5 validation accuracy does not improve\"\n),\n)\nparser.add_argument(\n\"--contrastive-learning-temperature\",\ntype=float,\ndefault=0.07,\nhelp=utils.compose_cli_help_string(\"Temperature for InfoNCE loss\"),\n)\nparser.add_argument(\n\"--contrastive-learning-epochs\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\n\"Epochs for contrastive learning pretraining\"\n),\n)\n####################################\n# finetune configs\n####################################\nparser.add_argument(\n\"--finetune-lr\",\ntype=float,\ndefault=0.01,\nhelp=utils.compose_cli_help_string(\"Learning for for finetune\"),\n)\nparser.add_argument(\n\"--finetune-patience-steps\",\ntype=int,\ndefault=5,\nhelp=utils.compose_cli_help_string(\n\"Max steps to wait before stopping training training loss does not improve\"\n),\n)\nparser.add_argument(\n\"--finetune-patience-min-delta\",\ntype=float,\ndefault=0.001,\nhelp=utils.compose_cli_help_string(\"Min improvement for training loss\"),\n)\nparser.add_argument(\n\"--finetune-train-samples\",\ntype=int,\ndefault=10,\nhelp=utils.compose_cli_help_string(\n\"Number of samples per-class for finetune training\"\n),\n)\nparser.add_argument(\n\"--finetune-epochs\",\ntype=int,\ndefault=50,\nhelp=utils.compose_cli_help_string(\"Epochs for finetune training\"),\n)\nparser.add_argument(\n\"--finetune-seed\",\ntype=int,\ndefault=12345,\nhelp=utils.compose_cli_help_string(\"Sed for finetune training\"),\n)\nreturn parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/","title":"utils","text":"<p>This modules contains a set of utility function to support a variety of tasks</p>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.compose_cli_help_string","title":"<code>compose_cli_help_string(text)</code>","text":"<p>Attach to the input text the default formatting string used by argparse to print help strings</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the input text to process</p> required Return <p>The input text modified by appending \"(default: %(default)s)\"</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def compose_cli_help_string(text: str) -&gt; str:\n\"\"\"Attach to the input text the default formatting string used by argparse to print help strings\n    Arguments:\n        text: the input text to process\n    Return:\n        The input text modified by appending \"(default: %(default)s)\"\n    \"\"\"\nreturn text + \" \" + ARGPARSE_HELP_DEFAULT\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.load_yaml","title":"<code>load_yaml(fname)</code>","text":"<p>Load an input YAML file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the YAML filename to load</p> required Return <p>The YAML object loaded</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def load_yaml(fname: pathlib.Path) -&gt; Dict[Any, Any]:\n\"\"\"Load an input YAML file\n    Arguments:\n        fname: the YAML filename to load\n    Return:\n        The YAML object loaded\n    \"\"\"\nwith open(fname) as fin:\nreturn yaml.safe_load(fin)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.load_config","title":"<code>load_config(fname)</code>","text":"<p>Load the configuration file of the framework</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the YAML config file to load</p> required Return <p>The loaded config file</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def load_config(fname: pathlib.Path) -&gt; Dict:\n\"\"\"Load the configuration file of the framework\n    Arguments:\n        fname: the YAML config file to load\n    Return:\n        The loaded config file\n    \"\"\"\nreturn load_yaml(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.get_logger","title":"<code>get_logger(fname)</code>","text":"<p>Create a logger attached to the console and also binds it to the filename passed as input. Anything printed via the logger will appear on the console and in the file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>pathlib.Path</code> <p>the file name to bind to the logger</p> required Return <p>A new logger object associated to both console and the specified filename</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def get_logger(fname: pathlib.Path) -&gt; logging.Logger:\n\"\"\"Create a logger attached to the console\n    and also binds it to the filename passed as input.\n    Anything printed via the logger will appear on the\n    console and in the file\n    Arguments:\n        fname: the file name to bind to the logger\n    Return:\n        A new logger object associated to both\n        console and the specified filename\n    \"\"\"\nfname = pathlib.Path(fname)\nlogger = logging.getLogger(\"imc22_flowpic\")\n# loggers are kept internally\n# and consistently returned based on name\n# thus, if handlers are set, it means\n# the logger was previously created.\n# In this case we close them\n# and create new ones\nif logger.handlers:\nlogger.handlers[0].close()\nlogger.handlers[1].close()\nlogger.removeHandler(logger.handlers[0])\nlogger.removeHandler(logger.handlers[0])\n# logger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nif not fname.parent.exists():\nfname.parent.mkdir(parents=True)\nfh = logging.FileHandler(fname)\nfh.setLevel(logging.DEBUG)\nprint(f\"opened log at {fname}\")\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nlogger.addHandler(fh)\nlogger.addHandler(ch)\nreturn logger\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.log_msg","title":"<code>log_msg(msg, logger=None)</code>","text":"<p>A generic function to print a message via a logger</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>the text to print</p> required <code>logger</code> <code>logging.Logger</code> <p>the logger handling the printing</p> <code>None</code> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def log_msg(msg: str, logger: logging.Logger = None) -&gt; None:\n\"\"\"A generic function to print a message via a logger\n    Arguments:\n        msg: the text to print\n        logger: the logger handling the printing\n    \"\"\"\nif logger is None:\nprint(msg, flush=True)\nelse:\nlogger.debug(msg)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.seed_everything","title":"<code>seed_everything(seed)</code>","text":"<p>Set the seed for pytorch, numpy and python</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the seed to use for the initialization</p> required Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def seed_everything(seed: int) -&gt; None:\n\"\"\"Set the seed for pytorch, numpy and python\n    Arguments:\n        seed: the seed to use for the initialization\n    \"\"\"\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.get_aim_run_hash","title":"<code>get_aim_run_hash(run)</code>","text":"<p>Return the hash number of an AIM run based on its name</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>aim.Run</code> <p>the AIM run from which extract the hash</p> required Return <p>The hash of the run</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def get_aim_run_hash(run: aim.Run) -&gt; str:\n\"\"\"Return the hash number of an AIM run based on its name\n    Arguments:\n        run: the AIM run from which extract the hash\n    Return:\n        The hash of the run\n    \"\"\"\nreturn run.name.split(\" \")[1]\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.dump_cli_args","title":"<code>dump_cli_args(cli_args, save_as, logger)</code>","text":"<p>Transform a argparse Namespace object into a dictionary which is saved as YAML file as well as printed by the logger</p> <p>Parameters:</p> Name Type Description Default <code>cli_args</code> <code>argparse.Namespace</code> <p>a Namespace objected obtained by calling .parse_args() on a argparse parser</p> required <code>save_as</code> <code>pathlib.Path</code> <p>a file where to save the arguments</p> required <code>logger</code> <code>logging.Logger</code> <p>a logger instance use for printing the parsed CLI arguments</p> required Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def dump_cli_args(\ncli_args: argparse.Namespace, save_as: pathlib.Path, logger: logging.Logger\n) -&gt; None:\n\"\"\"Transform a argparse Namespace object into a dictionary\n    which is saved as YAML file as well as printed by the logger\n    Arguments:\n        cli_args: a Namespace objected obtained by calling .parse_args() on a argparse parser\n        save_as: a file where to save the arguments\n        logger: a logger instance use for printing the parsed CLI arguments\n    \"\"\"\nparams = dict()\nfor key in dir(cli_args):\nif key[0] == \"_\":\ncontinue\nparams[key] = getattr(cli_args, key)\nif isinstance(params[key], (pathlib.Path, DATASETS, MODELING_INPUT_REPR_TYPE)):\nparams[key] = str(params[key])\nsave_as = pathlib.Path(save_as)\nif not save_as.parent.exists():\nsave_as.parent.mkdir(parents=True)\nlog_msg(f\"saving: {save_as}\", logger)\nwith open(save_as, \"w\") as fout:\nyaml.dump(params, fout)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.classification_reports","title":"<code>classification_reports(net, dset, batch_size, device='cuda:0', context='train', save_to=None, logger=None, method='monolithic', xgboost_model=None)</code>","text":"<p>Compute scikit learn classification report and confusion matrix</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>backbone.BaseNet</code> <p>the trained network to use</p> required <code>dset</code> <code>dataprep.FlowpicDataset</code> <p>the dataset to use</p> required <code>batch_size</code> <code>int</code> <p>the batch_size to use</p> required <code>device</code> <code>str</code> <p>the device where to operate inferece</p> <code>'cuda:0'</code> <code>context</code> <code>str</code> <p>a text string used for AIM tracking</p> <code>'train'</code> <code>save_to</code> <code>pathlib.Path</code> <p>a folder where to store the report</p> <code>None</code> <code>logger</code> <code>pathlib.Path</code> <p>the logger where to print the reports</p> <code>None</code> <code>method</code> <code>str</code> <p>e.g. 'monolithic' (NN) or 'xgboost'</p> <code>'monolithic'</code> <code>xgboost_model</code> <code>Any</code> <p>the xgboost model in case method=='xgboost'</p> <code>None</code> Return <p>A dictionary with two keys: \"class_rep\" and \"conf_mtx\". Each key is associated to a pandas DataFrame containing the classification report and confusion matrix computed based on inference</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def classification_reports(\nnet: backbone.BaseNet,\ndset: dataprep.FlowpicDataset,\nbatch_size: int,\ndevice: str = \"cuda:0\",\ncontext: str = \"train\",\nsave_to: pathlib.Path = None,\nlogger: pathlib.Path = None,\nmethod: str = \"monolithic\",\nxgboost_model: Any=None,\n) -&gt; Dict[str, pd.DataFrame]:\n\"\"\"Compute scikit learn classification report\n    and confusion matrix\n    Arguments:\n        net: the trained network to use\n        dset: the dataset to use\n        batch_size: the batch_size to use\n        device: the device where to operate inferece\n        context: a text string used for AIM tracking\n        save_to: a folder where to store the report\n        logger: the logger where to print the reports\n        method: e.g. 'monolithic' (NN) or 'xgboost'\n        xgboost_model: the xgboost model in case method=='xgboost'\n    Return:\n        A dictionary with two keys: \"class_rep\"\n        and \"conf_mtx\". Each key is associated\n        to a pandas DataFrame containing the\n        classification report and confusion matrix\n        computed based on inference\n    \"\"\"\nsave_to = pathlib.Path(save_to)\nif not save_to.exists():\nsave_to.mkdir(parents=True)\ndummy_trainer = methods.trainer_factory(\nmethod, net=net, device=device, logger=logger, xgboost_model=xgboost_model\n)\nloader = torch.utils.data.DataLoader(dset, batch_size, shuffle=False)\n_, reports = dummy_trainer.test_loop(loader, with_reports=True)\nlabels = dset.df[\"app\"].dtype.categories\nmapping = {str(idx): lab for idx, lab in enumerate(labels)}\nreports[\"class_rep\"] = reports[\"class_rep\"].rename(mapping, axis=0)\n###\nmapping = {idx: lab for idx, lab in enumerate(labels)}\nreports[\"conf_mtx\"] = reports[\"conf_mtx\"].rename(mapping).rename(mapping, axis=1)\nlog_msg(\"\", logger)\nlog_msg(f\"---{context} reports---\", logger)\nlog_msg(\"\", logger)\nlog_msg(reports[\"class_rep\"], logger)\nlog_msg(\"\", logger)\nlog_msg(reports[\"conf_mtx\"], logger)\nif save_to:\nlog_msg(\"\")\nfname = save_to / f\"./{context}_class_rep.csv\"\nlog_msg(f\"saving: {fname}\", logger)\nreports[\"class_rep\"].reset_index().to_csv(fname, index=None)\nfname = save_to / f\"./{context}_conf_mtx.csv\"\nlog_msg(f\"saving: {fname}\", logger)\nreports[\"conf_mtx\"].reset_index().to_csv(fname, index=None)\nreturn reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.compute_confidence_intervals","title":"<code>compute_confidence_intervals(array, alpha=0.05)</code>","text":"<p>Computes the confidence intervasl from an array of values.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>a list of values to process</p> required <code>alpha</code> <code>float</code> <p>the alpha of the confidence interval</p> <code>0.05</code> Return <p>The function is based on statsmodels.stats.api.DescrStatsW() which reports lower and upper values of the interval. However we return the difference between mean value of the input array and the upper value of the confidence interval</p> Source code in <code>tcbench/modeling/utils.py</code> <pre><code>def compute_confidence_intervals(array: NDArray, alpha: float = 0.05) -&gt; float:\n\"\"\"Computes the confidence intervasl from an array of values.\n    Arguments:\n        array: a list of values to process\n        alpha: the alpha of the confidence interval\n    Return:\n        The function is based on statsmodels.stats.api.DescrStatsW()\n        which reports lower and upper values of the interval.\n        However we return the difference between mean value of the\n        input array and the upper value of the confidence interval\n    \"\"\"\narray = np.array(array)\nlow, high = sms.DescrStatsW(array).tconfint_mean(alpha)\nmean = array.mean()\nci = high - mean\nreturn ci\n</code></pre>"}]}