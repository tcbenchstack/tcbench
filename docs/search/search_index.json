{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"The tcbench framework","text":"<p>tcbench is a ML/DL framework specific for Traffic Classification (TC) created as research project by the AI4NET team of the Huawei Technologies research center in Paris, France.</p> <p>What is Traffic Classification?</p> <p>Nodes within a computer network operate by exchanging  information, namely packets, which is regulated according to standardized protocols (e.g., HTTP for the web). So to understand  the network health it is required to constantly monitor this information flow and react accordingly. For instance, one might want to prioritize certain traffic (e.g., video meeting) or block it (e.g., social media in working environment).</p> <p>Traffic classification is the the act of labeling an exchange of packets  based on the Internet application which generated it.</p> <p>The academic literature is ripe with methods and proposals for TC. Yet, it is scarce of code artifacts and public datasets  do not offer common conventions of use.</p> <p>We designed tcbench with the following goals in mind:</p> Goal State of the art tcbench  Data curation There are a few public datasets for TC, yet no common format/schema, cleaning process, or standard train/val/test folds. An (opinionated) curation of datasets to create easy to use parquet files with associated train/val/test fold.  Code TC literature has no reference code base for ML/DL modeling tcbench is  open source with an easy to use CLI based on  click  Model tracking Most of ML framework requires integration with cloud environments and subscription services tcbench uses aimstack to save on local servers metrics during training which can be later explored via its web UI or aggregated in report summaries using tcbench"},{"location":"about/#features-and-roadmap","title":"Features and roadmap","text":"<p>tcbench is still under development, but (as suggested by its name) ultimately aims to be a reference framework for benchmarking multiple ML/DL solutions  related to TC.</p> <p>At the current stage, tcbench offers</p> <ul> <li> <p>Integration with 4 datasets, namely <code>ucdavis-icdm19</code>, <code>mirage19</code>, <code>mirage22</code> and <code>utmobilenet21</code>. You can use these datasets and their curated version independently from tcbench. Check out the dataset install process and dataset loading tutorial.</p> </li> <li> <p>Good support for flowpic input representation and minimal support for 1d time series (based on network packets properties) input representation.</p> </li> <li> <p>Data augmentation functionality for flowpic input representation.</p> </li> <li> <p>Modeling via XGBoost, vanilla DL supervision and contrastive learning (via SimCLR or SupCon).</p> </li> </ul> <p>Most of the above functionalities described relate to our  IMC23 paper.</p> <p>More exiting features including more datasets and algorithms will come in the next months. </p> <p>Stay tuned !</p>"},{"location":"artifacts/","title":"Artifacts","text":"<p>The submission is associated to three types of artifacts</p> <ul> <li> <p> Website: This website serves as a primary source of documentation. It collects</p> <ul> <li>Documentation about datasets .</li> <li>Documentation about our modeling framework called <code>tcbench</code>.</li> <li>Guides on how to run experiments  via <code>tcbench</code>.</li> </ul> </li> <li> <p> Code: This includes </p> <ul> <li>All source code related to <code>tcbench</code> .</li> <li>A collection of  Jupyter notebooks  used for the tables and figures of the submission.</li> </ul> </li> <li> <p> Data: This includes </p> <ul> <li>The datasets install, curation and split generation  used in our modeling</li> <li>All models and logs  generated through our modeling campaigns.</li> </ul> </li> </ul>"},{"location":"artifacts/#figshare-material","title":"Figshare material","text":"<p>A key objective of our submission is to made available all artifacts to the research community.  For instance, all code will be pushed to a  github repository, this website will be published on github pages or similar solutions, and data artifacts will be on a public cloud storage solution.</p> <p>Yet, due to double-blind policy, we temporarily uploaded our artifacts to a  figshare repository.</p> <p>More specifically, on figshare you find the following tarball.</p> <ul> <li> <p><code>website_documentation.tgz</code>: Well...if you are reading this page you already know the tarball contains this website .</p> </li> <li> <p><code>code_artifacts_paper132.tgz</code>: All code developed. See </p> <ul> <li>Quick tour for <code>tcbench</code>.</li> <li>Table and figures for jupyter notebooks.</li> </ul> </li> <li> <p><code>curated_datasets.tgz</code>: The preprocessed version of the datasets.  Please see the datasets pages in this website.</p> </li> <li> <p><code>ml_artifacts.tgz</code>: All output data generated via modeling campaigns. For fine grained view, those can be explored via AIM web UI  while results are generated via  Jupyter notebooks.</p> </li> </ul>"},{"location":"artifacts/#unpack-artifacts","title":"Unpack artifacts","text":"<p>In the figshare folder we also provide a <code>unpack_scripts.tgz</code>  tarball containing the following scripts</p> <pre><code>unpack_all.sh\n_unpack_code_artifacts_paper132.sh\n_unpack_curated_datasets.sh\n_unpack_ml_artifacts.sh\n</code></pre> <p>These are simple bash scripts to simplify the  extraction and installation of all material.</p> <p>Use the following process</p> <ol> <li> <p>First of all, prepare a python virtual environment, for example via  conda     <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> </li> <li> <p>Download all figshare tarballs in the same folder and run     <pre><code>tar -xzvf unpack_script.tgz\nbash ./unpack_all.sh\n</code></pre></p> </li> </ol>"},{"location":"install/","title":"Install and config","text":""},{"location":"install/#download-code-and-artifacts","title":"Download code and artifacts","text":"<p>If you see this documentation it means you downloaded the file from figshare so you already have the code in your hand :)</p> <p>Note</p> <p>It is our intent to push all the code into a proper repository</p>"},{"location":"install/#configure-a-python-environment","title":"Configure a python environment","text":"<p>We first create a <code>conda</code> environment to install all required dependencies</p> <pre><code>conda create -n replicating-imc22-flowpic python=3.10 pip\nconda activate replicating-imc22-flowpic\npython -m pip install -r ./requirements.txt\n</code></pre> <p>The code artifacts are also a python package that can be installed inside the environment. From inside <code>/replicate_imc22_flowpic</code> run</p> <pre><code>python -m pip install .\n</code></pre>"},{"location":"quick_tour/","title":"Quick tour","text":"<p>The code base is collected into a python package named <code>tcbench</code> which is designed to cover two functionalities</p> <ol> <li>Easy install and access to a curated set of traffic classification datasets</li> <li>Use the datasets to train/test ML and DL models</li> </ol>"},{"location":"quick_tour/#install-tcbench","title":"Install <code>tcbench</code>","text":"<p>If you unpacked the artifacts...</p> <p>When unpacking the artifacts, <code>tcbench</code> has been already installed</p> <p>First prepare a python virtual environment, for example via  conda <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> <p>Grab the latest <code>code_artifacts_paper132.tgz</code> from  figshare and unpack it. It contains a folder <code>/code_artifacts_paper132</code> from which you can trigger the installation.</p> <pre><code>cd code_artifacts_paper132\npython -m pip install .\n</code></pre> <p>All dependecies are automatically installed.</p>"},{"location":"quick_tour/#tcbench-cli","title":"<code>tcbench</code> cli","text":"<p>When installing the package you also install a <code>tcbench</code> CLI script which  acts as a universal entry point to interact with the framework via a nested commands structure.</p> <p>For instance <pre><code>tcbench --help\n</code></pre></p> <p>Output</p> <pre><code> Usage: tcbench [OPTIONS] COMMAND [ARGS]...\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version      Show tcbench version and exit.                                            \u2502\n\u2502 --help         Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 aimrepo         Investigate AIM repository content.                                      \u2502\n\u2502 campaign        Triggers a modeling campaign.                                            \u2502\n\u2502 datasets        Install/Remove traffic classification datasets.                          \u2502\n\u2502 run             Triggers a modeling run.                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>For instance command <code>datasets</code> offers the following sub-commands</p> <pre><code>tcbench datasets --help\n</code></pre> <p>Output</p> <pre><code>Usage: tcbench datasets [OPTIONS] COMMAND [ARGS]...\n\n Install/Remove traffic classification datasets\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 delete                Delete a dataset                                                   \u2502\n\u2502 import                Import datasets                                                    \u2502\n\u2502 info                  Show the meta-data related to supported datasets                   \u2502\n\u2502 install               Install a dataset                                                  \u2502\n\u2502 lsparquet             Tree view of the datasets parquet files                            \u2502\n\u2502 samples-count         Show report on number of samples per class                         \u2502\n\u2502 schema                Show datasets schemas                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Those sub-commands in turn offers different options. For instance for <code>install</code></p> <pre><code>tcbench datasets install --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench datasets install [OPTIONS]\n\n Install a dataset\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --name          -n  [ucdavis-icdm19|utmobilenet21|m  Dataset to install. [required]   \u2502\n\u2502                        irage19|mirage22]                                                 \u2502\n\u2502    --input-folder  -i  PATH                             Folder where to find             \u2502\n\u2502                                                         pre-downloaded tarballs.         \u2502\n\u2502    --help                                               Show this message and exit.      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":"<p>TCBench supports the following public traffic classification datasets</p>"},{"location":"datasets/#table-datasets-properties","title":"Table : Datasets properties","text":"Name Applications Links License Our curation <code>ucdavis-icdm19</code> 5 <code>mirage19</code> 20  NC-ND - <code>mirage22</code> 9  NC-ND - <code>utmobilenet21</code> 17  GPLv3 <p>At a glance, these datasets</p> <ul> <li> <p>Are collections of either CSV or JSON files.</p> </li> <li> <p>Are reporting individual packet level information or per-flow time series and metrics.</p> </li> <li> <p>May have been organized in subfolders, namely partitions, to reflect the related measurement campaign (see <code>ucdavis-icdm19</code>, <code>utmobilenet21</code>).</p> </li> <li> <p>May have file names carrying semantic.</p> </li> <li> <p>May require preprocessing to remove \"background\" noise, i.e., traffic unrelated to a target application (see <code>mirage19</code> and <code>mirage22</code>).</p> </li> <li> <p>Do not have reference train/validation/test splits.</p> </li> </ul> <p>In other words, these datasets need to be curated  to be used.</p> <p>Important</p> <p>The integration of these datasets in tcbench does not break the original licensing of the data nor it breaks their ownership. Rather, the integration aims at easing the access to these dataset. We thus encourage researchers and practitioners interesting in using these datasets to cite the original publications  (see links in the table above).</p>"},{"location":"datasets/#terminology","title":"Terminology","text":"<p>When describing datasets and related processing we use the following conventions:</p> <ul> <li> <p>A partition is a set of samples  pre-defined by the authors of the dataset. For instance, a partition can relate to a specific set of samples to use for training/test  (see <code>ucdavis-icdm19</code>).</p> </li> <li> <p>A split is a set of indexes of samples that need to be used for train/validation/test.</p> </li> <li> <p>An unfiltered dataset corresponds a monolithic parquet files containing the original raw data of a dataset (no filtering  is applied).</p> </li> <li> <p>A curated dataset is generated  processing the unfiltered parquet  to clean noise, remove small flows, etc., and each dataset have slightly different curation rules.</p> </li> </ul>"},{"location":"datasets/curation_and_metadata/","title":"Curation & Meta-data","text":""},{"location":"datasets/curation_and_metadata/#datasets-curation","title":"Datasets curation","text":"<p>The curation process operated by tcbench aims at the following objectives.</p> <p>Curation objectives</p> <ol> <li> <p>Transform all datasets into a common format, apply some clearning if/where needed and  create a reference set of data split for modeling.</p> </li> <li> <p>All data should reflect per-flow records where each flow is associated to packet time series input.</p> </li> </ol> <p>To do so, tcbench applies an opinionated preprocessing starting from the original raw version of the data (i.e., the data offered by the authors of each dataset).</p> <p>The raw data go through the following steps:</p> <ol> <li> <p>Install: tcbench can fetch the raw data from the Internet or  can take as input a folder where the raw data were already downloaded. This data is then \"installed\" by unpacking it into the python environment were tcbench is installed.</p> </li> <li> <p>Preprocess: Once unpacked, the raw data  is converted into monolithic packet files. Such files are left untouched, i.e., they simply serve as a re-organization of the original data (with a per-flow view where needed) with an homogeneous format across datasets.</p> </li> <li> <p>Filter and split: The monolithic parquet files are first filtered (e.g., removing very short flows or flow related to invalid IP addresses) and then used to train/validation/test splits.</p> </li> </ol> <p>We reiterate that all steps are a necessity to enable ML/DL modeling. Yet, they are opinionated  processes so tcbench embodies just on option to perform them.</p>"},{"location":"datasets/curation_and_metadata/#datasets-meta-data","title":"Datasets Meta-data","text":"<p>As part of the curation process,  tcbench can easily show the meta-data related to the datasets.</p> <p>For instance, you can see the  datasets summary table by running</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:           5\n\u2502        \ud83d\udd17 paper_url:         https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:           https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:              https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udd17 curated data:      https://figshare.com/ndownloader/files/42437043\n\u2502        \u2795 curated data MD5:  9828cce0c3a092ff19ed77f9e07f317c\n\u2502        \ud83d\udcc1 installed:         None\n\u2502        \ud83d\udcc1 preprocessed:      None\n\u2502        \ud83d\udcc1 data splits:       None\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:           17\n         \ud83d\udd17 paper_url:         https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:           https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:              https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udd17 curated data:      https://figshare.com/ndownloader/files/42436353\n         \u2795 curated data MD5:  e1fdcffa41a0f01d63eaf57c198485ce\n         \ud83d\udcc1 installed:         None\n         \ud83d\udcc1 preprocessed:      None\n         \ud83d\udcc1 data splits:       None\n</code></pre> <p>Notice the three properties <code>installed</code>, <code>preprocessed</code> and <code>data_splits</code>. These show the absolute path where the data is stored. As mentioned before, notice that the data is installed inside the python environment where tcbench is installed.</p> <p>The example above refers to an environment with no dataset installed yet.</p> <p>The following instead show a case where all datasets are installed.</p> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:           5\n\u2502        \ud83d\udd17 paper_url:         https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:           https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:              https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udd17 curated data:      https://figshare.com/ndownloader/files/42437043\n\u2502        \u2795 curated data MD5:  9828cce0c3a092ff19ed77f9e07f317c\n\u2502        \ud83d\udcc1 installed:         ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/raw\n\u2502        \ud83d\udcc1 preprocessed:      ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed\n\u2502        \ud83d\udcc1 data splits:       ./envs/tcbenchlib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed\n\u2502        \ud83d\udcc1 data splits:   ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed\n\u2502        \ud83d\udcc1 data splits:   ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:           17\n         \ud83d\udd17 paper_url:         https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:           https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:              https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udd17 curated data:      https://figshare.com/ndownloader/files/42436353\n         \u2795 curated data MD5:  e1fdcffa41a0f01d63eaf57c198485ce\n         \ud83d\udcc1 installed:         None\n         \ud83d\udcc1 preprocessed:      ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed\n         \ud83d\udcc1 data splits:       ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23\n</code></pre> <p>Notice that</p> <ul> <li> <p>installed may or may not be <code>None</code> depending on     if the dataset was installed from the raw data     or via tcbench artifacts (see XXX).</p> </li> <li> <p>preprocessed and data splits are always     reported unless the dataset was not installed.</p> </li> </ul>"},{"location":"datasets/curation_and_metadata/#dataset-files","title":"Dataset files","text":"<p>As mentioned, the datasets files are installed within the  python environment where tcbench is installed.</p> <p>Generally speaking, tcbench API are designed to ease the process of loading the data without the need to master the internal organization of a dataset. Check the XXX tutorial.</p> <p>In case of need, you can inspect the internal organization of a dataset using the  <code>datasets lsfiles</code> subcommand.</p> <p>For instance, for the <code>ucdavis-icdm19</code> datasets <pre><code>tcbench datasets lsfiles --name ucdavis-icdm19\n</code></pre></p> <p>Output</p> <pre><code>tcbench datasets lsfiles --name ucdavis-icdm19\nDatasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 ucdavis-icdm19.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 test_split_human.parquet\n            \u251c\u2500\u2500 test_split_script.parquet\n            \u251c\u2500\u2500 train_split_0.parquet\n            \u251c\u2500\u2500 train_split_1.parquet\n            \u251c\u2500\u2500 train_split_2.parquet\n            \u251c\u2500\u2500 train_split_3.parquet\n            \u2514\u2500\u2500 train_split_4.parquet\n</code></pre> <p>While the <code>info</code> subcommand show the absolute root path where datasets are installed, the <code>lsparquet</code> shows the relative paths and internal hierarchical structure of how the data is locally installed.</p> <p>In the sample above</p> <ul> <li> <p>the <code>raw/</code> folder where the original raw data of the dataset is never displayed.</p> </li> <li> <p>the <code>preprocessed/ucdavis-icdm19.parquet</code> corresponds to the monolithic parquet obtained reformatting (but not curating) the raw data.</p> </li> <li> <p>the files under the <code>preprocessed/imc23</code>  corresponds to the curation operated to the dataset based on our IMC23 paper.</p> </li> </ul>"},{"location":"datasets/datasets/","title":"Datasets","text":""},{"location":"datasets/datasets/#datasets-references","title":"Datasets references","text":"<p>We refer to the following datasets</p> Name Classes PDF Data Code Auto-download ucdavis-icdm19 5 pdf data code mirage19 20 pdf data - mirage22 9 pdf data - utmobilenet21 17 pdf data code <p>Warning</p> <p>Run the following commands from the root folder of the code repository (one of the scripts, namely mirage22_json_to_parquet.py has an harded dependency from mirage19_json_to_parquet.py).</p>"},{"location":"datasets/datasets/#why-and-how-to-preprocess-the-raw-data-from-each-dataset","title":"Why and how to preprocess the raw data from each dataset","text":"<p>Each dataset comes as either CSV or JSON files, with a mixed preference between per-packet and per-flow formating. Moreover, files can be organized in subfolders---namely partitions--- to reflect some aspect of the measuring campaign.</p> <p>We preprocess all dataset to create monolitich per-flow parquet files, associating to each flow numpy arrays for the packets time series used for modeling.</p> <p>The scripts for the conversion are collected in the <code>/datasets</code>  subfolder of the repository. The same folder is expected to gather the output parquet files and (later) the splits used for modeling.</p> <p>Note</p> <p>Our modeling framework provides some flexibility to bypass this limitation but, as of now, this is not fully supported yet.</p> <p>Note</p> <p>The code for generating the charts is in <code>/notebooks/datasets_properties.ipynb</code></p>"},{"location":"datasets/datasets/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<p>The dataset comprises 3 partitions with the following structure</p> <pre><code>&lt;root folder&gt;\n\u251c\u2500\u2500 pretraining\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u251c\u2500\u2500 Retraining(human-triggered)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u2514\u2500\u2500 Retraining(script-triggered)\n    \u251c\u2500\u2500 Google Doc\n    \u251c\u2500\u2500 Google Drive\n    \u251c\u2500\u2500 Google Music\n    \u251c\u2500\u2500 Google Search\n    \u2514\u2500\u2500 Youtube\n</code></pre> <p>Inside each nested folder there is a collection CSV files. Each file corresponds to a different flow,  where each row represent individual packet information.</p> <p>The aim of the pre-processing is to load all CSV into a single parquet file <code>ucdavis-icdm19.parquet</code>, and \"transpose\" the representation--- rather than having indivial packet for each row, we create one row per flow (the flow_id is the filename itself) encoding the packet time series into numpy arrays.</p> <p>The final dataset has the following columns</p> <ul> <li><code>row_id</code>: a unique row id</li> <li><code>app</code>: the label of the flow, encoded as pandas <code>category</code></li> <li><code>flow_id</code>: the original filename</li> <li><code>partition</code>: the partition related to the flow</li> <li><code>num_pkts</code>: number of packets in the flow</li> <li><code>duration</code>: the duration of the flow</li> <li><code>bytes</code>: the number of bytes of the flow</li> <li><code>unixtime</code>: numpy array with the absolute time of each packet</li> <li><code>timetofirst</code>: numpy array with the delta between a packet the first packet of the flow</li> <li><code>pkts_size</code>: numpy array for the packet size time series</li> <li><code>pkts_dir</code>: numpy array for the packet direction time series</li> <li><code>pkts_iat</code>: numpy array for the packet inter-arrival time series</li> </ul> <pre><code>python datasets/ucdavis-icdm19_csv-to-parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-csvs&gt; \\\n    --output-folder datasets/ucdavis-icdm19\n</code></pre> output <pre><code>found 6672 files to load\n\n................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\nloading complete\nsaving datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#mirage19","title":"mirage19","text":"<p>The dataset is a collection of JSON files for 20 applications</p> <p>Note</p> <p>Despite the website and the related paper mention that the dataset contains 40 application, the public version has only 20. With separate communication with the authors of the dataset, we understood that the remaining 20 are available only upon request (altough not explicitly specified). As result, we considered only the 20 publicly available.</p> <p>The files collection is organized as follows <pre><code>/mnt/storage/nfs/TLS/mirage-unina/mirage-19\n\u2514\u2500 MIRAGE-2019_traffic_dataset_downloadable\n\u00a0\u00a0 \u251c\u2500\u2500 Mi5_38_a4_ed_18_cc_bf\n\u00a0\u00a0 \u2514\u2500\u2500 Nexus7_bc_ee_7b_a4_09_47\n</code></pre></p> <p>Inside each nested folder there is a collection of JSON files with some semantical information embedded in the names themselves.</p> <p>Each JSON has fairly complicated nested structure which makes it very difficult to work with.</p> <p>The purpose of the preprocessing is to</p> <ol> <li>Combine all JSON into a monolithic parquet file <code>mirage19.parquet</code></li> <li>Flatten the nested structure. For instance, a dictionary     such as {\"a\":{\"b\":1, \"c\":2}} is transformed into two separate     columns \"a_b\" and \"a_c\" with the respective values</li> <li>Add a <code>\"background\"</code> class by processing the original     label compared the JSON filenames. More specifically,     each JSON file detail the name of the app used during     a measurement campaign. But the traffic in an experiment     can be related to a different app/service running in parallel.     The decoupling of the two is facilitated by the column     <code>flow_metadata_bf_label</code> which is collected using <code>netstat</code>     from the mobile phone: if <code>flow_metadata_bf_label</code> !=      the expected app name, we mark the flow as <code>background</code></li> <li>The dataset contains raw packet bytes across multiple packets     of a flow. We process these series to search for ASCII strings.     This can be usefull for extract (in a lazy way) TLS     handshake information (e.g., SNI or certificate info)</li> </ol> <p>The final parquet files has 127 columns, and most of them comes from the original dataset itself. They are not documented but fairly easy to understand based on the name.</p> <p>The most important one are</p> <ul> <li><code>packet_data_packet_dir</code>: the time series of the packet direction</li> <li><code>packet_data_l4_payload_bytes</code>: the time series of the packet size</li> <li><code>packet_data_iat</code>: the time series of the packet inter-arrival time</li> <li><code>flow_metadata_bf_label</code>: the label gathered via netstat</li> <li><code>strings</code>: the ASCII string recovered from the payload analysis</li> <li><code>android_name</code>: the app used for an experiment</li> <li><code>app</code>: the final label encoded as a pandas <code>category</code></li> </ul> <pre><code>python datasets/mirage19_json_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-json&gt;/MIRAGE-2019_traffic_dataset_downloadable \\\n    --output-f0lder datasets/mirage19 \\\n    --workers 30\n</code></pre> output <pre><code>found 1642 files\n..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done\nsaving: mirage19/mirage19.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#mirage22","title":"mirage22","text":"<p>The data has the same format as mirage-19, i.e., a collection of JSON files.</p> <p>The dataset contains the following structure</p> <pre><code>/mnt/storage/nfs/TLS/mirage-unina/mirage-22\n\u2514\u2500\u2500 MIRAGE-COVID-CCMA-2022\n\u00a0\u00a0 \u251c\u2500\u2500 Preprocessed_pickle\n\u00a0\u00a0 \u2514\u2500\u2500 Raw_JSON\n\u00a0\u00a0     \u251c\u2500\u2500 Discord\n\u00a0\u00a0     \u251c\u2500\u2500 GotoMeeting\n\u00a0\u00a0     \u251c\u2500\u2500 Meet\n\u00a0\u00a0     \u251c\u2500\u2500 Messenger\n\u00a0\u00a0     \u251c\u2500\u2500 Skype\n\u00a0\u00a0     \u251c\u2500\u2500 Slack\n\u00a0\u00a0     \u251c\u2500\u2500 Teams\n\u00a0\u00a0     \u251c\u2500\u2500 Webex\n\u00a0\u00a0     \u2514\u2500\u2500 Zoom\n</code></pre> <p>!!! warning:     We disegarded the pickle preprocessed version because (from what we reverse engineered)      encodes a series of object in the same pickle, but we found it cumbersome to work with.</p> <p>Please refer to mirage-19 for details about pre-processing</p> <pre><code>python datasets/mirage22_json_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-json&gt;/MIRAGE-COVID-CCMA-2022/Raw_JSON/ \\\n    --output-f0lder datasets/mirage22\n    --workers 30\n</code></pre> output <pre><code>found 998 files\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done\nsaving: datasets/mirage22/mirage22.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets/#utmobilenet21","title":"utmobilenet21","text":"<p>The dataset is a collection of per-packet CSV files divided into 4 partitions representing 4 different measurement campaigns.</p> <p>The files are organized as follows <pre><code>&lt;root&gt;\n\u2514\u2500\u2500 csvs\n \u00a0\u00a0 \u251c\u2500\u2500 Action-Specific Wild Test Data\n \u00a0\u00a0 \u251c\u2500\u2500 Deterministic Automated Data\n \u00a0\u00a0 \u251c\u2500\u2500 Randomized Automated Data\n \u00a0\u00a0 \u2514\u2500\u2500 Wild Test Data\n</code></pre></p> <p>Broadly speaking, the dataset has the same preprocessing needs of ucdavid-icmd19, i.e., being formatted per-packet, we pre-process it into per-flow and create numpy time series.</p> <p>The final <code>utmobilenet21.parquet</code> files contains the following columns</p> <ul> <li><code>row_id</code>: a unique flow id</li> <li><code>src_ip</code>: the source ip of the flow</li> <li><code>src_port</code>: the source port of the flow</li> <li><code>dst_ip</code>: the destination ip of the flow</li> <li><code>dst_port</code>: the destination port of the flow</li> <li><code>ip_proto</code>: the protocol of the flow (TCP or UDP)</li> <li><code>first</code>: timestamp of the first packet</li> <li><code>last</code>: timestamp of the last packet</li> <li><code>duration</code>: duration of the flow</li> <li><code>packets</code>: number of packets in the flow</li> <li><code>bytes</code>: number of bytes in the flow</li> <li><code>partition</code>: from which folder the flow was originally stored</li> <li><code>location</code>: a label originally provided by the dataset (see the related paper for details)</li> <li><code>fname</code>: the original filename where the packets of the flow come from </li> <li><code>app</code>: the final label of the flow, encoded as pandas <code>category</code></li> <li><code>pkts_size</code>: the numpy array for the packet size time series</li> <li><code>pkts_dir</code>: the numpy array for the packet diretion time series</li> <li><code>timetofirst</code>: the numpy array for the delta between the each packet timestamp the first packet of the flow</li> </ul> <pre><code>python datasets/utmobilenet21_csv_to_parquet.py \\\n    --input-folder &lt;where-you-unpacked-the-dataset&gt;/csvs \\\n    --output-folder ./datasets/utmobilenet21 \\\n    --tmp-staging-folder /tmp/processing-utmobilenet21 \\\n    --num-workers 10\n</code></pre> output <pre><code>processing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Wild Test Data\nfound 14 files\n..............\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/wild_test_data.parquet\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Deterministic Automated Data\nfound 3438 files\n..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/deterministic_automated_data.parquet\n\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Action-Specific Wild Test Data\nfound 43 files\n...........................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/action-specific_wild_test_data.parquet\n\nprocessing: /mnt/storage/nfs/TLS/utmobilenet-21/csvs/Randomized Automated Data\nfound 288 files\n................................................................................................................................................................................................................................................................................................\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/randomized_automated_data.parquet\nmerging all partitions\nsaving: datasets/utmobilenet21/utmobilenet21.parquet\n</code></pre> <p>Here some aggregate stats about traffic volumes per-class (click to magnify the charts)</p> <p></p>"},{"location":"datasets/datasets_splits/","title":"Datasets splits","text":"<p>The splits described here are specific for our submission and the aim to replicate the previous IMC22 paper.</p>"},{"location":"datasets/datasets_splits/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<p>Differently from the other datasets inhere described, <code>ucdavis-icdm19</code> does NOT require any filtering/adaptation after transforming the original CSV into a monolithic parquet.</p> <p>The testing partition are also predefined (\"human\" and \"script\").</p> <p>We need however to define splits of 100 samples per class for modeling. To do so we perform a random shuffle of  the data and generate 5 non overlapping groups of 100 samples.</p> <pre><code>python datasets/generate_splits.py --config config.yml\n</code></pre> output <pre><code>loading: datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\nsaving: datasets/ucdavis-icdm19/train_split_0.parquet\nsaving: datasets/ucdavis-icdm19/train_split_1.parquet\nsaving: datasets/ucdavis-icdm19/train_split_2.parquet\nsaving: datasets/ucdavis-icdm19/train_split_3.parquet\nsaving: datasets/ucdavis-icdm19/train_split_4.parquet\nloading: datasets/ucdavis-icdm19/ucdavis-icdm19.parquet\nsaving: datasets/ucdavis-icdm19/test_split_human.parquet\nsaving: datasets/ucdavis-icdm19/test_split_script.parquet\n</code></pre>"},{"location":"datasets/import/","title":"Import curated datasets","text":"<p>The <code>datasets</code> command offers also the option to import a pre-computed curation of datasets.</p> <p>This is </p> <ul> <li> <p>To avoid spending computation.  Some of the preprocessing requires ingenuity and multiprocessing/multicore architecture. </p> </li> <li> <p>Further strength replicability (although the curation process of tcbench is deterministic).</p> </li> </ul> <p>The datasets summary table indicates that the not all datasets have the curated data already available. This is because some datasets (namely MIRAGE) has tighter licensing. For these datasets please refer to the related installation page.</p>"},{"location":"datasets/import/#the-import-subcommand","title":"The <code>import</code> subcommand","text":"<p>For datasets which licensing allows to redistribute modified version, the curated data is stored in a public  figshare collection.</p> <p>You can manually fetch the datasets from the collection or use automate their installation with the <code>datasets import</code> subcommand.</p> <pre><code>tcbench datasets import --name ucdavis-icdm19\n</code></pre> <p>Info</p> <pre><code>Downloading... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 554.2 MB / 554.2 MB eta 0:00:00\nopening: /tmp/tmpb586lqhh/42438621\n\nFiles installed\nDatasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 ucdavis-icdm19.parquet\n        \u251c\u2500\u2500 LICENSE\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 test_split_human.parquet\n            \u251c\u2500\u2500 test_split_script.parquet\n            \u251c\u2500\u2500 train_split_0.parquet\n            \u251c\u2500\u2500 train_split_1.parquet\n            \u251c\u2500\u2500 train_split_2.parquet\n            \u251c\u2500\u2500 train_split_3.parquet\n            \u2514\u2500\u2500 train_split_4.parquet\n</code></pre> <p>Notice that <code>installed</code> is not set. Indeed the prepared curated datasets do NOT repack the original datasets, just the preprocessed ones  (see the meta-data page).</p> <p>You can also import the curated data by downloading the individual archives from figshare and use the <code>--archive</code> option</p> <pre><code>tcbench datasets import \\\n    --name ucdavis-icdm19 \\\n    --archive &lt;tarball&gt;\n</code></pre> <p> Figshare versioning</p> <p>Figshare updates the version of a published entry for any modification to any of the elements related to the entry (including changes to  description). </p> <p>tcbench is configured to automatically fetch the latest version of the curated datasets. But if you download them manually make sure to download the latest versions</p>"},{"location":"datasets/import/#the-delete-subcommand","title":"The <code>delete</code> subcommand","text":"<p>You can use the <code>delete</code> subcommand to remove installed/imported datasets.</p> <p>For instance, continuing the example above</p> <pre><code>tcbench datasets delete --name ucdavis-icdm19\n</code></pre> <p>...now <code>info</code> stats all data for <code>ucdavis-icdm19</code> has been removed</p> <pre><code>tcbench datasets info --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500  \ud83d\udea9 classes:           5\n         \ud83d\udd17 paper_url:         https://arxiv.org/pdf/1812.09761.pdf\n         \ud83d\udd17 website:           https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n         \ud83d\udd17 data:              https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n         \ud83d\udd17 curated data:      https://figshare.com/ndownloader/files/42437043\n         \u2795 curated data MD5:  9828cce0c3a092ff19ed77f9e07f317c\n         \ud83d\udcc1 installed:         None\n         \ud83d\udcc1 preprocessed:      None\n         \ud83d\udcc1 data splits:       None\n</code></pre>"},{"location":"datasets/install/","title":"Install","text":""},{"location":"datasets/install/#datasets-meta-data","title":"Datasets meta-data","text":"<p>As part of the curation process,  <code>tcbench</code> enables you to show meta-data related to the datasets.</p> <p>For instance, the refences collected in the summary table reported at the top of this page can be visualized issuing</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  None\n         \ud83d\udcc1 data splits:   None\n</code></pre> <p>Beside showing the a set of static properties (e.g., URL links),  the 3 properties <code>installed</code>, <code>preprocessed</code> nd <code>data_splits</code>  reports the absolute path where the related data is stored. The example refers to the initial setup where no dataset is yet installed.</p> <p>However, when unpacking artifacts with the  provided scripts,  the curated datasets are automatically installed</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed\n\u2502        \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed\n         \ud83d\udcc1 data splits:   /home/johndoe/.conda/envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23\n</code></pre> <p>Notice that * installed is <code>None</code> because this refers to the     original raw data of the datasets (which is not     provided with the curated artifacts).</p> <ul> <li>preprocessed and data splits are two specific types     of curation of the datasets.</li> </ul> <p>In fact, the artifacts unpacking uses a dataset import process.</p> <p>When installing a dataset, <code>tcbench</code> also shows two types of reports as formatted tables.</p> <ul> <li> <p>Samples count: This tables collect the number of samples (i.e., flows) available.</p> </li> <li> <p>Stats: The curation process can filter out flows (e.g., based on a minum number of packets or remove classes without a minimum number of flows). As such, when  installing, <code>tcbench</code> is showing general stats (mean, std, percentiles) about number of packets for each flow across classes.</p> </li> </ul> <p>Please check out the datasets meta-data page for more details.</p>"},{"location":"datasets/metadata/","title":"Metadata","text":"<p>The <code>tcbench</code> CLI can show 4 types of meta-data:</p> <ul> <li> <p> Static information: A collection of  URL links with datasets documentation and folders path related the installation.</p> </li> <li> <p> List of curated files: An organized view of the files generated during installation.</p> </li> <li> <p> Schemas: A formatted view of the schemas of installed files.</p> </li> <li> <p> Samples count report: A per-app breakdown of the number of samples.</p> </li> </ul>"},{"location":"datasets/metadata/#static-information","title":"Static information","text":"<p>The static information corresponds to the information displayed in the  datasets properties shown in the installation page.</p> <p>To show it in the console run</p> <pre><code>tcbench datasets info\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n\u2502        \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n\u2502        \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n\u2502        \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       20\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/MIRAGE_ICCCS_2019.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-2019.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500  \ud83d\udea9 classes:       9\n\u2502        \ud83d\udd17 paper_url:     http://wpage.unina.it/antonio.montieri/pubs/_C__IEEE_CAMAD_2021___Traffic_Classification_Covid_app.pdf\n\u2502        \ud83d\udd17 website:       https://traffic.comics.unina.it/mirage/mirage-covid-ccma-2022.html\n\u2502        \ud83d\udd17 data:          https://traffic.comics.unina.it/mirage/MIRAGE/MIRAGE-COVID-CCMA-2022.zip\n\u2502        \ud83d\udcc1 installed:     None\n\u2502        \ud83d\udcc1 preprocessed:  None\n\u2502        \ud83d\udcc1 data splits:   None\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       17\n         \ud83d\udd17 paper_url:     https://ieeexplore.ieee.org/abstract/document/9490678/\n         \ud83d\udd17 website:       https://github.com/YuqiangHeng/UTMobileNetTraffic2021\n         \ud83d\udd17 data:          https://utexas.app.box.com/s/okrimcsz1mn9ec4j667kbb00d9gt16ii\n         \ud83d\udcc1 installed:     None\n         \ud83d\udcc1 preprocessed:  None\n         \ud83d\udcc1 data splits:   None\n</code></pre> <p>The example above corresponds to the case when no dataset is installed.</p> <p>After a dataset is installed, the remaining properties are filled. Specifically, as suggested by the icon, the last 3 properties of each dataset  correspond to folders generated via the curation:</p> <ul> <li> <p><code>\"installed\"</code> is the path where the raw data of the dataset is unpacked.</p> </li> <li> <p><code>\"preprocessed\"</code> is the path where the preprocessed data is stored, i.e., the monolithic per-flow parquet files (with no filtering applied).</p> </li> <li> <p><code>\"data splits\"</code> is the folder where filtered data and data splits are stored, i.e., the data used for modeling.</p> </li> </ul> <p>The <code>datasets info</code> sub-command supports the option <code>--name</code> to filter out information for just one dataset.</p> <p>For instance, after installing <code>ucdavis-icdm19</code>, its information are:</p> <pre><code>tcbench datasets info --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500  \ud83d\udea9 classes:       5\n         \ud83d\udd17 paper_url:     https://arxiv.org/pdf/1812.09761.pdf\n         \ud83d\udd17 website:       https://github.com/shrezaei/Semi-supervised-Learning-QUIC-\n         \ud83d\udd17 data:          https://drive.google.com/drive/folders/1Pvev0hJ82usPh6dWDlz7Lv8L6h3JpWhE\n         \ud83d\udcc1 installed:     /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/raw\n         \ud83d\udcc1 preprocessed:  /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed\n         \ud83d\udcc1 data splits:   /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23\n</code></pre>"},{"location":"datasets/metadata/#list-of-curated-files","title":"List of curated files","text":"<p>As reported by <code>datasets info</code>,  both datasets raw data and curated parquet files  are stored into a subfolder of the python environment.</p> <p>Specifically, the folder is structure is as follows: <pre><code>/datasets\n  \u2514\u2500\u2500 &lt;dataset-name&gt; \n        \u2514\u2500\u2500 /raw\n        \u2514\u2500\u2500 /preprocessed\n             \u2514\u2500\u2500 /imc23\n</code></pre></p> <p>where</p> <ul> <li><code>/raw</code> contains the raw data of the datasets.</li> <li><code>/preprocessed</code> contains the monolithic parquet files.</li> <li><code>/imc23</code> contains the filtererd monolithic parquet files and the splits generated for the submission.</li> </ul> <p>One can better inspect this structure via the <code>datasets lsparquet</code> sub-command</p> <pre><code>tcbench datasets lsparquet\n</code></pre> <p>Output</p> <pre><code>Datasets\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 ucdavis-icdm19.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 test_split_human.parquet\n\u2502           \u251c\u2500\u2500 test_split_script.parquet\n\u2502           \u251c\u2500\u2500 train_split_0.parquet\n\u2502           \u251c\u2500\u2500 train_split_1.parquet\n\u2502           \u251c\u2500\u2500 train_split_2.parquet\n\u2502           \u251c\u2500\u2500 train_split_3.parquet\n\u2502           \u2514\u2500\u2500 train_split_4.parquet\n\u251c\u2500\u2500 mirage19\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 mirage19.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 mirage19_filtered_minpkts10.parquet\n\u2502           \u2514\u2500\u2500 mirage19_filtered_minpkts10_splits.parquet\n\u251c\u2500\u2500 mirage22\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n\u2502       \u251c\u2500\u2500 mirage22.parquet\n\u2502       \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts10.parquet\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts1000.parquet\n\u2502           \u251c\u2500\u2500 mirage22_filtered_minpkts1000_splits.parquet\n\u2502           \u2514\u2500\u2500 mirage22_filtered_minpkts10_splits.parquet\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 utmobilenet21.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 utmobilenet21_filtered_minpkts10.parquet\n            \u2514\u2500\u2500 utmobilenet21_filtered_minpkts10_splits.parquet\n</code></pre> <p>While all datasets have a file <code>&lt;dataset-name&gt;.parquet</code>  which corresponds to the monolithic version of the raw data, the content of the <code>/imc23</code> folder is slightly different between datasets</p> <ul> <li> <p>For <code>ucdavis-icdm19</code> split are \"materialized\". This means that the files <code>train_split_[0-4].parquet</code> contains the data to use for training (the actual train/val split is operated at runtime) while <code>test_split_human.parquet</code> and <code>text_split_script.parquet</code> are predefined test split already available in the raw dataset.</p> </li> <li> <p>For all other datasets, the files <code>xyz_minpkts&lt;N&gt;.parquet</code> contains a filtered version of the monolithic data (see install page for more details on the filtering) and the related <code>xyz_minpkts&lt;N&gt;_split.parquet</code> contains the index of the rows to use for train/val/test splits.</p> </li> </ul> <p>The tutorial about load and explore data offers more details about how to handle those differences.</p>"},{"location":"datasets/metadata/#schemas","title":"Schemas","text":"<p>Via the <code>datasets schema</code> sub-command is possible to visualize the schema of the curated parquet files.</p> <pre><code>tcbench datasets schema --help\n\n Usage: tcbench datasets schema [OPTIONS]\n\n Show datasets schemas\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --name  -n  [ucdavis-icdm19|utmobilenet21|mirage19|mirage22]  Dataset to install                                         \u2502\n\u2502 --type  -t  [unfiltered|filtered|splits]                      Schema type (unfiltered: original raw data; filtered:      \u2502\n\u2502                                                               curated data; splits: train/val/test splits)               \u2502\n\u2502 --help                                                        Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Beside the dataset name <code>--name</code>, the selection of the schema is simplified via a single parameter <code>--type</code> which matches the parquet files as follows</p> <ul> <li> <p><code>\"unfiltered\"</code> corresponds to the monolithic  before any filtering (i.e., the files under <code>/preprocessed</code>)</p> </li> <li> <p><code>\"filtered\"</code> corresponds to the filtered  version of the monolithic files (i.e., the files having <code>minpkts&lt;N&gt;</code> in the filename).</p> </li> <li> <p><code>\"splits\"</code> corresponds to the split files (i.e., the files having <code>xyz_split.parquet</code> in the filename).</p> </li> </ul> <p>While for <code>ucdavis-icdm19</code> the three schema types are the same, for the other datasets there are differences.</p> <p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p>"},{"location":"datasets/metadata/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<pre><code>tcbench datasets schema --name ucdavis-icdm19 --type unfiltered\n</code></pre> <p>Output</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type filtered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type splits <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#mirage19","title":"<code>mirage19</code>","text":"tcbench datasets schema --name mirage19 <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 float    \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 float    \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 float    \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 float    \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 float    \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 float    \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 float    \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 float    \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 float    \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protocol                                                \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 float    \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#mirage22","title":"<code>mirage22</code>","text":"tcbench datasets schema --name mirage22 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_timestamp                                     \u2502 np.array \u2502 Time series of packet unixtime                             \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_ip_packet_bytes                               \u2502 np.array \u2502 Time series pkts bytes (as from IP len field)              \u2502\n\u2502 packet_data_ip_header_bytes                               \u2502 np.array \u2502 Time series of IP header bytes                             \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_l4_header_bytes                               \u2502 np.array \u2502 Time series of L4 header bytes                             \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_tcp_flags                                     \u2502 np.array \u2502 Time series of TCP flags                                   \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 packet_data_is_clear                                      \u2502 np.array \u2502 n.a.                                                       \u2502\n\u2502 packet_data_heuristic                                     \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 packet_data_annotations                                   \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_device                                   \u2502 str      \u2502 Ethernet address                                           \u2502\n\u2502 flow_metadata_bf_label_source                             \u2502 str      \u2502 Constant value 'netstate'                                  \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_sublabel                                 \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_code                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_name                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 int      \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 int      \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 int      \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 int      \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 int      \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 int      \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_uf_mss                                      \u2502 float    \u2502 Upload maximum segment size                                \u2502\n\u2502 flow_metadata_uf_ws                                       \u2502 float    \u2502 Upload window scaling                                      \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 int      \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 int      \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 int      \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 flow_metadata_df_mss                                      \u2502 float    \u2502 Download maximum segment size                              \u2502\n\u2502 flow_metadata_df_ws                                       \u2502 float    \u2502 Download window scaling                                    \u2502\n\u2502 flow_metadata_bf_activity                                 \u2502 str      \u2502 Experiment activity                                        \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protol                                                  \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_activity         \u2502 str      \u2502 Experiment activity                                                  \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 int      \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#utmobilenet21","title":"<code>utmobilenet21</code>","text":"tcbench datasets schema --name utmobilenet21 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#samples-count-report","title":"Samples count report","text":"<p>As the name suggests, a samples count report details how many samples (i.e., flows) are available for each app. These reports are shown during installation, but can  be retrieved at any time using the subcommand <code>datasets samples-count</code>.</p> <p>They can be generated for unfiltered, filtered or based on splits, but the command requires familiarity with the parametrization semantic.</p>"},{"location":"datasets/metadata/#ucdavis-icdm19_1","title":"ucdavis-icdm19","text":"<p>For instance, the following provides the unfitered view for the <code>ucdavis-icdm19</code> dataset</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>While to obtain the breakdown of the first train split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split 0\n</code></pre> <p>Output</p> <pre><code>filtered, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>...or the <code>human</code> test split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split human\n</code></pre> <p>Output</p> <pre><code>filtered, split: human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/metadata/#examples-for-other-datasets","title":"Examples for other datasets","text":"<p>Other datasets can be filtered based on the <code>--min_pkts</code> options.</p> <p>For instance, the following is the overall view for <code>mirage22</code></p> <pre><code>tcbench datasets samples-count --name mirage22\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 background                       \u2502   18882 \u2502\n\u2502 com.microsoft.teams              \u2502    6541 \u2502\n\u2502 com.skype.raider                 \u2502    6203 \u2502\n\u2502 us.zoom.videomeetings            \u2502    5066 \u2502\n\u2502 com.cisco.webex.meetings         \u2502    4789 \u2502\n\u2502 com.discord                      \u2502    4337 \u2502\n\u2502 com.facebook.orca                \u2502    4321 \u2502\n\u2502 com.gotomeeting                  \u2502    3695 \u2502\n\u2502 com.Slack                        \u2502    2985 \u2502\n\u2502 com.google.android.apps.meetings \u2502    2252 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   59071 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This counts reduce when filtering by <code>--min-pkts 1000</code></p> <pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 1000\n</code></pre> <p>Output</p> <pre><code>min_pkts: 1000\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502    2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502     425 \u2502\n\u2502 com.google.android.apps.meetings \u2502     379 \u2502\n\u2502 com.microsoft.teams              \u2502     321 \u2502\n\u2502 com.gotomeeting                  \u2502     297 \u2502\n\u2502 com.facebook.orca                \u2502     280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502     259 \u2502\n\u2502 com.Slack                        \u2502     198 \u2502\n\u2502 com.skype.raider                 \u2502     190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502    4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>...and you can also obtain the breakdown from a specific split <pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 1000 --split 0\n</code></pre></p> <p>Output</p> <pre><code>min_pkts: 1000, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502          1798 \u2502         200 \u2502          222 \u2502        2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502           344 \u2502          39 \u2502           42 \u2502         425 \u2502\n\u2502 com.google.android.apps.meetings \u2502           307 \u2502          34 \u2502           38 \u2502         379 \u2502\n\u2502 com.microsoft.teams              \u2502           260 \u2502          29 \u2502           32 \u2502         321 \u2502\n\u2502 com.gotomeeting                  \u2502           240 \u2502          27 \u2502           30 \u2502         297 \u2502\n\u2502 com.facebook.orca                \u2502           227 \u2502          25 \u2502           28 \u2502         280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502           210 \u2502          23 \u2502           26 \u2502         259 \u2502\n\u2502 com.Slack                        \u2502           160 \u2502          18 \u2502           20 \u2502         198 \u2502\n\u2502 com.skype.raider                 \u2502           154 \u2502          17 \u2502           19 \u2502         190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502          3700 \u2502         412 \u2502          457 \u2502        4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/guides/","title":"Guides","text":"<p> Datasets loading: A jupyter notebook showing the APIs used loading the parquet files composing a dataset.</p>"},{"location":"datasets/guides/tutorial_load_datasets/","title":"Tutorial load datasets","text":""},{"location":"datasets/guides/tutorial_load_datasets/#tutorial-loading-datasets-apis","title":"Tutorial: loading datasets APIs","text":"<p>Let's import <code>tcbench</code> and map its alias <code>tcb</code></p> <p>The module automatically import a few functions and constants.</p> <pre><code>import tcbench as tcb\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#the-get_datasets_root_folder-method","title":"The <code>.get_datasets_root_folder()</code> method","text":"<p>You can first discover the  path where the datasets are installed using <code>.get_datasets_root_folder()</code> <pre><code>root_folder = tcb.get_datasets_root_folder()\nroot_folder\n</code></pre> <pre><code>PosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets')\n</code></pre> <p>The function returns a <code>pathlib</code> path so you can take advantage of it to navigate the subfolders structure.</p> <p>For instance:</p> <pre><code>list(root_folder.iterdir())\n</code></pre> <pre><code>[PosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21.BACKUP'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19')]\n</code></pre> <p>As from the output, each dataset is mapped to a different folder  named after the dataset itself. Meaning, again taking advantage of <code>pathlib</code>,  you can compose path based on strings.</p> <p>For instance:</p> <pre><code>list((root_folder / 'ucdavis-icdm19').iterdir())\n</code></pre> <pre><code>[PosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/raw')]\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#the-datasets-enum","title":"The <code>.DATASETS</code> enum","text":"<p>A more polished way to reference datasets is via the <code>tcbench.DATASETS</code> attribute which corresponds to a python enumeration object</p> <pre><code>type(tcb.DATASETS), list(tcb.DATASETS)\n</code></pre> <pre><code>(enum.EnumMeta,\n[&lt; DATASETS.UCDAVISICDM19: 'ucdavis-icdm19' &gt;,\n&lt; DATASETS.UTMOBILENET21: 'utmobilenet21' &gt;,\n&lt; DATASETS.MIRAGE19: 'mirage19' &gt;,\n&lt; DATASETS.MIRAGE22: 'mirage22' &gt;])\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#the-get_dataset_folder-method","title":"The <code>.get_dataset_folder()</code> method","text":"<p>For instance, you can bypass the composition of a dataset folder path and call directly <code>.get_dataset_folder()</code> to find the specific  dataset folder you look for.</p> <pre><code>dataset_folder = tcb.get_dataset_folder(tcb.DATASETS.UCDAVISICDM19)\ndataset_folder\n</code></pre> <pre><code>PosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19')\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#listing-files","title":"Listing files","text":"<p>Via <code>pathlib</code> you can easily discover all parquet files composing a dataset</p> <pre><code>list(dataset_folder.rglob('*.parquet'))\n</code></pre> <pre><code>[PosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_1.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_4.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_3.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet'),\nPosixPath('./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_2.parquet')]\n</code></pre> <p>But you can also programmatically call the the <code>datasets lsparquet</code> subcommand of the CLI using <code>get_rich_tree_parquet_files()</code></p> <pre><code>from tcbench.libtcdatasets.datasets_utils import get_rich_tree_parquet_files\nget_rich_tree_parquet_files(tcb.DATASETS.UCDAVISICDM19)\n</code></pre> <pre>Datasets\n\u2514\u2500\u2500 ucdavis-icdm19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 ucdavis-icdm19.parquet\n        \u251c\u2500\u2500 LICENSE\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 test_split_human.parquet\n            \u251c\u2500\u2500 test_split_script.parquet\n            \u251c\u2500\u2500 train_split_0.parquet\n            \u251c\u2500\u2500 train_split_1.parquet\n            \u251c\u2500\u2500 train_split_2.parquet\n            \u251c\u2500\u2500 train_split_3.parquet\n            \u2514\u2500\u2500 train_split_4.parquet\n</pre>"},{"location":"datasets/guides/tutorial_load_datasets/#the-load_parquet-method","title":"The <code>.load_parquet()</code> method","text":"<p>Finally, the generic <code>.load_parquet()</code> can be used to load one of the parquet files.</p> <p>For instance, the following load the unfiltered monolithic file of the <code>ucdavis-icdm19</code> dataset</p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n</code></pre> <pre><code>df.head(2)\n</code></pre> row_id app flow_id partition num_pkts duration bytes unixtime timetofirst pkts_size pkts_dir pkts_iat 0 0 google-doc GoogleDoc-100 pretraining 2925 116.348 816029 [1527993495.652867, 1527993495.685678, 1527993... [0.0, 0.0328109, 0.261392, 0.262656, 0.263943,... [354, 87, 323, 1412, 1412, 107, 1412, 180, 141... [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ... [0.0, 0.0328109, 0.2285811, 0.0012639999999999... 1 1 google-doc GoogleDoc-1000 pretraining 2813 116.592 794628 [1527987720.40456, 1527987720.422811, 15279877... [0.0, 0.0182509, 0.645106, 0.646344, 0.647689,... [295, 87, 301, 1412, 1412, 1412, 180, 113, 141... [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0.0, 0.0182509, 0.6268551, 0.0012380000000000... <pre><code>df.groupby(['partition', 'app'])['app'].value_counts()\n</code></pre> <pre><code>partition                    app\npretraining                  google-doc       1221\ngoogle-drive     1634\ngoogle-music      592\ngoogle-search    1915\nyoutube          1077\nretraining-human-triggered   google-doc         15\ngoogle-drive       18\ngoogle-music       15\ngoogle-search      15\nyoutube            20\nretraining-script-triggered  google-doc         30\ngoogle-drive       30\ngoogle-music       30\ngoogle-search      30\nyoutube            30\nName: count, dtype: int64\n</code></pre> <p>Beside the dataset name, the function only has 2 other parameters, but their semantic and values are \"mingled\" with the curation process adopted.</p> <pre><code>tcb.load_parquet?\n</code></pre> <pre><code>Signature:\ntcb.load_parquet(\n    dataset_name: 'str | DATASETS',\n    min_pkts: 'int' = -1,\n    split: 'str' = None,\n    columns: 'List[str]' = None,\n    animation: 'bool' = False,\n) -&gt; 'pd.DataFrame'\nDocstring:\nLoad and returns a dataset parquet file\n\nArguments:\n    dataset_name: The name of the dataset\n    min_pkts: the filtering rule applied when curating the datasets.\n        If -1, load the unfiltered dataset\n    split: if min_pkts!=-1, is used to request the loading of\n        the split file. For DATASETS.UCDAVISICDM19\n        values can be \"human\", \"script\" or a number\n        between 0 and 4.\n        For all other dataset split can be anything\n        which is not None (e.g., True)\n    columns: A list of columns to load (if None, load all columns)\n    animation: if True, create a loading animation on the console\n\nReturns:\n    A pandas dataframe and the related parquet file used to load the dataframe\nFile:      ~/.conda/envs/super-tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets_utils.py\nType:      function\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#how-load_parquet-maps-to-parquet-files","title":"How <code>.load_parquet()</code> maps to parquet files","text":"<p>The logic to follow to load specific files can be confusing. The table below report a global view across datasets:</p> Dataset min_pkts=-1 min_pkts=10 min_pkts=1000 split=True split=0..4 split=human split=script <code>ucdavis-icdm19</code> yes - - - yes (train+val) yes (test) yes (test) <code>mirage19</code> yes yes - yes (train/val/test) - - - <code>mirage22</code> yes yes yes yes (train/val/test) - - - <code>utmobilenet21</code> yes yes - yes (train/val/test) - - - <ul> <li> <p><code>min_pkts=-1</code> is set by default and corresponds to loading the unfiltered parquet files, i.e., the files stored immediately under <code>/preprocessed</code>. All other files are stored under the <code>imc23</code> subfolders</p> </li> <li> <p>For <code>ucdavis-icdm19</code>, the parameter <code>min_pkts</code> is not used. The loading of training(+validation) and test data is controlled by <code>split</code></p> </li> <li> <p>For all other datasets, <code>min_pkts</code> specifies which filtered version of the data to use, while <code>split=True</code> load the split indexes</p> </li> </ul>"},{"location":"datasets/guides/tutorial_load_datasets/#loading-ucdavis-icdm19","title":"Loading <code>ucdavis-icdm19</code>","text":"<p>For instance, to load the <code>human</code> test split of <code>ucdavid-icdm19</code> you can run</p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='human')\ndf['app'].value_counts()\n</code></pre> <pre><code>app\nyoutube          20\ngoogle-drive     18\ngoogle-doc       15\ngoogle-music     15\ngoogle-search    15\nName: count, dtype: int64\n</code></pre> <p>And the logic is very similar for the <code>script</code> partition</p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='script')\ndf['app'].value_counts()\n</code></pre> <pre><code>app\ngoogle-doc       30\ngoogle-drive     30\ngoogle-music     30\ngoogle-search    30\nyoutube          30\nName: count, dtype: int64\n</code></pre> <p>However to load a specific train split</p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split='0')\ndf['app'].value_counts()\n</code></pre> <pre><code>app\ngoogle-doc       100\ngoogle-drive     100\ngoogle-music     100\ngoogle-search    100\nyoutube          100\nName: count, dtype: int64\n</code></pre>"},{"location":"datasets/guides/tutorial_load_datasets/#loading-other-datasets","title":"Loading other datasets","text":"<p>By default, without any parameter beside the dataset name, the function loads the unfiltered version of a dataset</p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.MIRAGE19)\ndf.shape\n</code></pre> <pre><code>(122007, 135)\n</code></pre> <p>Recall the structure of the <code>mirage19</code> dataset</p> <pre><code>get_rich_tree_parquet_files(tcb.DATASETS.MIRAGE19)\n</code></pre> <pre>Datasets\n\u2514\u2500\u2500 mirage19\n    \u2514\u2500\u2500 \ud83d\udcc1 preprocessed/\n        \u251c\u2500\u2500 mirage19.parquet\n        \u2514\u2500\u2500 \ud83d\udcc1 imc23/\n            \u251c\u2500\u2500 mirage19_filtered_minpkts10.parquet\n            \u2514\u2500\u2500 mirage19_filtered_minpkts10_splits.parquet\n</pre> <p>So there is only one filtering with <code>min_pkts=10</code></p> <pre><code>df = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)\ndf.shape\n</code></pre> <pre><code>(64172, 20)\n</code></pre> <p>Based on the dataframe shape, we can see that (indeed) we loaded a reduced version of the unfiltered dataset.</p> <p>While for <code>ucdavis-icdm19</code> the \"split\" files represent 100 samples selected for training (because there are two ad-hoc test split), for all other dataset the \"split\" files contains indexes indicating the rows to use for train/val/test.</p> <p>Thus, issuing <code>split=True</code> is enough to indicate the need to load the split table.</p> <pre><code>df_split = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10, split=True)\n</code></pre>"},{"location":"datasets/install/","title":"Datasets installation","text":"<p>Dataset installation is triggered with the <code>datasets install</code> subcommand</p> <pre><code>tcbench datasets install --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench datasets install [OPTIONS]\n\n Install a dataset.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --name          -n  [ucdavis-icdm19|utmobilenet21|mirage19|mirage22]  Dataset to install. [required]                         \u2502\n\u2502    --input-folder  -i  PATH                                              Folder where to find pre-downloaded tarballs.          \u2502\n\u2502    --help                                                                Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The raw data of the datasets is either hosted on websites or cloud environments. The automatic download from those locations is available only for some of the datasets.</p> Name Auto download <code>ucdavis-icdm19</code> <code>mirage19</code> <code>mirage22</code> <code>utmobilenet21</code> <p>If auto download is not possible, to install the dataset you need to manually fetch the related archives, place them in a folder, e.g., <code>/download</code>, and provide the <code>--input-folder</code> option when triggering installation.</p> <p>When installing a dataset, <code>tcbench</code> also shows two types of reports as formatted tables.</p> <ul> <li> <p>Samples count: This tables collect the number of samples (i.e., flows) available.</p> </li> <li> <p>Stats: The curation process can filter out flows (e.g., based on a minum number of packets or remove classes without a minimum number of flows). As such, when  installing, <code>tcbench</code> is showing general stats (mean, std, percentiles) about number of packets for each flow across classes.</p> </li> </ul> <p>Please check the specific install page for each dataset for more details.</p>"},{"location":"datasets/install/#datasets-deletion","title":"Datasets deletion","text":"<p>The datasets files are installed within the  python environment where tcbench is installed.</p> <p>You can delete a dataset using the following command</p> <pre><code>tcbench datasets delete --name &lt;dataset-name&gt;\n</code></pre>"},{"location":"datasets/install/mirage19/","title":"<code>mirage19</code>","text":"<p>The dataset collect traffic from 20 mobile Android apps ( accuweather, comixology, dropbox, duolingo, facebook, foursquared, groupon, iliga, messenger, pinterest, slither, spotify, subito.it, trello, tripadvisor, twitter, viber, waze, wish.com, youtube).</p> <p>The authors of the dataset (Aceto et. al) describe it as follows</p> QuoteBibtex <p> We have collected the MIRAGE-2019 dataset in the ARCLAB laboratory at the University of Napoli \u201cFederico II\u201d. The capture sessions (cf. Sec. III-A) span from May 2017 to May 2019. We employed three devices to generate the mobile traffic, namely: (i) Xiaomi Mi5, (ii) Google Nexus 7, and (iii) Samsung Galaxy A5. In detail, we installed the custom firmware CyanogenMod v13.0 (corresponding to the Android version 6.0.1) on all the devices and enabled the root mode. More than 280 experimenters took part to the dataset construction on a voluntary basis, by performing one or two experimental sessions each. The experimenters involved in this activity were students of three different courses9 held at the University of Napoli \u201cFederico II\u201d, aged 19\u00f725 years, with a 85/15% share between males and females. Each experimental session lasted two hours, at most. Altogether, during each experimental session, each experimenter performed 12 capture sessions of 5\u00f710 minutes (each resulting in one PCAP traffic trace and one strace log-file, cf. Sec. III). In each capture session the experimenter was asked to perform activities mimicking common uses of a single app with the intent to explore its functionalities in addition to first-time install, login, registration. We report the ethical considerations underlying the aforementioned traffic-capture procedure in Sec. VI. Overall, the MIRAGE-2019 dataset gathers the traffic generated by 40 Android apps belonging to 16 different categories according to Google Play apps distribution portal [17] </p> <p>@INPROCEEDINGS{aceto2019mirage, author={G. {Aceto} and D. {Ciuonzo} and A. {Montieri} and V. {Persico} and A. {Pescap{`e}}}, booktitle={IEEE 4th International Conference on Computing, Communication and Security (ICCCS 2019)}, title={MIRAGE: Mobile-app Traffic Capture and Ground-truth Creation}, year={2019}, volume={}, number={}, pages={}, abstract={Network traffic analysis, i.e. the umbrella of procedures for distilling information from network traffic, represents the enabler for highly-valuable profiling information, other than being the workhorse for several key network management tasks. While it is currently being revolutionized in its nature by the rising share of traffic generated by mobile and hand-held devices, existing design solutions are mainly evaluated on private traffic traces, and only a few public datasets are available, thus clearly limiting repeatability and further advances on the topic. To this end, this paper introduces and describes MIRAGE, a reproducible architecture for mobile-app traffic capture and ground-truth creation. The outcome of this system is MIRAGE-2019, a human-generated dataset for mobile traffic analysis (with associated ground-truth) having the goal of advancing the state-of-the-art in mobile app traffic analysis. A first statistical characterization of the mobile-app traffic in the dataset is provided in this paper. Still, MIRAGE is expected to be capitalized by the networking community for different tasks related to mobile traffic analysis.}, keywords={Android apps; encrypted traffic; mobile apps; mobile traffic; reproducible research; open dataset; traffic classification}, doi={}, ISSN={}, month={Oct},}</p> <p>20 or 40 apps?</p> <p>The quote reports that the datasets has traffic from 40 apps but the website describing the dataset is actually reporting only 20.</p> <p>Through communication with Aceto et al. we gathered that the public version of the dataset only contains 20 apps. Although not reported on the website, the data for the remaining 20 is available only upon request.</p> <p>That said, tcbench considers only portion of the dataset which public.</p>"},{"location":"datasets/install/mirage19/#raw-data","title":"Raw data","text":"<p>The dataset is a single tarball that once unpacked has the following structure </p> <pre><code>MIRAGE-2019_traffic_dataset_downloadable\n\u251c\u2500\u2500 Mi5_38_a4_ed_18_cc_bf\n\u2514\u2500\u2500 Nexus7_bc_ee_7b_a4_09_47\n</code></pre> <p>The subfolders contain collections of JSON files, each representing a different experiment.</p> <p>The JSON schema of each file is not officially documented. The semantic of the JSON schema is not very difficult to reverse engineer (especially if you have domain knowledge in traffic processing). That said, the JSON schema has  a nested structure that makes it not easy to process.</p> <p>The target Android app for an experiment is encoded both in the filename as well as metadata in the JSON schema. The actual structure in the schema is <code>flow</code> <code>metadata</code> <code>bf</code> <code>label</code>.</p> <p>Each JSON file collects per-flow data, but  metrics are scattered across different nested layers. For example, aggregate flow metrics are hierarchially separated from packet time series, which are further separated from other metadata.</p> <p>Last, each JSON reports time series of packet properties (e.g., packets size and direction) but also packet payload (each payload is encoded as a list of integers).</p>"},{"location":"datasets/install/mirage19/#curation","title":"Curation","text":"<p>The curation process has the following objectives:</p> <ol> <li> <p>Combine all JSON files into a monolithic parquet file.</p> </li> <li> <p>Flatten the JSON nested structure.      For instance, the nested input dictionary     <code>{\"layer1\":{\"col1\":1, \"col2\":2}}</code>      would be flattened into a table     with columns \"layer1_col1\" and \"layer1_col2\"      with the respective values \"1\" and \"2\".</p> </li> <li> <p>Remove \"background\" traffic. More specifically,     each JSON file details the Android      app name in the file name. But the traffic in an experiment     can be related to a different app/service running in parallel.     However, the dataset offers the column     <code>flow_metadata_bf_label</code> which contains the     Android app name that <code>netstat</code> linked to each     network socket during an experiment.     This implies that, by knowing the expected app     of an experiment, one can define as \"background\"       <code>flow_metadata_bf_label</code> !=      expected Android app name.</p> </li> <li> <p>Remove \"small data\". This include filtering out data     based on the following rules:</p> <ul> <li> <p>Remove ACK packets from time series.</p> </li> <li> <p>Remove flows with &lt; 10 samples.</p> </li> <li> <p>Remove apps generating &lt; 100 samples.</p> </li> </ul> </li> <li> <p>As mentioned, the dataset contains raw packet bytes across multiple packets     of a flow. We process these series to search for      ASCII strings.     This represents a layman approach to search for TLS     handshake information (i.e., rather than actually decoding     packet headers, we simply search for sequence of bytes     that looks like ASCII strings).</p> </li> </ol> <p>Given the curation requires filtering, we provides two version of the dataset: </p> <ul> <li> <p>unfiltered contains all data (including background, ACK packets, etc.) and the related parquet has 135 columns (most generated by  unfolding the JSON nested structure).</p> </li> <li> <p>preprocessed contains the curated data  and an opinionated selection of 20 columns to make the parquet files more maneageable.</p> </li> </ul> <p>For both formats, the most important columns of  the datasets are the following.</p> Field Description <code>packet_data_packet_dir</code> The time series of the packet direction <code>packet_data_l4_payload_bytes</code> The time series of the packet size <code>packet_data_iat</code> The time series of the packet inter-arrival time <code>flow_metadata_bf_label</code> The label gathered via netstat <code>strings</code> The ASCII string recovered from the payload analysis <code>android_name</code> The app used for an experiment <code>app</code> The final label encoded as a pandas <code>category</code> <code>row_id</code> A unique row identifier <p>Please refer to the datasets schema page for more details.</p>"},{"location":"datasets/install/mirage19/#splits","title":"Splits","text":"<p>The preprocessed parquet file is associated with an 80/10/10 train/validation/test splits created with the following logic.</p> <ol> <li> <p>Shuffle the rows.</p> </li> <li> <p>Perform a 90/10 split where the 10-part is used for testing.</p> </li> <li> <p>From the 90-part, do a second 90/10 to define train and validation.</p> </li> </ol> <p>The splits are a collection of row indexes that needs to be applied on the filtered monolithic parquet in order to obtain the data for modeling.</p> <p>The structure of the splits table is as follows</p> Field Description <code>train_indexes</code> A numpy array with the <code>row_id</code> related to the train split <code>val_indexes</code> ... validation split <code>test_indexes</code> ... test split <code>split_index</code> The index of the split (0..4)"},{"location":"datasets/install/mirage19/#install","title":"Install","text":"<p>The installation does not requires you to pre-download the dataset tarball and can be triggered with the following command</p> <pre><code>tcbench datasets install --name mirage19\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502download &amp; unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDownloading... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5 GB / 1.5 GB eta 0:00:00\nopening: /tmp/tmpxcdzy8tw/MIRAGE-2019_traffic_dataset_downloadable_v2.tar.gz\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 1642 JSON files to load\nConverting JSONs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1642/1642 0:00:11\nmerging files...\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/mirage19.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/mirage19.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.waze                    \u2502   11737 \u2502\n\u2502 de.motain.iliga             \u2502   10810 \u2502\n\u2502 com.accuweather.android     \u2502   10631 \u2502\n\u2502 com.duolingo                \u2502    8319 \u2502\n\u2502 it.subito                   \u2502    8167 \u2502\n\u2502 com.contextlogic.wish       \u2502    6507 \u2502\n\u2502 com.spotify.music           \u2502    6431 \u2502\n\u2502 com.joelapenna.foursquared  \u2502    6399 \u2502\n\u2502 com.google.android.youtube  \u2502    6346 \u2502\n\u2502 com.iconology.comics        \u2502    5516 \u2502\n\u2502 com.facebook.katana         \u2502    5368 \u2502\n\u2502 com.dropbox.android         \u2502    4815 \u2502\n\u2502 com.twitter.android         \u2502    4734 \u2502\n\u2502 background                  \u2502    4439 \u2502\n\u2502 com.pinterest               \u2502    4078 \u2502\n\u2502 com.facebook.orca           \u2502    4018 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502    3572 \u2502\n\u2502 air.com.hypah.io.slither    \u2502    3088 \u2502\n\u2502 com.viber.voip              \u2502    2740 \u2502\n\u2502 com.trello                  \u2502    2306 \u2502\n\u2502 com.groupon                 \u2502    1986 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502  122007 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503    value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502 122007.0 \u2502\n\u2502 mean  \u2502    23.11 \u2502\n\u2502 std   \u2502     9.73 \u2502\n\u2502 min   \u2502      1.0 \u2502\n\u2502 25%   \u2502     17.0 \u2502\n\u2502 50%   \u2502     26.0 \u2502\n\u2502 75%   \u2502     32.0 \u2502\n\u2502 max   \u2502     32.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=10...\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23/mirage19_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 de.motain.iliga             \u2502    7505 \u2502\n\u2502 com.waze                    \u2502    7214 \u2502\n\u2502 com.duolingo                \u2502    4583 \u2502\n\u2502 it.subito                   \u2502    4299 \u2502\n\u2502 com.contextlogic.wish       \u2502    3927 \u2502\n\u2502 com.accuweather.android     \u2502    3737 \u2502\n\u2502 com.joelapenna.foursquared  \u2502    3627 \u2502\n\u2502 com.spotify.music           \u2502    3300 \u2502\n\u2502 com.dropbox.android         \u2502    3189 \u2502\n\u2502 com.facebook.katana         \u2502    2878 \u2502\n\u2502 com.iconology.comics        \u2502    2812 \u2502\n\u2502 com.twitter.android         \u2502    2805 \u2502\n\u2502 com.google.android.youtube  \u2502    2728 \u2502\n\u2502 com.pinterest               \u2502    2450 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502    2052 \u2502\n\u2502 com.facebook.orca           \u2502    1783 \u2502\n\u2502 com.viber.voip              \u2502    1618 \u2502\n\u2502 com.trello                  \u2502    1478 \u2502\n\u2502 com.groupon                 \u2502    1174 \u2502\n\u2502 air.com.hypah.io.slither    \u2502    1013 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502   64172 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503   value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502 64172.0 \u2502\n\u2502 mean  \u2502   17.01 \u2502\n\u2502 std   \u2502    4.43 \u2502\n\u2502 min   \u2502    11.0 \u2502\n\u2502 25%   \u2502    14.0 \u2502\n\u2502 50%   \u2502    17.0 \u2502\n\u2502 75%   \u2502    19.0 \u2502\n\u2502 max   \u2502    32.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage19/preprocessed/imc23/mirage19_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 de.motain.iliga             \u2502          6079 \u2502         675 \u2502          751 \u2502        7505 \u2502\n\u2502 com.waze                    \u2502          5844 \u2502         649 \u2502          721 \u2502        7214 \u2502\n\u2502 com.duolingo                \u2502          3712 \u2502         413 \u2502          458 \u2502        4583 \u2502\n\u2502 it.subito                   \u2502          3482 \u2502         387 \u2502          430 \u2502        4299 \u2502\n\u2502 com.contextlogic.wish       \u2502          3181 \u2502         353 \u2502          393 \u2502        3927 \u2502\n\u2502 com.accuweather.android     \u2502          3027 \u2502         336 \u2502          374 \u2502        3737 \u2502\n\u2502 com.joelapenna.foursquared  \u2502          2938 \u2502         326 \u2502          363 \u2502        3627 \u2502\n\u2502 com.spotify.music           \u2502          2673 \u2502         297 \u2502          330 \u2502        3300 \u2502\n\u2502 com.dropbox.android         \u2502          2583 \u2502         287 \u2502          319 \u2502        3189 \u2502\n\u2502 com.facebook.katana         \u2502          2331 \u2502         259 \u2502          288 \u2502        2878 \u2502\n\u2502 com.iconology.comics        \u2502          2278 \u2502         253 \u2502          281 \u2502        2812 \u2502\n\u2502 com.twitter.android         \u2502          2272 \u2502         252 \u2502          281 \u2502        2805 \u2502\n\u2502 com.google.android.youtube  \u2502          2209 \u2502         246 \u2502          273 \u2502        2728 \u2502\n\u2502 com.pinterest               \u2502          1984 \u2502         221 \u2502          245 \u2502        2450 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502          1662 \u2502         185 \u2502          205 \u2502        2052 \u2502\n\u2502 com.facebook.orca           \u2502          1444 \u2502         161 \u2502          178 \u2502        1783 \u2502\n\u2502 com.viber.voip              \u2502          1310 \u2502         146 \u2502          162 \u2502        1618 \u2502\n\u2502 com.trello                  \u2502          1197 \u2502         133 \u2502          148 \u2502        1478 \u2502\n\u2502 com.groupon                 \u2502           951 \u2502         106 \u2502          117 \u2502        1174 \u2502\n\u2502 air.com.hypah.io.slither    \u2502           821 \u2502          91 \u2502          101 \u2502        1013 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502         51978 \u2502        5776 \u2502         6418 \u2502       64172 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The console output is showing a few samples count reports related to the processing performed on the datasets</p> <ol> <li> <p>The first report relates to the unfiltered dataset, i.e.,  the monolithic parquet files obtained consolidating all JSON files but before applying any curation. At first glance, it looks like this dataset has a lot of flows. However, the following report shows the number of packets per flow and suggests that most of the flows in the raw dataset are very very short (thus meaningless for a classification task).</p> </li> <li> <p>The second group of reports show similar information when removing flows with less than 10 packets.</p> </li> <li> <p>The last report shows the number of train/validation/test samples by each application for the first split (the same counters are true for all splits).</p> </li> </ol>"},{"location":"datasets/install/mirage22/","title":"<code>mirage22</code>","text":"<p>The dataset collect traffic from 9 mobile Android apps  (webex, skype, microsoft teams, zoom, discord, messenger, gotomeeting, google meetings, slack).</p> <p>The authors of the dataset (Guarino et. al) describe it as follows</p> QuoteBibtex <p> The dataset was collected by students and researchers within April\u2013June 2021 leveraging the MIRAGE architecture [16] (conveniently optimized to capture traffic of communication and collaboration apps) in the ARCLAB laboratory at the University of Napoli \u201cFederico II\u201d.1 Experimenters used three mobile devices: a Google Nexus 6 (Android 10) and two Samsung Galaxy A5 (Android 6.0.1). In each capture session\u2014 whose duration spanned from 15 to 80 minutes based on the activity\u2014the experimenters performed a specific activity, so as to obtain a traffic dataset that reflects the common usage of considered apps.2 Each session resulted in a PCAP traffic trace and additional system log-files with ground-truth information. Based on the latter, each biflow3 was reliably labeled with the corresponding Android package-name by considering established network-connections (via netstat).</p> <p>[...]</p> <p>Communication and collaboration apps\u2014used for business meeting, classes, and social interaction\u2014have experienced a huge utilization increment when \u201cstay-at-home\u201d orders were issued worldwide. Based on both popularity and utilization boost, herein we focus on five of them: GotoMeeting (Gm), Skype (Sk), Teams (Tm), Webex (Wb), and Zoom (Zm). Indeed, Zoom has obtained the steepest increment with its traffic scaling to orders of magnitude, followed by Webex, GotoMeeting, Teams, BlueJeans (whose traffic we are currently collecting), and Skype [17] </p> <pre><code>@INPROCEEDINGS{guarino2021classification,  \nauthor={Guarino, Idio and Aceto, Giuseppe and Ciuonzo, Domenico and Montieri, Antonio and Persico, Valerio and Pescap\u00c3\u00a9, Antonio},  \nbooktitle={2021 IEEE 26th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)},   \ntitle={Classification of Communication and Collaboration Apps via Advanced Deep-Learning Approaches},   \nyear={2021},  \nvolume={},  \nnumber={},  \npages={1-6},  \ndoi={10.1109/CAMAD52502.2021.9617789}\n}\n</code></pre> <p>As suggested by the name, the dataset is from the same research group of <code>mirage19</code>  so the two datasets share many properties.</p> <p>The major difference is the target of applications as <code>mirage22</code> focuses only on on video meeting Android apps with experiments annotated with respect to different interactions the the apps (voice, chat, etc.) while <code>mirage19</code> is more diversified set of apps.</p>"},{"location":"datasets/install/mirage22/#raw-data","title":"Raw data","text":"<p>The dataset is a single zip. Once unpacked it has the following structure <pre><code>MIRAGE-COVID-CCMA-2022\n\u251c\u2500\u2500 Preprocessed_pickle\n\u2514\u2500\u2500 Raw_JSON\n    \u251c\u2500\u2500 Discord\n    \u251c\u2500\u2500 GotoMeeting\n    \u251c\u2500\u2500 Meet\n    \u251c\u2500\u2500 Messenger\n    \u251c\u2500\u2500 Skype\n    \u251c\u2500\u2500 Slack\n    \u251c\u2500\u2500 Teams\n    \u251c\u2500\u2500 Webex\n    \u2514\u2500\u2500 Zoom\n</code></pre></p> <p>Notice the two subfolders:</p> <ul> <li> <p><code>Raw_JSON</code> gathers the nested JSON files for each experiment.</p> </li> <li> <p><code>Preprocessed_pickle</code> is a pickle serialization of the  data but unfortunately is undocumented.</p> </li> </ul>"},{"location":"datasets/install/mirage22/#curation-splits","title":"Curation &amp; splits","text":"<p>We follow the same processes described for <code>mirage19</code> curation, i.e., consolidation and flattening of the JSON files, removal of the  background, etc.</p> <p>However, next to the unfiltered and filtered version imposing a minimum of 10 packets per flow, we also create a second filtered version imposing a minimum of 1,000 packets per flow.</p> <p>Once the parquet files are generate we create 80/10/10 train/validation/test splits with the same process  described for the <code>mirage19</code> splits.</p>"},{"location":"datasets/install/mirage22/#install","title":"Install","text":"<p>The installation does not requires you to pre-download the dataset tarball and can be triggered with the following command</p> <pre><code>tcbench datasets install --name mirage22\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502download &amp; unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDownloading... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1 GB / 3.1 GB eta 0:00:00\nopening: /tmp/tmp3marsp7l/MIRAGE-COVID-CCMA-2022.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Discord.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/GotoMeeting.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Meet.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Messenger.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Skype.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Slack.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Teams.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Webex.zip\nopening: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/raw/MIRAGE-COVID-CCMA-2022/Raw_JSON/Zoom.zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 998 JSON files to load\nConverting JSONs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 998/998 0:00:28\nmerging files...\nsaving: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/mirage22/preprocessed/mirage22.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/mirage22.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 background                       \u2502   18882 \u2502\n\u2502 com.microsoft.teams              \u2502    6541 \u2502\n\u2502 com.skype.raider                 \u2502    6203 \u2502\n\u2502 us.zoom.videomeetings            \u2502    5066 \u2502\n\u2502 com.cisco.webex.meetings         \u2502    4789 \u2502\n\u2502 com.discord                      \u2502    4337 \u2502\n\u2502 com.facebook.orca                \u2502    4321 \u2502\n\u2502 com.gotomeeting                  \u2502    3695 \u2502\n\u2502 com.Slack                        \u2502    2985 \u2502\n\u2502 com.google.android.apps.meetings \u2502    2252 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   59071 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   59071.0 \u2502\n\u2502 mean  \u2502   3068.32 \u2502\n\u2502 std   \u2502  25416.43 \u2502\n\u2502 min   \u2502       1.0 \u2502\n\u2502 25%   \u2502      20.0 \u2502\n\u2502 50%   \u2502      27.0 \u2502\n\u2502 75%   \u2502      42.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=10...\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.cisco.webex.meetings         \u2502    4437 \u2502\n\u2502 com.skype.raider                 \u2502    4117 \u2502\n\u2502 com.microsoft.teams              \u2502    3857 \u2502\n\u2502 us.zoom.videomeetings            \u2502    3587 \u2502\n\u2502 com.discord                      \u2502    3387 \u2502\n\u2502 com.facebook.orca                \u2502    2623 \u2502\n\u2502 com.gotomeeting                  \u2502    2557 \u2502\n\u2502 com.google.android.apps.meetings \u2502    1238 \u2502\n\u2502 com.Slack                        \u2502     970 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   26773 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   26773.0 \u2502\n\u2502 mean  \u2502   6598.23 \u2502\n\u2502 std   \u2502  37290.08 \u2502\n\u2502 min   \u2502      11.0 \u2502\n\u2502 25%   \u2502      15.0 \u2502\n\u2502 50%   \u2502      21.0 \u2502\n\u2502 75%   \u2502     186.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.cisco.webex.meetings         \u2502          3594 \u2502         399 \u2502          444 \u2502        4437 \u2502\n\u2502 com.skype.raider                 \u2502          3334 \u2502         371 \u2502          412 \u2502        4117 \u2502\n\u2502 com.microsoft.teams              \u2502          3124 \u2502         347 \u2502          386 \u2502        3857 \u2502\n\u2502 us.zoom.videomeetings            \u2502          2905 \u2502         323 \u2502          359 \u2502        3587 \u2502\n\u2502 com.discord                      \u2502          2743 \u2502         305 \u2502          339 \u2502        3387 \u2502\n\u2502 com.facebook.orca                \u2502          2125 \u2502         236 \u2502          262 \u2502        2623 \u2502\n\u2502 com.gotomeeting                  \u2502          2072 \u2502         230 \u2502          255 \u2502        2557 \u2502\n\u2502 com.google.android.apps.meetings \u2502          1002 \u2502         112 \u2502          124 \u2502        1238 \u2502\n\u2502 com.Slack                        \u2502           786 \u2502          87 \u2502           97 \u2502         970 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502         21685 \u2502        2410 \u2502         2678 \u2502       26773 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nfiltering min_pkts=1000...\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts1000.parquet\nsamples count : filtered (min_pkts=1000)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502    2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502     425 \u2502\n\u2502 com.google.android.apps.meetings \u2502     379 \u2502\n\u2502 com.microsoft.teams              \u2502     321 \u2502\n\u2502 com.gotomeeting                  \u2502     297 \u2502\n\u2502 com.facebook.orca                \u2502     280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502     259 \u2502\n\u2502 com.Slack                        \u2502     198 \u2502\n\u2502 com.skype.raider                 \u2502     190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502    4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=1000)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502    4569.0 \u2502\n\u2502 mean  \u2502  38321.32 \u2502\n\u2502 std   \u2502   83282.0 \u2502\n\u2502 min   \u2502    1001.0 \u2502\n\u2502 25%   \u2502    2863.0 \u2502\n\u2502 50%   \u2502    6303.0 \u2502\n\u2502 75%   \u2502   35392.0 \u2502\n\u2502 max   \u2502 1665842.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/mirage22/preprocessed/imc23/mirage22_filtered_minpkts1000_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502          1798 \u2502         200 \u2502          222 \u2502        2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502           344 \u2502          39 \u2502           42 \u2502         425 \u2502\n\u2502 com.google.android.apps.meetings \u2502           307 \u2502          34 \u2502           38 \u2502         379 \u2502\n\u2502 com.microsoft.teams              \u2502           260 \u2502          29 \u2502           32 \u2502         321 \u2502\n\u2502 com.gotomeeting                  \u2502           240 \u2502          27 \u2502           30 \u2502         297 \u2502\n\u2502 com.facebook.orca                \u2502           227 \u2502          25 \u2502           28 \u2502         280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502           210 \u2502          23 \u2502           26 \u2502         259 \u2502\n\u2502 com.Slack                        \u2502           160 \u2502          18 \u2502           20 \u2502         198 \u2502\n\u2502 com.skype.raider                 \u2502           154 \u2502          17 \u2502           19 \u2502         190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502          3700 \u2502         412 \u2502          457 \u2502        4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The console output is showing a few samples count reports related to the processing performed on the datasets</p> <ol> <li> <p>The first report relates to the unfiltered dataset, i.e.,  the monolithic parquet files obtained consolidating all JSON files but before applying any curation. At first glance, it looks like this dataset has a lot of flows. However, the following report shows the number of packets per flow and suggests that there are many flows which are very short.</p> </li> <li> <p>The second and third group of reports show similar information to the first group but relates to the filtering out of flows with less than 10 and 1,000 packets.</p> </li> <li> <p>The last report shows the number of train/validation/test samples by each application for the first split (the same counters are true for all splits) when focusing on flows with more than 1,000 packets.</p> </li> </ol>"},{"location":"datasets/install/ucdavis-icdm19/","title":"<code>ucdavis-icdm19</code>","text":"<p>In the literature this dataset is also known as UCDAVIS19 or QUIC-DATASET and refers to 5 quic-based Google services (Google Drive, Google Docs, Google Search, Google Music, YouTube).</p> <p>The authors of the dataset (Rezaei et. al) describe it as follows</p> QuoteBibtex <p> This is a dataset captured in our lab at UC Davis and contains 5 Google services: Google Drive, Youtube, Google Docs, Google Search, and Google Music [5]. We used several systems with various configurations, including Windows 7, 8, 10, Ubuntu 16.4, and 17 operating systems. We wrote several scripts using Selenium WebDriver [17] and AutoIt [1] tools to mimic human behavior when capturing data. This approach allowed us to capture a large dataset without significant human effort. Such approach has been used in many other studies [14, 8, 3]. Furthermore, we also captured a few samples of real human interactions to show how much the accuracy of a model trained on scripted samples will degrade when it is tested on real human samples. During preprocessing, we removed all non-QUIC traffic. Note that all flows in our dataset are labeled, but we did not use labels during the pre-training step. We used class labels of all flows to show the accuracy gap between a fully-supervised and semi-supervised approach. </p> <pre><code>@article{DBLP:journals/corr/abs-1812-09761,\n  author       = {Shahbaz Rezaei and\n                  Xin Liu},\n  title        = {How to Achieve High Classification Accuracy with Just a Few Labels:\n                  {A} Semi-supervised Approach Using Sampled Packets},\n  journal      = {CoRR},\n  volume       = {abs/1812.09761},\n  year         = {2018},\n  url          = {http://arxiv.org/abs/1812.09761},\n  eprinttype    = {arXiv},\n  eprint       = {1812.09761},\n  timestamp    = {Thu, 07 Nov 2019 09:05:08 +0100},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-09761.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"datasets/install/ucdavis-icdm19/#raw-data","title":"Raw data","text":"<p>The original dataset is a collection of three different zip archives.</p> <pre><code>&lt;root&gt;/\n\u251c\u2500\u2500 pretraining.zip\n\u251c\u2500\u2500 Retraining(human-triggered).zip\n\u2514\u2500\u2500 Retraining(script-triggered).zip\n</code></pre> <p>Each archive is a different partition that Rezaei et al. named to reflect different scopes for modeling: * <code>pretraining</code> contains thousands of samples and is meant for pre-training models. * <code>Retraining(human-triggered)</code> and <code>Retraining(script-triggered)</code> contain  tens of samples and they are meant for testing or fine-tuning models.</p> <p>When all zips are unpacked, the folder structure becomes <pre><code>downloads/\n\u251c\u2500\u2500 pretraining\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u251c\u2500\u2500 Retraining(human-triggered)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Drive\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Music\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Google Search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Youtube\n\u2514\u2500\u2500 Retraining(script-triggered)\n    \u251c\u2500\u2500 Google Doc\n    \u251c\u2500\u2500 Google Drive\n    \u251c\u2500\u2500 Google Music\n    \u251c\u2500\u2500 Google Search\n    \u2514\u2500\u2500 Youtube\n</code></pre></p> <p>Inside each nested folder there is a collection of CSV files.</p> <p>Here an extract from one of those CSVs</p> <pre><code>head &lt;root&gt;/pretraining/Google Doc/GoogleDoc-1000.txt\n</code></pre> <p>Output</p> <pre><code>1527987720.404560000    0       295     1\n1527987720.422811000    0.0182509       87      0\n1527987721.049666000    0.645106        301     0\n1527987721.050904000    0.646344        1412    0\n1527987721.052249000    0.647689        1412    0\n1527987721.053456000    0.648896        1412    0\n1527987721.054619000    0.650059        180     0\n1527987721.055299000    0.650739        113     1\n1527987721.055848000    0.651288        1412    0\n1527987721.057053000    0.652493        1412    0\n</code></pre> <p>Each file represent an individual flow and each row in a file has information for an individual packet of that flow. Specifically, the columns correspond to </p> <ul> <li> <p>The packet unixtime (in seconds).</p> </li> <li> <p>The packet relative time with respect to the first packet of the flow.</p> </li> <li> <p>The packet size.</p> </li> <li> <p>The packet direction (either 0 or 1).</p> </li> </ul>"},{"location":"datasets/install/ucdavis-icdm19/#curation","title":"Curation","text":"<p>The raw dataset provided by Rezaei et al. is already cleaned dataset, i.e., the authors already filtered the data they collected and provide logs referring only to traffic generated by the 5 targetted Google services.</p> <p>As such, tcbench does NOT perform any additional filtering.</p> <p>However, the organization of the raw data can be improved as it is a collection of many individual CSVs files and class labels are encoded in folder and file names.</p> <p>So, the curation process performed by tcbench aim to</p> <ul> <li> <p>Create a monolithic parquet files where each row represent one flow, and  packet series are collected into <code>numpy</code> arrays.</p> </li> <li> <p>Retain the original folders structure has semantic, this is preserved during curation by adding extra columns (<code>partition</code> and <code>flow_id</code>).</p> </li> </ul> <p>The following table describes the schema of the curated datasets.</p> Field Description <code>row_id</code> A unique row id <code>app</code> The label of the flow, encoded as pandas <code>category</code> <code>flow_id</code> The original filename <code>partition</code> The partition related to the flow <code>num_pkts</code> Number of packets in the flow <code>duration</code> The duration of the flow <code>bytes</code> The number of bytes of the flow <code>unixtime</code> Numpy array with the absolute time of each packet <code>timetofirst</code> Numpy array with the delta between a packet the first packet of the flow <code>pkts_size</code> Numpy array for the packet size time series <code>pkts_dir</code> Numpy array for the packet direction time series <code>pkts_iat</code> Numpy array for the packet inter-arrival time series"},{"location":"datasets/install/ucdavis-icdm19/#splits","title":"Splits","text":"<p>The 3 partition created by Rezaei et al. needs to be complemented with actual folds before being able to train models.</p> <p>The splits generated for this datasets relate to our IMC23 paper.</p> <p>Specifically:</p> <ul> <li> <p>From <code>pretraining</code> we generate 5 random splits, each with 100 samples per class.</p> </li> <li> <p>The other two partitions are left as is and are used for testing.</p> </li> </ul> <p>Both training and testing splits are \"materialized\", i.e., the splits are NOT collection or row indexes but rather already filtered views of the monolithic  parquet files.</p> <p>Hence, all splits have the same columns as from the previous table.</p>"},{"location":"datasets/install/ucdavis-icdm19/#install","title":"Install","text":"<p>The dataset zip archives are stored in a google Google Drive folder.</p> <p>To install them in tcbench you need to download the 3 zip files manually and place them into a local folder, e.g., <code>/downloads</code>.</p> <p>To trigger the installation run the following</p> <pre><code>tcbench datasets install \\\n    --name ucdavis-icdm19 \\\n    --input-folder ./downloads/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nopening: downloads/pretraining.zip\nopening: downloads/Retraining(human-triggered).zip\nopening: downloads/Retraining(script-triggered).zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfound 6672 CSV files to load\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00\nconcatenating files\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_1.parquet\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_2.parquet\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_3.parquet\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_4.parquet\nsamples count : train_split = 0 to 4\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet\nsamples count : test_split_human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet\nsamples count : test_split_script\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502      30 \u2502\n\u2502 google-drive  \u2502      30 \u2502\n\u2502 google-music  \u2502      30 \u2502\n\u2502 google-search \u2502      30 \u2502\n\u2502 youtube       \u2502      30 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The console output is showing a few samples count reports related to the processing performed on the datasets</p> <ol> <li> <p>The first report relates to the overall monolithic parquet files obtained consolidating all CSVs. This is labeled as unfiltered in the console output.</p> </li> <li> <p>The next report relates to the generation of the 5 splits (obtained processing the pretraining partition only).</p> </li> <li> <p>The last two reports relate to the two predefined testing partitions.</p> </li> </ol>"},{"location":"datasets/install/utmobilenet21/","title":"<code>utmobilenet21</code>","text":"<p>The dataset collect traffic from 17 mobile Android apps  (youtube, reddit, google-maps, spotify, netflix, pinterest, hulu, instagram, dropbox, facebook, twitter, gmail, pandora, messenger, google-drive, hangout, skype)</p> <p>The authors of the dataset (Heng et. al) describe it as follows</p> QuoteBibtex <p> We select 16 of the most popular mobile applications listed on [18], so  that the collected data is representative of the modern data consumption patterns.  Typical user activities are selected for each application, such as browsing  and posting for Reddit and sending and opening emails for Gmail.  For each activity, a series of actions are implemented through the  Android API to emulate a sequence of user interactions with the smartphone.  Examples of actions include scrolling the smartphone screen, clicking on  part of the displayed content to go to a different screen, and waiting  for the displayed media to play for a certain time.</p> <p>[...]</p> <p>After packet recording is complete, the raw pcap data files are transferred  to the laptop, where they are processed using TShark and converted to the csv format.</p> <p>[...]</p> <p>Three different sets of data were collected. In the deterministic automated dataset, the action parameters for each activity are fixed. For instance, when performing the activity of scrolling news feed on Facebook, the BASH script will always scroll the feed 3 times, wait for 5 seconds, and repeat for 5 times. Although the actions are fixed, the context and content displayed are up to the application. In the randomized automated dataset, the action parameters for each activity are randomized, such as the number of scrolls, the wait time and the number of repetitions for Facebook news feed scrolling. This makes the collected data more diverse and realistic. The third dataset is generated by human users and is the most realistic in terms of representing user activity. It includes two subsets: an application-specific dataset and an activity-specific dataset. In the former, human users perform each activity using applications in Table III. In the latter, human users use each application normally without constraints on the activities to perform. </p> <pre><code>@ARTICLE{9490678,\n  author={Heng, Yuqiang and Chandrasekhar, Vikram and Andrews, Jeffrey G.},\n  journal={IEEE Networking Letters}, \n  title={UTMobileNetTraffic2021: A Labeled Public Network Traffic Dataset}, \n  year={2021},\n  volume={3},\n  number={3},\n  pages={156-160},\n  doi={10.1109/LNET.2021.3098455}}\n</code></pre>"},{"location":"datasets/install/utmobilenet21/#raw-data","title":"Raw data","text":"<p>The dataset is a single zip file. Once unpacked it contains the following structure. <pre><code>csvs\n\u251c\u2500\u2500 Action-Specific Wild Test Data\n\u251c\u2500\u2500 Deterministic Automated Data\n\u251c\u2500\u2500 Randomized Automated Data\n\u2514\u2500\u2500 Wild Test Data\n</code></pre></p> <p>The structure reflects the four actions used to interact with the apps. Within each folder there is a collection of CSV files with ground-truth labels encoded in the filename.</p> <p>For instance <pre><code>&gt; ls -1 csvs/Action-Specific\\ Wild\\ Test\\ Data/ | head\ndropbox_man-download_2019-04-30_19-07-09_4fd1c357.csv\ndropbox_man-upload_2019-04-30_19-16-06_4fd1c357.csv\nfacebook_man-scroll-newsfeed_2019-04-19_14-36-52_d56097ed.csv\ngmail_man-open-email_2019-04-19_15-08-28_d56097ed.csv\ngmail_man-send-email_2019-04-19_15-26-04_d56097ed.csv\ngoogle-drive_man-download_2019-04-30_19-22-16_4fd1c357.csv\ngoogle-drive_man-upload_2019-04-19_15-40-09_d56097ed.csv\ngoogle-drive_man-upload_2019-04-30_19-27-21_4fd1c357.csv\ngoogle-maps_man-explore_2019-04-24_15-55-57_4fd1c357.csv\ngoogle-maps_man-explore_2019-04-24_16-26-39_4fd1c357.csv\n</code></pre></p> <p>Each CSV is generated with <code>tshark</code> so it gathers per-packet information across all flows of a pcap.</p> <p>For instance <pre><code>&gt; head csvs/Action-Specific\\ Wild\\ Test\\ Data/dropbox_man-download_2019-04-30_19-07-09_4fd1c357.csv\n,frame.number,frame.time,frame.len,frame.cap_len,sll.pkttype,sll.hatype,sll.halen,sll.src.eth,sll.unused,sll.etype,ip.hdr_len,ip.dsfield.ecn,ip.len,ip.id,ip.frag_offset,ip.ttl,ip.proto,ip.checksum,ip.src,ip.dst,tcp.hdr_len,tcp.len,tcp.srcport,tcp.dstport,tcp.seq,tcp.ack,tcp.flags.ns,tcp.flags.fin,tcp.window_size_value,tcp.checksum,tcp.urgent_pointer,tcp.option_kind,tcp.option_len,tcp.options.timestamp.tsval,tcp.options.timestamp.tsecr,udp.srcport,udp.dstport,udp.length,udp.checksum,gquic.puflags.rsv,gquic.packet_number,location\n0,1,\"Apr 30, 2019 19:07:17.184823000 CDT\",886,68,4,1,6,98:f1:70:7c:4b:27,0000,0x00000800,20.0,0.0,870.0,0x00004405,0.0,64.0,6.0,0x00001eb4,10.145.31.196,162.125.8.7,32.0,818.0,59576.0,443.0,1.0,1.0,0.0,0.0,406.0,0x000016e0,0.0,\"1,1,8\",10,19415791.0,4190900276.0,,,,,,,EER\n1,2,\"Apr 30, 2019 19:07:17.207030000 CDT\",68,68,0,1,6,00:6c:bc:1c:5f:b9,0000,0x00000800,20.0,0.0,52.0,0x00001556,0.0,55.0,6.0,0x0000594d,162.125.8.7,10.145.31.196,32.0,0.0,443.0,59576.0,1.0,819.0,0.0,0.0,66.0,0x0000ce00,0.0,\"1,1,8\",10,4190904398.0,19415791.0,,,,,,,EER\n2,3,\"Apr 30, 2019 19:07:17.338454000 CDT\",550,68,0,1,6,00:6c:bc:1c:5f:b9,0000,0x00000800,20.0,0.0,534.0,0x00001557,0.0,55.0,6.0,0x0000576a,162.125.8.7,10.145.31.196,32.0,482.0,443.0,59576.0,1.0,819.0,0.0,0.0,66.0,0x00001382,0.0,\"1,1,8\",10,4190904525.0,19415791.0,,,,,,,EER\n3,4,\"Apr 30, 2019 19:07:17.338645000 CDT\",102,68,0,1,6,00:6c:bc:1c:5f:b9,0000,0x00000800,20.0,0.0,86.0,0x00001558,0.0,55.0,6.0,0x00005929,162.125.8.7,10.145.31.196,32.0,34.0,443.0,59576.0,483.0,819.0,0.0,0.0,66.0,0x00005a59,0.0,\"1,1,8\",10,4190904525.0,19415791.0,,,,,,,EER\n4,5,\"Apr 30, 2019 19:07:17.340360000 CDT\",68,68,4,1,6,98:f1:70:7c:4b:27,0000,0x00000800,20.0,0.0,52.0,0x00004406,0.0,64.0,6.0,0x000021e5,10.145.31.196,162.125.8.7,32.0,0.0,59576.0,443.0,819.0,517.0,0.0,0.0,415.0,0x0000d4ff,0.0,\"1,1,8\",10,19415838.0,4190904525.0,,,,,,,EER\n5,6,\"Apr 30, 2019 19:07:17.398784000 CDT\",80,44,4,1,6,98:f1:70:7c:4b:27,0000,0x00000800,20.0,0.0,64.0,0x0000589f,0.0,64.0,17.0,0x00007e3c,10.145.31.196,128.83.185.41,,,,,,,,,,,,,,,,56035.0,53.0,44.0,0x00009d5d,,,EER\n6,7,\"Apr 30, 2019 19:07:17.399295000 CDT\",80,44,4,1,6,98:f1:70:7c:4b:27,0000,0x00000800,20.0,0.0,64.0,0x000058a0,0.0,64.0,17.0,0x00007e3b,10.145.31.196,128.83.185.41,,,,,,,,,,,,,,,,48733.0,53.0,44.0,0x00009158,,,EER\n7,8,\"Apr 30, 2019 19:07:17.400595000 CDT\",126,44,0,1,6,00:6c:bc:1c:5f:b9,0000,0x00000800,20.0,0.0,110.0,0x0000f478,0.0,62.0,17.0,0x000023ed,128.83.185.41,10.145.31.196,,,,,,,,,,,,,,,,53.0,56035.0,90.0,0x0000c3e8,,,EER\n8,9,\"Apr 30, 2019 19:07:17.400908000 CDT\",126,44,0,1,6,00:6c:bc:1c:5f:b9,0000,0x00000800,20.0,0.0,110.0,0x0000f479,0.0,62.0,17.0,0x000023ec,128.83.185.41,10.145.31.196,,,,,,,,,,,,,,,,53.0,48733.0,90.0,0x0000b7e3,,,EER\n</code></pre></p>"},{"location":"datasets/install/utmobilenet21/#curation","title":"Curation","text":"<p>The curation process has the following objectives:</p> <ol> <li> <p>The CSVs can generate problem when loaded. For instance, some rows have broken formats (e.g., utilities such as <code>pandas.read_csv()</code> fail parsing), columns have missing values or mixed types (e.g., ports can be either int of floats).  So extra care is required to properly ingest the CSVs.</p> </li> <li> <p>The CSVs contains packets with protocols other than TCP/UDP. These need to be filtered out.</p> </li> <li> <p>As CSVs are per-packet, they neet to be reassembled into flows to obtain packet time series.</p> </li> <li> <p>Remove \"small data\". This include filtering out data     based on the following rules:</p> <ul> <li> <p>Remove flows with &lt; 10 samples.</p> </li> <li> <p>Remove apps generating &lt; 100 samples.</p> </li> </ul> </li> </ol> <p>The final monolithic parquet files has the following columns</p> Field Description <code>row_id</code> A unique flow id <code>src_ip</code> The source ip of the flow <code>src_port</code> The source port of the flow <code>dst_ip</code> The destination ip of the flow <code>dst_port</code> The destination port of the flow <code>ip_proto</code> The protocol of the flow (TCP or UDP) <code>first</code> Timestamp of the first packet <code>last</code> Timestamp of the last packet <code>duration</code> Duration of the flow <code>packets</code> Number of packets in the flow <code>bytes</code> Number of bytes in the flow <code>partition</code> From which folder the flow was originally stored <code>location</code> A label originally provided by the dataset (see the related paper for details) <code>fname</code> The original filename where the packets of the flow come from <code>app</code> The final label of the flow, encoded as pandas <code>category</code> <code>pkts_size</code> The numpy array for the packet size time series <code>pkts_dir</code> The numpy array for the packet diretion time series <code>timetofirst</code> The numpy array for the delta between the each packet timestamp the first packet of the flow"},{"location":"datasets/install/utmobilenet21/#splits","title":"Splits","text":"<p>Once preprocessed, the monolithic dataset is further processed to: define five 80/10/10 train/val/test splits with the following logic</p> <ol> <li>Shuffle the rows.</li> <li>Perform a 90/10 split where the 10-part is used for testing.</li> <li>From the 90-part, do a second 90/10 to define train and validation.</li> </ol> <p>The splits are NOT materialized, i.e.,  splits are a collection of row indexes that needs to be applied on the filtered monolithic parquet in order to obtain the data for modeling</p> <p>The structure of the split table is</p> Field Description <code>train_indexes</code> A numpy array with the <code>row_id</code> related to the train split <code>val_indexes</code> A numpy array with the <code>row_id</code> related to the validation split <code>test_indexes</code> A numpy array with the <code>row_id</code> related to the test split <code>split_index</code> The index of the split (0..4)"},{"location":"datasets/install/utmobilenet21/#install","title":"Install","text":"<p>The dataset zip archive is stored in a box.com folder. You need to manually download the archive and save it into a local folder, e.g., <code>/downloads</code></p> <p>To trigger the installation run the following <pre><code>tcbench datasets install \\\n    --name utmobilenet21 \\\n    --input-folder downloads/\n</code></pre></p> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502unpack\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nopening: downloads/UTMobileNet2021.zip\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502preprocess\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nprocessing: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Action-Specific Wild Test Data\nfound 43 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43/43 0:01:15\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/action-specific_wild_test_data.parquet\n\nprocessing: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Wild Test Data\nfound 14 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14/14 0:03:12\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/wild_test_data.parquet\n\nprocessing: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Randomized Automated Data\nfound 288 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 288/288 0:01:35\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/randomized_automated_data.parquet\n\nprocessing: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/raw/Deterministic Automated Data\nfound 3438 files\nConverting CSVs... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3438/3438 0:08:26\nstage1 completed\nstage2 completed\nstage3 completed\nstage4 completed\nsaving: /tmp/processing-utmobilenet21/deterministic_automated_data.parquet\nmerging all partitions\nsaving: ./envs/tcbench/lib/python3.10/site-packages/libtcdatasets/datasets/utmobilenet21/preprocessed/utmobilenet21.parquet\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502filter &amp; generate splits\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nloading: ./envs/super-tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/utmobilenet21.parquet\nsamples count : unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502    5591 \u2502\n\u2502 reddit       \u2502    4370 \u2502\n\u2502 google-maps  \u2502    4347 \u2502\n\u2502 spotify      \u2502    2550 \u2502\n\u2502 netflix      \u2502    2237 \u2502\n\u2502 pinterest    \u2502    2165 \u2502\n\u2502 hulu         \u2502    1839 \u2502\n\u2502 instagram    \u2502    1778 \u2502\n\u2502 dropbox      \u2502    1752 \u2502\n\u2502 facebook     \u2502    1654 \u2502\n\u2502 twitter      \u2502    1494 \u2502\n\u2502 gmail        \u2502    1133 \u2502\n\u2502 pandora      \u2502     949 \u2502\n\u2502 messenger    \u2502     837 \u2502\n\u2502 google-drive \u2502     803 \u2502\n\u2502 hangout      \u2502     720 \u2502\n\u2502 skype        \u2502     159 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502   34378 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (unfiltered)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502   34378.0 \u2502\n\u2502 mean  \u2502    663.96 \u2502\n\u2502 std   \u2502  18455.95 \u2502\n\u2502 min   \u2502       1.0 \u2502\n\u2502 25%   \u2502       2.0 \u2502\n\u2502 50%   \u2502       2.0 \u2502\n\u2502 75%   \u2502      18.0 \u2502\n\u2502 max   \u2502 1973657.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: ./envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23/utmobilenet21_filtered_minpkts10.parquet\nsamples count : filtered (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502    2496 \u2502\n\u2502 google-maps  \u2502    1798 \u2502\n\u2502 hulu         \u2502    1169 \u2502\n\u2502 reddit       \u2502     816 \u2502\n\u2502 spotify      \u2502     664 \u2502\n\u2502 netflix      \u2502     483 \u2502\n\u2502 pinterest    \u2502     436 \u2502\n\u2502 twitter      \u2502     365 \u2502\n\u2502 instagram    \u2502     274 \u2502\n\u2502 hangout      \u2502     254 \u2502\n\u2502 dropbox      \u2502     238 \u2502\n\u2502 pandora      \u2502     200 \u2502\n\u2502 facebook     \u2502     137 \u2502\n\u2502 google-drive \u2502     130 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502    9460 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nstats : number packets per-flow (min_pkts=10)\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 stat  \u2503     value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 count \u2502    9460.0 \u2502\n\u2502 mean  \u2502   2366.32 \u2502\n\u2502 std   \u2502  35109.17 \u2502\n\u2502 min   \u2502      11.0 \u2502\n\u2502 25%   \u2502      25.0 \u2502\n\u2502 50%   \u2502      51.0 \u2502\n\u2502 75%   \u2502     182.0 \u2502\n\u2502 max   \u2502 1973657.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nsaving: ./envs/tcbench-johndoe/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/utmobilenet21/preprocessed/imc23/utmobilenet21_filtered_minpkts10_splits.parquet\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502          2021 \u2502         225 \u2502          250 \u2502        2496 \u2502\n\u2502 google-maps  \u2502          1456 \u2502         162 \u2502          180 \u2502        1798 \u2502\n\u2502 hulu         \u2502           947 \u2502         105 \u2502          117 \u2502        1169 \u2502\n\u2502 reddit       \u2502           661 \u2502          73 \u2502           82 \u2502         816 \u2502\n\u2502 spotify      \u2502           538 \u2502          60 \u2502           66 \u2502         664 \u2502\n\u2502 netflix      \u2502           391 \u2502          44 \u2502           48 \u2502         483 \u2502\n\u2502 pinterest    \u2502           353 \u2502          39 \u2502           44 \u2502         436 \u2502\n\u2502 twitter      \u2502           296 \u2502          33 \u2502           36 \u2502         365 \u2502\n\u2502 instagram    \u2502           222 \u2502          25 \u2502           27 \u2502         274 \u2502\n\u2502 hangout      \u2502           206 \u2502          23 \u2502           25 \u2502         254 \u2502\n\u2502 dropbox      \u2502           193 \u2502          21 \u2502           24 \u2502         238 \u2502\n\u2502 pandora      \u2502           162 \u2502          18 \u2502           20 \u2502         200 \u2502\n\u2502 facebook     \u2502           111 \u2502          12 \u2502           14 \u2502         137 \u2502\n\u2502 google-drive \u2502           105 \u2502          12 \u2502           13 \u2502         130 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502          7662 \u2502         852 \u2502          946 \u2502        9460 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The console output is showing a few samples count reports related to the processing performed on the datasets</p> <ol> <li> <p>The first report relates to the unfiltered dataset, i.e.,  the monolithic parquet files obtained consolidating all JSON files but before applying any curation. At first glance, it looks like this dataset has a lot of flows. However, the following report shows the number of packets per flow and suggests that there are many flows which are very short.</p> </li> <li> <p>The second and third group of reports show similar information to the first group but relates to the filtering out of flows with less than 10 packets.</p> </li> <li> <p>The last report shows the number of train/validation/test samples by each application for the first split (the same counters are true for all splits).</p> </li> </ol>"},{"location":"datasets/samples_count/","title":"Samples count report","text":"<p>An important dataset property to keep an eye on when aiming for modeling is the number of  samples for each class available in the datasets.</p> <p>You can easily recover this using the <code>datasets samples-count</code> subcommand.</p> <p>For instance,  the following command computes the samples count for the unfitered  version of the <code>ucdavis-icdm19</code> dataset.</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>While to obtain the breakdown of the first train split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split 0\n</code></pre> <p>Output</p> <pre><code>filtered, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>...or the <code>human</code> test split</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split human\n</code></pre> <p>Output</p> <pre><code>filtered, split: human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/mirage19/","title":"<code>mirage19</code>","text":"<p>Below we report the samples count for each version of the dataset.</p> <p>Semantic of the splits</p> <p>The split available for this datasets relate to our  IMC23 paper.</p>"},{"location":"datasets/samples_count/mirage19/#unfiltered","title":"unfiltered","text":"<p>The unfitered version contains all data before curation.</p> <pre><code>tcbench datasets samples-count --name mirage19\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.waze                    \u2502   11737 \u2502\n\u2502 de.motain.iliga             \u2502   10810 \u2502\n\u2502 com.accuweather.android     \u2502   10631 \u2502\n\u2502 com.duolingo                \u2502    8319 \u2502\n\u2502 it.subito                   \u2502    8167 \u2502\n\u2502 com.contextlogic.wish       \u2502    6507 \u2502\n\u2502 com.spotify.music           \u2502    6431 \u2502\n\u2502 com.joelapenna.foursquared  \u2502    6399 \u2502\n\u2502 com.google.android.youtube  \u2502    6346 \u2502\n\u2502 com.iconology.comics        \u2502    5516 \u2502\n\u2502 com.facebook.katana         \u2502    5368 \u2502\n\u2502 com.dropbox.android         \u2502    4815 \u2502\n\u2502 com.twitter.android         \u2502    4734 \u2502\n\u2502 background                  \u2502    4439 \u2502\n\u2502 com.pinterest               \u2502    4078 \u2502\n\u2502 com.facebook.orca           \u2502    4018 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502    3572 \u2502\n\u2502 air.com.hypah.io.slither    \u2502    3088 \u2502\n\u2502 com.viber.voip              \u2502    2740 \u2502\n\u2502 com.trello                  \u2502    2306 \u2502\n\u2502 com.groupon                 \u2502    1986 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502  122007 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/mirage19/#first-training-split","title":"First training split","text":"<pre><code>tcbench datasets samples-count --name mirage19 --split 0\n</code></pre> <p>Output</p> <pre><code>min_pkts: 10, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                         \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 de.motain.iliga             \u2502          6079 \u2502         675 \u2502          751 \u2502        7505 \u2502\n\u2502 com.waze                    \u2502          5844 \u2502         649 \u2502          721 \u2502        7214 \u2502\n\u2502 com.duolingo                \u2502          3712 \u2502         413 \u2502          458 \u2502        4583 \u2502\n\u2502 it.subito                   \u2502          3482 \u2502         387 \u2502          430 \u2502        4299 \u2502\n\u2502 com.contextlogic.wish       \u2502          3181 \u2502         353 \u2502          393 \u2502        3927 \u2502\n\u2502 com.accuweather.android     \u2502          3027 \u2502         336 \u2502          374 \u2502        3737 \u2502\n\u2502 com.joelapenna.foursquared  \u2502          2938 \u2502         326 \u2502          363 \u2502        3627 \u2502\n\u2502 com.spotify.music           \u2502          2673 \u2502         297 \u2502          330 \u2502        3300 \u2502\n\u2502 com.dropbox.android         \u2502          2583 \u2502         287 \u2502          319 \u2502        3189 \u2502\n\u2502 com.facebook.katana         \u2502          2331 \u2502         259 \u2502          288 \u2502        2878 \u2502\n\u2502 com.iconology.comics        \u2502          2278 \u2502         253 \u2502          281 \u2502        2812 \u2502\n\u2502 com.twitter.android         \u2502          2272 \u2502         252 \u2502          281 \u2502        2805 \u2502\n\u2502 com.google.android.youtube  \u2502          2209 \u2502         246 \u2502          273 \u2502        2728 \u2502\n\u2502 com.pinterest               \u2502          1984 \u2502         221 \u2502          245 \u2502        2450 \u2502\n\u2502 com.tripadvisor.tripadvisor \u2502          1662 \u2502         185 \u2502          205 \u2502        2052 \u2502\n\u2502 com.facebook.orca           \u2502          1444 \u2502         161 \u2502          178 \u2502        1783 \u2502\n\u2502 com.viber.voip              \u2502          1310 \u2502         146 \u2502          162 \u2502        1618 \u2502\n\u2502 com.trello                  \u2502          1197 \u2502         133 \u2502          148 \u2502        1478 \u2502\n\u2502 com.groupon                 \u2502           951 \u2502         106 \u2502          117 \u2502        1174 \u2502\n\u2502 air.com.hypah.io.slither    \u2502           821 \u2502          91 \u2502          101 \u2502        1013 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                   \u2502         51978 \u2502        5776 \u2502         6418 \u2502       64172 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/mirage22/","title":"<code>mirage22</code>","text":"<p>Below we report the samples count for each version of the dataset.</p> <p>Semantic of the splits</p> <p>The split available for this datasets relate to our  IMC23 paper.</p>"},{"location":"datasets/samples_count/mirage22/#unfiltered","title":"unfiltered","text":"<p>The unfitered version contains all data before curation.</p> <pre><code>tcbench datasets samples-count --name mirage22\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 background                       \u2502   18882 \u2502\n\u2502 com.microsoft.teams              \u2502    6541 \u2502\n\u2502 com.skype.raider                 \u2502    6203 \u2502\n\u2502 us.zoom.videomeetings            \u2502    5066 \u2502\n\u2502 com.cisco.webex.meetings         \u2502    4789 \u2502\n\u2502 com.discord                      \u2502    4337 \u2502\n\u2502 com.facebook.orca                \u2502    4321 \u2502\n\u2502 com.gotomeeting                  \u2502    3695 \u2502\n\u2502 com.Slack                        \u2502    2985 \u2502\n\u2502 com.google.android.apps.meetings \u2502    2252 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502   59071 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/mirage22/#first-training-split-min_pkts10","title":"First training split @ <code>min_pkts=10</code>","text":"<pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 10 --split 0\n</code></pre> <p>Output</p> <pre><code>min_pkts: 10, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.cisco.webex.meetings         \u2502          3594 \u2502         399 \u2502          444 \u2502        4437 \u2502\n\u2502 com.skype.raider                 \u2502          3334 \u2502         371 \u2502          412 \u2502        4117 \u2502\n\u2502 com.microsoft.teams              \u2502          3124 \u2502         347 \u2502          386 \u2502        3857 \u2502\n\u2502 us.zoom.videomeetings            \u2502          2905 \u2502         323 \u2502          359 \u2502        3587 \u2502\n\u2502 com.discord                      \u2502          2743 \u2502         305 \u2502          339 \u2502        3387 \u2502\n\u2502 com.facebook.orca                \u2502          2125 \u2502         236 \u2502          262 \u2502        2623 \u2502\n\u2502 com.gotomeeting                  \u2502          2072 \u2502         230 \u2502          255 \u2502        2557 \u2502\n\u2502 com.google.android.apps.meetings \u2502          1002 \u2502         112 \u2502          124 \u2502        1238 \u2502\n\u2502 com.Slack                        \u2502           786 \u2502          87 \u2502           97 \u2502         970 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502         21685 \u2502        2410 \u2502         2678 \u2502       26773 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/mirage22/#first-training-split-min_pkts1000","title":"First training split @ <code>min_pkts=1000</code>","text":"<pre><code>tcbench datasets samples-count --name mirage22 --min-pkts 1000 --split 0\n</code></pre> <p>Output</p> <pre><code>min_pkts: 1000, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app                              \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 com.discord                      \u2502          1798 \u2502         200 \u2502          222 \u2502        2220 \u2502\n\u2502 us.zoom.videomeetings            \u2502           344 \u2502          39 \u2502           42 \u2502         425 \u2502\n\u2502 com.google.android.apps.meetings \u2502           307 \u2502          34 \u2502           38 \u2502         379 \u2502\n\u2502 com.microsoft.teams              \u2502           260 \u2502          29 \u2502           32 \u2502         321 \u2502\n\u2502 com.gotomeeting                  \u2502           240 \u2502          27 \u2502           30 \u2502         297 \u2502\n\u2502 com.facebook.orca                \u2502           227 \u2502          25 \u2502           28 \u2502         280 \u2502\n\u2502 com.cisco.webex.meetings         \u2502           210 \u2502          23 \u2502           26 \u2502         259 \u2502\n\u2502 com.Slack                        \u2502           160 \u2502          18 \u2502           20 \u2502         198 \u2502\n\u2502 com.skype.raider                 \u2502           154 \u2502          17 \u2502           19 \u2502         190 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__                        \u2502          3700 \u2502         412 \u2502          457 \u2502        4569 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/ucdavis-icdm19/","title":"<code>ucdavis-icdm19</code>","text":"<p>Below we report the samples count for each version of the dataset.</p> <p>Semantic of the splits</p> <p>The split available for this datasets relate to our  IMC23 paper.</p>"},{"location":"datasets/samples_count/ucdavis-icdm19/#unfiltered","title":"unfiltered","text":"<p>The unfitered version contains all data before curation.</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 partition                   \u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 pretraining                 \u2502 google-doc    \u2502    1221 \u2502\n\u2502                             \u2502 google-drive  \u2502    1634 \u2502\n\u2502                             \u2502 google-music  \u2502     592 \u2502\n\u2502                             \u2502 google-search \u2502    1915 \u2502\n\u2502                             \u2502 youtube       \u2502    1077 \u2502\n\u2502                             \u2502 __total__     \u2502    6439 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-human-triggered  \u2502 google-doc    \u2502      15 \u2502\n\u2502                             \u2502 google-drive  \u2502      18 \u2502\n\u2502                             \u2502 google-music  \u2502      15 \u2502\n\u2502                             \u2502 google-search \u2502      15 \u2502\n\u2502                             \u2502 youtube       \u2502      20 \u2502\n\u2502                             \u2502 __total__     \u2502      83 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 retraining-script-triggered \u2502 google-doc    \u2502      30 \u2502\n\u2502                             \u2502 google-drive  \u2502      30 \u2502\n\u2502                             \u2502 google-music  \u2502      30 \u2502\n\u2502                             \u2502 google-search \u2502      30 \u2502\n\u2502                             \u2502 youtube       \u2502      30 \u2502\n\u2502                             \u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/ucdavis-icdm19/#first-training-split","title":"First training split","text":"<pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split 0\n</code></pre> <p>Output</p> <pre><code>filtered, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502     100 \u2502\n\u2502 google-drive  \u2502     100 \u2502\n\u2502 google-music  \u2502     100 \u2502\n\u2502 google-search \u2502     100 \u2502\n\u2502 youtube       \u2502     100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     500 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/ucdavis-icdm19/#human-test-split","title":"<code>human</code> test split","text":"<p>This is equivalent to the <code>human</code> partition of the unfiltered dataset.</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split human\n</code></pre> <p>Output</p> <pre><code>filtered, split: human\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube       \u2502      20 \u2502\n\u2502 google-drive  \u2502      18 \u2502\n\u2502 google-doc    \u2502      15 \u2502\n\u2502 google-music  \u2502      15 \u2502\n\u2502 google-search \u2502      15 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502      83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/ucdavis-icdm19/#script-test-split","title":"<code>script</code> test split","text":"<p>This is equivalent to the <code>script</code> partition of the unfiltered dataset.</p> <pre><code>tcbench datasets samples-count --name ucdavis-icdm19 --split script\n</code></pre> <p>Output</p> <pre><code>filtered, split: script\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app           \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 google-doc    \u2502      30 \u2502\n\u2502 google-drive  \u2502      30 \u2502\n\u2502 google-music  \u2502      30 \u2502\n\u2502 google-search \u2502      30 \u2502\n\u2502 youtube       \u2502      30 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__     \u2502     150 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/utmobilenet21/","title":"<code>utmobilenet21</code>","text":"<p>Below we report the samples count for each version of the dataset.</p> <p>Semantic of the splits</p> <p>The split available for this datasets relate to our  IMC23 paper.</p>"},{"location":"datasets/samples_count/utmobilenet21/#unfiltered","title":"unfiltered","text":"<p>The unfitered version contains all data before curation.</p> <pre><code>tcbench datasets samples-count --name utmobilenet21\n</code></pre> <p>Output</p> <pre><code>unfiltered\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502    5591 \u2502\n\u2502 reddit       \u2502    4370 \u2502\n\u2502 google-maps  \u2502    4347 \u2502\n\u2502 spotify      \u2502    2550 \u2502\n\u2502 netflix      \u2502    2237 \u2502\n\u2502 pinterest    \u2502    2165 \u2502\n\u2502 hulu         \u2502    1839 \u2502\n\u2502 instagram    \u2502    1778 \u2502\n\u2502 dropbox      \u2502    1752 \u2502\n\u2502 facebook     \u2502    1654 \u2502\n\u2502 twitter      \u2502    1494 \u2502\n\u2502 gmail        \u2502    1133 \u2502\n\u2502 pandora      \u2502     949 \u2502\n\u2502 messenger    \u2502     837 \u2502\n\u2502 google-drive \u2502     803 \u2502\n\u2502 hangout      \u2502     720 \u2502\n\u2502 skype        \u2502     159 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502   34378 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/samples_count/utmobilenet21/#first-training-split","title":"First training split","text":"<pre><code>tcbench datasets samples-count --name utmobilenet21 --split 0\n</code></pre> <p>Output</p> <pre><code>min_pkts: 10, split: 0\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 app          \u2503 train_samples \u2503 val_samples \u2503 test_samples \u2503 all_samples \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 youtube      \u2502          2021 \u2502         225 \u2502          250 \u2502        2496 \u2502\n\u2502 google-maps  \u2502          1456 \u2502         162 \u2502          180 \u2502        1798 \u2502\n\u2502 hulu         \u2502           947 \u2502         105 \u2502          117 \u2502        1169 \u2502\n\u2502 reddit       \u2502           661 \u2502          73 \u2502           82 \u2502         816 \u2502\n\u2502 spotify      \u2502           538 \u2502          60 \u2502           66 \u2502         664 \u2502\n\u2502 netflix      \u2502           391 \u2502          44 \u2502           48 \u2502         483 \u2502\n\u2502 pinterest    \u2502           353 \u2502          39 \u2502           44 \u2502         436 \u2502\n\u2502 twitter      \u2502           296 \u2502          33 \u2502           36 \u2502         365 \u2502\n\u2502 instagram    \u2502           222 \u2502          25 \u2502           27 \u2502         274 \u2502\n\u2502 hangout      \u2502           206 \u2502          23 \u2502           25 \u2502         254 \u2502\n\u2502 dropbox      \u2502           193 \u2502          21 \u2502           24 \u2502         238 \u2502\n\u2502 pandora      \u2502           162 \u2502          18 \u2502           20 \u2502         200 \u2502\n\u2502 facebook     \u2502           111 \u2502          12 \u2502           14 \u2502         137 \u2502\n\u2502 google-drive \u2502           105 \u2502          12 \u2502           13 \u2502         130 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 __total__    \u2502          7662 \u2502         852 \u2502          946 \u2502        9460 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/schemas/","title":"Datasets schemas","text":"<p>Despite the curation, datasets can have intrinsically different schemas.</p> <p>You can investigate those on the command line via the <code>datasets schema</code> sub-command.</p> <pre><code>tcbench datasets schema --help\n\n Usage: tcbench datasets schema [OPTIONS]\n\n Show datasets schemas\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --name  -n  [ucdavis-icdm19|utmobilenet21|mirage19|mirage22]  Dataset to install                                         \u2502\n\u2502 --type  -t  [unfiltered|filtered|splits]                      Schema type (unfiltered: original raw data; filtered:      \u2502\n\u2502                                                               curated data; splits: train/val/test splits)               \u2502\n\u2502 --help                                                        Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Beside the dataset name <code>--name</code>, the selection of the schema is simplified via a single parameter <code>--type</code> which matches the parquet files as follows</p> <ul> <li> <p><code>\"unfiltered\"</code> corresponds to the monolithic  before any filtering (i.e., the files under <code>/preprocessed</code>)</p> </li> <li> <p><code>\"filtered\"</code> corresponds to the filtered  version of the monolithic files (i.e., the files having <code>minpkts&lt;N&gt;</code> in the filename).</p> </li> <li> <p><code>\"splits\"</code> corresponds to the split files (i.e., the files having <code>xyz_split.parquet</code> in the filename).</p> </li> </ul>"},{"location":"datasets/schemas/mirage19/","title":"<code>mirage19</code>","text":"<p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p> tcbench datasets schema --name mirage19 <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 float    \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 float    \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 float    \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 float    \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 float    \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 float    \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 float    \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 float    \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 float    \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protocol                                                \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 float    \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage19 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/schemas/mirage22/","title":"<code>mirage22</code>","text":"<p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p> tcbench datasets schema --name mirage22 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                                                     \u2503 Dtype    \u2503 Description                                                \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                                                    \u2502 int      \u2502 Unique flow id                                             \u2502\n\u2502 conn_id                                                   \u2502 str      \u2502 Flow 5-tuple                                               \u2502\n\u2502 packet_data_timestamp                                     \u2502 np.array \u2502 Time series of packet unixtime                             \u2502\n\u2502 packet_data_src_port                                      \u2502 np.array \u2502 Time series of the source ports                            \u2502\n\u2502 packet_data_dst_port                                      \u2502 np.array \u2502 Time series of the destination ports                       \u2502\n\u2502 packet_data_packet_dir                                    \u2502 np.array \u2502 Time series of pkts direction (0 or 1)                     \u2502\n\u2502 packet_data_ip_packet_bytes                               \u2502 np.array \u2502 Time series pkts bytes (as from IP len field)              \u2502\n\u2502 packet_data_ip_header_bytes                               \u2502 np.array \u2502 Time series of IP header bytes                             \u2502\n\u2502 packet_data_l4_payload_bytes                              \u2502 np.array \u2502 Time series of payload pkts size                           \u2502\n\u2502 packet_data_l4_header_bytes                               \u2502 np.array \u2502 Time series of L4 header bytes                             \u2502\n\u2502 packet_data_iat                                           \u2502 np.array \u2502 Time series of pkts inter arrival times                    \u2502\n\u2502 packet_data_tcp_win_size                                  \u2502 np.array \u2502 Time series of TCP window size                             \u2502\n\u2502 packet_data_tcp_flags                                     \u2502 np.array \u2502 Time series of TCP flags                                   \u2502\n\u2502 packet_data_l4_raw_payload                                \u2502 np.array \u2502 List of list with each packet payload                      \u2502\n\u2502 packet_data_is_clear                                      \u2502 np.array \u2502 n.a.                                                       \u2502\n\u2502 packet_data_heuristic                                     \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 packet_data_annotations                                   \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_features_packet_length_biflow_min                    \u2502 float    \u2502 Bidirectional min frame (i.e., pkt with headers) size      \u2502\n\u2502 flow_features_packet_length_biflow_max                    \u2502 float    \u2502 Bidirectional max frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_mean                   \u2502 float    \u2502 Bidirectional mean frame size                              \u2502\n\u2502 flow_features_packet_length_biflow_std                    \u2502 float    \u2502 Bidirectional std frame size                               \u2502\n\u2502 flow_features_packet_length_biflow_var                    \u2502 float    \u2502 Bidirectional variance frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_mad                    \u2502 float    \u2502 Bidirectional median absolute deviation frame size         \u2502\n\u2502 flow_features_packet_length_biflow_skew                   \u2502 float    \u2502 Bidirection skew frame size                                \u2502\n\u2502 flow_features_packet_length_biflow_kurtosis               \u2502 float    \u2502 Bidirectional kurtosi frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_10_percentile          \u2502 float    \u2502 Bidirection 10%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_20_percentile          \u2502 float    \u2502 Bidirection 20%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_30_percentile          \u2502 float    \u2502 Bidirection 30%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_40_percentile          \u2502 float    \u2502 Bidirection 40%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_50_percentile          \u2502 float    \u2502 Bidirection 50%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_60_percentile          \u2502 float    \u2502 Bidirection 60%-le of frame size                           \u2502\n\u2502 flow_features_packet_length_biflow_70_percentile          \u2502 float    \u2502 Bidirection 70%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_80_percentile          \u2502 float    \u2502 Bidirection 80%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_biflow_90_percentile          \u2502 float    \u2502 Bidirection 90%-ile of frame size                          \u2502\n\u2502 flow_features_packet_length_upstream_flow_min             \u2502 float    \u2502 Upstream min frame (i.e., pkt with headers) size           \u2502\n\u2502 flow_features_packet_length_upstream_flow_max             \u2502 float    \u2502 Upstream max frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_mean            \u2502 float    \u2502 Upstream mean frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_std             \u2502 float    \u2502 Upstream std frame size                                    \u2502\n\u2502 flow_features_packet_length_upstream_flow_var             \u2502 float    \u2502 Upstream variance frame size                               \u2502\n\u2502 flow_features_packet_length_upstream_flow_mad             \u2502 float    \u2502 Upstream median absolute deviation frame size              \u2502\n\u2502 flow_features_packet_length_upstream_flow_skew            \u2502 float    \u2502 Upstream skew frame size                                   \u2502\n\u2502 flow_features_packet_length_upstream_flow_kurtosis        \u2502 float    \u2502 Upstream kurtosi frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_10_percentile   \u2502 float    \u2502 Upstream 10%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_20_percentile   \u2502 float    \u2502 Upstream 20%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_30_percentile   \u2502 float    \u2502 Upstream 30%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_40_percentile   \u2502 float    \u2502 Upstream 40%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_50_percentile   \u2502 float    \u2502 Upstream 50%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_60_percentile   \u2502 float    \u2502 Upstream 60%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_70_percentile   \u2502 float    \u2502 Upstream 70%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_80_percentile   \u2502 float    \u2502 Upstream 80%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_upstream_flow_90_percentile   \u2502 float    \u2502 Upstream 90%-ile frame size                                \u2502\n\u2502 flow_features_packet_length_downstream_flow_min           \u2502 float    \u2502 Downstream min frame (i.e., pkt with headers) size         \u2502\n\u2502 flow_features_packet_length_downstream_flow_max           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_mean          \u2502 float    \u2502 Downstream mean frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_std           \u2502 float    \u2502 Downstream std frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_var           \u2502 float    \u2502 Downstream variance frame size                             \u2502\n\u2502 flow_features_packet_length_downstream_flow_mad           \u2502 float    \u2502 Downstream max frame size                                  \u2502\n\u2502 flow_features_packet_length_downstream_flow_skew          \u2502 float    \u2502 Downstream skew frame size                                 \u2502\n\u2502 flow_features_packet_length_downstream_flow_kurtosis      \u2502 float    \u2502 Downstream kurtosi frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_10_percentile \u2502 float    \u2502 Downstream 10%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_20_percentile \u2502 float    \u2502 Downstream 20%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_30_percentile \u2502 float    \u2502 Downstream 30%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_40_percentile \u2502 float    \u2502 Downstream 40%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_50_percentile \u2502 float    \u2502 Downstream 50%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_60_percentile \u2502 float    \u2502 Downstream 60%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_70_percentile \u2502 float    \u2502 Downstream 70%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_80_percentile \u2502 float    \u2502 Downstream 80%-ile frame size                              \u2502\n\u2502 flow_features_packet_length_downstream_flow_90_percentile \u2502 float    \u2502 Downstream 90%-ile frame size                              \u2502\n\u2502 flow_features_iat_biflow_min                              \u2502 float    \u2502 Bidirectional min inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_max                              \u2502 float    \u2502 Bidirectional max inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_mean                             \u2502 float    \u2502 Bidirectional mean inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_std                              \u2502 float    \u2502 Bidirectional std inter arrival time                       \u2502\n\u2502 flow_features_iat_biflow_var                              \u2502 float    \u2502 Bidirectional variance inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_mad                              \u2502 float    \u2502 Bidirectional median absolute deviation inter arrival time \u2502\n\u2502 flow_features_iat_biflow_skew                             \u2502 float    \u2502 Bidirectional skew inter arrival time                      \u2502\n\u2502 flow_features_iat_biflow_kurtosis                         \u2502 float    \u2502 Bidirectional kurtosi inter arrival time                   \u2502\n\u2502 flow_features_iat_biflow_10_percentile                    \u2502 float    \u2502 Bidirectional 10%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_20_percentile                    \u2502 float    \u2502 Bidirectional 20%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_30_percentile                    \u2502 float    \u2502 Bidirectional 30%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_40_percentile                    \u2502 float    \u2502 Bidirectional 40%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_50_percentile                    \u2502 float    \u2502 Bidirectional 50%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_60_percentile                    \u2502 float    \u2502 Bidirectional 60%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_70_percentile                    \u2502 float    \u2502 Bidirectional 70%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_80_percentile                    \u2502 float    \u2502 Bidirectional 80%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_biflow_90_percentile                    \u2502 float    \u2502 Bidirectional 90%-tile inter arrival time                  \u2502\n\u2502 flow_features_iat_upstream_flow_min                       \u2502 float    \u2502 Upstream min inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_max                       \u2502 float    \u2502 Upstream max inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_mean                      \u2502 float    \u2502 Upstream avg inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_std                       \u2502 float    \u2502 Upstream std inter arrival time                            \u2502\n\u2502 flow_features_iat_upstream_flow_var                       \u2502 float    \u2502 Upstream variance inter arrival time                       \u2502\n\u2502 flow_features_iat_upstream_flow_mad                       \u2502 float    \u2502 Upstream median absolute deviation inter arrival time      \u2502\n\u2502 flow_features_iat_upstream_flow_skew                      \u2502 float    \u2502 Upstream skew inter arrival time                           \u2502\n\u2502 flow_features_iat_upstream_flow_kurtosis                  \u2502 float    \u2502 Upstream kurtosi inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_10_percentile             \u2502 float    \u2502 Upstream 10%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_20_percentile             \u2502 float    \u2502 Upstream 20%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_30_percentile             \u2502 float    \u2502 Upstream 30%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_40_percentile             \u2502 float    \u2502 Upstream 40%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_50_percentile             \u2502 float    \u2502 Upstream 50%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_60_percentile             \u2502 float    \u2502 Upstream 60%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_70_percentile             \u2502 float    \u2502 Upstream 70%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_80_percentile             \u2502 float    \u2502 Upstream 80%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_upstream_flow_90_percentile             \u2502 float    \u2502 Upstream 90%-ile inter arrival time                        \u2502\n\u2502 flow_features_iat_downstream_flow_min                     \u2502 float    \u2502 Downstream min inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_max                     \u2502 float    \u2502 Downstream max inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_mean                    \u2502 float    \u2502 Downstream mean inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_std                     \u2502 float    \u2502 Downstream std inter arrival time                          \u2502\n\u2502 flow_features_iat_downstream_flow_var                     \u2502 float    \u2502 Downstream variance inter arrival time                     \u2502\n\u2502 flow_features_iat_downstream_flow_mad                     \u2502 float    \u2502 Downstream median absolute deviation inter arrival time    \u2502\n\u2502 flow_features_iat_downstream_flow_skew                    \u2502 float    \u2502 Downstream skew inter arrival time                         \u2502\n\u2502 flow_features_iat_downstream_flow_kurtosis                \u2502 float    \u2502 Downstream kurtosi inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_10_percentile           \u2502 float    \u2502 Downstream 10%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_20_percentile           \u2502 float    \u2502 Downstream 20%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_30_percentile           \u2502 float    \u2502 Downstream 30%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_40_percentile           \u2502 float    \u2502 Downstream 40%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_50_percentile           \u2502 float    \u2502 Downstream 50%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_60_percentile           \u2502 float    \u2502 Downstream 60%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_70_percentile           \u2502 float    \u2502 Downstream 70%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_80_percentile           \u2502 float    \u2502 Downstream 80%-ile inter arrival time                      \u2502\n\u2502 flow_features_iat_downstream_flow_90_percentile           \u2502 float    \u2502 Downstream 90%-ile inter arrival time                      \u2502\n\u2502 flow_metadata_bf_device                                   \u2502 str      \u2502 Ethernet address                                           \u2502\n\u2502 flow_metadata_bf_label_source                             \u2502 str      \u2502 Constant value 'netstate'                                  \u2502\n\u2502 flow_metadata_bf_label                                    \u2502 str      \u2502 original mirage label                                      \u2502\n\u2502 flow_metadata_bf_sublabel                                 \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_code                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_label_version_name                       \u2502 str      \u2502 n.a.                                                       \u2502\n\u2502 flow_metadata_bf_labeling_type                            \u2502 str      \u2502 exact=via netstat; most-common=via experiment              \u2502\n\u2502 flow_metadata_bf_num_packets                              \u2502 int      \u2502 Bidirectional number of pkts                               \u2502\n\u2502 flow_metadata_bf_ip_packet_bytes                          \u2502 int      \u2502 Bidirectional bytes (including headers)                    \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes                         \u2502 int      \u2502 Bidirectional payload bytes                                \u2502\n\u2502 flow_metadata_bf_duration                                 \u2502 float    \u2502 Bidirectional duration                                     \u2502\n\u2502 flow_metadata_uf_num_packets                              \u2502 int      \u2502 Upload number of pkts                                      \u2502\n\u2502 flow_metadata_uf_ip_packet_bytes                          \u2502 int      \u2502 Upload bytes (including headers)                           \u2502\n\u2502 flow_metadata_uf_l4_payload_bytes                         \u2502 int      \u2502 Upload payload bytes                                       \u2502\n\u2502 flow_metadata_uf_duration                                 \u2502 float    \u2502 Upload duration                                            \u2502\n\u2502 flow_metadata_uf_mss                                      \u2502 float    \u2502 Upload maximum segment size                                \u2502\n\u2502 flow_metadata_uf_ws                                       \u2502 float    \u2502 Upload window scaling                                      \u2502\n\u2502 flow_metadata_df_num_packets                              \u2502 int      \u2502 Download number of packets                                 \u2502\n\u2502 flow_metadata_df_ip_packet_bytes                          \u2502 int      \u2502 Download bytes (including headers)                         \u2502\n\u2502 flow_metadata_df_l4_payload_bytes                         \u2502 int      \u2502 Download payload bytes                                     \u2502\n\u2502 flow_metadata_df_duration                                 \u2502 float    \u2502 Download duration                                          \u2502\n\u2502 flow_metadata_df_mss                                      \u2502 float    \u2502 Download maximum segment size                              \u2502\n\u2502 flow_metadata_df_ws                                       \u2502 float    \u2502 Download window scaling                                    \u2502\n\u2502 flow_metadata_bf_activity                                 \u2502 str      \u2502 Experiment activity                                        \u2502\n\u2502 strings                                                   \u2502 list     \u2502 ASCII string extracted from payload                        \u2502\n\u2502 android_name                                              \u2502 str      \u2502 app name (based on filename)                               \u2502\n\u2502 device_name                                               \u2502 str      \u2502 device name (based on filename)                            \u2502\n\u2502 app                                                       \u2502 category \u2502 label (background|android app)                             \u2502\n\u2502 src_ip                                                    \u2502 str      \u2502 Source IP                                                  \u2502\n\u2502 src_port                                                  \u2502 str      \u2502 Source port                                                \u2502\n\u2502 dst_ip                                                    \u2502 str      \u2502 Destination IP                                             \u2502\n\u2502 dst_port                                                  \u2502 str      \u2502 Destination port                                           \u2502\n\u2502 proto                                                     \u2502 str      \u2502 L4 protol                                                  \u2502\n\u2502 packets                                                   \u2502 int      \u2502 Number of (bidirectional) packets                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field                             \u2503 Dtype    \u2503 Description                                                          \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id                            \u2502 int      \u2502 Unique flow id                                                       \u2502\n\u2502 conn_id                           \u2502 str      \u2502 Flow 5-tuple                                                         \u2502\n\u2502 packet_data_l4_raw_payload        \u2502 np.array \u2502 List of list with each packet payload                                \u2502\n\u2502 flow_metadata_bf_label            \u2502 str      \u2502 original mirage label                                                \u2502\n\u2502 flow_metadata_bf_activity         \u2502 str      \u2502 Experiment activity                                                  \u2502\n\u2502 flow_metadata_bf_labeling_type    \u2502 str      \u2502 exact=via netstat; most-common=via experiment                        \u2502\n\u2502 flow_metadata_bf_l4_payload_bytes \u2502 int      \u2502 Bidirectional payload bytes                                          \u2502\n\u2502 flow_metadata_bf_duration         \u2502 float    \u2502 Bidirectional duration                                               \u2502\n\u2502 strings                           \u2502 list     \u2502 ASCII string extracted from payload                                  \u2502\n\u2502 android_name                      \u2502 str      \u2502 app name (based on filename)                                         \u2502\n\u2502 device_name                       \u2502 str      \u2502 device name (based on filename)                                      \u2502\n\u2502 app                               \u2502 category \u2502 label (background|android app)                                       \u2502\n\u2502 src_ip                            \u2502 str      \u2502 Source IP                                                            \u2502\n\u2502 src_port                          \u2502 str      \u2502 Source port                                                          \u2502\n\u2502 dst_ip                            \u2502 str      \u2502 Destination IP                                                       \u2502\n\u2502 dst_port                          \u2502 str      \u2502 Destination port                                                     \u2502\n\u2502 proto                             \u2502 str      \u2502 L4 protocol                                                          \u2502\n\u2502 packets                           \u2502 int      \u2502 Number of (bidirectional) packets                                    \u2502\n\u2502 pkts_size                         \u2502 str      \u2502 Packet size time series                                              \u2502\n\u2502 pkts_dir                          \u2502 str      \u2502 Packet diretion time series                                          \u2502\n\u2502 timetofirst                       \u2502 str      \u2502 Delta between the each packet timestamp the first packet of the flow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name mirage22 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/schemas/ucdavis-icdm19/","title":"<code>ucdavis-icdm19</code>","text":"<p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p> <p>For <code>ucdavis-icdm19</code> the three types (unfiltered, filtered, splits) have the same schema because the splits are materialized.</p> <pre><code>tcbench datasets schema --name ucdavis-icdm19 --type unfiltered\n</code></pre> <p>Output</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type filtered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> tcbench datasets schema --name ucdavis-icdm19 --type splits <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                         \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique row id                                       \u2502\n\u2502 app         \u2502 category \u2502 Label of the flow                                   \u2502\n\u2502 flow_id     \u2502 str      \u2502 Original filename                                   \u2502\n\u2502 partition   \u2502 str      \u2502 Partition related to the flow                       \u2502\n\u2502 num_pkts    \u2502 int      \u2502 Number of packets in the flow                       \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes of the flow                         \u2502\n\u2502 unixtime    \u2502 str      \u2502 Absolute time of each packet                        \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between a packet the first packet of the flow \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                             \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet direction time series                        \u2502\n\u2502 pkts_iat    \u2502 np.array \u2502 Packet inter-arrival time series                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"datasets/schemas/utmobilenet21/","title":"<code>utmobilenet21</code>","text":"<p>Below we report all schemas for all datasets. The section expanded suggest the datasets to be used, while highlighted rows suggest which fields are more useful for modeling.</p> tcbench datasets schema --name utmobilenet21 --type unfiltered <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type filtered</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field       \u2503 Dtype    \u2503 Description                                                                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 row_id      \u2502 int      \u2502 Unique flow id                                                               \u2502\n\u2502 src_ip      \u2502 str      \u2502 Source ip of the flow                                                        \u2502\n\u2502 src_port    \u2502 int      \u2502 Source port of the flow                                                      \u2502\n\u2502 dst_ip      \u2502 str      \u2502 Destination ip of the flow                                                   \u2502\n\u2502 dst_port    \u2502 int      \u2502 Destination port of the flow                                                 \u2502\n\u2502 ip_proto    \u2502 int      \u2502 Protocol of the flow (TCP or UDP)                                            \u2502\n\u2502 first       \u2502 float    \u2502 Timestamp of the first packet                                                \u2502\n\u2502 last        \u2502 float    \u2502 Timestamp of the last packet                                                 \u2502\n\u2502 duration    \u2502 float    \u2502 Duration of the flow                                                         \u2502\n\u2502 packets     \u2502 int      \u2502 Number of packets in the flow                                                \u2502\n\u2502 bytes       \u2502 int      \u2502 Number of bytes in the flow                                                  \u2502\n\u2502 partition   \u2502 str      \u2502 From which folder the flow was originally stored                             \u2502\n\u2502 location    \u2502 str      \u2502 Label originally provided by the dataset (see the related paper for details) \u2502\n\u2502 fname       \u2502 str      \u2502 Original filename where the packets of the flow come from                    \u2502\n\u2502 app         \u2502 category \u2502 Final label of the flow, encoded as pandas category                          \u2502\n\u2502 pkts_size   \u2502 np.array \u2502 Packet size time series                                                      \u2502\n\u2502 pkts_dir    \u2502 np.array \u2502 Packet diretion time series                                                  \u2502\n\u2502 timetofirst \u2502 np.array \u2502 Delta between the each packet timestamp the first packet of the flow         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>tcbench datasets schema --name utmobilenet21 --type splits</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Field         \u2503 Dtype    \u2503 Description                  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 train_indexes \u2502 np.array \u2502 row_id of training samples   \u2502\n\u2502 val_indexes   \u2502 np.array \u2502 row_id of validation samples \u2502\n\u2502 test_indexes  \u2502 np.array \u2502 row_id of test samples       \u2502\n\u2502 split_index   \u2502 int      \u2502 Split id                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"modeling/","title":"Modeling introduction","text":"<p>When training ML/DL models,  finding the right combination of data preprocessing/splitting, algorithms and hyper-parameters can be challenging. Even more so when the modeling process  aims to be repeatable/replicable/reproducible.</p> <p>To ease this process, tcbench is designed to ease the</p> <ul> <li> <p>Collection of  telemetry and metadata. This includes bind the generated output to  the input parameters used to create models.</p> </li> <li> <p>Collection of  artifacts such as  the models created as well performance reports (e.g., loss evolution during training  and testing confusion matrixes).</p> </li> </ul> <p>This is possible thanks to a tight integration with AIM with some extra ad-hoc components.</p>"},{"location":"modeling/#aim-tracking","title":"AIM tracking","text":"<p>AIM stack is an open-source self-hosted model tracking framework enabling logging of metrics  related to model training. </p> <p>Such telemetry  can later be explored via a web interface or programmatically extracted via AIM SKD.</p> <p>Why AIM?</p> <p>There are many solutions for model tracking. While frameworks such as Weights &amp; Biases or Neptune.ai are extremely rich with features, unfortunately they typically  are cloud-based solutions and not necessarily open-sourced.</p> <p>Alternative frameworks such as Tensorboard and MLFlow have only primitive functionalities with respect to AIM.</p> <p>AIM is sitting in the middle of this spectrum: it is self-hosted (i.e., no need to push data to the cloud) and provides nice data exploration features.</p> <p>AIM collects modeling metadata into repositories fully under the control of the end-user:</p> <ul> <li> <p>Repositories are not tied to specific projects and it is up to the end-user define the semantic of the repository. In other words, users can decide to track in a repository metrics related to multiple experiments of the same model (e.g., different hyper parametrization) but single repository can be used to track completely different experiments  (e.g., different metrics, hyper-parameters, properties across experiments).</p> </li> <li> <p>There is no limit on the amount of repositories  can be created. All repository are local to end-users infrastructure so the only limitation is the end-users storage.</p> </li> </ul> <p>AIM repositories are collection of \"runs\", each representing a different modeling experiment and identifies by a unique ID automatically generated by the framework. </p>"},{"location":"modeling/#runs-and-campaigns","title":"Runs and campaigns","text":"<p><code>tcbench</code> tracks in an AIM repository two types of tasks, namely runs and campaigns:</p> <ul> <li>A  run has a 1:1 matching with the run defined by AIM, i.e., it corresponds to the training of an individual ML/DL model and is \"minimal experiment object\" used by AIM, i.e., any tracked metadata need to be associated to an AIM run.</li> </ul> <p>A run is associated to both individual values (e.g., best validation loss observed or the final accuracy score) as well as series (e.g., loss value for each epoch).</p> <p>Morever, tracked metrics are associated to a context  expressing if they are generate using train, validation or test set.</p> <ul> <li>A  campaign corresponds to a collection of runs. </li> </ul> <p>Runs -vs- Collections</p> <p>Runs are the fundamental building block for collecting modeling results. But they are also the fundamental unit when developing/debugging modeling tasks.</p> <p>Conversely, campaigns are intended for  grouping semantically similar runs  and store them into a single repository. Hence, different campaigns are stored into different repositories.</p>"},{"location":"modeling/aim_repositories_content/","title":"Aim repositories content","text":"<p>Exploring AIM repositories via the web UI or run artifacts are great ways as long as a precise target is already defined.</p> <p>To complement those options, <code>tcbench</code> offers a few sub-commands to overview an AIM repository  content.</p> <pre><code>tcbench aimrepo --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo [OPTIONS] COMMAND [ARGS]...\n\n Investigate AIM repository content.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ls                    List a subset of properties of each run.                                  \u2502\n\u2502 properties            List properties across all runs.                                          \u2502\n\u2502 report                Summarize runs performance metrics.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In the following we illustrate each sub-command using the  <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository</p>"},{"location":"modeling/aim_repositories_content/#ls","title":"<code>ls</code>","text":"<p>The <code>ls</code> sub-command simply list  the hash, creation time and end time of each run.</p> <pre><code> tcbench aimrepo ls \\\n    --aim-repo code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code> hash                      creation_time      end_time\n 8ba8f05ec1604574b71139b4  1684486220.186745  1684487165.465486\n 67503d171467435091714d45  1684485354.995328  1684486219.662343\n 9a106b1a209b4a47abb5c17f  1684484451.951014  1684485354.490275\n bfe6d326e299461cb56cdc5f  1684483871.600086  1684484451.518072\n 87fa4af7e99d478aaf22f4da  1684483362.006482  1684483871.393643\n c13844d1704c45fc95653b1f  1684482566.637334  1684483361.812976\n c7bdc0d409c4403bbe5b9cc4  1684482432.924283  1684484618.028195\n 043419ccd36541fd8a4057cb  1684482146.1981    1684482566.387968\n 963f7797f19d4976a3efcca3  1684481692.788042  1684482145.885839\n 4d81379229814f409b7647d3  1684481202.271289  1684481692.537144\n fe35f9380c344b8aa9c23f03  1684481101.181884  1684482432.517077\n 2f6d76f632c64391b7cb4973  1684480123.949839  1684481201.997798\n b3df56698ec6416cac21add4  1684479475.427219  1684481101.055126\n 72ac1602329b415dbf17524f  1684479393.541195  1684480123.490255\n 68832122ee434d799da3ec6c  1684478564.503901  1684479474.865144\n 98033a73d4d1433ab3779742  1684478415.758284  1684479393.015683\n c6aea7ff760b4ad090d9dd36  1684477658.120382  1684478415.53678\n 7e0386cdcbaa454e892b5fb9  1684477439.749558  1684478563.989332\n d985dfe5a5a6422293ff34eb  1684476688.49235   1684477657.471275\n 889dc5128693473f9fd79299  1684476501.215957  1684477439.58382\n 922fef07ff264188a2e370a5  1684475858.118653  1684476688.059573\n cfb5812f3ca84c2bb440e6c4  1684475376.248467  1684476501.070375\n 99fc7d6feea64a07a9f4499d  1684475282.908762  1684475857.977606\n 728dfe6ff0584b0db57b8f22  1684474888.182631  1684475282.672584\n 9dde5426e9424f9aa0eda8ef  1684474417.41854   1684474887.912377\n d8845f81ebbc4fbdbb7efc6e  1684474280.592608  1684475375.693128\n 4ca5aff6b9c442808f39d2c3  1684473415.875285  1684474417.168511\n f24dfd8ef3e44cd6ae37c4d7  1684473045.421477  1684474280.137564\n 9d530cacbbfc4300846c412a  1684472369.171916  1684473415.375791\n b222cda5054744ee923cc1a5  1684471606.606575  1684473045.147482\n 5c9c03c663ea493480ff176c  1684471433.960361  1684472736.412599\n 1e13e4b8c60a4dd084ca607b  1684471290.69116   1684472368.905358\n 4c42b65f85e444579adabd51  1684470601.838798  1684471289.976878\n 4f759432db0743c7b3d95f6b  1684470452.527724  1684471433.756871\n 1a71035742a940c7a74c6a9b  1684470331.022178  1684471606.113162\n 1f2bc263b9ed450abcf1c1d3  1684469705.56874   1684470601.122689\n 9951ad2c1f894efcb04d39bb  1684469472.837658  1684470451.971661\n dd4ec191cd0945a184c9aef8  1684469140.912997  1684469472.087474\n 60b55181ebaa4ccebc214875  1684468906.545817  1684470330.375135\n d74b79343de8419b874f5210  1684468808.347594  1684469705.18729\n 0a30bf4e578445239c3e91cb  1684468778.110371  1684469140.702514\n 1d0327a026444eee800dc9a1  1684468500.060794  1684468777.756142\n e40d612c892644f59370cd68  1684468394.226275  1684468808.078585\n 67c4857856d445d4809e1fd2  1684467913.759307  1684468394.08247\n cad4cd97d854419e91869d85  1684467543.981347  1684468905.978667\n a7018c0846304408bc75e8c8  1684467526.249917  1684467913.503346\n 39c042def7b94710bc211b0e  1684466951.957943  1684468499.463829\n e61e21d87ccd4aa8b8a2ed35  1684466335.110533  1684467526.054343\n b1869271d50644c696fb5246  1684466131.933304  1684467543.395299\n e0dac9c82bf04bafb4aade4c  1684465619.566809  1684466951.462375\n ed199b3b6a71488893e1abfb  1684465434.028724  1684466333.918455\n 7dc2a753e3f74ed69f069ea8  1684464725.062243  1684466131.24298\n 03a5cdeb32354bfdba1de136  1684464569.689175  1684465433.542933\n 37c00f6c1ebe44fda8a9618d  1684464407.673333  1684465618.991646\n f620bcb6afe24d2a87a778c0  1684464064.526061  1684464406.941772\n 9cdde43cddd640e2b440b1c4  1684463741.103362  1684464064.054385\n ffb1a45a1fb249c0bf3f463c  1684463607.054282  1684464569.411643\n 395fc5799efc4f34823b3655  1684463449.453486  1684463740.94038\n 5a0a0a5c3bb244d497538de4  1684463312.066392  1684464724.410855\n 0638ad838f4a429b91131a5b  1684462917.872626  1684463606.433408\n 51aa03f39a3e42e2b99db486  1684462233.665966  1684463311.043898\n 57eb602e1dad4933b5eca155  1684462207.043093  1684463448.792002\n 01fea90ec67544d3a39ef997  1684461943.457484  1684462917.499634\n d792e20d40604c78956ab5a2  1684461464.25762   1684461943.313054\n 637badf3dfec4b93a78d9919  1684461042.264266  1684461464.126894\n f87edacfdacf4b5faee625b4  1684460961.382713  1684462233.005103\n 444c2e1ebfaa43a78c6b4c96  1684460467.126398  1684461042.125671\n 9e5135774a89458b8dae96ef  1684459980.097812  1684462206.830038\n ca77db360e8f45d5a8d2f3b7  1684459693.93209   1684460466.346116\n adf0905c3c8349d590574c38  1684459300.071951  1684460960.695716\n 916583de1a524ed092408d72  1684459069.160377  1684459693.416057\n 7c25e7eca05e49ffa82e1dda  1684458823.182091  1684459979.576512\n e77cf8e464b2496a94229ca8  1684458538.286254  1684458823.017745\n 70fef369b9e741e59a9cb092  1684458437.369592  1684459067.97458\n 73c779ffb561448281772cca  1684458244.2756    1684458537.96287\n c21d1b9a093c42cbb1421b99  1684458241.060798  1684459299.580839\n e0c07fd0f3664789a68a97fd  1684457885.407864  1684458243.499636\n 7b52359ef3ef42219b6297ef  1684457777.173311  1684458436.747236\n aecb143222764ac2badda996  1684457079.7536    1684457776.789753\n 83850279a1fd43e1892771ea  1684457024.45417   1684458240.375002\n c80ccdfec89b4ac68013dc57  1684456500.647965  1684457079.174438\n bd8b6929f7bb41eeb00e8451  1684456429.302551  1684457884.834434\n cca820ec5ae24ee793b45ef3  1684456007.859746  1684456500.321697\n ad89e8df0be34133bb61e589  1684455782.341968  1684457023.582\n d9613c247ffc49ae83b1864d  1684455514.437144  1684456007.704841\n f4f28ffbd85041d78d032bf7  1684455198.659235  1684456428.772103\n 17aa80f35ba240d6a3c27681  1684454983.894229  1684455514.144682\n 376d83d39ee34bf6a5f14553  1684454900.637969  1684454978.036799\n d66de4d4c0f043289d0f9671  1684454828.019291  1684454900.501477\n 53e969d8626c41df92814c61  1684454756.332229  1684454827.858425\n 988b1457caba480b817ba385  1684454684.082903  1684454756.197079\n f60fc0f56310461ebd15ccb1  1684454602.697385  1684454683.934212\n 4231173e05fe4466bb3592e5  1684454519.796744  1684454602.561087\n b7bcc2cb30054c9d89b81e71  1684454436.86775   1684454519.639057\n 0eb416e6597f4205a4eb30e5  1684454350.202147  1684454436.706402\n 01d4bb96f3d547b0b67f3219  1684454275.221716  1684454350.058379\n 2863d94709a241549ea43d1e  1684454180.176362  1684454275.095384\n 608b4ff892424b909c37195b  1684454131.059445  1684455781.74821\n 2f4819882cab4dbc8bb4e599  1684454101.90471   1684454180.044018\n dc3e8dfccc964658b1094227  1684454031.000489  1684454101.748034\n 95e5b09ba7d44fabbb055f93  1684453973.055474  1684454030.873004\n 3ab8c90f935845bc85d9e7c0  1684453916.309955  1684453972.93122\n 8ed7f1729fd5456cbf9bf9f0  1684453858.633202  1684453916.18064\n 2838f1c9925a4276b596f29a  1684453857.763272  1684453999.474313\n fb37ebad53be4b8bb0922c43  1684453854.938354  1684455198.416701\n dbc9c91611dd40d18da3a563  1684453800.945448  1684453857.692718\n 0922df9981cb4dada7cb8e70  1684453783.490234  1684453858.502442\n 577b94c1058543fbbbac556c  1684453740.054494  1684453800.871211\n 1e8636dca105456d96c9d911  1684453708.58691   1684453783.36737\n 2927b88ec0064c1b96f2420d  1684453669.588055  1684453739.989276\n 1f79705713004475b0f4b423  1684453641.653473  1684453708.452254\n 816d410679ce4d3d93c9fb13  1684453609.559607  1684453669.528333\n f20a0864a0154d14bc62bef9  1684453603.103946  1684453641.526248\n 7ab9bc7b21f84284a8c06e2c  1684453562.770793  1684453602.990837\n b7dbd5917875476a9f331342  1684453554.43456   1684453609.487829\n e711aaf96ecc418cb1629a36  1684453522.761069  1684453562.665403\n c4d648e857fa404dbec12d64  1684453500.782744  1684453554.364838\n 810da0118ed7417db7c4771e  1684453469.291014  1684453854.683895\n 5d8a70cabc464fcbbd0a904d  1684453444.704697  1684453500.709896\n 5634546f547945849b466cf5  1684453420.998128  1684453522.623544\n c4ed1533ddbf43b5ba8f83ab  1684453389.201033  1684453444.618217\n cc13b218cbfd4cf08e5d20a4  1684453337.641638  1684453389.137557\n 610b507713cb47f4963ef671  1684453316.818646  1684453420.841483\n 36f2e4b6045b4f01b70e4680  1684453287.041721  1684453337.567413\n 37f99c3ed79a45f3b7d40a5f  1684453246.21599   1684453316.660344\n 5fe2730f842e416c8af9c9b3  1684453237.076659  1684453286.959855\n e25a532da42c4327bc11528a  1684453190.214313  1684453237.018725\n 7aea23e113cc4a289d7bfa79  1684453161.004546  1684453246.06685\n 45429199088f4c0c8b705537  1684453142.490356  1684453190.154943\n ab079de736834eef899c21bb  1684453101.882433  1684454130.262774\n fa468f7b03e3475daf215746  1684453095.863897  1684453142.425248\n add75d33ef8b4d638e770aaf  1684453075.861901  1684453468.361486\n 2aeb5a16085f42e9ab4094df  1684453065.579356  1684453160.874607\n 8e87b04207e54931b865d237  1684453016.800851  1684453095.802078\n 76f30367c8ab44b0bbe67d03  1684452977.614285  1684453065.424347\n deaffa25f04b4aae856c632f  1684452942.818858  1684453016.74139\n 5cae649ac7bb46d5a60c72ac  1684452877.834861  1684452942.748545\n 4a0efe2a9ecb499983245191  1684452870.028023  1684452977.473543\n 2e0474d626b94b5e957a3774  1684452847.893095  1684452877.790754\n 50f15777a7174531bbf50a1c  1684452818.958093  1684452847.832531\n b924f0f921e04b64828975ba  1684452789.569578  1684452818.902016\n b418ebaa6576432290bfe7ae  1684452774.458869  1684452869.895356\n b9f6b0815edf4ddfb34dc177  1684452728.81128   1684452789.506862\n 01b5d3ec52da4424a37125df  1684452700.006671  1684452774.319969\n f93d353566c24408bb3fc997  1684452663.618535  1684452728.740588\n dca1cc1dc9cc45f69bf45471  1684452629.495522  1684453074.989142\n 61a54b25733740b586ae3788  1684452604.757812  1684452663.541514\n 762d10ed26ae46ab8752cbfb  1684452601.345309  1684452699.879312\n 6cbe5bb9f82141b8abb5d2c8  1684452537.019976  1684452604.693619\n f2d476a93fef4c7abc727453  1684452508.867201  1684452601.21922\n 3d69205834a84112b65fcaef  1684452472.610688  1684452536.957576\n f2c44a9bc9f14ad4b3b827b8  1684452419.986378  1684452508.722681\n 4d2f061684c842d793d53458  1684452405.098763  1684452472.542425\n f05cbbb211a545d8b726a301  1684452365.427215  1684452419.85173\n 5f451dce01f04ee9b0169828  1684452346.577028  1684452405.012775\n 234669be443d4f08ac6dd900  1684452308.986717  1684452365.299411\n e8d6da0defd049028aaea9d5  1684452290.203461  1684452346.517978\n bd57bec0663e479ea2934678  1684452252.92531   1684452308.85739\n 352f015ecc004b7f922e9675  1684452225.314651  1684452290.121976\n 47a4a0891c024ff9ad3e10a5  1684452168.422513  1684452252.796179\n 06da93cc03a44040b0c74178  1684452157.739649  1684452225.250497\n e3d645b0dc16421ca3209896  1684452137.461206  1684453101.39133\n 7bdb5341a2614a888c577a09  1684452097.493495  1684452157.677333\n 4bbd8caafebe4379b130c6b0  1684452090.542063  1684452168.28824\n 7b25cd869f00439fafbed535  1684452029.411407  1684452097.43651\n 92c9588486534986a7a5eacf  1684451984.658156  1684452029.353263\n 61d85dcf712b434fac9a5b1f  1684451983.953664  1684452090.41075\n a12b3724c60d4958996de377  1684451944.910572  1684451983.848708\n 66dda78e98484ac695ac28c2  1684451937.460639  1684451984.606509\n 6693f2a4a645455cada34389  1684451908.020474  1684451944.789062\n e1169972d26140e18d897023  1684451886.957104  1684451937.392489\n 822c84da50f74667920f03bf  1684451871.324137  1684451907.890575\n 77bb1cec15af4c23b8be0f2f  1684451821.675738  1684451886.891239\n cf3545083d794dee8e0d2e9a  1684451812.399059  1684451871.18422\n 38171ed6da8f44e1a555b9d8  1684451748.611645  1684451821.617548\n a94ec554111b4a1a89cca0f6  1684451744.945461  1684451812.24693\n f20aca505c37459abb281e08  1684451656.412687  1684451744.789916\n 5c4aa95035c841d4a5e96b0c  1684451556.350265  1684451656.276747\n 6aa44846db614702867c9623  1684451546.981732  1684451748.381051\n c727a871d64b4b84ab39ce7b  1684451472.885078  1684451556.217995\n f1444a30a0b64ddebb0ee893  1684451451.528096  1684451546.751202\n 83ecc5c955ec4023bc82b30d  1684451382.832542  1684451472.73465\n 6c7b1cbe72004a0898b331e1  1684451373.367619  1684451451.294249\n 55a0901ebd8c4a14a006bda0  1684451332.395285  1684452629.058694\n 9c0d10c9e8f04194b6cf322c  1684451332.169061  1684451373.247294\n 1fe3f3cff5364183b91b0161  1684451311.409357  1684451382.69011\n e64349682cac4e96bdb3a195  1684451305.523624  1684452136.669685\n b6f5f861bfd045b79910163f  1684451268.06238   1684451332.097281\n 24db445e86664a5688d5b058  1684451244.011303  1684451311.247374\n 707b1d8f242c4c569397818f  1684451201.961014  1684451267.986849\n b5870e5dd6ac4433a59e949e  1684451151.07773   1684451243.875269\n 972de7c3956d4d96b27fa2e7  1684451147.277512  1684451201.870307\n 007ff3eb7d3644c9bfa97590  1684451089.133811  1684451147.206315\n be295d1045b64e5da2dad253  1684451086.977876  1684451150.932065\n 008ab3a058424242b2c74721  1684451024.406991  1684451089.065439\n 8587efea57d444b0a37a4d87  1684451012.747797  1684451086.845493\n 46cfa3ec77604b18b5cbb952  1684450970.955052  1684451024.350877\n 473c10c9d83b4a2f9f7aaacd  1684450935.840727  1684451012.61568\n 9749d96f77754ee3a1dd82dc  1684450910.230388  1684450970.876716\n bd622c8c454a43b2a526e255  1684450878.062955  1684450935.71623\n 1b69360731f64dbabd3e6b4a  1684450856.297448  1684450910.156479\n f1c8afdee5f843dea2cd6326  1684450820.288095  1684450877.926006\n 8719913ab19c470090e2cbba  1684450789.546818  1684450856.227493\n 64cc636a24de4af085a90012  1684450764.779803  1684450820.161206\n 38f7ad203bd3441cb27e8477  1684450726.736646  1684450789.489278\n 2006e773a42a45169c8af288  1684450700.498349  1684450764.63308\n f48a8088282a4f22a3060aad  1684450628.084315  1684450726.679969\n 07741993d3ff4d4c85cd2378  1684450625.213713  1684450700.367589\n 78897d14594d4d8ab0511b09  1684450532.651023  1684450625.066728\n c058300f4d55480183fe7d0a  1684450500.728736  1684450627.9136\n 70457b6d2ca34c9ca54faa9c  1684450494.905926  1684450532.530795\n fae0d0098235451582379e49  1684450457.158045  1684450494.787555\n eb0656be43d64f4abc07d913  1684450420.256119  1684450457.035452\n 3f6b15bdbb4c4f7990139390  1684450373.169881  1684450500.508234\n 37fb6128cf0940b0a8e02a6d  1684450346.729378  1684450420.123028\n 59850e2e072c4315bcfd44f9  1684450317.249483  1684451331.988945\n eb23366973d54adcb7ae41dc  1684450309.69558   1684450372.983297\n 4c51c9d6344741a3b4167ace  1684450270.958624  1684450346.587809\n 0adf8ec7b2d44e83b9d1ecd6  1684450264.164186  1684450309.641298\n 7663b098261c42b6ae806446  1684450255.519111  1684451304.891808\n 8d87effdeaf045bd8e1793be  1684450191.755371  1684450270.815829\n 4afff20583a7446a87f532bb  1684450174.269007  1684450264.10297\n a12f8c41aa7645fb8dce11db  1684450113.72971   1684450191.620866\n 278b560f285e4b3e968d2500  1684450112.787798  1684450174.213632\n 8232054671804f4a9f049ac8  1684450038.59175   1684450113.590469\n 8a2aaa0359c14bee82d7a01b  1684450038.553256  1684450112.718496\n cc14c289c387497299647aff  1684450009.504617  1684450038.503996\n c1d541e31bf04c34a8361277  1684449981.075677  1684450009.447578\n f6ae1ffbf09b4b829845768a  1684449960.174022  1684450038.453731\n e55525d57f8f4c0fbe62925c  1684449951.684003  1684449981.027956\n 3b3c950211ec4ac1861d2ef2  1684449896.906696  1684449951.618761\n a6156d0fedfb4da58d7ed157  1684449886.685146  1684449960.026201\n 34fb83c203bb4950bd629d93  1684449847.082951  1684449896.838466\n 32315d3421a643f8a60e3cc6  1684449808.13473   1684449886.545201\n 6db2eb1e1dd347acba48e369  1684449790.974563  1684449847.000392\n 63c5de9bc5ae4bca84a04a12  1684449737.9346    1684449807.970432\n ec202059cb874f4f9ca4205e  1684449725.824672  1684449790.911293\n ae692ba4c260457d9a3e442a  1684449668.959527  1684449725.763858\n a96434d27b8e45d3ad8b81ea  1684449659.840383  1684449737.633578\n 5c28fb0c4cae4a5eb3e2bf37  1684449609.2497    1684449668.911\n 401d87dc0f9d4f7fb3aa52df  1684449583.918668  1684449659.708532\n c9c4b346ff394c8488e9790d  1684449545.541382  1684449609.186583\n 74ba1657c61644e2a5e7da30  1684449520.169214  1684449583.771104\n ae7a6eb3e85a48228d3371b4  1684449492.929299  1684449545.465121\n 52615c37cb4c403a8bf7a025  1684449464.493325  1684449520.040155\n 4969fb8c374141028594d962  1684449437.814941  1684449492.858044\n 1e96377382f443ccbe7e85f0  1684449405.594734  1684449464.356868\n 776699b253614175b7f827ff  1684449387.696488  1684449437.748665\n 5687406675c34ef8bead5633  1684449346.890198  1684449405.47788\n 387e01ef08454e2eb2cff697  1684449333.183408  1684449387.6353\n 685af93f85ac4b67bb99144e  1684449302.386924  1684450255.027677\n 4ea944b1be754ab48bcb7d45  1684449276.832263  1684449346.737273\n db1c87ebb5494363a2fb2d3b  1684449237.813694  1684449333.12579\n 3e1533ac0f164dc092b7b681  1684449204.473322  1684449276.677444\n 61882760258d44d793c721dd  1684449151.709934  1684449237.749622\n 385286c19cb14a0c9571e0e9  1684449129.216098  1684449204.342291\n 63a50b8c64884ec5aa61cfa1  1684449091.168293  1684449129.090858\n a9b40ede978e4fb4bfc120b7  1684449055.11729   1684449091.054672\n cb78f86b48284696aa938fcc  1684449029.36249   1684449151.468267\n 47595fc5c64146e9805b51e6  1684449017.959478  1684449054.990691\n bf2c9394ea3742e683452a22  1684448936.264374  1684449029.147291\n c8f67a99057c49a2944bf39a  1684448932.684268  1684449017.804864\n a4cf32694e6e411ea84aa47a  1684448931.706416  1684450316.541687\n 4b3570a3a50c41c78f0000f4  1684448858.610413  1684448936.197166\n b21d99a3202e4da3b484ae4a  1684448837.720911  1684448932.548398\n f099eb327fd7494db384bb14  1684448765.679447  1684448858.545372\n 47eccc984daf4834aec19f67  1684448756.36372   1684448837.570065\n 7aaac8ecd2e644eda14968cf  1684448674.903293  1684448765.625554\n 43a503bf40aa4f518a97cf4e  1684448667.212994  1684448756.204382\n 230c734960ea4c849e3af2e1  1684448645.431733  1684448674.860142\n a6a20facc3f34662aa660fee  1684448615.158763  1684448645.376438\n cd65b0169cf441bfb6e3dcd8  1684448585.157683  1684448615.106011\n a0bc2e31538249aab969fc0d  1684448580.386622  1684448667.054529\n 3ee8b28573704e4ba79cf698  1684448527.514667  1684448585.087628\n 0712bcf076c44d239f8def39  1684448505.486607  1684448580.237805\n 45cf18b3d09a40098e621079  1684448465.274798  1684448527.439574\n 40cde1e6fa674cc3bfd82bb6  1684448423.393276  1684448931.366076\n d59fb822d0c346e496361cc6  1684448421.81773   1684448505.32149\n 13d14051be32424d92613289  1684448410.573326  1684448465.203814\n f05bc193e27d4178986a5662  1684448379.497623  1684449301.840416\n 3e378ebe488442e59997f8d2  1684448349.959204  1684448421.651724\n e5fc7761ea6a4e04bbb16b74  1684448346.800847  1684448410.504434\n 7538f4623894469ea5db1024  1684448284.44457   1684448346.71941\n c81913cba557469a90999dfa  1684448266.979889  1684448349.801789\n 6a742ccb640e4594a353c49a  1684448229.388576  1684448284.376865\n 75d1aa21751a4d0085f7c78c  1684448190.969628  1684448266.848555\n 321f7acb616040cda1232be3  1684448178.230579  1684448229.301771\n f325127637554978a28c7c4c  1684448123.997501  1684448178.150143\n ba4003ab5d674388913ad196  1684448122.292348  1684448190.811933\n ecf0408ff75948cdad056c2a  1684448069.866839  1684448123.924316\n a96a951613e947c596cd48fa  1684448051.908431  1684448122.161052\n 274ed9aae2324ebdb6b0e59a  1684448023.663688  1684448423.171355\n 81e993bee47043319ff1ab6e  1684448007.683417  1684448069.801989\n a8bdcf0e5d6f4645a7a76382  1684447991.732175  1684448051.786494\n a1f84df603a3455b8bc91007  1684447955.241488  1684448007.621067\n df5e5a6eed6542739b67f833  1684447937.451409  1684447991.609256\n f1e61e62d6fb484c9122547e  1684447898.394327  1684447955.175681\n c59bc63eb8f041049543f1d2  1684447880.325154  1684447937.315866\n 1e5a6ac24e9540f89c0c54cf  1684447853.491841  1684447898.333804\n c21a103dfa35473390e9d1fe  1684447807.994382  1684447853.423596\n e5f3fc273b114ace8bcb8478  1684447807.815813  1684447880.187383\n 0d92ca4591404ba889239d2f  1684447760.682189  1684447807.938637\n 53046971da764da1b343c6c5  1684447727.930714  1684447807.672719\n f65eae9d11414922a5cc531e  1684447662.420696  1684447760.621397\n 5e9105ff533e4b44a0dfc577  1684447636.207182  1684447727.776517\n 7e7fd7824169464b9602d7cc  1684447598.500283  1684447636.086839\n a669d0df5eaa4e47a20f5d94  1684447572.937839  1684448379.216737\n 426257bb89474ca8a339523b  1684447565.763093  1684447662.363762\n 8cc32087458448c7bd165ea0  1684447561.149784  1684447598.372651\n caaffcdf9919419cb3616619  1684447520.023255  1684447561.032779\n 3ace3b501ffc4d3d9f006a4c  1684447477.174045  1684447565.696864\n 2ec24496c7b44bc1abb9d07d  1684447446.502455  1684447477.122609\n d74763a98ac54a899facc16e  1684447417.775803  1684447446.44637\n 7e702018ebfe4d96b96b84c5  1684447395.8736    1684448023.427643\n 18dd6fca9c6944fb9ac0b77f  1684447384.730554  1684447417.721719\n</code></pre>"},{"location":"modeling/aim_repositories_content/#properties","title":"<code>properties</code>","text":"<p>The <code>properties</code> sub-command show aggregate information and the list of unique values of the properties of an AIM repo (i.e., run hyper params and other meta data).</p> <pre><code>tcbench aimrepo properties \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name                   \u2502 No. unique \u2502 Value                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 runs                   \u2502          - \u2502 315                                                       \u2502\n\u2502 duration (mean \u00b1 std)  \u2502          - \u2502 5m44s \u00b1 7m40s                                             \u2502\n\u2502 metrics                \u2502          4 \u2502 ['acc', 'best_epoch', 'best_loss', 'loss']                \u2502\n\u2502 contexts               \u2502          5 \u2502 ['test-human', 'test-script', 'test-train-val-leftover',  \u2502\n\u2502                        \u2502            \u2502 'train', 'val']                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 experiment             \u2502          1 \u2502 ['augmentation-at-loading']                               \u2502\n\u2502 aug_name               \u2502          7 \u2502 ['changertt', 'colorjitter', 'horizontalflip', 'noaug',   \u2502\n\u2502                        \u2502            \u2502 'packetloss', 'rotate', 'timeshift']                      \u2502\n\u2502 campaign_exp_idx       \u2502        105 \u2502 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,   \u2502\n\u2502                        \u2502            \u2502 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,   \u2502\n\u2502                        \u2502            \u2502 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,   \u2502\n\u2502                        \u2502            \u2502 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,   \u2502\n\u2502                        \u2502            \u2502 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,   \u2502\n\u2502                        \u2502            \u2502 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,   \u2502\n\u2502                        \u2502            \u2502 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,  \u2502\n\u2502                        \u2502            \u2502 101, 102, 103, 104, 105]                                  \u2502\n\u2502 campaign_id            \u2502          1 \u2502 ['1684447037']                                            \u2502\n\u2502 dataset                \u2502          1 \u2502 ['ucdavis-icdm19']                                        \u2502\n\u2502 flowpic_block_duration \u2502          1 \u2502 [15]                                                      \u2502\n\u2502 flowpic_dim            \u2502          3 \u2502 [32, 64, 1500]                                            \u2502\n\u2502 patience_steps         \u2502          1 \u2502 [5]                                                       \u2502\n\u2502 seed                   \u2502          3 \u2502 [42, 666, 12345]                                          \u2502\n\u2502 split_index            \u2502          5 \u2502 [0, 1, 2, 3, 4]                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The table is split into two parts to separate general properties (top) from hyper parameters (bottom). General properties are common across repositories while hyper parameters vary depending of the campaign.</p> <p>What is a context?</p> <p>The term is borrored from AIM terminology and refers to ability to group metadata into categories. </p> <p>For instance, in the example above, the 4 listed metrics are separately stored for each listed context.</p>"},{"location":"modeling/aim_repositories_content/#report","title":"<code>report</code>","text":"<p>The <code>report</code> sub-command provide an aggregated summary across metrics grouped by properties</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                                    hparams                           acc                  duration\n                   split        aug_name   flowpic_dim  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              test-human       changertt            32    15  70.04   4.41   2.44    83.93      7.8     4.32\n                                                    64    15  72.05    2.1   1.16    61.57      5.2     2.88\n                                                  1500    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  68.84   4.69   2.59    78.01     10.9     6.03\n                                                    64    15  71.33   3.35   1.86    67.15    22.49    12.45\n                                                  1500    15  68.59   3.17   1.76   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                                    64    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                                  1500    15  73.82   1.47   0.82    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  69.48   2.12   1.17    37.97     1.46     0.81\n                                                    64    15  69.88   2.28   1.26    38.06    20.21    11.19\n                                                  1500    15  68.67   1.93   1.07   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                                    64    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                                  1500    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                                    64    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                                  1500    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  70.36   2.98   1.65    80.08    12.72     7.04\n                                                    64    15  72.53   1.83   1.02    64.23    21.91    12.13\n                                                  1500    15  70.84   2.42   1.34   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             test-script       changertt            32    15  97.33   0.71   0.39    83.93      7.8     4.32\n                                                    64    15  97.29   0.64   0.35    61.57      5.2     2.88\n                                                  1500    15   96.8   0.63   0.35  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.87    0.8   0.45    78.01     10.9     6.03\n                                                    64    15  97.42    1.2   0.67    67.15    22.49    12.45\n                                                  1500    15  94.89    1.5   0.83   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.11   0.74   0.41     56.9     1.64     0.91\n                                                    64    15  95.96   0.89   0.49    63.86    28.97    16.04\n                                                  1500    15  95.11   1.23   0.68    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.73   0.49   0.27    37.97     1.46     0.81\n                                                    64    15  95.96   0.53   0.29    38.06    20.21    11.19\n                                                  1500    15  94.44   1.63    0.9   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.98   0.87   0.48    80.83    11.11     6.15\n                                                    64    15  96.89   0.96   0.53    57.08     4.75     2.63\n                                                  1500    15  95.96   1.27    0.7  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.36   0.71   0.39    78.52     11.1     6.15\n                                                    64    15  96.89    0.7   0.39    88.49    33.41     18.5\n                                                  1500    15  95.47   0.84   0.47  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  96.71   0.92   0.51    80.08    12.72     7.04\n                                                    64    15  97.11   0.65   0.36    64.23    21.91    12.13\n                                                  1500    15   96.8   0.57   0.32   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-train-val-leftover       changertt            32    15  98.24   0.56   0.31    83.93      7.8     4.32\n                                                    64    15  98.29   0.71   0.39    61.57      5.2     2.88\n                                                  1500    15  98.43   0.22   0.12  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.46    0.6   0.33    78.01     10.9     6.03\n                                                    64    15  96.82   0.74   0.41    67.15    22.49    12.45\n                                                  1500    15  95.79   0.91    0.5   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.88   0.46   0.25     56.9     1.64     0.91\n                                                    64    15  96.38   0.91    0.5    63.86    28.97    16.04\n                                                  1500    15  96.47   1.02   0.57    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  96.05   0.35   0.19    37.97     1.46     0.81\n                                                    64    15  96.22   0.57   0.31    38.06    20.21    11.19\n                                                  1500    15  95.62   0.91   0.51   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  97.47   0.64   0.35    80.83    11.11     6.15\n                                                    64    15  97.48    0.5   0.28    57.08     4.75     2.63\n                                                  1500    15  97.29   0.49   0.27  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  97.01   0.43   0.24    78.52     11.1     6.15\n                                                    64    15  97.28   0.62   0.34    88.49    33.41     18.5\n                                                  1500    15  95.93   0.74   0.41  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  97.44   0.75   0.42    80.08    12.72     7.04\n                                                    64    15  97.78   0.68   0.38    64.23    21.91    12.13\n                                                  1500    15  97.94   0.34   0.19   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The console output is informing about the <code>campain_id</code>  found in the repository and the total number of runs. If the repository was containing more than one campaign, a different report table would have been created for each campaign.</p> <p>The report is always grouped by <code>test-&lt;XYZ&gt;</code> contexts. By default all hyper parameters with more than one value are also added (see the previous description about properties).</p> <p>In the example, the <code>acc</code> metric is measured with mean, standard deviation and 95 %tile confidence intervals. Next to the metric is reported also <code>duration</code> which  corresponds to the overall time for train/validation/test  in the run execution.</p> <p>One can rearrange the table composition using the <code>--groupby</code> and <code>--contexts</code> options For instance, in the following we swap the order of the <code>hparams</code> used and report only on one context.</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/ \\\n    --groupby flowpic_dim,aug_name \\\n    --contexts test-human\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                       hparams                           acc                  duration\n      split  flowpic_dim         aug_name  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-human           32        changertt    15  70.04   4.41   2.44    83.93      7.8     4.32\n                              colorjitter    15  68.84   4.69   2.59    78.01     10.9     6.03\n                           horizontalflip    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                    noaug    15  69.48   2.12   1.17    37.97     1.46     0.81\n                               packetloss    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                   rotate    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                timeshift    15  70.36   2.98   1.65    80.08    12.72     7.04\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      64        changertt    15  72.05    2.1   1.16    61.57      5.2     2.88\n                              colorjitter    15  71.33   3.35   1.86    67.15    22.49    12.45\n                           horizontalflip    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                    noaug    15  69.88   2.28   1.26    38.06    20.21    11.19\n                               packetloss    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                   rotate    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                timeshift    15  72.53   1.83   1.02    64.23    21.91    12.13\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    1500        changertt    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                              colorjitter    15  68.59   3.17   1.76   765.43   152.53    84.47\n                           horizontalflip    15  73.82   1.47   0.82    471.8     58.6    32.45\n                                    noaug    15  68.67   1.93   1.07   374.88    94.41    52.28\n                               packetloss    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                                   rotate    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                                timeshift    15  70.84   2.42   1.34   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The <code>report</code> sub-command also creates output artifacts.</p> <ul> <li> <p>An output folder is created based on the <code>campaign_id</code> value.</p> </li> <li> <p>A set of <code>runinfo_&lt;XYZ&gt;.parquet</code> files collect runs     hyper param and metrics. </p> </li> <li> <p>A set of <code>summary_&lt;XYZ&gt;.csv</code> files collect the     aggregate table reported on the console.</p> </li> </ul>"},{"location":"modeling/campaigns/","title":"Campaigns","text":"<p>Individual modeling campaings can be triggered from the subcommand <code>campaign</code> sub-command.</p> <pre><code>tcbench campaign --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench campaign [OPTIONS] COMMAND [ARGS]...\n\n Triggers a modeling campaign.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 augment-at-loading        Modeling by applying data augmentation when loading the training set.  \u2502\n\u2502 contralearn-and-finetune  Modeling by pre-training via constrative learning and then finetune    \u2502\n\u2502                           the final classifier from the pre-trained model.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The <code>campaign</code> sub-command is an \"opinionated version\" of <code>run</code> sub-command. Meaning, is currently targetting only the needs for the campaigns which are part of the submission. So, the options exposed by the <code>campaign</code> sub-commands are a selected subset of the one options available for the related <code>run</code> sub-commands.</p> <p>For <code>augment-at-loading</code> campaign supports the following options <pre><code>tcbench campaign augment-at-loading --help\n</code></pre></p> <p>Output</p> <pre><code>Usage: tcbench campaign augment-at-loading [OPTIONS]                                                                                                                                                    \nModeling by applying data augmentation when loading the training set.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                   \u2502\n\u2502                                   [default: augmentations-at-loading]                            \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                 \u2502\n\u2502                                   [default: aim-repo]                                            \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]                \u2502\n\u2502 --campaign-id            TEXT     A campaign id to mark all experiments.                         \u2502\n\u2502 --dry-run                         Show the number of experiments and then quit.                  \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).     \u2502\n\u2502                                   [default: 0]                                                   \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20]  \u2502\n\u2502 --seeds                  TEXT     Coma separated list of seed for experiments.                   \u2502\n\u2502                                   [default: 12345,42,666]                                        \u2502\n\u2502 --help                            Show this message and exit.                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --augmentations       TEXT                                  Coma separated list of augmentations \u2502\n\u2502                                                             for experiments. Choices:            \u2502\n\u2502                                                             [noaug|rotate|horizontalflip|colorj\u2026 \u2502\n\u2502                                                             [default:                            \u2502\n\u2502                                                             noaug,rotate,horizontalflip,colorji\u2026 \u2502\n\u2502 --dataset             [ucdavis-icdm19|utmobilenet21|mirage  Dataset to use for modeling.         \u2502\n\u2502                       19|mirage22]                          [default: ucdavis-icdm19]            \u2502\n\u2502 --dataset-minpkts     [-1|10|100|1000]                      In combination with --dataset,       \u2502\n\u2502                                                             refines preprocessed and split       \u2502\n\u2502                                                             dataset to use.                      \u2502\n\u2502                                                             [default: -1]                        \u2502\n\u2502 --flowpic-dims        TEXT                                  Coma separated list of flowpic       \u2502\n\u2502                                                             dimensions for experiments.          \u2502\n\u2502                                                             [default: 32,64,1500]                \u2502\n\u2502 --max-train-splits    INTEGER                               The maximum number of training       \u2502\n\u2502                                                             splits to experiment with. If -1,    \u2502\n\u2502                                                             use all available.                   \u2502\n\u2502                                                             [default: -1]                        \u2502\n\u2502 --split-indexes       TEXT                                  Coma separted list of split indexes  \u2502\n\u2502                                                             (by default all splits are used).    \u2502\n\u2502 --no-test-leftover                                          Skip test on leftover split          \u2502\n\u2502                                                             (specific for ucdavis-icdm19, and    \u2502\n\u2502                                                             default enabled for all other        \u2502\n\u2502                                                             datasets).                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Modeling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --method    [monolithic|xgboost]  Method to use for training. [default: monolithic]              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 DL hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size        INTEGER  Training batch size. [default: 32]                                  \u2502\n\u2502 --epochs            INTEGER  Number of epochs for training. [default: 50]                        \u2502\n\u2502 --learning-rate     FLOAT    Training learning rate. [default: 0.001]                            \u2502\n\u2502 --patience-steps    INTEGER  Max. number of epochs without improvement before stopping training. \u2502\n\u2502                              [default: 5]                                                        \u2502\n\u2502 --no-dropout                 Mask dropout layers with Identity layers.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 XGBoost hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --input-repr       TEXT     Input representation. [default: pktseries]                           \u2502\n\u2502 --pktseries-len    INTEGER  Number of packets (when using time series as input).                 \u2502\n\u2502                             [default: 10,30]                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --max-samples-per-class    INTEGER  Activated when --split-indexes is -1 to define how many      \u2502\n\u2502                                     samples to select for train+val (with a 80/20 split between  \u2502\n\u2502                                     train and val).                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>For <code>contralearn-and-finetune</code> campaign supports the following options <pre><code>tcbench campaign contralearn-and-finetune --help\n</code></pre></p> <p>Output</p> <pre><code>tcbench campaign contralearn-and-finetune --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench campaign contralearn-and-finetune [OPTIONS]\n\n Modeling by pre-training via constrative learning and then finetune the final classifier from the\n pre-trained model.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                   \u2502\n\u2502                                   [default: contrastive-learning-and-finetune]                   \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                 \u2502\n\u2502                                   [default: aim-repo]                                            \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]                \u2502\n\u2502 --campaign-id            TEXT     A campaign id to mark all experiments.                         \u2502\n\u2502 --dry-run                         Show the number of experiments and then quit.                  \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).     \u2502\n\u2502                                   [default: 0]                                                   \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 50]  \u2502\n\u2502 --help                            Show this message and exit.                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --augmentations            TEXT     Coma separated list of augmentations. Choices:               \u2502\n\u2502                                     [noaug|rotate|horizontalflip|colorjitter|packetloss|changer\u2026 \u2502\n\u2502                                     [default: changertt,timeshift]                               \u2502\n\u2502 --flowpic-dims             TEXT     Coma separated list of flowpic dimensions for experiments.   \u2502\n\u2502                                     [default: 32]                                                \u2502\n\u2502 --max-train-splits         INTEGER  The maximum number of training splits to experiment with. If \u2502\n\u2502                                     -1, use all available.                                       \u2502\n\u2502                                     [default: -1]                                                \u2502\n\u2502 --split-indexes            TEXT     Coma separted list of split indexes (by default all splits   \u2502\n\u2502                                     are used).                                                   \u2502\n\u2502 --train-val-split-ratio    FLOAT    If not predefined by the selected split, the ratio data to   \u2502\n\u2502                                     use for training (rest is for validation).                   \u2502\n\u2502                                     [default: 0.8]                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Training hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size                  INTEGER  Training batch size. [default: 32]                        \u2502\n\u2502 --cl-projection-layer-dims    TEXT     Coma separate list of contrastive learning projection     \u2502\n\u2502                                        layer dimensions.                                         \u2502\n\u2502                                        [default: 30]                                             \u2502\n\u2502 --cl-seeds                    TEXT     Coma separated list of seeds to use for contrastive       \u2502\n\u2502                                        learning pretraining.                                     \u2502\n\u2502                                        [default: 12345,1,2,3,4]                                  \u2502\n\u2502 --ft-seeds                    TEXT     Coma separated list of seeds to use for finetune          \u2502\n\u2502                                        training.                                                 \u2502\n\u2502                                        [default: 12345,1,2,3,4]                                  \u2502\n\u2502 --dropout                     TEXT     Coma separated list. Choices:[enabled|disabled].          \u2502\n\u2502                                        [default: disabled]                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"modeling/campaigns/#campaign-composition-and-progress","title":"Campaign composition and progress","text":"<p>As mentioned, campaigns are essentially just an array of runs.</p>"},{"location":"modeling/campaigns/#using-dry-run","title":"Using <code>--dry-run</code>","text":"<p>The <code>--dry-run</code> option allows to verify the composition of a campaign.</p> <p>For instance <pre><code> tcbench campaign augment-at-loading --dry-run --method monolithic\n</code></pre></p> <p>Output</p> <pre><code>##########\n# campaign_id: 1696169422 | run 1/315 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (3): [32, 64, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre> <p>From the output we can see that the  default configuration for an <code>augment-at-loading</code> campaign corresponds to 315 runs = 5 splits x 7 augmentations x 3 flowpic dimensions x 3 seeds.</p> <p>This can be adapted based on the campaign sub-commands options.</p> <p>For instance <pre><code>tcbench campaign augment-at-loading \\\n    --dry-run \\\n    --method monolithic \\\n    --split-indexes 0,3 \\\n    --seeds 12345 \\\n    --flowpic-dims 32 \\\n    --augmentations noaug,rotate\n</code></pre></p> <p>Output</p> <pre><code>##########\n# campaign_id: 1696169491 | run 1/4 - time to completion 0:00:00\n##########\n\nsplit_indexes (2): [0, 3]\naugmentations (2): ['noaug', 'rotate']\nflowpic_dims  (1): [32]\nseeds         (1): [12345]\n</code></pre>"},{"location":"modeling/campaigns/#campaign-id","title":"Campaign id","text":"<p>Notice also how a campaign is associated to a <code>campaign_id</code> which by default is the unixtime when the campaign is launched. This can be changed using the <code>--campaign-id</code> options</p> <pre><code>tcbench campaign augment-at-loading \\\n    --dry-run \\\n    --method monolithic \\\n    --split-indexes 0,3 \\\n    --seeds 12345 \\\n    --flowpic-dims 32 \\\n    --augmentations noaug,rotate \\\n    --campaign-id my-wonderful-campaign\n</code></pre> <p>Output</p> <pre><code>##########\n# campaign_id: my-wonderful-campaign | run 1/4 - time to completion 0:00:00\n##########\n\nsplit_indexes (2): [0, 3]\naugmentations (2): ['noaug', 'rotate']\nflowpic_dims  (1): [32]\nseeds         (1): [12345]\n</code></pre>"},{"location":"modeling/campaigns/#estimated-completion","title":"Estimated completion","text":"<p>When a campaign is running, the console output reports a status update about the number of runs left and an estimation of the time to complete the whole campaign.</p> <p>This estimation is based on the average duration of the runs already completed. As such, it is simply a rough value.</p>"},{"location":"modeling/campaigns/#multi-server-multi-gpu","title":"Multi-server / Multi-GPU","text":"<p>The runs composing a campaign are run sequentially.  This is clearly suboptimal in a multi-server multi-gpu environment especially if individual runs are not very computational intensive.</p> <p>At this stage tcbench does not offer a mechanisms to distribute jobs across multiple servers. However, you can launch separate campaigns and then merge their output using the <code>aimrepo merge</code> subcommand.</p> <p>AIM remote server</p> <p>AIM version 3.17.4 has the ability to spawn a  remote tracking server but the functionality is still under development.</p>"},{"location":"modeling/exploring_artifacts/","title":"Exploring artifacts","text":"<p>The ML artifacts provided with the submission  (<code>ml_artifacts.tgz</code> on  figshare) corresponds to data gathered by the modeling campaigns created for the submission.</p> <p>You can explore/investigate ML artifacts in three ways</p> <ol> <li> Using AIM Web UI.</li> <li> Inspecting individual run artifacts.</li> <li> Inspecting AIM repository reports.</li> </ol>"},{"location":"modeling/exploring_artifacts/#ml-artifacts-overview","title":"ML artifacts overview","text":"<p>First of all, make sure to install <code>tcbench</code> and unpack the ML artifacts.</p> <p>You can verify that everything is ok by running two checks.</p> <p>Check #1: When installing <code>tcbench</code> you also installed <code>aim</code> so you can invoke it on the command line. </p> <p>For instance <pre><code>aim version\n</code></pre></p> <p>Output</p> <p><pre><code>Aim v3.17.5\n</code></pre> Note: You might get a different version as  we explicitly do NOT pin dependency packages version, as <code>tcbench</code> does not require strict dependencies.</p> <p>Check #2: You should have all ML artifacts located under  <code>code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/</code> with the following structure.</p> <pre><code>&lt;root&gt;\n\u2514\u2500\u2500 campaigns\n    \u251c\u2500\u2500 mirage19\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts10\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 mirage22\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 minpkts10\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts1000\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 ucdavis-icdm19\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-suppress-dropout\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-with-dropout\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 larger-trainset\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-dropout-and-projection\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-other-augmentation-pairs\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 |   \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter-packetloss\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 colorjitter-rotate\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rotate-changertt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rotate-packetloss\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 xgboost\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 noaugmentation-flowpic\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 noaugmentation-timeseries\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n    \u251c\u2500\u2500 ucdavis-icdm19-git-repo-forked\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(script-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_20\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(script-triggered)_10\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 RandomSampling_Retraining(human-triggered)_10\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 RandomSampling_Retraining(script-triggered)_10\n    \u2514\u2500\u2500 utmobilenet21\n        \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n            \u2514\u2500\u2500 minpkts10\n                \u251c\u2500\u2500 .aim\n                \u251c\u2500\u2500 artifacts\n                \u2514\u2500\u2500 campaign_summary\n</code></pre> <p>Each subfolder relates to a different campaign with some semantic encoded in the folder names themselves.</p> <ul> <li> <p>Subfolders containing an <code>.aim/</code> folder are AIM repositories and can be explored via the AIM web UI.</p> </li> <li> <p>Subfolders named <code>artifacts/</code> are ML file artifacts (see below).</p> </li> <li> <p>Subfolders named <code>campaign_summary/</code> contains reports summarizing a campaign (see below).</p> </li> </ul> <p>The following reference table details how the different campaigns map to the results of the submission.</p>"},{"location":"modeling/exploring_artifacts/#mapping-campaigns-folder-to-submission-results","title":"Mapping campaigns folder to submission results","text":"Subfolder Submission reference CLI trigger <code>ucdavis-icdm19/xgboost/noaugmentation-flowpic</code> Table 2 <code>ucdavis-icdm19/xgboost/noaugmentation-timeseries</code> Table 2 <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> Table 3,9; Figure 1,3,9 <code>ucdavis-icdm19/simclr-dropout-and-projection</code> Table 4 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss</code> Table 5 <code>ucdavis-icdm19/larger-trainset/augmentation-at-loading</code> Table 6 <code>mirage19/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>mirage22/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>mirage22/augmentation-at-loading-no-dropout/minpkts1000</code> Table 7; Figure 4,5 <code>utmobilenet21/augmentation-at-loading-no-dropout/minpkts10</code> Table 7; Figure 4,5 <code>ucdavis-icdm19-git-repo-forked</code> Table 8; Figure 8 <code>ucdavis-icdm19/augmentation-at-loading-suppress-dropout</code> Figure 9"},{"location":"modeling/exploring_artifacts/#aim-web-ui","title":"AIM Web UI","text":"<p>AIM web interface is quite intuitive and  the official documentation already provides  a general purpose tutorial.</p> <p>In this mini guide we limit to showcase a basic set  of operations to navigate the ML artifacts using the <code>campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository as example. The same operations can be  performed on all other campaings too.</p> <p>First of all, we need to spawn the web UI for the repository.</p> <p>Assuming we are in the root folder of the jupyter notebooks <code>notebooks/</code></p> <pre><code>aim up --repo submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>Running Aim UI on repo `&lt;Repo#-3653246895908991301 path=/home/johndoe/code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/.aim read_only=None&gt;`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n</code></pre> <p>Run <code>aim up --help</code> for more options (e.g., specifying a different port or hostname).</p> <p>When visiting the URL reported in the output  you land on the home page of the AIM repository.</p> <p>This collects a variety of aggregate metrics  and track activity over time.  Hence, in our scenario the home page of the ML artifacts are mostly empty because all campaigns were generated in a specific moment in time.</p> <p></p> <p>The left side bar allows switch the view. In particular, \"Runs\" show a tabular view of the runs collected in the repository.</p> <p></p> <p>From the view you can see the hash of each run and scrolling horizontally you can glance  over the metadata stored for each run.</p> <p></p> <p>The search bar on the top of the page allows to filter runs. It accept python expression bounded to a <code>run</code> entry point.</p> <p>For instance, in the following example we filter one specific run based on hyper parameters.</p> <p></p> <p>When clicking the hash of a run (e.g., the one we filtered) we switch to a per-run view which further details the collected metadata of the selected run.</p> <p></p> <p>For instance, when scrolling at the bottom of the per-run page we can see that AIM details</p> <ul> <li> <p>The specific git commit used when executing the run.</p> </li> <li> <p>The specific python packages and related versions available in the environment when executing the run.</p> </li> </ul> <p>Both are automatically tracked by AIM with no extra code required (beside activating the  their collection when creating the run).</p> <p></p> <p>The per-run view offers a variety of information organized in multiple tabs.</p> <p>For instance, the tab \"Logs\" details the console output.</p> <p></p>"},{"location":"modeling/exploring_artifacts/#run-artifacts","title":"Run artifacts","text":"<p>Unfortunately, AIM has little support for  tracking general file artifacts and no native support for storing trained models.</p> <p>For instance from the last screenshot above we can see that we could bind to a run some images, audio files and some text.  While the first two are out of the scope for network traffic-related datasets, text tracking is meant for free form \"blob\" output.</p> <p>Hence, <code>tcbench</code> associates to a run  an \"artifacts folder\", named as the hash of the run, where a variety of files are saved.</p> <p>Specifically, each run is associated to the following files:</p> <ul> <li> <p><code>params.yml</code> is a YAML file collecting ALL parameters used when triggering a run, i.e., both the arguments explicitly defined on the command line, as well the ones with default values.</p> </li> <li> <p><code>log.txt</code> collects the console output generated by the run.</p> </li> <li> <p><code>best_model_weights_split_&lt;split-index&gt;.pt</code> stores the weights of the best  trained pytorch model (for a deep learning model). The filename is bounded to the specific split index configured when triggering the run.</p> </li> <li> <p><code>xgb_model_split_&lt;split-index&gt;.json</code> stores an XGBoost model (when training  via xgboost). The filename is bounded to the specific split index configured when triggering the run.</p> </li> <li> <p><code>&lt;context&gt;_class_rep.csv</code> contains a classification report. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> <li> <p><code>&lt;context&gt;_conf_mtx.csv</code> contains confusion matrix. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> </ul> <p>Why saving extra files?</p> <p>Future version of <code>tcbench</code> will likely integrate with solutions such as DVC to further strenghtening file artifacts handling and tracking also the input data version.</p>"},{"location":"modeling/exploring_artifacts/#aim-repository-reports","title":"AIM repository reports","text":"<p>To complement AIM web UI and manual inspection of ML  artifacts, <code>tcbench</code> offers a few sub-commands to overview an AIM repository  content.</p> <pre><code>tcbench aimrepo --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo [OPTIONS] COMMAND [ARGS]...\n\n Investigate AIM repository content.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ls                    List a subset of properties of each run.                                  \u2502\n\u2502 properties            List properties across all runs.                                          \u2502\n\u2502 report                Summarize runs performance metrics.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In the following we illustrate each sub-command using the  <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository</p>"},{"location":"modeling/exploring_artifacts/#ls","title":"<code>ls</code>","text":"<p>The <code>ls</code> sub-command simply list  the hash, creation time and end time of each run.</p> <pre><code> tcbench aimrepo ls \\\n    --aim-repo code_artifacts_paper132/notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code> hash                      creation_time      end_time\n 8ba8f05ec1604574b71139b4  1684486220.186745  1684487165.465486\n 67503d171467435091714d45  1684485354.995328  1684486219.662343\n 9a106b1a209b4a47abb5c17f  1684484451.951014  1684485354.490275\n bfe6d326e299461cb56cdc5f  1684483871.600086  1684484451.518072\n 87fa4af7e99d478aaf22f4da  1684483362.006482  1684483871.393643\n c13844d1704c45fc95653b1f  1684482566.637334  1684483361.812976\n c7bdc0d409c4403bbe5b9cc4  1684482432.924283  1684484618.028195\n 043419ccd36541fd8a4057cb  1684482146.1981    1684482566.387968\n 963f7797f19d4976a3efcca3  1684481692.788042  1684482145.885839\n 4d81379229814f409b7647d3  1684481202.271289  1684481692.537144\n fe35f9380c344b8aa9c23f03  1684481101.181884  1684482432.517077\n 2f6d76f632c64391b7cb4973  1684480123.949839  1684481201.997798\n b3df56698ec6416cac21add4  1684479475.427219  1684481101.055126\n 72ac1602329b415dbf17524f  1684479393.541195  1684480123.490255\n 68832122ee434d799da3ec6c  1684478564.503901  1684479474.865144\n 98033a73d4d1433ab3779742  1684478415.758284  1684479393.015683\n c6aea7ff760b4ad090d9dd36  1684477658.120382  1684478415.53678\n 7e0386cdcbaa454e892b5fb9  1684477439.749558  1684478563.989332\n d985dfe5a5a6422293ff34eb  1684476688.49235   1684477657.471275\n 889dc5128693473f9fd79299  1684476501.215957  1684477439.58382\n 922fef07ff264188a2e370a5  1684475858.118653  1684476688.059573\n cfb5812f3ca84c2bb440e6c4  1684475376.248467  1684476501.070375\n 99fc7d6feea64a07a9f4499d  1684475282.908762  1684475857.977606\n 728dfe6ff0584b0db57b8f22  1684474888.182631  1684475282.672584\n 9dde5426e9424f9aa0eda8ef  1684474417.41854   1684474887.912377\n d8845f81ebbc4fbdbb7efc6e  1684474280.592608  1684475375.693128\n 4ca5aff6b9c442808f39d2c3  1684473415.875285  1684474417.168511\n f24dfd8ef3e44cd6ae37c4d7  1684473045.421477  1684474280.137564\n 9d530cacbbfc4300846c412a  1684472369.171916  1684473415.375791\n b222cda5054744ee923cc1a5  1684471606.606575  1684473045.147482\n 5c9c03c663ea493480ff176c  1684471433.960361  1684472736.412599\n 1e13e4b8c60a4dd084ca607b  1684471290.69116   1684472368.905358\n 4c42b65f85e444579adabd51  1684470601.838798  1684471289.976878\n 4f759432db0743c7b3d95f6b  1684470452.527724  1684471433.756871\n 1a71035742a940c7a74c6a9b  1684470331.022178  1684471606.113162\n 1f2bc263b9ed450abcf1c1d3  1684469705.56874   1684470601.122689\n 9951ad2c1f894efcb04d39bb  1684469472.837658  1684470451.971661\n dd4ec191cd0945a184c9aef8  1684469140.912997  1684469472.087474\n 60b55181ebaa4ccebc214875  1684468906.545817  1684470330.375135\n d74b79343de8419b874f5210  1684468808.347594  1684469705.18729\n 0a30bf4e578445239c3e91cb  1684468778.110371  1684469140.702514\n 1d0327a026444eee800dc9a1  1684468500.060794  1684468777.756142\n e40d612c892644f59370cd68  1684468394.226275  1684468808.078585\n 67c4857856d445d4809e1fd2  1684467913.759307  1684468394.08247\n cad4cd97d854419e91869d85  1684467543.981347  1684468905.978667\n a7018c0846304408bc75e8c8  1684467526.249917  1684467913.503346\n 39c042def7b94710bc211b0e  1684466951.957943  1684468499.463829\n e61e21d87ccd4aa8b8a2ed35  1684466335.110533  1684467526.054343\n b1869271d50644c696fb5246  1684466131.933304  1684467543.395299\n e0dac9c82bf04bafb4aade4c  1684465619.566809  1684466951.462375\n ed199b3b6a71488893e1abfb  1684465434.028724  1684466333.918455\n 7dc2a753e3f74ed69f069ea8  1684464725.062243  1684466131.24298\n 03a5cdeb32354bfdba1de136  1684464569.689175  1684465433.542933\n 37c00f6c1ebe44fda8a9618d  1684464407.673333  1684465618.991646\n f620bcb6afe24d2a87a778c0  1684464064.526061  1684464406.941772\n 9cdde43cddd640e2b440b1c4  1684463741.103362  1684464064.054385\n ffb1a45a1fb249c0bf3f463c  1684463607.054282  1684464569.411643\n 395fc5799efc4f34823b3655  1684463449.453486  1684463740.94038\n 5a0a0a5c3bb244d497538de4  1684463312.066392  1684464724.410855\n 0638ad838f4a429b91131a5b  1684462917.872626  1684463606.433408\n 51aa03f39a3e42e2b99db486  1684462233.665966  1684463311.043898\n 57eb602e1dad4933b5eca155  1684462207.043093  1684463448.792002\n 01fea90ec67544d3a39ef997  1684461943.457484  1684462917.499634\n d792e20d40604c78956ab5a2  1684461464.25762   1684461943.313054\n 637badf3dfec4b93a78d9919  1684461042.264266  1684461464.126894\n f87edacfdacf4b5faee625b4  1684460961.382713  1684462233.005103\n 444c2e1ebfaa43a78c6b4c96  1684460467.126398  1684461042.125671\n 9e5135774a89458b8dae96ef  1684459980.097812  1684462206.830038\n ca77db360e8f45d5a8d2f3b7  1684459693.93209   1684460466.346116\n adf0905c3c8349d590574c38  1684459300.071951  1684460960.695716\n 916583de1a524ed092408d72  1684459069.160377  1684459693.416057\n 7c25e7eca05e49ffa82e1dda  1684458823.182091  1684459979.576512\n e77cf8e464b2496a94229ca8  1684458538.286254  1684458823.017745\n 70fef369b9e741e59a9cb092  1684458437.369592  1684459067.97458\n 73c779ffb561448281772cca  1684458244.2756    1684458537.96287\n c21d1b9a093c42cbb1421b99  1684458241.060798  1684459299.580839\n e0c07fd0f3664789a68a97fd  1684457885.407864  1684458243.499636\n 7b52359ef3ef42219b6297ef  1684457777.173311  1684458436.747236\n aecb143222764ac2badda996  1684457079.7536    1684457776.789753\n 83850279a1fd43e1892771ea  1684457024.45417   1684458240.375002\n c80ccdfec89b4ac68013dc57  1684456500.647965  1684457079.174438\n bd8b6929f7bb41eeb00e8451  1684456429.302551  1684457884.834434\n cca820ec5ae24ee793b45ef3  1684456007.859746  1684456500.321697\n ad89e8df0be34133bb61e589  1684455782.341968  1684457023.582\n d9613c247ffc49ae83b1864d  1684455514.437144  1684456007.704841\n f4f28ffbd85041d78d032bf7  1684455198.659235  1684456428.772103\n 17aa80f35ba240d6a3c27681  1684454983.894229  1684455514.144682\n 376d83d39ee34bf6a5f14553  1684454900.637969  1684454978.036799\n d66de4d4c0f043289d0f9671  1684454828.019291  1684454900.501477\n 53e969d8626c41df92814c61  1684454756.332229  1684454827.858425\n 988b1457caba480b817ba385  1684454684.082903  1684454756.197079\n f60fc0f56310461ebd15ccb1  1684454602.697385  1684454683.934212\n 4231173e05fe4466bb3592e5  1684454519.796744  1684454602.561087\n b7bcc2cb30054c9d89b81e71  1684454436.86775   1684454519.639057\n 0eb416e6597f4205a4eb30e5  1684454350.202147  1684454436.706402\n 01d4bb96f3d547b0b67f3219  1684454275.221716  1684454350.058379\n 2863d94709a241549ea43d1e  1684454180.176362  1684454275.095384\n 608b4ff892424b909c37195b  1684454131.059445  1684455781.74821\n 2f4819882cab4dbc8bb4e599  1684454101.90471   1684454180.044018\n dc3e8dfccc964658b1094227  1684454031.000489  1684454101.748034\n 95e5b09ba7d44fabbb055f93  1684453973.055474  1684454030.873004\n 3ab8c90f935845bc85d9e7c0  1684453916.309955  1684453972.93122\n 8ed7f1729fd5456cbf9bf9f0  1684453858.633202  1684453916.18064\n 2838f1c9925a4276b596f29a  1684453857.763272  1684453999.474313\n fb37ebad53be4b8bb0922c43  1684453854.938354  1684455198.416701\n dbc9c91611dd40d18da3a563  1684453800.945448  1684453857.692718\n 0922df9981cb4dada7cb8e70  1684453783.490234  1684453858.502442\n 577b94c1058543fbbbac556c  1684453740.054494  1684453800.871211\n 1e8636dca105456d96c9d911  1684453708.58691   1684453783.36737\n 2927b88ec0064c1b96f2420d  1684453669.588055  1684453739.989276\n 1f79705713004475b0f4b423  1684453641.653473  1684453708.452254\n 816d410679ce4d3d93c9fb13  1684453609.559607  1684453669.528333\n f20a0864a0154d14bc62bef9  1684453603.103946  1684453641.526248\n 7ab9bc7b21f84284a8c06e2c  1684453562.770793  1684453602.990837\n b7dbd5917875476a9f331342  1684453554.43456   1684453609.487829\n e711aaf96ecc418cb1629a36  1684453522.761069  1684453562.665403\n c4d648e857fa404dbec12d64  1684453500.782744  1684453554.364838\n 810da0118ed7417db7c4771e  1684453469.291014  1684453854.683895\n 5d8a70cabc464fcbbd0a904d  1684453444.704697  1684453500.709896\n 5634546f547945849b466cf5  1684453420.998128  1684453522.623544\n c4ed1533ddbf43b5ba8f83ab  1684453389.201033  1684453444.618217\n cc13b218cbfd4cf08e5d20a4  1684453337.641638  1684453389.137557\n 610b507713cb47f4963ef671  1684453316.818646  1684453420.841483\n 36f2e4b6045b4f01b70e4680  1684453287.041721  1684453337.567413\n 37f99c3ed79a45f3b7d40a5f  1684453246.21599   1684453316.660344\n 5fe2730f842e416c8af9c9b3  1684453237.076659  1684453286.959855\n e25a532da42c4327bc11528a  1684453190.214313  1684453237.018725\n 7aea23e113cc4a289d7bfa79  1684453161.004546  1684453246.06685\n 45429199088f4c0c8b705537  1684453142.490356  1684453190.154943\n ab079de736834eef899c21bb  1684453101.882433  1684454130.262774\n fa468f7b03e3475daf215746  1684453095.863897  1684453142.425248\n add75d33ef8b4d638e770aaf  1684453075.861901  1684453468.361486\n 2aeb5a16085f42e9ab4094df  1684453065.579356  1684453160.874607\n 8e87b04207e54931b865d237  1684453016.800851  1684453095.802078\n 76f30367c8ab44b0bbe67d03  1684452977.614285  1684453065.424347\n deaffa25f04b4aae856c632f  1684452942.818858  1684453016.74139\n 5cae649ac7bb46d5a60c72ac  1684452877.834861  1684452942.748545\n 4a0efe2a9ecb499983245191  1684452870.028023  1684452977.473543\n 2e0474d626b94b5e957a3774  1684452847.893095  1684452877.790754\n 50f15777a7174531bbf50a1c  1684452818.958093  1684452847.832531\n b924f0f921e04b64828975ba  1684452789.569578  1684452818.902016\n b418ebaa6576432290bfe7ae  1684452774.458869  1684452869.895356\n b9f6b0815edf4ddfb34dc177  1684452728.81128   1684452789.506862\n 01b5d3ec52da4424a37125df  1684452700.006671  1684452774.319969\n f93d353566c24408bb3fc997  1684452663.618535  1684452728.740588\n dca1cc1dc9cc45f69bf45471  1684452629.495522  1684453074.989142\n 61a54b25733740b586ae3788  1684452604.757812  1684452663.541514\n 762d10ed26ae46ab8752cbfb  1684452601.345309  1684452699.879312\n 6cbe5bb9f82141b8abb5d2c8  1684452537.019976  1684452604.693619\n f2d476a93fef4c7abc727453  1684452508.867201  1684452601.21922\n 3d69205834a84112b65fcaef  1684452472.610688  1684452536.957576\n f2c44a9bc9f14ad4b3b827b8  1684452419.986378  1684452508.722681\n 4d2f061684c842d793d53458  1684452405.098763  1684452472.542425\n f05cbbb211a545d8b726a301  1684452365.427215  1684452419.85173\n 5f451dce01f04ee9b0169828  1684452346.577028  1684452405.012775\n 234669be443d4f08ac6dd900  1684452308.986717  1684452365.299411\n e8d6da0defd049028aaea9d5  1684452290.203461  1684452346.517978\n bd57bec0663e479ea2934678  1684452252.92531   1684452308.85739\n 352f015ecc004b7f922e9675  1684452225.314651  1684452290.121976\n 47a4a0891c024ff9ad3e10a5  1684452168.422513  1684452252.796179\n 06da93cc03a44040b0c74178  1684452157.739649  1684452225.250497\n e3d645b0dc16421ca3209896  1684452137.461206  1684453101.39133\n 7bdb5341a2614a888c577a09  1684452097.493495  1684452157.677333\n 4bbd8caafebe4379b130c6b0  1684452090.542063  1684452168.28824\n 7b25cd869f00439fafbed535  1684452029.411407  1684452097.43651\n 92c9588486534986a7a5eacf  1684451984.658156  1684452029.353263\n 61d85dcf712b434fac9a5b1f  1684451983.953664  1684452090.41075\n a12b3724c60d4958996de377  1684451944.910572  1684451983.848708\n 66dda78e98484ac695ac28c2  1684451937.460639  1684451984.606509\n 6693f2a4a645455cada34389  1684451908.020474  1684451944.789062\n e1169972d26140e18d897023  1684451886.957104  1684451937.392489\n 822c84da50f74667920f03bf  1684451871.324137  1684451907.890575\n 77bb1cec15af4c23b8be0f2f  1684451821.675738  1684451886.891239\n cf3545083d794dee8e0d2e9a  1684451812.399059  1684451871.18422\n 38171ed6da8f44e1a555b9d8  1684451748.611645  1684451821.617548\n a94ec554111b4a1a89cca0f6  1684451744.945461  1684451812.24693\n f20aca505c37459abb281e08  1684451656.412687  1684451744.789916\n 5c4aa95035c841d4a5e96b0c  1684451556.350265  1684451656.276747\n 6aa44846db614702867c9623  1684451546.981732  1684451748.381051\n c727a871d64b4b84ab39ce7b  1684451472.885078  1684451556.217995\n f1444a30a0b64ddebb0ee893  1684451451.528096  1684451546.751202\n 83ecc5c955ec4023bc82b30d  1684451382.832542  1684451472.73465\n 6c7b1cbe72004a0898b331e1  1684451373.367619  1684451451.294249\n 55a0901ebd8c4a14a006bda0  1684451332.395285  1684452629.058694\n 9c0d10c9e8f04194b6cf322c  1684451332.169061  1684451373.247294\n 1fe3f3cff5364183b91b0161  1684451311.409357  1684451382.69011\n e64349682cac4e96bdb3a195  1684451305.523624  1684452136.669685\n b6f5f861bfd045b79910163f  1684451268.06238   1684451332.097281\n 24db445e86664a5688d5b058  1684451244.011303  1684451311.247374\n 707b1d8f242c4c569397818f  1684451201.961014  1684451267.986849\n b5870e5dd6ac4433a59e949e  1684451151.07773   1684451243.875269\n 972de7c3956d4d96b27fa2e7  1684451147.277512  1684451201.870307\n 007ff3eb7d3644c9bfa97590  1684451089.133811  1684451147.206315\n be295d1045b64e5da2dad253  1684451086.977876  1684451150.932065\n 008ab3a058424242b2c74721  1684451024.406991  1684451089.065439\n 8587efea57d444b0a37a4d87  1684451012.747797  1684451086.845493\n 46cfa3ec77604b18b5cbb952  1684450970.955052  1684451024.350877\n 473c10c9d83b4a2f9f7aaacd  1684450935.840727  1684451012.61568\n 9749d96f77754ee3a1dd82dc  1684450910.230388  1684450970.876716\n bd622c8c454a43b2a526e255  1684450878.062955  1684450935.71623\n 1b69360731f64dbabd3e6b4a  1684450856.297448  1684450910.156479\n f1c8afdee5f843dea2cd6326  1684450820.288095  1684450877.926006\n 8719913ab19c470090e2cbba  1684450789.546818  1684450856.227493\n 64cc636a24de4af085a90012  1684450764.779803  1684450820.161206\n 38f7ad203bd3441cb27e8477  1684450726.736646  1684450789.489278\n 2006e773a42a45169c8af288  1684450700.498349  1684450764.63308\n f48a8088282a4f22a3060aad  1684450628.084315  1684450726.679969\n 07741993d3ff4d4c85cd2378  1684450625.213713  1684450700.367589\n 78897d14594d4d8ab0511b09  1684450532.651023  1684450625.066728\n c058300f4d55480183fe7d0a  1684450500.728736  1684450627.9136\n 70457b6d2ca34c9ca54faa9c  1684450494.905926  1684450532.530795\n fae0d0098235451582379e49  1684450457.158045  1684450494.787555\n eb0656be43d64f4abc07d913  1684450420.256119  1684450457.035452\n 3f6b15bdbb4c4f7990139390  1684450373.169881  1684450500.508234\n 37fb6128cf0940b0a8e02a6d  1684450346.729378  1684450420.123028\n 59850e2e072c4315bcfd44f9  1684450317.249483  1684451331.988945\n eb23366973d54adcb7ae41dc  1684450309.69558   1684450372.983297\n 4c51c9d6344741a3b4167ace  1684450270.958624  1684450346.587809\n 0adf8ec7b2d44e83b9d1ecd6  1684450264.164186  1684450309.641298\n 7663b098261c42b6ae806446  1684450255.519111  1684451304.891808\n 8d87effdeaf045bd8e1793be  1684450191.755371  1684450270.815829\n 4afff20583a7446a87f532bb  1684450174.269007  1684450264.10297\n a12f8c41aa7645fb8dce11db  1684450113.72971   1684450191.620866\n 278b560f285e4b3e968d2500  1684450112.787798  1684450174.213632\n 8232054671804f4a9f049ac8  1684450038.59175   1684450113.590469\n 8a2aaa0359c14bee82d7a01b  1684450038.553256  1684450112.718496\n cc14c289c387497299647aff  1684450009.504617  1684450038.503996\n c1d541e31bf04c34a8361277  1684449981.075677  1684450009.447578\n f6ae1ffbf09b4b829845768a  1684449960.174022  1684450038.453731\n e55525d57f8f4c0fbe62925c  1684449951.684003  1684449981.027956\n 3b3c950211ec4ac1861d2ef2  1684449896.906696  1684449951.618761\n a6156d0fedfb4da58d7ed157  1684449886.685146  1684449960.026201\n 34fb83c203bb4950bd629d93  1684449847.082951  1684449896.838466\n 32315d3421a643f8a60e3cc6  1684449808.13473   1684449886.545201\n 6db2eb1e1dd347acba48e369  1684449790.974563  1684449847.000392\n 63c5de9bc5ae4bca84a04a12  1684449737.9346    1684449807.970432\n ec202059cb874f4f9ca4205e  1684449725.824672  1684449790.911293\n ae692ba4c260457d9a3e442a  1684449668.959527  1684449725.763858\n a96434d27b8e45d3ad8b81ea  1684449659.840383  1684449737.633578\n 5c28fb0c4cae4a5eb3e2bf37  1684449609.2497    1684449668.911\n 401d87dc0f9d4f7fb3aa52df  1684449583.918668  1684449659.708532\n c9c4b346ff394c8488e9790d  1684449545.541382  1684449609.186583\n 74ba1657c61644e2a5e7da30  1684449520.169214  1684449583.771104\n ae7a6eb3e85a48228d3371b4  1684449492.929299  1684449545.465121\n 52615c37cb4c403a8bf7a025  1684449464.493325  1684449520.040155\n 4969fb8c374141028594d962  1684449437.814941  1684449492.858044\n 1e96377382f443ccbe7e85f0  1684449405.594734  1684449464.356868\n 776699b253614175b7f827ff  1684449387.696488  1684449437.748665\n 5687406675c34ef8bead5633  1684449346.890198  1684449405.47788\n 387e01ef08454e2eb2cff697  1684449333.183408  1684449387.6353\n 685af93f85ac4b67bb99144e  1684449302.386924  1684450255.027677\n 4ea944b1be754ab48bcb7d45  1684449276.832263  1684449346.737273\n db1c87ebb5494363a2fb2d3b  1684449237.813694  1684449333.12579\n 3e1533ac0f164dc092b7b681  1684449204.473322  1684449276.677444\n 61882760258d44d793c721dd  1684449151.709934  1684449237.749622\n 385286c19cb14a0c9571e0e9  1684449129.216098  1684449204.342291\n 63a50b8c64884ec5aa61cfa1  1684449091.168293  1684449129.090858\n a9b40ede978e4fb4bfc120b7  1684449055.11729   1684449091.054672\n cb78f86b48284696aa938fcc  1684449029.36249   1684449151.468267\n 47595fc5c64146e9805b51e6  1684449017.959478  1684449054.990691\n bf2c9394ea3742e683452a22  1684448936.264374  1684449029.147291\n c8f67a99057c49a2944bf39a  1684448932.684268  1684449017.804864\n a4cf32694e6e411ea84aa47a  1684448931.706416  1684450316.541687\n 4b3570a3a50c41c78f0000f4  1684448858.610413  1684448936.197166\n b21d99a3202e4da3b484ae4a  1684448837.720911  1684448932.548398\n f099eb327fd7494db384bb14  1684448765.679447  1684448858.545372\n 47eccc984daf4834aec19f67  1684448756.36372   1684448837.570065\n 7aaac8ecd2e644eda14968cf  1684448674.903293  1684448765.625554\n 43a503bf40aa4f518a97cf4e  1684448667.212994  1684448756.204382\n 230c734960ea4c849e3af2e1  1684448645.431733  1684448674.860142\n a6a20facc3f34662aa660fee  1684448615.158763  1684448645.376438\n cd65b0169cf441bfb6e3dcd8  1684448585.157683  1684448615.106011\n a0bc2e31538249aab969fc0d  1684448580.386622  1684448667.054529\n 3ee8b28573704e4ba79cf698  1684448527.514667  1684448585.087628\n 0712bcf076c44d239f8def39  1684448505.486607  1684448580.237805\n 45cf18b3d09a40098e621079  1684448465.274798  1684448527.439574\n 40cde1e6fa674cc3bfd82bb6  1684448423.393276  1684448931.366076\n d59fb822d0c346e496361cc6  1684448421.81773   1684448505.32149\n 13d14051be32424d92613289  1684448410.573326  1684448465.203814\n f05bc193e27d4178986a5662  1684448379.497623  1684449301.840416\n 3e378ebe488442e59997f8d2  1684448349.959204  1684448421.651724\n e5fc7761ea6a4e04bbb16b74  1684448346.800847  1684448410.504434\n 7538f4623894469ea5db1024  1684448284.44457   1684448346.71941\n c81913cba557469a90999dfa  1684448266.979889  1684448349.801789\n 6a742ccb640e4594a353c49a  1684448229.388576  1684448284.376865\n 75d1aa21751a4d0085f7c78c  1684448190.969628  1684448266.848555\n 321f7acb616040cda1232be3  1684448178.230579  1684448229.301771\n f325127637554978a28c7c4c  1684448123.997501  1684448178.150143\n ba4003ab5d674388913ad196  1684448122.292348  1684448190.811933\n ecf0408ff75948cdad056c2a  1684448069.866839  1684448123.924316\n a96a951613e947c596cd48fa  1684448051.908431  1684448122.161052\n 274ed9aae2324ebdb6b0e59a  1684448023.663688  1684448423.171355\n 81e993bee47043319ff1ab6e  1684448007.683417  1684448069.801989\n a8bdcf0e5d6f4645a7a76382  1684447991.732175  1684448051.786494\n a1f84df603a3455b8bc91007  1684447955.241488  1684448007.621067\n df5e5a6eed6542739b67f833  1684447937.451409  1684447991.609256\n f1e61e62d6fb484c9122547e  1684447898.394327  1684447955.175681\n c59bc63eb8f041049543f1d2  1684447880.325154  1684447937.315866\n 1e5a6ac24e9540f89c0c54cf  1684447853.491841  1684447898.333804\n c21a103dfa35473390e9d1fe  1684447807.994382  1684447853.423596\n e5f3fc273b114ace8bcb8478  1684447807.815813  1684447880.187383\n 0d92ca4591404ba889239d2f  1684447760.682189  1684447807.938637\n 53046971da764da1b343c6c5  1684447727.930714  1684447807.672719\n f65eae9d11414922a5cc531e  1684447662.420696  1684447760.621397\n 5e9105ff533e4b44a0dfc577  1684447636.207182  1684447727.776517\n 7e7fd7824169464b9602d7cc  1684447598.500283  1684447636.086839\n a669d0df5eaa4e47a20f5d94  1684447572.937839  1684448379.216737\n 426257bb89474ca8a339523b  1684447565.763093  1684447662.363762\n 8cc32087458448c7bd165ea0  1684447561.149784  1684447598.372651\n caaffcdf9919419cb3616619  1684447520.023255  1684447561.032779\n 3ace3b501ffc4d3d9f006a4c  1684447477.174045  1684447565.696864\n 2ec24496c7b44bc1abb9d07d  1684447446.502455  1684447477.122609\n d74763a98ac54a899facc16e  1684447417.775803  1684447446.44637\n 7e702018ebfe4d96b96b84c5  1684447395.8736    1684448023.427643\n 18dd6fca9c6944fb9ac0b77f  1684447384.730554  1684447417.721719\n</code></pre>"},{"location":"modeling/exploring_artifacts/#properties","title":"<code>properties</code>","text":"<p>The <code>properties</code> sub-command shows aggregate information and the list of unique values of the properties of an AIM repo (i.e., run hyper params and other meta data).</p> <pre><code>tcbench aimrepo properties \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name                   \u2502 No. unique \u2502 Value                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 runs                   \u2502          - \u2502 315                                                       \u2502\n\u2502 duration (mean \u00b1 std)  \u2502          - \u2502 5m44s \u00b1 7m40s                                             \u2502\n\u2502 metrics                \u2502          4 \u2502 ['acc', 'best_epoch', 'best_loss', 'loss']                \u2502\n\u2502 contexts               \u2502          5 \u2502 ['test-human', 'test-script', 'test-train-val-leftover',  \u2502\n\u2502                        \u2502            \u2502 'train', 'val']                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 experiment             \u2502          1 \u2502 ['augmentation-at-loading']                               \u2502\n\u2502 aug_name               \u2502          7 \u2502 ['changertt', 'colorjitter', 'horizontalflip', 'noaug',   \u2502\n\u2502                        \u2502            \u2502 'packetloss', 'rotate', 'timeshift']                      \u2502\n\u2502 campaign_exp_idx       \u2502        105 \u2502 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,   \u2502\n\u2502                        \u2502            \u2502 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,   \u2502\n\u2502                        \u2502            \u2502 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,   \u2502\n\u2502                        \u2502            \u2502 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,   \u2502\n\u2502                        \u2502            \u2502 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,   \u2502\n\u2502                        \u2502            \u2502 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,   \u2502\n\u2502                        \u2502            \u2502 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,  \u2502\n\u2502                        \u2502            \u2502 101, 102, 103, 104, 105]                                  \u2502\n\u2502 campaign_id            \u2502          1 \u2502 ['1684447037']                                            \u2502\n\u2502 dataset                \u2502          1 \u2502 ['ucdavis-icdm19']                                        \u2502\n\u2502 flowpic_block_duration \u2502          1 \u2502 [15]                                                      \u2502\n\u2502 flowpic_dim            \u2502          3 \u2502 [32, 64, 1500]                                            \u2502\n\u2502 patience_steps         \u2502          1 \u2502 [5]                                                       \u2502\n\u2502 seed                   \u2502          3 \u2502 [42, 666, 12345]                                          \u2502\n\u2502 split_index            \u2502          5 \u2502 [0, 1, 2, 3, 4]                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The table is split into two parts to separate general properties (top) from hyper parameters (bottom). General properties are common across repositories while hyper parameters vary depending of the campaign.</p> <p>What is a context?</p> <p>The term is borrored from AIM terminology and refers to ability to group metadata into categories. </p> <p>For instance, in the example above, the 4 listed metrics are separately stored for each listed context.</p>"},{"location":"modeling/exploring_artifacts/#report","title":"<code>report</code>","text":"<p>The <code>report</code> sub-command provides an aggregated summary across metrics grouped by properties</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                                    hparams                           acc                  duration\n                   split        aug_name   flowpic_dim  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              test-human       changertt            32    15  70.04   4.41   2.44    83.93      7.8     4.32\n                                                    64    15  72.05    2.1   1.16    61.57      5.2     2.88\n                                                  1500    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  68.84   4.69   2.59    78.01     10.9     6.03\n                                                    64    15  71.33   3.35   1.86    67.15    22.49    12.45\n                                                  1500    15  68.59   3.17   1.76   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                                    64    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                                  1500    15  73.82   1.47   0.82    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  69.48   2.12   1.17    37.97     1.46     0.81\n                                                    64    15  69.88   2.28   1.26    38.06    20.21    11.19\n                                                  1500    15  68.67   1.93   1.07   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                                    64    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                                  1500    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                                    64    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                                  1500    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  70.36   2.98   1.65    80.08    12.72     7.04\n                                                    64    15  72.53   1.83   1.02    64.23    21.91    12.13\n                                                  1500    15  70.84   2.42   1.34   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             test-script       changertt            32    15  97.33   0.71   0.39    83.93      7.8     4.32\n                                                    64    15  97.29   0.64   0.35    61.57      5.2     2.88\n                                                  1500    15   96.8   0.63   0.35  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.87    0.8   0.45    78.01     10.9     6.03\n                                                    64    15  97.42    1.2   0.67    67.15    22.49    12.45\n                                                  1500    15  94.89    1.5   0.83   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.11   0.74   0.41     56.9     1.64     0.91\n                                                    64    15  95.96   0.89   0.49    63.86    28.97    16.04\n                                                  1500    15  95.11   1.23   0.68    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.73   0.49   0.27    37.97     1.46     0.81\n                                                    64    15  95.96   0.53   0.29    38.06    20.21    11.19\n                                                  1500    15  94.44   1.63    0.9   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.98   0.87   0.48    80.83    11.11     6.15\n                                                    64    15  96.89   0.96   0.53    57.08     4.75     2.63\n                                                  1500    15  95.96   1.27    0.7  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.36   0.71   0.39    78.52     11.1     6.15\n                                                    64    15  96.89    0.7   0.39    88.49    33.41     18.5\n                                                  1500    15  95.47   0.84   0.47  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  96.71   0.92   0.51    80.08    12.72     7.04\n                                                    64    15  97.11   0.65   0.36    64.23    21.91    12.13\n                                                  1500    15   96.8   0.57   0.32   907.03   165.48    91.64\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-train-val-leftover       changertt            32    15  98.24   0.56   0.31    83.93      7.8     4.32\n                                                    64    15  98.29   0.71   0.39    61.57      5.2     2.88\n                                                  1500    15  98.43   0.22   0.12  1303.72   336.55   186.37\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.46    0.6   0.33    78.01     10.9     6.03\n                                                    64    15  96.82   0.74   0.41    67.15    22.49    12.45\n                                                  1500    15  95.79   0.91    0.5   765.43   152.53    84.47\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.88   0.46   0.25     56.9     1.64     0.91\n                                                    64    15  96.38   0.91    0.5    63.86    28.97    16.04\n                                                  1500    15  96.47   1.02   0.57    471.8     58.6    32.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  96.05   0.35   0.19    37.97     1.46     0.81\n                                                    64    15  96.22   0.57   0.31    38.06    20.21    11.19\n                                                  1500    15  95.62   0.91   0.51   374.88    94.41    52.28\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  97.47   0.64   0.35    80.83    11.11     6.15\n                                                    64    15  97.48    0.5   0.28    57.08     4.75     2.63\n                                                  1500    15  97.29   0.49   0.27  1164.89   245.69   136.06\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  97.01   0.43   0.24    78.52     11.1     6.15\n                                                    64    15  97.28   0.62   0.34    88.49    33.41     18.5\n                                                  1500    15  95.93   0.74   0.41  1313.58    301.7   167.08\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  97.44   0.75   0.42    80.08    12.72     7.04\n                                                    64    15  97.78   0.68   0.38    64.23    21.91    12.13\n                                                  1500    15  97.94   0.34   0.19   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The console output is informing about the <code>campaign_id</code>  found in the repository and the total number of runs. If the repository was containing more than one campaign, a different report table would have been created for each campaign.</p> <p>The report is always grouped by <code>test-&lt;XYZ&gt;</code> contexts. By default all hyper parameters with more than one value are also added (see the previous description about properties).</p> <p>In the example, the <code>acc</code> metric is measured with mean, standard deviation and 95 %tile confidence intervals. Next to the metric is reported also <code>duration</code> which  corresponds to the overall time for train/validation/test  in the run execution.</p> <p>One can rearrange the table composition using the <code>--groupby</code> and <code>--contexts</code> options For instance, in the following we swap the order of the <code>hparams</code> used and report only on one context.</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/submission_tables_and_figures/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/ \\\n    --groupby flowpic_dim,aug_name \\\n    --contexts test-human\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                       hparams                           acc                  duration\n      split  flowpic_dim         aug_name  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-human           32        changertt    15  70.04   4.41   2.44    83.93      7.8     4.32\n                              colorjitter    15  68.84   4.69   2.59    78.01     10.9     6.03\n                           horizontalflip    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                    noaug    15  69.48   2.12   1.17    37.97     1.46     0.81\n                               packetloss    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                   rotate    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                timeshift    15  70.36   2.98   1.65    80.08    12.72     7.04\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      64        changertt    15  72.05    2.1   1.16    61.57      5.2     2.88\n                              colorjitter    15  71.33   3.35   1.86    67.15    22.49    12.45\n                           horizontalflip    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                    noaug    15  69.88   2.28   1.26    38.06    20.21    11.19\n                               packetloss    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                   rotate    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                timeshift    15  72.53   1.83   1.02    64.23    21.91    12.13\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    1500        changertt    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                              colorjitter    15  68.59   3.17   1.76   765.43   152.53    84.47\n                           horizontalflip    15  73.82   1.47   0.82    471.8     58.6    32.45\n                                    noaug    15  68.67   1.93   1.07   374.88    94.41    52.28\n                               packetloss    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                                   rotate    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                                timeshift    15  70.84   2.42   1.34   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The <code>report</code> sub-command also creates output artifacts.</p> <ul> <li> <p>An output folder is created based on the <code>campaign_id</code> value.</p> </li> <li> <p>A set of <code>runinfo_&lt;XYZ&gt;.parquet</code> files collect runs     hyper param and metrics. </p> </li> <li> <p>A set of <code>summary_&lt;XYZ&gt;.csv</code> files collect the     aggregate table reported on the console.</p> </li> </ul>"},{"location":"modeling/overview/","title":"Modeling overview","text":"<p>When training ML/DL models,  finding the right combination of data preprocessing/splitting, algorithms and hyper-parameters can be challenging. Even more so when the modeling process  aims to be repeatable/replicable/reproducible.</p> <p>To ease this process is key to</p> <ul> <li> <p>Collect telemetry and metadata. This includes both the parameters used to create models as well as lower level metrics such as the evolution of the training loss over time.</p> </li> <li> <p>Generate artifacts such as  reports about the overall performance (e.g., confusion matrixes).</p> </li> </ul>"},{"location":"modeling/overview/#aim-stack-tracking","title":"AIM stack tracking","text":"<p><code>tcbench</code> integrates with AIM stack, an open-source and self-hosted model tracking framework enabling logging of metrics  related to model training. Such telemetry  can later be explored via a web interface or programmatically extracted via AIM SKD.</p> <p>Why not using more popular frameworks?</p> <p>There are many solutions for model tracking. While frameworks such as Weights &amp; Biases or Neptune.ai are extremely rich with features, unfortunately they typically  are cloud-based solutions and not necessarily open-sourced.</p> <p>Alternative frameworks such as Tensorboard and MLFlow have only primitive functionalities with respect to AIM stack.</p> <p>Aim stack is sitting in the middle of this spectrum: It is self-hosted (i.e., no need to push data to the cloud) and provides nice data exploration features.</p>"},{"location":"modeling/overview/#runs-and-campaigns","title":"Runs and campaigns","text":"<p>AIM collects modeling metadata into repositories which are fully controlled by end-users:</p> <ul> <li> <p>Repositories are not tied to specific projects. In other words, the end-user can store in a repository models completely unrelated to each other.</p> </li> <li> <p>There is no limit on the amount of repositories  can be created. </p> </li> </ul> <p><code>tcbench</code> tracks in an AIM repository two types of tasks, namely runs and campaigns:</p> <ul> <li> <p>A run corresponds to the training of an individual ML/DL model and is \"minimal experiment object\" used by AIM, i.e., any tracked metadata need to be associated to an AIM run.</p> </li> <li> <p>A campaign corresponds to a collection of runs. </p> </li> </ul> <p>AIM assign a unique hash code to a run, but a run object be further enriched with  extra metadata using AIM SDK or web UI.</p> <p>A run can be enriched with both individual values (e.g., best validation loss observed or the final accuracy score) as well as series (e.g., loss value for each epoch). Morever, values can have a context to further specify semantic (e.g., define if a registered metric relates to trainining, validation or test).</p> <p>While run is at term borrowed from AIM terminology, <code>tcbench</code> introduces campaign to  group runs which are semantically related and need to be summarized together (e.g., results collected across different train/val/test splits).</p> <p>It follows that:</p> <ul> <li> <p>Runs are the fundamental building block for collecting modeling results. But they are also the fundamental unit when developing/debugging modeling tasks.</p> </li> <li> <p>Campaigns bind multiple runs together. Hence, are meant to be stored in separate AIM repositories (although this is NOT a strict requirement for <code>tcbench</code>).</p> </li> </ul>"},{"location":"modeling/runs/","title":"Runs","text":"<p>Individual modeling run can be triggered using the subcommand <code>run</code></p> <pre><code>tcbench run --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench run [OPTIONS] COMMAND [ARGS]...\n\n Triggers a modeling run.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help      Show this message and exit.                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 augment-at-loading        Modeling by applying data augmentation when loading the training set.  \u2502\n\u2502 contralearn-and-finetune  Modeling by pre-training via constrative learning and then finetune    \u2502\n\u2502                           the final classifier from the pre-trained model.                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>As from the help string of the sub-commands</p> <ul> <li> <p><code>augment-at-loading</code> applies data augmentation to  boost the number of samples in the training set.</p> </li> <li> <p><code>contralearn-and-finetune</code> pre-train a model (via SimCLR) and then uses it to finetune the final classifier.</p> </li> </ul> <p>Both run types are associated to a variety of parameters which,  for readability, are organized in groups when printing the <code>--help</code>.</p> <p>Each parameter help string should be sufficient for understanding their purpose so we skip their detailed discussion.</p> <p>Yet, for each run type we report a reference example of how to trigger it.</p> <p>Runs and campaigns are repeatable</p> <p>When executing the reference examples (or any of the runs collected in the ML artifacts) we expect you to obtain the  exact same results! </p>"},{"location":"modeling/runs/#augment-at-loading","title":"<code>augment-at-loading</code>","text":"<pre><code>tcbench run augment-at-loading --help\n</code></pre> <p>Output</p> <pre><code>Usage: tcbench run augment-at-loading [OPTIONS]                                                                                                                                                         \nModeling by applying data augmentation when loading the training set.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                   \u2502\n\u2502                                   [default: augmentation-at-loading]                             \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                 \u2502\n\u2502                                   [default: aim-repo]                                            \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]                \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).     \u2502\n\u2502                                   [default: 0]                                                   \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20]  \u2502\n\u2502 --seed                   INTEGER  Seed to initialize random generators. [default: 12345]         \u2502\n\u2502 --help                            Show this message and exit.                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --dataset                   [ucdavis-icdm19|utmobilenet21|mi  Dataset to use for modeling.       \u2502\n\u2502                             rage19|mirage22]                  [default: ucdavis-icdm19]          \u2502\n\u2502 --dataset-minpkts           [-1|10|100|1000]                  In combination with --dataset,     \u2502\n\u2502                                                               refines preprocessed and split     \u2502\n\u2502                                                               dataset to use.                    \u2502\n\u2502                                                               [default: -1]                      \u2502\n\u2502 --flowpic-dim               [32|64|1500]                      Flowpic dimension. [default: 32]   \u2502\n\u2502 --flowpic-block-duration    INTEGER                           Number of seconds for the head of  \u2502\n\u2502                                                               a flow (i.e., block) to use for a  \u2502\n\u2502                                                               flowpic.                           \u2502\n\u2502                                                               [default: 15]                      \u2502\n\u2502 --split-index               INTEGER                           Data split index. [default: 0]     \u2502\n\u2502 --train-val-split-ratio     FLOAT                             If not predefined by the selected  \u2502\n\u2502                                                               split, the ratio data to use for   \u2502\n\u2502                                                               training (rest is for validation). \u2502\n\u2502                                                               [default: 0.8]                     \u2502\n\u2502 --aug-name                  [noaug|rotate|horizontalflip|col  Name of the augmentation to use.   \u2502\n\u2502                             orjitter|packetloss|timeshift|ch  [default: noaug]                   \u2502\n\u2502                             angertt]                                                             \u2502\n\u2502 --no-test-leftover                                            Skip test on leftover split        \u2502\n\u2502                                                               (specific for ucdavis-icdm19, and  \u2502\n\u2502                                                               default enabled for all other      \u2502\n\u2502                                                               datasets).                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Modeling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --method    [monolithic|xgboost]  Method to use for training. [default: monolithic]              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 DL hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size        INTEGER  Training batch size [default: 32]                                   \u2502\n\u2502 --learning-rate     FLOAT    Training learning rate. [default: 0.001]                            \u2502\n\u2502 --patience-steps    INTEGER  Max. number of epochs without improvement before stopping training. \u2502\n\u2502                              [default: 5]                                                        \u2502\n\u2502 --epochs            INTEGER  Number of epochs for training. [default: 50]                        \u2502\n\u2502 --no-dropout                 Mask dropout layers with Identity layers.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 XGBoost hyper params \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --input-repr       [flowpic|pktseries]  Input representation. [default: pktseries]               \u2502\n\u2502 --pktseries-len    INTEGER              Number of packets (when using time series as input).     \u2502\n\u2502                                         [default: 10]                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"modeling/runs/#reference-example","title":"Reference example","text":"<pre><code>tcbench run augment-at-loading \\\n    --dataset ucdavis-icdm19 \\\n    --learning-rate 0.001 \\\n    --batch-size 32 \\\n    --flowpic-dim 32 \\\n    --split-index 0 \\\n    --seed 12345 \\\n    --aug-name noaug \\\n    --method monolithic\n</code></pre> Output <pre><code>connecting to AIM repo at: aim-repo\ncreated aim run hash=d5fa0dae7540485682e0869e\nartifacts folder at: aim-repo/artifacts/d5fa0dae7540485682e0869e\nWARNING: the artifact folder is not a subfolder of the AIM repo\n--- run hparams ---\nflowpic_dim: 32\nflowpic_block_duration: 15\nsplit_index: 0\nmax_samples_per_class: -1\naug_name: noaug\npatience_steps: 5\nsuppress_val_augmentation: False\ndataset: ucdavis-icdm19\ndataset_minpkts: -1\nseed: 12345\nwith_dropout: True\n-------------------\nopened log at aim-repo/artifacts/d5fa0dae7540485682e0869e/log.txt\nloaded: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\nno augmentation\nno augmentation\ndataset samples count\n               train  val\napp                      \ngoogle-doc        80   20\ngoogle-drive      80   20\ngoogle-music      80   20\ngoogle-search     80   20\nyoutube           80   20\n\nnetwork architecture\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n         Dropout2d-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n           Linear-11                   [-1, 84]          10,164\n             ReLU-12                   [-1, 84]               0\n        Dropout1d-13                   [-1, 84]               0\n           Linear-14                    [-1, 5]             425\n================================================================\nTotal params: 61,281\nTrainable params: 61,281\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.23\nEstimated Total Size (MB): 0.36\n----------------------------------------------------------------\n\n---\nWARNING: Detected Dropout layer!\nWARNING: During supervised training, the monitored train_acc will be inaccurate\n---\n\nepoch:   0 | train_loss: 1.484430 | train_acc:  38.5% | val_loss: 1.207108 | val_acc:  95.0% | *\nepoch:   1 | train_loss: 1.234462 | train_acc:  49.2% | val_loss: 0.782972 | val_acc:  96.0% | *\nepoch:   2 | train_loss: 1.023239 | train_acc:  56.8% | val_loss: 0.531561 | val_acc:  95.0% | *\nepoch:   3 | train_loss: 1.015719 | train_acc:  54.0% | val_loss: 0.408276 | val_acc:  95.0% | *\nepoch:   4 | train_loss: 0.952349 | train_acc:  53.5% | val_loss: 0.292073 | val_acc:  97.0% | *\nepoch:   5 | train_loss: 0.911262 | train_acc:  57.5% | val_loss: 0.359489 | val_acc:  96.0%\nepoch:   6 | train_loss: 0.920104 | train_acc:  59.8% | val_loss: 0.256316 | val_acc:  97.0% | *\nepoch:   7 | train_loss: 0.964038 | train_acc:  53.8% | val_loss: 0.233604 | val_acc:  97.0% | *\nepoch:   8 | train_loss: 0.868652 | train_acc:  59.5% | val_loss: 0.273222 | val_acc:  98.0%\nepoch:   9 | train_loss: 0.878158 | train_acc:  58.8% | val_loss: 0.191980 | val_acc:  97.0% | *\nepoch:  10 | train_loss: 0.814328 | train_acc:  59.5% | val_loss: 0.193263 | val_acc:  98.0%\nepoch:  11 | train_loss: 0.873851 | train_acc:  54.8% | val_loss: 0.179443 | val_acc:  97.0% | *\nepoch:  12 | train_loss: 0.889558 | train_acc:  57.0% | val_loss: 0.161395 | val_acc:  97.0% | *\nepoch:  13 | train_loss: 0.862803 | train_acc:  56.8% | val_loss: 0.201240 | val_acc:  98.0%\nepoch:  14 | train_loss: 0.952843 | train_acc:  51.8% | val_loss: 0.144734 | val_acc:  97.0% | *\nepoch:  15 | train_loss: 0.893776 | train_acc:  56.8% | val_loss: 0.130068 | val_acc:  98.0% | *\nepoch:  16 | train_loss: 0.834928 | train_acc:  59.2% | val_loss: 0.176018 | val_acc:  96.0%\nepoch:  17 | train_loss: 0.883773 | train_acc:  57.8% | val_loss: 0.129550 | val_acc:  98.0%\nepoch:  18 | train_loss: 0.822181 | train_acc:  59.5% | val_loss: 0.134569 | val_acc:  97.0%\nepoch:  19 | train_loss: 0.861686 | train_acc:  58.5% | val_loss: 0.117548 | val_acc:  98.0% | *\nepoch:  20 | train_loss: 0.856357 | train_acc:  60.2% | val_loss: 0.182993 | val_acc:  96.0%\nepoch:  21 | train_loss: 0.929739 | train_acc:  55.0% | val_loss: 0.144799 | val_acc:  98.0%\nepoch:  22 | train_loss: 0.806559 | train_acc:  60.8% | val_loss: 0.112128 | val_acc:  98.0% | *\nepoch:  23 | train_loss: 0.893108 | train_acc:  56.8% | val_loss: 0.110627 | val_acc:  98.0% | *\nepoch:  24 | train_loss: 0.814123 | train_acc:  62.0% | val_loss: 0.125326 | val_acc:  98.0%\nepoch:  25 | train_loss: 0.804388 | train_acc:  59.2% | val_loss: 0.119205 | val_acc:  98.0%\nepoch:  26 | train_loss: 0.868501 | train_acc:  57.8% | val_loss: 0.101536 | val_acc:  98.0% | *\nepoch:  27 | train_loss: 0.799067 | train_acc:  61.8% | val_loss: 0.105484 | val_acc:  98.0%\nepoch:  28 | train_loss: 0.842477 | train_acc:  58.0% | val_loss: 0.086314 | val_acc:  98.0% | *\nepoch:  29 | train_loss: 0.845000 | train_acc:  57.2% | val_loss: 0.134341 | val_acc:  98.0%\nepoch:  30 | train_loss: 0.731437 | train_acc:  64.2% | val_loss: 0.079428 | val_acc:  97.0% | *\nepoch:  31 | train_loss: 0.791915 | train_acc:  62.8% | val_loss: 0.090225 | val_acc:  98.0%\nepoch:  32 | train_loss: 0.816266 | train_acc:  62.2% | val_loss: 0.085245 | val_acc:  98.0%\nepoch:  33 | train_loss: 0.866204 | train_acc:  56.0% | val_loss: 0.080400 | val_acc:  98.0%\nepoch:  34 | train_loss: 0.780020 | train_acc:  62.2% | val_loss: 0.109240 | val_acc:  98.0%\nepoch:  35 | train_loss: 0.865760 | train_acc:  56.5% | val_loss: 0.076947 | val_acc:  98.0% | *\nepoch:  36 | train_loss: 0.849319 | train_acc:  58.8% | val_loss: 0.078938 | val_acc:  98.0%\nepoch:  37 | train_loss: 0.833253 | train_acc:  59.0% | val_loss: 0.102355 | val_acc:  98.0%\nepoch:  38 | train_loss: 0.815432 | train_acc:  59.8% | val_loss: 0.088092 | val_acc:  98.0%\nepoch:  39 | train_loss: 0.788713 | train_acc:  61.0% | val_loss: 0.077858 | val_acc:  97.0%\nepoch:  40 | train_loss: 0.788635 | train_acc:  61.5% | val_loss: 0.090270 | val_acc:  98.0%\nrun out of patience\n\n\n---train reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.975610   1.000  0.987654   80.000\ngoogle-drive    1.000000   0.900  0.947368   80.000\ngoogle-music    0.917647   0.975  0.945455   80.000\ngoogle-search   0.987654   1.000  0.993789   80.000\nyoutube         1.000000   1.000  1.000000   80.000\naccuracy        0.975000   0.975  0.975000    0.975\nmacro avg       0.976182   0.975  0.974853  400.000\nweighted avg    0.976182   0.975  0.974853  400.000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             80             0             0              0        0\ngoogle-drive            1            72             7              0        0\ngoogle-music            1             0            78              1        0\ngoogle-search           0             0             0             80        0\nyoutube                 0             0             0              0       80\n\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/train_class_rep.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/train_conf_mtx.csv\n\n\n---val reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.952381    1.00  0.975610    20.00\ngoogle-drive    1.000000    0.95  0.974359    20.00\ngoogle-music    0.952381    1.00  0.975610    20.00\ngoogle-search   1.000000    0.95  0.974359    20.00\nyoutube         1.000000    1.00  1.000000    20.00\naccuracy        0.980000    0.98  0.980000     0.98\nmacro avg       0.980952    0.98  0.979987   100.00\nweighted avg    0.980952    0.98  0.979987   100.00\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             20             0             0              0        0\ngoogle-drive            0            19             1              0        0\ngoogle-music            0             0            20              0        0\ngoogle-search           1             0             0             19        0\nyoutube                 0             0             0              0       20\n\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/val_class_rep.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/val_conf_mtx.csv\nloading: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_human.parquet\nloading: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/test_split_script.parquet\n               human  script\napp                         \nyoutube           20      30\ngoogle-drive      18      30\ngoogle-doc        15      30\ngoogle-music      15      30\ngoogle-search     15      30\n\nTest dataset human | loss: 0.981303 | acc: 68.7\n\n\n---test-human reports---\n\n               precision    recall  f1-score    support\ngoogle-doc      0.500000  1.000000  0.666667  15.000000\ngoogle-drive    0.736842  0.777778  0.756757  18.000000\ngoogle-music    0.764706  0.866667  0.812500  15.000000\ngoogle-search   0.000000  0.000000  0.000000  15.000000\nyoutube         0.937500  0.750000  0.833333  20.000000\naccuracy        0.686747  0.686747  0.686747   0.686747\nmacro avg       0.587810  0.678889  0.613851  83.000000\nweighted avg    0.614262  0.686747  0.632238  83.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             15             0             0              0        0\ngoogle-drive            0            14             3              0        1\ngoogle-music            0             1            13              1        0\ngoogle-search          15             0             0              0        0\nyoutube                 0             4             1              0       15\n\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-human_class_rep.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-human_conf_mtx.csv\n\nTest dataset script | loss: 0.142414 | acc: 95.3\n\n\n---test-script reports---\n\n               precision    recall  f1-score     support\ngoogle-doc      0.882353  1.000000  0.937500   30.000000\ngoogle-drive    1.000000  0.900000  0.947368   30.000000\ngoogle-music    0.933333  0.933333  0.933333   30.000000\ngoogle-search   1.000000  0.933333  0.965517   30.000000\nyoutube         0.967742  1.000000  0.983607   30.000000\naccuracy        0.953333  0.953333  0.953333    0.953333\nmacro avg       0.956686  0.953333  0.953465  150.000000\nweighted avg    0.956686  0.953333  0.953465  150.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             30             0             0              0        0\ngoogle-drive            1            27             2              0        0\ngoogle-music            1             0            28              0        1\ngoogle-search           2             0             0             28        0\nyoutube                 0             0             0              0       30\n\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-script_class_rep.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-script_conf_mtx.csv\nloaded: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet\n\nTest dataset train-val-leftover | loss: 0.160271 | acc: 96.2\n\n\n---test-train-val-leftover reports---\n\n               precision    recall  f1-score      support\ngoogle-doc      0.952576  0.999142  0.975303  1166.000000\ngoogle-drive    0.993827  0.915929  0.953289  1582.000000\ngoogle-music    0.845395  0.957169  0.897817   537.000000\ngoogle-search   0.978529  0.980108  0.979318  1860.000000\nyoutube         0.966667  0.960078  0.963361  1027.000000\naccuracy        0.961925  0.961925  0.961925     0.961925\nmacro avg       0.947399  0.962485  0.953818  6172.000000\nweighted avg    0.963990  0.961925  0.962142  6172.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc           1165             0             1              0        0\ngoogle-drive           24          1449            90              0       19\ngoogle-music            2             4           514              7       10\ngoogle-search          31             1             0           1823        5\nyoutube                 1             4             3             33      986\n\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-train-val-leftover_class_rep.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/test-train-val-leftover_conf_mtx.csv\nsaving: aim-repo/artifacts/d5fa0dae7540485682e0869e/params.yml\n</code></pre>"},{"location":"modeling/runs/#contralearn-and-finetune","title":"<code>contralearn-and-finetune</code>","text":"<pre><code>tcbench run contralearn-and-finetune --help\n</code></pre> <p>Output</p> <pre><code>Usage: tcbench run contralearn-and-finetune [OPTIONS]                                                                                                                                                   \nModeling by pre-training via constrative learning and then finetune the final classifier from the\npre-trained model.\n\n\u256d\u2500 General options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --aim-experiment-name    TEXT     The name of the experiment for AIM tracking.                   \u2502\n\u2502                                   [default: contrastive-learning-and-finetune]                   \u2502\n\u2502 --aim-repo               PATH     AIM repository location (local folder or URL).                 \u2502\n\u2502                                   [default: aim-repo]                                            \u2502\n\u2502 --artifacts-folder       PATH     Artifacts folder. [default: aim-repo/artifacts]                \u2502\n\u2502 --gpu-index              TEXT     The id of the GPU to use (if training with deep learning).     \u2502\n\u2502                                   [default: 0]                                                   \u2502\n\u2502 --workers                INTEGER  Number of parallel worker for loading the data. [default: 20]  \u2502\n\u2502 --help                            Show this message and exit.                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --dataset                   [ucdavis-icdm19]  Dataset to use for modeling.                       \u2502\n\u2502                                               [default: ucdavis-icdm19]                          \u2502\n\u2502 --flowpic-dim               [32]              Flowpic dimension. [default: 32]                   \u2502\n\u2502 --flowpic-block-duration    INTEGER           Number of seconds for the head of a flow (i.e.,    \u2502\n\u2502                                               block) to use for a flowpic.                       \u2502\n\u2502                                               [default: 15]                                      \u2502\n\u2502 --split-index               INTEGER           Data split index. [default: 0]                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 General Deeplearning hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --batch-size    INTEGER  Training batch size [default: 32]                                       \u2502\n\u2502 --no-dropout             Mask dropout layers with Identity layers.                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Contrastive learning hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --cl-aug-names               TEXT     Coma separated list of augmentations pool for contrastive  \u2502\n\u2502                                       learning.                                                  \u2502\n\u2502                                       [default: changertt,timeshift]                             \u2502\n\u2502 --cl-projection-layer-dim    INTEGER  The number of units in the contrastive learning projection \u2502\n\u2502                                       layer.                                                     \u2502\n\u2502                                       [default: 30]                                              \u2502\n\u2502 --cl-learning-rate           FLOAT    Learning rate for pretraining. [default: 0.001]            \u2502\n\u2502 --cl-seed                    INTEGER  Seed for contrastive learning pretraining.                 \u2502\n\u2502                                       [default: 12345]                                           \u2502\n\u2502 --cl-patience-steps          INTEGER  Max steps to wait before stopping training if the top5     \u2502\n\u2502                                       validation accuracy does not improve.                      \u2502\n\u2502                                       [default: 3]                                               \u2502\n\u2502 --cl-temperature             FLOAT    Temperature for InfoNCE loss. [default: 0.07]              \u2502\n\u2502 --cl-epochs                  INTEGER  Epochs for contrastive learning pretraining. [default: 50] \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Finetune hyperparams \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --ft-learning-rate         FLOAT    Learning rate for finetune. [default: 0.01]                  \u2502\n\u2502 --ft-patience-steps        INTEGER  Max steps to wait before stopping finetune training loss     \u2502\n\u2502                                     does not improve.                                            \u2502\n\u2502                                     [default: 5]                                                 \u2502\n\u2502 --ft-patience-min-delta    FLOAT    Minimum decrease of training loss to be considered as        \u2502\n\u2502                                     improvement.                                                 \u2502\n\u2502                                     [default: 0.001]                                             \u2502\n\u2502 --ft-train-samples         INTEGER  Number of samples per-class for finetune training.           \u2502\n\u2502                                     [default: 10]                                                \u2502\n\u2502 --ft-epochs                INTEGER  Epochs for finetune training. [default: 50]                  \u2502\n\u2502 --ft-seed                  INTEGER  Seed for finetune training. [default: 12345]                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"modeling/runs/#reference-example_1","title":"Reference example","text":"<pre><code>tcbench run contralearn-and-finetune \\\n    --dataset ucdavis-icdm19 \\\n    --batch-size 32 \\\n    --flowpic-dim 32 \\\n    --split-index 0 \\\n    --no-dropout \\\n    --cl-seed 12345 \\\n    --ft-seed 12345 \\\n    --cl-projection-layer-dim 30\n</code></pre> Output <pre><code>connecting to AIM repo at: aim-repo\ncreated aim run hash=f2e52ab2cbfe4788aa642075\nartifacts folder at: aim-repo/artifacts/f2e52ab2cbfe4788aa642075\nWARNING: the artifact folder is not a subfolder of the AIM repo\n--- run hparams ---\nflowpic_dim: 32\nsplit_index: 0\ndataset: ucdavis-icdm19\ndataset_minpkts: -1\ncontrastive_learning_seed: 12345\nfinetune_seed: 12345\nfinetune_train_samples: 10\nwith_dropout: False\nprojection_layer_dim: 30\nfinetune_augmentation: none\naugmentations: ['changertt', 'timeshift']\ntrain_val_split_ratio: 0.8\n-------------------\nopened log at aim-repo/artifacts/f2e52ab2cbfe4788aa642075/log.txt\nloaded: ./envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/imc23/train_split_0.parquet\ndataset samples count\n               train  val\napp                      \ngoogle-doc        80   20\ngoogle-drive      80   20\ngoogle-music      80   20\ngoogle-search     80   20\nyoutube           80   20\n\n==== network adapted for pretrain ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n           Linear-11                  [-1, 120]          14,520\n             ReLU-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                   [-1, 30]           3,630\n================================================================\nTotal params: 68,842\nTrainable params: 68,842\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.26\nEstimated Total Size (MB): 0.39\n----------------------------------------------------------------\n\nepoch:   0 | train_loss: 2.792354 | train_acc_top_1:  22.1% | train_acc_top_5:  58.2% | val_loss: 2.770429 | val_acc_top_1:  24.6% | val_acc_top_5:  51.2% | *\nepoch:   1 | train_loss: 2.389633 | train_acc_top_1:  27.4% | train_acc_top_5:  62.3% | val_loss: 2.722881 | val_acc_top_1:  31.2% | val_acc_top_5:  54.3% | *\nepoch:   2 | train_loss: 2.277134 | train_acc_top_1:  26.1% | train_acc_top_5:  64.1% | val_loss: 2.687306 | val_acc_top_1:  26.6% | val_acc_top_5:  52.3%\nepoch:   3 | train_loss: 2.155350 | train_acc_top_1:  30.5% | train_acc_top_5:  68.5% | val_loss: 2.353693 | val_acc_top_1:  25.4% | val_acc_top_5:  60.5% | *\nepoch:   4 | train_loss: 2.128647 | train_acc_top_1:  30.8% | train_acc_top_5:  70.3% | val_loss: 2.409730 | val_acc_top_1:  30.9% | val_acc_top_5:  57.8%\nepoch:   5 | train_loss: 2.033960 | train_acc_top_1:  32.5% | train_acc_top_5:  72.5% | val_loss: 2.468154 | val_acc_top_1:  30.5% | val_acc_top_5:  57.8%\nepoch:   6 | train_loss: 1.933769 | train_acc_top_1:  36.7% | train_acc_top_5:  74.2% | val_loss: 2.246096 | val_acc_top_1:  34.8% | val_acc_top_5:  64.1% | *\nepoch:   7 | train_loss: 1.913906 | train_acc_top_1:  37.7% | train_acc_top_5:  78.6% | val_loss: 2.082685 | val_acc_top_1:  45.7% | val_acc_top_5:  68.4% | *\nepoch:   8 | train_loss: 1.800368 | train_acc_top_1:  36.2% | train_acc_top_5:  81.1% | val_loss: 2.157686 | val_acc_top_1:  35.2% | val_acc_top_5:  67.6%\nepoch:   9 | train_loss: 1.739876 | train_acc_top_1:  43.5% | train_acc_top_5:  81.7% | val_loss: 2.286904 | val_acc_top_1:  39.5% | val_acc_top_5:  67.6%\nepoch:  10 | train_loss: 1.773573 | train_acc_top_1:  41.8% | train_acc_top_5:  79.8% | val_loss: 2.208454 | val_acc_top_1:  30.5% | val_acc_top_5:  68.4%\nrun out of patience\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/best_model_weights_pretrain_split_0.pt\n               human_test  human_train  script_test  script_train\napp                                                              \ngoogle-doc              5           10           20            10\ngoogle-drive            8           10           20            10\ngoogle-music            5           10           20            10\ngoogle-search           5           10           20            10\nyoutube                10           10           20            10\n\n--- finetune (train) on human ---\napp\ngoogle-doc       10\ngoogle-drive     10\ngoogle-music     10\ngoogle-search    10\nyoutube          10\nName: count, dtype: int64\n\n==== network adapted for fine-tuning ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n         Identity-11                  [-1, 120]               0\n         Identity-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                    [-1, 5]             605\n================================================================\nTotal params: 51,297\nTrainable params: 51,297\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.20\nEstimated Total Size (MB): 0.33\n----------------------------------------------------------------\n\n\nepoch:   0 | train_loss: 1.615836 | train_acc:   8.0% | *\nepoch:   1 | train_loss: 1.537240 | train_acc:  32.0% | *\nepoch:   2 | train_loss: 1.484336 | train_acc:  40.0% | *\nepoch:   3 | train_loss: 1.439574 | train_acc:  40.0% | *\nepoch:   4 | train_loss: 1.391847 | train_acc:  42.0% | *\nepoch:   5 | train_loss: 1.352634 | train_acc:  36.0% | *\nepoch:   6 | train_loss: 1.330688 | train_acc:  36.0% | *\nepoch:   7 | train_loss: 1.281217 | train_acc:  38.0% | *\nepoch:   8 | train_loss: 1.262554 | train_acc:  40.0% | *\nepoch:   9 | train_loss: 1.238030 | train_acc:  42.0% | *\nepoch:  10 | train_loss: 1.225150 | train_acc:  46.0% | *\nepoch:  11 | train_loss: 1.198335 | train_acc:  50.0% | *\nepoch:  12 | train_loss: 1.188114 | train_acc:  58.0% | *\nepoch:  13 | train_loss: 1.175914 | train_acc:  64.0% | *\nepoch:  14 | train_loss: 1.191627 | train_acc:  64.0%\nepoch:  15 | train_loss: 1.124196 | train_acc:  64.0% | *\nepoch:  16 | train_loss: 1.111688 | train_acc:  64.0% | *\nepoch:  17 | train_loss: 1.121508 | train_acc:  66.0%\nepoch:  18 | train_loss: 1.079309 | train_acc:  68.0% | *\nepoch:  19 | train_loss: 1.061186 | train_acc:  70.0% | *\nepoch:  20 | train_loss: 1.039081 | train_acc:  72.0% | *\nepoch:  21 | train_loss: 1.043780 | train_acc:  72.0%\nepoch:  22 | train_loss: 1.026590 | train_acc:  72.0% | *\nepoch:  23 | train_loss: 0.976669 | train_acc:  76.0% | *\nepoch:  24 | train_loss: 1.016128 | train_acc:  80.0%\nepoch:  25 | train_loss: 0.983972 | train_acc:  82.0%\nepoch:  26 | train_loss: 1.009065 | train_acc:  82.0%\nepoch:  27 | train_loss: 0.929888 | train_acc:  82.0% | *\nepoch:  28 | train_loss: 0.961760 | train_acc:  82.0%\nepoch:  29 | train_loss: 0.922811 | train_acc:  82.0% | *\nepoch:  30 | train_loss: 0.963608 | train_acc:  82.0%\nepoch:  31 | train_loss: 0.952226 | train_acc:  82.0%\nepoch:  32 | train_loss: 0.917957 | train_acc:  84.0% | *\nepoch:  33 | train_loss: 0.920033 | train_acc:  84.0%\nepoch:  34 | train_loss: 0.914727 | train_acc:  84.0% | *\nepoch:  35 | train_loss: 0.873566 | train_acc:  84.0% | *\nepoch:  36 | train_loss: 0.914860 | train_acc:  84.0%\nepoch:  37 | train_loss: 0.886750 | train_acc:  84.0%\nepoch:  38 | train_loss: 0.863592 | train_acc:  84.0% | *\nepoch:  39 | train_loss: 0.875310 | train_acc:  84.0%\nepoch:  40 | train_loss: 0.897717 | train_acc:  82.0%\nepoch:  41 | train_loss: 0.875007 | train_acc:  82.0%\nepoch:  42 | train_loss: 0.840733 | train_acc:  84.0% | *\nepoch:  43 | train_loss: 0.810166 | train_acc:  84.0% | *\nepoch:  44 | train_loss: 0.819367 | train_acc:  84.0%\nepoch:  45 | train_loss: 0.823529 | train_acc:  84.0%\nepoch:  46 | train_loss: 0.813771 | train_acc:  84.0%\nepoch:  47 | train_loss: 0.846898 | train_acc:  84.0%\nepoch:  48 | train_loss: 0.804951 | train_acc:  86.0% | *\nepoch:  49 | train_loss: 0.814344 | train_acc:  86.0%\nreached max epochs\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/best_model_weights_finetune_human_from_split_0.pt\n\n--- finetune (test) on human ---\napp\nyoutube          10\ngoogle-drive      8\ngoogle-doc        5\ngoogle-music      5\ngoogle-search     5\nName: count, dtype: int64\n\nTest dataset human | loss: 1.122207 | acc: 75.8\n\n\n---test-human reports---\n\n               precision    recall  f1-score    support\ngoogle-doc      0.714286  1.000000  0.833333   5.000000\ngoogle-drive    0.600000  0.750000  0.666667   8.000000\ngoogle-music    0.714286  1.000000  0.833333   5.000000\ngoogle-search   1.000000  0.600000  0.750000   5.000000\nyoutube         1.000000  0.600000  0.750000  10.000000\naccuracy        0.757576  0.757576  0.757576   0.757576\nmacro avg       0.805714  0.790000  0.766667  33.000000\nweighted avg    0.816450  0.757576  0.755051  33.000000\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc              5             0             0              0        0\ngoogle-drive            0             6             2              0        0\ngoogle-music            0             0             5              0        0\ngoogle-search           2             0             0              3        0\nyoutube                 0             4             0              0        6\n\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/test-human_class_rep.csv\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/test-human_conf_mtx.csv\n\n--- finetune (train) on script ---\napp\ngoogle-doc       10\ngoogle-drive     10\ngoogle-music     10\ngoogle-search    10\nyoutube          10\nName: count, dtype: int64\n\n==== network adapted for fine-tuning ====\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]             156\n              ReLU-2            [-1, 6, 28, 28]               0\n         MaxPool2d-3            [-1, 6, 14, 14]               0\n            Conv2d-4           [-1, 16, 10, 10]           2,416\n              ReLU-5           [-1, 16, 10, 10]               0\n          Identity-6           [-1, 16, 10, 10]               0\n         MaxPool2d-7             [-1, 16, 5, 5]               0\n           Flatten-8                  [-1, 400]               0\n            Linear-9                  [-1, 120]          48,120\n             ReLU-10                  [-1, 120]               0\n         Identity-11                  [-1, 120]               0\n         Identity-12                  [-1, 120]               0\n         Identity-13                  [-1, 120]               0\n           Linear-14                    [-1, 5]             605\n================================================================\nTotal params: 51,297\nTrainable params: 51,297\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.13\nParams size (MB): 0.20\nEstimated Total Size (MB): 0.33\n----------------------------------------------------------------\n\nepoch:   0 | train_loss: 1.610058 | train_acc:  20.0% | *\nepoch:   1 | train_loss: 1.551162 | train_acc:  34.0% | *\nepoch:   2 | train_loss: 1.494622 | train_acc:  44.0% | *\nepoch:   3 | train_loss: 1.432214 | train_acc:  46.0% | *\nepoch:   4 | train_loss: 1.373513 | train_acc:  52.0% | *\nepoch:   5 | train_loss: 1.349639 | train_acc:  54.0% | *\nepoch:   6 | train_loss: 1.294582 | train_acc:  56.0% | *\nepoch:   7 | train_loss: 1.281295 | train_acc:  70.0% | *\nepoch:   8 | train_loss: 1.241570 | train_acc:  72.0% | *\nepoch:   9 | train_loss: 1.181321 | train_acc:  76.0% | *\nepoch:  10 | train_loss: 1.124716 | train_acc:  78.0% | *\nepoch:  11 | train_loss: 1.144935 | train_acc:  78.0%\nepoch:  12 | train_loss: 1.103080 | train_acc:  78.0% | *\nepoch:  13 | train_loss: 1.061779 | train_acc:  78.0% | *\nepoch:  14 | train_loss: 1.030383 | train_acc:  76.0% | *\nepoch:  15 | train_loss: 1.030399 | train_acc:  76.0%\nepoch:  16 | train_loss: 0.954889 | train_acc:  76.0% | *\nepoch:  17 | train_loss: 0.955167 | train_acc:  76.0%\nepoch:  18 | train_loss: 0.959741 | train_acc:  80.0%\nepoch:  19 | train_loss: 0.896621 | train_acc:  84.0% | *\nepoch:  20 | train_loss: 0.886190 | train_acc:  92.0% | *\nepoch:  21 | train_loss: 0.909035 | train_acc:  92.0%\nepoch:  22 | train_loss: 0.888979 | train_acc:  92.0%\nepoch:  23 | train_loss: 0.860065 | train_acc:  92.0% | *\nepoch:  24 | train_loss: 0.820559 | train_acc:  92.0% | *\nepoch:  25 | train_loss: 0.800317 | train_acc:  92.0% | *\nepoch:  26 | train_loss: 0.761238 | train_acc:  92.0% | *\nepoch:  27 | train_loss: 0.759209 | train_acc:  92.0% | *\nepoch:  28 | train_loss: 0.789361 | train_acc:  92.0%\nepoch:  29 | train_loss: 0.751218 | train_acc:  92.0% | *\nepoch:  30 | train_loss: 0.743825 | train_acc:  94.0% | *\nepoch:  31 | train_loss: 0.758159 | train_acc:  94.0%\nepoch:  32 | train_loss: 0.726163 | train_acc:  92.0% | *\nepoch:  33 | train_loss: 0.700075 | train_acc:  92.0% | *\nepoch:  34 | train_loss: 0.657465 | train_acc:  92.0% | *\nepoch:  35 | train_loss: 0.656949 | train_acc:  92.0%\nepoch:  36 | train_loss: 0.671816 | train_acc:  92.0%\nepoch:  37 | train_loss: 0.675776 | train_acc:  92.0%\nepoch:  38 | train_loss: 0.621407 | train_acc:  92.0% | *\nepoch:  39 | train_loss: 0.642558 | train_acc:  92.0%\nepoch:  40 | train_loss: 0.624327 | train_acc:  92.0%\nepoch:  41 | train_loss: 0.639679 | train_acc:  92.0%\nepoch:  42 | train_loss: 0.594589 | train_acc:  92.0% | *\nepoch:  43 | train_loss: 0.617626 | train_acc:  92.0%\nepoch:  44 | train_loss: 0.597697 | train_acc:  92.0%\nepoch:  45 | train_loss: 0.598853 | train_acc:  92.0%\nepoch:  46 | train_loss: 0.629960 | train_acc:  92.0%\nepoch:  47 | train_loss: 0.596823 | train_acc:  92.0%\nrun out of patience\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/best_model_weights_finetune_script_from_split_0.pt\n\n--- finetune (test) on script ---\napp\ngoogle-doc       20\ngoogle-drive     20\ngoogle-music     20\ngoogle-search    20\nyoutube          20\nName: count, dtype: int64\n\nTest dataset script | loss: 0.565789 | acc: 93.0\n\n\n---test-script reports---\n\n               precision  recall  f1-score  support\ngoogle-doc      0.900000    0.90  0.900000    20.00\ngoogle-drive    1.000000    0.90  0.947368    20.00\ngoogle-music    0.904762    0.95  0.926829    20.00\ngoogle-search   0.904762    0.95  0.926829    20.00\nyoutube         0.950000    0.95  0.950000    20.00\naccuracy        0.930000    0.93  0.930000     0.93\nmacro avg       0.931905    0.93  0.930205   100.00\nweighted avg    0.931905    0.93  0.930205   100.00\n\n               google-doc  google-drive  google-music  google-search  youtube\ngoogle-doc             18             0             0              2        0\ngoogle-drive            0            18             1              0        1\ngoogle-music            1             0            19              0        0\ngoogle-search           1             0             0             19        0\nyoutube                 0             0             1              0       19\n\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/test-script_class_rep.csv\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/test-script_conf_mtx.csv\nsaving: aim-repo/artifacts/f2e52ab2cbfe4788aa642075/params.yml\n</code></pre>"},{"location":"modeling/runs/#limitations","title":"Limitations","text":"<p>Currently tcbench supports only the modeling functionalities related to our IMC23 paper and the  parametrization of those runs is strictly limited to  what was required for the purpose of the paper.</p> <p>We are currently working for lifting those limitations so stay tuned </p>"},{"location":"modeling/aim_repos/","title":"Explore AIM repos","text":"<p>An AIM repository is merely a folder where AIM stores a rocksdb database (see AIM reference doc for more info).</p> <p>AIM has great functionality for tracking metrics but has very little support for  tracking general artifacts outside console output and nor has native support for storing trained models files.</p> <p>Hence tcbench complement AIM by collecting runs artifacts into run-specific folders.</p> <p>Specifically, a tcbench repository has the following structure</p> <pre><code>&lt;root&gt;\n\u251c\u2500\u2500 .aim\n\u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 001baa39ed8d4b8bb9966e94\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 025830cb840b4f3f8f0a1625\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 050bae064b5246f88e821a29\n...\n\u2514\u2500\u2500 campaign_summary\n    \u2514\u2500\u2500 &lt;campaign_id&gt;\n</code></pre> <ul> <li> <p>Each subfolder of <code>/artifacts</code> collects the artificats of a specific run and is named with the hash of the run itself.</p> </li> <li> <p>The <code>/campaign_summary</code> subfolder collects reports generated by the <code>aimrepo report</code> subcommand.</p> </li> </ul> <p>Investigating the content of one run artifact folder </p> <pre><code>ls -1 &lt;root&gt;/artifacts/001baa39ed8d4b8bb9966e94\n</code></pre> <p>Output</p> <pre><code>log.txt\nparams.yml\ntest-human_class_rep.csv\ntest-human_conf_mtx.csv\ntest-script_class_rep.csv\ntest-script_conf_mtx.csv\ntest-train-val-leftover_class_rep.csv\ntest-train-val-leftover_conf_mtx.csv\ntrain_class_rep.csv\ntrain_conf_mtx.csv\nval_class_rep.csv\nval_conf_mtx.csv\nbest_model_weights_split_2.pt\n</code></pre> <p>For each run tcbench creates the following artifacts:</p> <ul> <li> <p><code>params.yml</code> is a YAML file collecting  parameters used when triggering a run, i.e., both the arguments explicitly defined on the command line, as well the ones with default values.</p> </li> <li> <p><code>log.txt</code> collects the console output generated by the run.</p> </li> <li> <p><code>&lt;context&gt;_class_rep.csv</code> contains a classification report. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> <li> <p><code>&lt;context&gt;_conf_mtx.csv</code> contains confusion matrix. The filename is bounded to the context (i.e., train, val, test) used to generate it.</p> </li> <li> <p><code>best_model_weights_split_&lt;split-index&gt;.pt</code> stores the weights of the best  trained pytorch model (for a deep learning model). The filename is bounded to the specific split index configured when triggering the run.</p> </li> <li> <p><code>xgb_model_split_&lt;split-index&gt;.json</code> stores an XGBoost model (when training  via xgboost). The filename is bounded to the specific split index configured when triggering the run.</p> </li> </ul>"},{"location":"modeling/aim_repos/aim_webui/","title":"AIM Web UI","text":"<p>AIM web interface is quite intuitive and  the official documentation already provides  a general purpose tutorial.</p> <p>In this mini guide we limit to showcase a basic set  of operations to navigate the ML artifacts using some artifacts from our IMC23 paper.</p> <p>To replicate the following, make sure you installed the needed artifacts.</p> <pre><code>aim up --repo notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>Running Aim UI on repo `&lt;Repo#-3653246895908991301 path=./notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/.aim read_only=None&gt;`\nOpen http://127.0.0.1:43800\nPress Ctrl+C to exit\n</code></pre> <p>Run <code>aim up --help</code> for more options (e.g., specifying a different port or hostname).</p> <p>When visiting the URL reported in the output  you land on the home page of the AIM repository.</p> <p>This collects a variety of aggregate metrics  and track activity over time.  Hence, in our scenario the home page of the ML artifacts are mostly empty because all campaigns were generated in a specific moment in time.</p> <p></p> <p>The left side bar allows switch the view. In particular, \"Runs\" show a tabular view of the runs collected in the repository.</p> <p></p> <p>From the view you can see the hash of each run and scrolling horizontally you can glance  over the metadata stored for each run.</p> <p></p> <p>The search bar on the top of the page allows to filter runs. It accept python expression bounded to a <code>run</code> entry point.</p> <p>For instance, in the following example we filter one specific run based on hyper parameters.</p> <p></p> <p>Using the search box</p> <p>The search box accept python expressions and <code>run.hparams</code>  is a dictionary of key-value pairs related to the different runs.</p> <p>As from the example, you can use the traditional python syntax of <code>dict[&lt;key&gt;] == &lt;value&gt;</code> to filter, but the search box supports also a dot-notated syntax <code>hparams.&lt;key&gt; == &lt;value&gt;</code> which has an autocomplete.</p> <p>In the example, the search is based on equality but any other python operation is allowed.</p> <p>When clicking the hash of a run (e.g., the one we filtered) we switch to a per-run view which further details the collected metadata of the selected run.</p> <p></p> <p>For instance, when scrolling at the bottom of the per-run page we can see that AIM details</p> <ul> <li> <p>The specific git commit used when executing the run.</p> </li> <li> <p>The specific python packages and related versions available in the environment when executing the run.</p> </li> </ul> <p>Both are automatically tracked by AIM with no extra code required (beside activating the  their collection when creating the run).</p> <p></p> <p>The per-run view offers a variety of information organized in multiple tabs.</p> <p>For instance, the tab \"Logs\" details the console output.</p> <p></p>"},{"location":"modeling/aim_repos/aimrepo_subcmd/","title":"Repository reports","text":"<p><code>tcbench</code> offers the <code>aimrepo</code> subcommand to  interact with the content of a repository.</p> <pre><code>tcbench aimrepo --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo [OPTIONS] COMMAND [ARGS]...\n\n Investigate AIM repository content.\n\n \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n \u2502 --help      Show this message and exit.                                                          \u2502\n \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n \u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n \u2502 ls                List a subset of properties of each run.                                       \u2502\n \u2502 merge             Coalesce different AIM repos into a single new repo.                           \u2502\n \u2502 properties        List properties across all runs.                                               \u2502\n \u2502 report            Summarize runs performance metrics.                                            \u2502\n \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In the following we illustrate each sub-command using the  <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> repository from our IMC23 paper.</p>"},{"location":"modeling/aim_repos/aimrepo_subcmd/#listing-runs-hash","title":"Listing runs hash","text":"<p>The <code>ls</code> sub-command simply list  the hash, creation time and end time of each run.</p> <pre><code> tcbench aimrepo ls \\\n    --aim-repo notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout \\\n    | head -5\n</code></pre> <p>Output</p> <pre><code>hash                      creation_time      end_time\n9cf03ef2a61848e8975ea90c  1693512039.103155  1693513226.327002\n3a5fd371e8064c4a89b1df6a  1693510803.708324  1693512038.63982\na845811424ee4e13a454ab79  1693509166.65158   1693510144.254626\nc1ecfb414b8f4f809c4eb142  1693508952.200609  1693510803.343346\n</code></pre>"},{"location":"modeling/aim_repos/aimrepo_subcmd/#listing-properties","title":"Listing properties","text":"<p>The <code>properties</code> sub-command shows aggregate information and the list of unique values of the properties of an AIM repo (i.e., run hyper params and other meta data).</p> <pre><code>tcbench aimrepo properties \\\n    --aim-repo notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout \\\n</code></pre> <p>Output</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name                      \u2502 No. unique \u2502 Value                                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 runs                      \u2502          - \u2502 315                                                     \u2502\n\u2502 run duration (mean \u00b1 std) \u2502          - \u2502 9m6s \u00b1 21m3s                                            \u2502\n\u2502 metrics                   \u2502          7 \u2502 ['acc', 'best_epoch', 'best_loss', 'f1', 'loss',        \u2502\n\u2502                           \u2502            \u2502 'precision', 'recall']                                  \u2502\n\u2502 contexts                  \u2502          5 \u2502 ['test-human', 'test-script',                           \u2502\n\u2502                           \u2502            \u2502 'test-train-val-leftover', 'train', 'val']              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 experiment                \u2502          1 \u2502 ['augmentation-at-loading']                             \u2502\n\u2502 aug_name                  \u2502          7 \u2502 ['changertt', 'colorjitter', 'horizontalflip', 'noaug', \u2502\n\u2502                           \u2502            \u2502 'packetloss', 'rotate', 'timeshift']                    \u2502\n\u2502 campaign_exp_idx          \u2502        105 \u2502 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, \u2502\n\u2502                           \u2502            \u2502 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, \u2502\n\u2502                           \u2502            \u2502 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, \u2502\n\u2502                           \u2502            \u2502 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, \u2502\n\u2502                           \u2502            \u2502 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, \u2502\n\u2502                           \u2502            \u2502 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, \u2502\n\u2502                           \u2502            \u2502 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,     \u2502\n\u2502                           \u2502            \u2502 100, 101, 102, 103, 104, 105]                           \u2502\n\u2502 campaign_id               \u2502          1 \u2502 ['augment-at-loading-with-dropout']                     \u2502\n\u2502 dataset                   \u2502          1 \u2502 ['ucdavis-icdm19']                                      \u2502\n\u2502 dataset_minpkts           \u2502          1 \u2502 [-1]                                                    \u2502\n\u2502 flowpic_block_duration    \u2502          1 \u2502 [15]                                                    \u2502\n\u2502 flowpic_dim               \u2502          3 \u2502 [32, 64, 1500]                                          \u2502\n\u2502 max_samples_per_class     \u2502          1 \u2502 [-1]                                                    \u2502\n\u2502 patience_steps            \u2502          1 \u2502 [5]                                                     \u2502\n\u2502 seed                      \u2502          3 \u2502 [42, 666, 12345]                                        \u2502\n\u2502 split_index               \u2502          5 \u2502 [0, 1, 2, 3, 4]                                         \u2502\n\u2502 suppress_val_augmentation \u2502          1 \u2502 [False]                                                 \u2502\n\u2502 with_dropout              \u2502          1 \u2502 [True]                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The table is split into two parts to separate general properties (top) from hyper parameters (bottom). General properties are common across repositories while hyper parameters vary depending of the campaign.</p> <p>Considering the general properties</p> <ul> <li> <p>runs indicates the total number of runs in the repository (i.e., 315 = 3 seeds * 3 resolutions * 5 splits * 7 augmentations).</p> </li> <li> <p>run duration indicates the average duration of each run.</p> </li> <li> <p>metrics lists which metrics are tracked for each run.</p> </li> <li> <p>context is term borrored from AIM terminology and refers to ability to  bind a metric to multiple semantic. For instance, in this repository the metrics are bounded to the data used to measure them.</p> </li> </ul>"},{"location":"modeling/aim_repos/aimrepo_subcmd/#summary-reports","title":"Summary reports","text":"<p>The <code>report</code> sub-command provides a summary across metrics grouped by properties.</p> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/\n</code></pre> <p>Output</p> <pre><code>campaign_id: augment-at-loading-with-dropout\nruns: 315\n\n                                    hparams                           acc                 run_duration\n                   split        aug_name   flowpic_dim  runs   mean    std   ci95     mean       std      ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              test-human       changertt            32    15  70.76    3.6   1.99    71.32      8.52      4.72\n                                                    64    15  71.49   2.87   1.59    76.95      7.93      4.39\n                                                  1500    15  71.97   1.96   1.08  2308.22   1921.22   1063.94\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  68.43    5.1   2.82    59.65     11.83      6.55\n                                                    64    15   70.2    3.6   1.99    72.37     11.55       6.4\n                                                  1500    15  69.08   3.11   1.72  2017.51   2215.73   1227.03\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15   69.4   2.94   1.63    42.27      3.93      2.18\n                                                    64    15  70.52   3.67   2.03    57.49      3.44       1.9\n                                                  1500    15   73.9   1.91   1.06   501.25    105.56     58.46\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  68.84   2.61   1.45    25.48      2.37      1.31\n                                                    64    15  69.08   2.44   1.35    34.87      1.57      0.87\n                                                  1500    15  69.32   2.95   1.63   353.54    106.06     58.73\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  70.68   2.44   1.35    60.18      8.05      4.46\n                                                    64    15  71.33   2.62   1.45    72.91      7.44      4.12\n                                                  1500    15  71.08   2.04   1.13  1072.35    273.49    151.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  71.65   3.58   1.98    65.27     13.04      7.22\n                                                    64    15  71.08   2.73   1.51   104.31      7.95       4.4\n                                                  1500    15  68.19   1.75   0.97   1375.8    617.11    341.74\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  70.36   2.94   1.63    62.17     10.46      5.79\n                                                    64    15  71.89   2.87   1.59    75.28      8.35      4.62\n                                                  1500    15  71.08   2.41   1.33  2961.36   3200.62   1772.44\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             test-script       changertt            32    15  97.29   0.64   0.35    71.32      8.52      4.72\n                                                    64    15  97.02   0.83   0.46    76.95      7.93      4.39\n                                                  1500    15  96.93   0.55   0.31  2308.22   1921.22   1063.94\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  97.56    1.0   0.55    59.65     11.83      6.55\n                                                    64    15  97.16   1.11   0.62    72.37     11.55       6.4\n                                                  1500    15  94.93   1.23   0.68  2017.51   2215.73   1227.03\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.47    0.8   0.45    42.27      3.93      2.18\n                                                    64    15   96.0   1.07   0.59    57.49      3.44       1.9\n                                                  1500    15  94.89   1.42   0.79   501.25    105.56     58.46\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.64   0.66   0.37    25.48      2.37      1.31\n                                                    64    15  95.87   0.52   0.29    34.87      1.57      0.87\n                                                  1500    15  94.93    1.3   0.72   353.54    106.06     58.73\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.89   0.93   0.52    60.18      8.05      4.46\n                                                    64    15  96.84   1.14   0.63    72.91      7.44      4.12\n                                                  1500    15  95.96   0.92   0.51  1072.35    273.49    151.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.31   0.79   0.44    65.27     13.04      7.22\n                                                    64    15  96.93   0.83   0.46   104.31      7.95       4.4\n                                                  1500    15  95.69   0.71   0.39   1375.8    617.11    341.74\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  96.71   1.08    0.6    62.17     10.46      5.79\n                                                    64    15  97.16   0.89   0.49    75.28      8.35      4.62\n                                                  1500    15  96.89   0.48   0.27  2961.36   3200.62   1772.44\n test-train-val-leftover       changertt            32    15  98.38   0.32   0.18    71.32      8.52      4.72\n                                                    64    15  97.97   0.71   0.39    76.95      7.93      4.39\n                                                  1500    15  98.19   0.39   0.22  2308.22   1921.22   1063.94\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                             colorjitter            32    15  96.93   1.01   0.56    59.65     11.83      6.55\n                                                    64    15  96.46   0.82   0.46    72.37     11.55       6.4\n                                                  1500    15  95.47   0.88   0.49  2017.51   2215.73   1227.03\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                          horizontalflip            32    15  95.68   0.72    0.4    42.27      3.93      2.18\n                                                    64    15  96.32   1.06   0.59    57.49      3.44       1.9\n                                                  1500    15  95.97   1.45    0.8   501.25    105.56     58.46\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                   noaug            32    15  95.78   0.53   0.29    25.48      2.37      1.31\n                                                    64    15  96.09   0.68   0.38    34.87      1.57      0.87\n                                                  1500    15  95.79   0.92   0.51   353.54    106.06     58.73\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                              packetloss            32    15  96.99    0.7   0.39    60.18      8.05      4.46\n                                                    64    15  97.25    0.7   0.39    72.91      7.44      4.12\n                                                  1500    15  96.84   0.89   0.49  1072.35    273.49    151.45\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                  rotate            32    15  96.74   0.63   0.35    65.27     13.04      7.22\n                                                    64    15   97.0   0.69   0.38   104.31      7.95       4.4\n                                                  1500    15  95.79   0.56   0.31   1375.8    617.11    341.74\n                          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                               timeshift            32    15  97.02    0.9    0.5    62.17     10.46      5.79\n                                                    64    15  97.51   0.83   0.46    75.28      8.35      4.62\n                                                  1500    15  97.67   0.53   0.29  2961.36   3200.62   1772.44\n\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/runsinfo_flowpic_dim_1500.parquet\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/summary_flowpic_dim_1500.csv\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/runsinfo_flowpic_dim_64.parquet\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/summary_flowpic_dim_64.csv\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/runsinfo_flowpic_dim_32.parquet\nsaving: notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/summary_flowpic_dim_32.csv\n</code></pre> <p>Starting from the top, the console output is informing about the <code>campaign_id</code>  and the number of runs in the repository.</p> <p>Then it follows a table grouped by the  <code>test-&lt;XYZ&gt;</code> contexts found. By default all hyper parameters with more than one value are also added (see the previous description about properties).</p> <p>In the example, the <code>acc</code> metric is measured with mean, standard deviation and 95th %tile confidence intervals. Next to the metric is reported also <code>run_duration</code> which  corresponds to the overall time for train/validation/test  in the run execution (hence values looks duplicated across different partitions of the table).</p> <p>The example above show the default configuration of th report. You can use </p> <ul> <li> <p>The <code>--groupby</code> option to change the order of the grouping (e.g., swap augmentation and resolution).</p> </li> <li> <p>The <code>--contexts</code> option to add/remove context (e.g.,  add training and validation).</p> </li> <li> <p>The <code>--metrics</code> option allows to specify which metrics to use (e.g., use f1 rather than accuracy).</p> </li> </ul> <pre><code>tcbench aimrepo report \\\n    --aim-repo notebooks/imc23/campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/ \\\n    --groupby flowpic_dim,aug_name \\\n    --contexts test-human\n    --metrics acc,f1\n</code></pre> <p>Output</p> <pre><code>campaign_id: 1684447037\nruns: 315\n\n                       hparams                           acc                  duration\n      split  flowpic_dim         aug_name  runs   mean    std   ci95     mean      std     ci95\n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n test-human           32        changertt    15  70.04   4.41   2.44    83.93      7.8     4.32\n                              colorjitter    15  68.84   4.69   2.59    78.01     10.9     6.03\n                           horizontalflip    15   69.8   2.51   1.39     56.9     1.64     0.91\n                                    noaug    15  69.48   2.12   1.17    37.97     1.46     0.81\n                               packetloss    15   71.0   1.85   1.02    80.83    11.11     6.15\n                                   rotate    15  71.57   3.52   1.95    78.52     11.1     6.15\n                                timeshift    15  70.36   2.98   1.65    80.08    12.72     7.04\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      64        changertt    15  72.05    2.1   1.16    61.57      5.2     2.88\n                              colorjitter    15  71.33   3.35   1.86    67.15    22.49    12.45\n                           horizontalflip    15  70.92   3.31   1.83    63.86    28.97    16.04\n                                    noaug    15  69.88   2.28   1.26    38.06    20.21    11.19\n                               packetloss    15  73.17   1.61   0.89    57.08     4.75     2.63\n                                   rotate    15   71.0   2.43   1.35    88.49    33.41     18.5\n                                timeshift    15  72.53   1.83   1.02    64.23    21.91    12.13\n             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    1500        changertt    15  72.69   2.68   1.48  1303.72   336.55   186.37\n                              colorjitter    15  68.59   3.17   1.76   765.43   152.53    84.47\n                           horizontalflip    15  73.82   1.47   0.82    471.8     58.6    32.45\n                                    noaug    15  68.67   1.93   1.07   374.88    94.41    52.28\n                               packetloss    15  72.13   1.87   1.04  1164.89   245.69   136.06\n                                   rotate    15  67.87   1.56   0.86  1313.58    301.7   167.08\n                                timeshift    15  70.84   2.42   1.34   907.03   165.48    91.64\n\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_1500.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_1500.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_32.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_32.csv\nsaving: campaing_summary/1684447037/runsinfo_flowpic_dim_64.parquet\nsaving: campaing_summary/1684447037/summary_flowpic_dim_64.csv\n</code></pre> <p>The <code>report</code> sub-command also creates output artifacts.</p> <ul> <li> <p>An output folder is created based on the <code>campaign_id</code> value.</p> </li> <li> <p>A set of <code>runinfo_&lt;XYZ&gt;.parquet</code> files collect runs     hyper param and metrics. </p> </li> <li> <p>A set of <code>summary_&lt;XYZ&gt;.csv</code> files collect the     aggregate table reported on the console.</p> </li> </ul>"},{"location":"modeling/aim_repos/aimrepo_subcmd/#merge-repositories","title":"Merge repositories","text":"<p>Currently tcbench does not have a scheduler able to  automatically distribute runs of a campaign across servers or GPUs.</p> <p>You can however split your workload across multiple repositories and then use the <code>aimrepo merge</code> subcommand to consolidate all repositories into a single one.</p> <pre><code>tcbench aimrepo merge --help\n</code></pre> <p>Output</p> <pre><code> Usage: tcbench aimrepo merge [OPTIONS]\n\n Coalesce different AIM repos into a single new repo.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --src     PATH  AIM repository to merge. [required]                     \u2502\n\u2502    --dst     PATH  New AIM repository to create. [default: aim-repo]       \u2502\n\u2502    --help          Show this message and exit.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>If the destionation repository does not exists, it will be created. When using merge, the recommendation is to create a new destination (so that the primary source is always available in case something goes wrong).</p>"},{"location":"papers/","title":"Research articles featuring tcbench","text":"<p>Replication: Contrastive Learning and Data Augmentation in Traffic Classification A. Finamore, C. Wang, J. Krolikowki, J. M. Navarro, F. Cheng, D. Rossi,   ACM Internet Measurement Conference (IMC), 2023  Artifacts PDF</p> BibtexAbstract <pre><code>@misc{finamore2023contrastive,\n  title={\n    Contrastive Learning and Data Augmentation \n    in Traffic Classification Using a \n    Flowpic Input Representation\n  }, \n  author={\n    Alessandro Finamore and \n    Chao Wang and \n    Jonatan Krolikowski \n    and Jose M. Navarro \n    and Fuxing Chen and \n    Dario Rossi\n  },\n  year={2023},\n  eprint={2309.09733},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG}\n}\n</code></pre> <p>Over the last years we witnessed a renewed interest towards Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) meth- ods. Among those works, a recent study from IMC'22 [17] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-sup ervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [17] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called \"flowpic\" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we rep roduce [17] on the same datasets and (ii) we rep licate its most salient aspect (the importance of data augmentation) on three additional public datasets, MIRAGE-19, MIRAGE-22 and UTMOBILENET21. While we con- firm most of the original results, we also found a 20% ac- curacy drop on some of the investigated scenarios due to a data shift of the original dataset that we uncovered. Ad- ditionally, our study validates that the data augmentation strategies studied in [17] perform well on other datasets too. In the spirit of reproducibility and replicability we make all artifacts (code and data) available at [10].</p>"},{"location":"papers/imc23/","title":"Contrastive Learning and Data Augmentation in Traffic Classification, IMC23","text":"<p>This work investigates the role of data augmentation by using both supervised and contrastive learning techniques across 4 datasets, namely <code>ucdavis-icdm19</code>,  <code>mirage19</code>,  <code>mirage22</code> and  <code>utmobilenet21</code>.</p> BibtexAbstract <pre><code>@misc{finamore2023contrastive,\n  title={\n    Contrastive Learning and Data Augmentation \n    in Traffic Classification Using a \n    Flowpic Input Representation\n  }, \n  author={\n    Alessandro Finamore and \n    Chao Wang and \n    Jonatan Krolikowski \n    and Jose M. Navarro \n    and Fuxing Chen and \n    Dario Rossi\n  },\n  year={2023},\n  eprint={2309.09733},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG}\n}\n</code></pre> <p>Over the last years we witnessed a renewed interest towards Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) meth- ods. Among those works, a recent study from IMC'22 [17] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-sup ervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [17] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called \"flowpic\" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we rep roduce [17] on the same datasets and (ii) we rep licate its most salient aspect (the importance of data augmentation) on three additional public datasets, MIRAGE-19, MIRAGE-22 and UTMOBILENET21. While we con- firm most of the original results, we also found a 20% ac- curacy drop on some of the investigated scenarios due to a data shift of the original dataset that we uncovered. Ad- ditionally, our study validates that the data augmentation strategies studied in [17] perform well on other datasets too. In the spirit of reproducibility and replicability we make all artifacts (code and data) available at [10].</p>"},{"location":"papers/imc23/#scope-of-the-study","title":"Scope of the study","text":"<p>This paper replicates and reproduces the following paper from the IMC22 program</p> BibtexAbstract <pre><code>@inproceedings{10.1145/3517745.3561436,\nauthor = {\n    Horowicz, Eyal and \n    Shapira, Tal and \n    Shavitt, Yuval\n},\ntitle = {A Few Shots Traffic Classification with Mini-FlowPic Augmentations},\nyear = {2022},\nisbn = {9781450392594},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3517745.3561436},\ndoi = {10.1145/3517745.3561436},\nbooktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},\npages = {647\u2013654},\nnumpages = {8},\nlocation = {Nice, France},\nseries = {IMC '22}\n}\n</code></pre> <p>Internet traffic classification has been intensively studied over the past decade due to its importance for traffic engineering and cyber security. One of the best solutions to several traffic classification problems is the FlowPic approach, where histograms of packet sizes in consecutive time slices are transformed into a picture that is fed into a Convolution Neural Network (CNN) model for classification. However, CNNs (and the FlowPic approach included) require a relatively large labeled flow dataset, which is not always easy to obtain. In this paper, we show that we can overcome this obstacle by replacing the large labeled dataset with a few samples of each class and by using augmentations in order to inflate the number of training samples. We show that common picture augmentation techniques can help, but accuracy improves further when introducing augmentation techniques that mimic network behavior such as changes in the RTT. Finally, we show that we can replace the large FlowPics suggested in the past with much smaller mini-FlowPics and achieve two advantages: improved model performance and easier engineering. Interestingly, this even improves accuracy in some cases.</p> <p>which</p> <ul> <li> <p>Considers a traffic classification use-case modeled with a CNN-based network using a flowpic input representation (a 2d summary of traffic flow dynamics)</p> </li> <li> <p>Compares a supervised setting against a contrastive learning </p> </li> <li> <p>fine-tuning setting, both in a few shot scenario (training with 100 input samples).</p> </li> <li> <p>Benchmarks 6 types of data augmentations (3 on time series and 3 on images) against training with no data augmentation.</p> </li> </ul> <p>In our paper we replicate and reproduce the IMC22 paper results and expand the previous analysis to more datasets.</p>"},{"location":"papers/imc23/#takeaways","title":"Takeaways","text":"<ol> <li> <p>We were able only to partially reproduce the results from the IMC22 paper.    Specifically, we found a 20% accuracy gap with respect to the IMC22    paper which relates to a data shift in the <code>ucdavis-icdm19</code> dataset    previously undetected in the reference paper.</p> </li> <li> <p>Simply using the <code>ucdavis-icdm19</code>, and differently    from the IMC22 paper, we do not find statistical significance differences across     the 6 augmentations under analysis.</p> </li> <li> <p>Contrastive learning can help to \"bootstrap\" a model in an unsupervised fashion. Yet,    relying on more samples (than the 100 required as from the IMC22 paper modeling scenarios)    is beneficial to boost performance, i.e., augmentations are not perfect replacement for     real input samples.</p> </li> <li> <p>Using multiple datasets (namely <code>mirage19</code>,     <code>mirage22</code> and     <code>utmobilenet21</code> allowed to confirm     Change RTT and Time Shift augmentations (used in the IMC22 paper)     as superior with respect to alternatives on a flowpic input representation.    Yet, the augmentations are not statistically different from each other.</p> </li> </ol>"},{"location":"papers/imc23/artifacts/","title":"IMC23 Paper Artifacts","text":"<p>The paper is associated to the following types of artifacts:</p> <ul> <li> <p> Data: This includes </p> <ul> <li>The datasets curation and splits for <code>ucdavis-icdm19</code>,  <code>mirage19</code>,  <code>mirage22</code> and  <code>utmobilenet21</code>. Please refer to the  datasets webpage and related pages for more details.</li> <li>All  models and logs generated through our modeling campaigns.</li> </ul> </li> <li> <p> Code: This includes </p> <ul> <li>A collection of  Jupyter notebooks  used for the tables and figures of the paper.</li> <li>A collection of data to support  pytest unittest related to the  results collected for the paper.</li> </ul> </li> </ul>"},{"location":"papers/imc23/artifacts/#figshare-material","title":"Figshare material","text":"<p>The artifacts are stored in a  figshare collection with the following items:</p> <ul> <li> <p><code>curated_datasets_ucdavis-icdm19.tgz</code>: A curated version of the dataset presented by Rezaei et al. in \"How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets\".</p> </li> <li> <p><code>curated_datasets_utmobilenet21.tgz</code>: A curated version of the dataset presented by Heng et al. in \"UTMobileNetTraffic2021: A Labeled Public Network Traffic Dataset\".</p> </li> <li> <p><code>imc23_ml_artifacts.tgz</code>: Models and output logs generated via tcbench.</p> </li> <li> <p><code>imc23_notebooks.tgz</code>: A collection of jupyter notebooks for recreating tables and figures from the paper.</p> </li> <li> <p><code>imc23_pytest_resources.tgz</code>: A collection of reference resources for pytest unit testing (to verify model training replicability).</p> </li> <li> <p><code>ucdavis-icdm19-git-repo-forked.tgz</code>: A fork of the repository https://github.com/shrezaei/Semi-supervised-Learning-QUIC- to verify results of \"How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets\" https://doi.org/10.48550/arXiv.1812.09761</p> </li> </ul>"},{"location":"papers/imc23/artifacts/#downloading-artifacts","title":"Downloading artifacts","text":"<p>Each artifact can be manually downloaded from the figshare collection. However,  make sure to refer to the latest version of an archive when downloading manually.</p> <p>tcbench offers automated procedures to fetch the right content from figshare:</p> <ul> <li> <p>For datasets please refer to datasets page page,  the specific page for each datasets and the import command.</p> </li> <li> <p>For the remaning, you can use the <code>fetch-artifacts</code> subcommand with the following process</p> </li> <li> <p>First of all, prepare a python virtual environment, for example via  conda     <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> </li> <li> <p>Clone the tcbench repo and use the <code>imc23</code> branch     <pre><code>git clone https://github.com/tcbenchstack/tcbench.git tcbench.git\ncd tcbench.git\ngit checkout imc23\n</code></pre></p> </li> <li> <p>Install tcbench     <pre><code>python -m pip install .[dev]\n</code></pre></p> </li> <li> <p>Fetch the artifacts     <pre><code>tcbench fetch-artifacts\n</code></pre></p> </li> </ul> <p>This will install locally</p> <ul> <li> <p>The notebooks for replicating tables and figures of the paper under <code>/notebooks/imc23</code>. The cloned repository already contains the notebooks but since the code might change, the version fetched from figshare is identical to what used for the submission.</p> </li> <li> <p>The ml-artifacts under <code>/notebooks/imc23/campaigns</code>.</p> </li> <li> <p>The pytest resources for enabling unit tests.</p> </li> </ul> <p>Packages depencency version and <code>/imc23</code> branch</p> <p>When installing tcbench via pypi of from the main branch of the repository, only a few sensible packages have a pinned version.</p> <p>If you are trying to replicate the results of the paper, please refer to the <code>/imc23</code> branch which also contains a <code>requirements-imc23.txt</code> generated via <code>pip freeze</code> from  the environment used for collecting results. </p> <p>Based on our experience, the most probable cause of results inconsistency is due to package version. </p>"},{"location":"papers/imc23/campaigns/","title":"Campaigns","text":""},{"location":"papers/imc23/campaigns/#submission-campaigns-commands","title":"Submission campaigns commands","text":"<p>We report below the command used to trigger the campaigns collected in the ML artifacts</p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19xgboostnoaugmentation-flowpic","title":"<code>ucdavis-icdm19/xgboost/noaugmentation-flowpic</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method xgboost \\\n    --augmentations noaug \\\n    --input-repr flowpic \\\n    --flowpic-dims 32,64,1500 \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/xgboost/noaugmentation-flowpic\n    --artifacts-folder ucdavis-icdm19/xgboost/noaugmentation-flowpic/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaigns has 45 runs</p> <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nflowpic_dims  (3): [32, 64, 1500]\n</code></pre> <p>In the submission we just reported results for flowpic with 32x32 resolution.</p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19xgboostnoaugmentation-timeseries","title":"<code>ucdavis-icdm19/xgboost/noaugmentation-timeseries</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method xgboost \\\n    --augmentations noaug \\\n    --input-repr pktseries \\\n    --pktseries-len 10,30 \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/xgboost/noaugmentation-timeseries \\\n    --artifacts-folder ucdavis-icdm19/xgboost/noaugmentation-timeseries/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 30 runs</p> <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nmax_n_pkts    (2): [10, 30]\n</code></pre> <p>In the submission we just reported results for time series with 10 packets</p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19augmentation-at-loading-with-dropout","title":"<code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset ucdavis-icdm19 \\\n    --aim-repo ucdavis-icdm19/augmentation-at-loading-with-dropout \\\n    --artifacts-folder ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 315 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (3): [32, 64, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#mirage19augmentation-at-loading-no-dropoutminpkts10","title":"<code>mirage19/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage19 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage19/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage19/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#mirage22augmentation-at-loading-no-dropoutminpkts10","title":"<code>mirage22/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage22/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage22/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#mirage22augmentation-at-loading-no-dropoutminpkts1000","title":"<code>mirage22/augmentation-at-loading-no-dropout/minpkts1000</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo mirage22/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder mirage22/augmentation-at-loading-no-dropout/minpkts10\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#utmobilenet21augmentation-at-loading-no-dropoutminpkts10","title":"<code>utmobilenet21/augmentation-at-loading-no-dropout/minpkts10</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --method monolithic \\\n    --seeds 12345,42,666 \\\n    --dataset utmobilenet21 \\\n    --dataset-minpkts 10 \\\n    --augmentations noaug \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo utmobilenet21/augmentation-at-loading-no-dropout/minpkts10 \\\n    --artifacts-folder utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 15 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-changertt","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,changertt \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-changertt/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-packetloss","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,packetloss \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-packetloss/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairscolorjitter-rotate","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations colorjitter,rotate \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/colorjitter-rotate/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairsrotate-changertt","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations rotate,changertt \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-changertt/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 125 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-other-augmentation-pairsrotate-packetloss","title":"<code>ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations rotate,packetloss \\\n    --flowpic-dims 32 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --cl-projection-layer-dims 30 \\\n    --dropout disable \\\n    --aim-repo ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss \\\n    --artifacts-folder ucdavis-icdm19/simclr-other-augmentation-pairs/rotate-packetloss/artifacts\n</code></pre> <p>Runs grid</p> <p>The campaign has 215 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19simclr-dropout-and-projection","title":"<code>ucdavis-icdm19/simclr-dropout-and-projection</code>","text":"<pre><code>tcbench campaign contralearn-and-finetune \\\n    --augmentations changertt,timeshift \\\n    --flowpic-dims 32 \\\n    --cl-projection-layer-dims 30,84 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --dropout disable,enable \\\n    --aim-repo ucdavis-icdm19/simclr-dropout-and-projection \\\n    --artifacts-folder ucdavis-icdm19/simclr-dropout-and-projection/artifacts\n</code></pre> <p>Run grid</p> <p>The campaign has 500 runs <pre><code>split_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (2): [30, 84]\ndropout                    (2): ['disable', 'enable']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19augmentation-at-loading-suppress-dropout","title":"<code>ucdavis-icdm19/augmentation-at-loading-suppress-dropout</code>","text":"<pre><code>tcbench campaign augment-at-loading \\\n    --flowpic-dims 32,1500 \\\n    --seeds 12345,42,666 \\\n    --no-dropout \\\n    --aim-repo ucdavis-icdm19/simclr-dropout-and-projection \\\n    --artifacts-folder ucdavis-icdm19/simclr-dropout-and-projection/artifacts\n</code></pre> <p>Run grid</p> <p>The campaign has 210 runs <pre><code>split_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (2): [32, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19larger-trainsetaugmentation-at-loading","title":"<code>ucdavis-icdm19/larger-trainset/augmentation-at-loading</code>","text":"<p>This campaign is a composition of two sub-campaigns  stored in the same AIM repository.</p> <p>The first is for augmentation at loading <pre><code>tcbench campaign augment-at-loading \\\n    --seeds 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 \\\n    --split-indexes -1 \\\n    --method monolithic \\\n    --no-dropout \\\n    --flowpic-dims 32 \\\n    --aim-repo ucdavis-icdm19/larger-trainset/augmentation-at-loading \\\n    --artifacts-folder ucdavis-icdm19/larger-trainset/augmentation-at-loading/artifacts\n</code></pre></p> <p>Run grid</p> <p>The campaign has 140 runs <pre><code>split_indexes (1): [-1]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (20): [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n</code></pre></p> <p>The second is for contrastive learning created with the following script to manually compose 20 runs.</p> <pre><code>#!/bin/bash\n\nCONTRALEARN_SEEDS=(32 33 34 35 6 7 8 9 10 11 12 13 14 15 16 17 18 20 43 64)\nFINETUNE_SEEDS=(2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 22 43 64)\n\n\nfor idx in {0..19}; do\n    tcbench run contralearn-and-finetune \\\n        --dataset ucdavis-icdm19 \\\n        --batch-size 32 \\\n        --flowpic-dim 32 \\\n        --no-dropout \\\n        --split-index -1 \\\n        --cl-projection-layer-dim 30 \\\n        --cl-seed ${CONTRALEARN_SEEDS[$idx]} \\\n        --ft-seed ${FINETUNE_SEEDS[$idx]} \\\n        --aim-repo ucdavis-icdm19/larger-trainset/augmentation-at-loading \\\n        --artifacts-folder ucdavis-icdm19/larger-trainset/augmentation-at-loading/artifacts\ndone\n</code></pre>"},{"location":"papers/imc23/campaigns/#ucdavis-icdm19-git-repo-forked","title":"<code>ucdavis-icdm19-git-repo-forked</code>","text":"<p>This campaign is different from all the others because is just repeating the experiments of the following paper ICDM19.</p> <pre><code>@misc{rezaei2020achieve,\ntitle={How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets}, \nauthor={Shahbaz Rezaei and Xin Liu},\nyear={2020},\neprint={1812.09761},\narchivePrefix={arXiv},\nprimaryClass={cs.NI}\n}\n</code></pre> <p>The related code is available at this   repository</p> <p>We just did minor modifications (mostly for changing output folders) without affecting how to execute the code.</p> <p>In other words, to generate the result run <pre><code>python dataProcessInMemoryQUIC.py \npython pre-training.py\npython re-training.py\n</code></pre></p> <p>All results are collected in an output <code>/artifacts</code> folder (but now AIM repository is generated).</p>"},{"location":"papers/imc23/ml_artifacts/","title":"ML artifacts","text":"<p>The ML artifacts correspond to a collection of 13 modeling campaigns.</p> <pre><code>&lt;root&gt;\n\u251c\u2500\u2500 mirage19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts10\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n\u251c\u2500\u2500 mirage22\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 minpkts10\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 minpkts1000\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n\u251c\u2500\u2500 ucdavis-icdm19\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-dropout-impact\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading-with-dropout\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 larger-trainset\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 augmentation-at-loading\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 simclr\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-dropout-and-projection\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 simclr-other-augmentation-pairs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 xgboost\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 noaugmentation-flowpic\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 noaugmentation-timeseries\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 .aim\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 artifacts\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 campaign_summary\n\u251c\u2500\u2500 ucdavis-icdm19-git-repo-forked\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 artifacts\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(human-triggered)_10\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 FixedStepSampling_Retraining(script-triggered)_10\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_10\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(human-triggered)_20\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 IncrementalSampling_Retraining(script-triggered)_10\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 RandomSampling_Retraining(human-triggered)_10\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 RandomSampling_Retraining(script-triggered)_10\n\u2514\u2500\u2500 utmobilenet21\n    \u2514\u2500\u2500 augmentation-at-loading-no-dropout\n        \u251c\u2500\u2500 minpkts10\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 .aim\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 artifacts\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 campaign_summary\n        \u2514\u2500\u2500 minpkts10.STILL-WITH-BUG\n            \u251c\u2500\u2500 .aim\n            \u251c\u2500\u2500 artifacts\n            \u2514\u2500\u2500 campaign_summary\n</code></pre> <p>Each subfolder relates to a different campaign with some semantic encoded in the folder names themselves.</p> <ul> <li> <p>Subfolders containing an <code>.aim/</code> folder are AIM repositories.</p> </li> <li> <p>Subfolders named <code>artifacts/</code> collect each run artifacts.</p> </li> <li> <p>Subfolders named <code>campaign_summary/</code> contains reports summarizing a campaign.</p> </li> </ul> <p>The following reference table details how the different campaigns map to the results in the paper.</p>"},{"location":"papers/imc23/ml_artifacts/#mapping-campaigns-folder-to-submission-results","title":"Mapping campaigns folder to submission results","text":"Subfolder Results CLI trigger <code>ucdavis-icdm19/xgboost/noaugmentation-flowpic</code> Table 3 <code>ucdavis-icdm19/xgboost/noaugmentation-timeseries</code> Table 3 <code>ucdavis-icdm19/augmentation-at-loading-with-dropout</code> Table 4Figure 3,5 <code>ucdavis-icdm19/simclr-dropout-and-projection</code> Table 5 <code>ucdavis-icdm19/simclr-other-augmentation-pairs</code> Table 6 <code>ucdavis-icdm19/larger-trainset/augmentation-at-loading</code> Table 7 <code>ucdavis-icdm19/larger-trainset/simclr</code> Table 7 <code>mirage19/augmentation-at-loading-no-dropout/minpkts10</code> Table 8Figure 6,7 <code>mirage22/augmentation-at-loading-no-dropout/minpkts10</code> Table 8Figure 6,7 <code>mirage22/augmentation-at-loading-no-dropout/minpkts1000</code> Table 8Figure 6,7 <code>utmobilenet21/augmentation-at-loading-no-dropout/minpkts10</code> Table 8Figure 6,7 <code>ucdavis-icdm19-git-repo-forked</code> Table 9Figure 10 <code>ucdavis-icdm19/augmentation-at-loading-dropout-impact</code> Figure 11"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19xgboostnoaugmentation-flowpic","title":"ucdavis-icdm19/xgboost/noaugmentation-flowpic","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --campaign-id 'noaugmentation-flowpic' \\\n    --aim-experiment-name 'xgboost-flowpic' \\\n    --split-indexes 0,1,2,3,4 \\\n    --dataset ucdavis-icdm19 \\\n    --method xgboost \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --augmentations noaug\n\ntcbench aimrepo report \\\n    --aim-repo $REPO\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: noaugmentation-flowpic | run 1/15 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nflowpic_dims  (1): [32]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19xgboostnoaugmentation-timeseries","title":"ucdavis-icdm19/xgboost/noaugmentation-timeseries","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --campaign-id 'noaugmentation-timeseries' \\\n    --aim-experiment-name 'xgboost-pktseries-10pkts' \\\n    --split-indexes 0,1,2,3,4 \\\n    --dataset ucdavis-icdm19 \\\n    --method xgboost \\\n    --input-repr pktseries \\\n    --pktseries-len 10 \\\n    --augmentations noaug\n\ntcbench aimrepo report \\\n    --aim-repo $REPO\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: noaugmentation-timeseries | run 1/15 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (1): ['noaug']\nseeds         (3): [12345, 42, 666]\nmax_n_pkts    (1): [10]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19augmentation-at-loading-with-dropout","title":"ucdavis-icdm19/augmentation-at-loading-with-dropout","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --campaign-id 'augment-at-loading-with-dropout' \\\n    --aim-experiment-name 'augment-at-loading' \\\n    --split-indexes 0,1,2,3,4 \\\n    --dataset ucdavis-icdm19 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32,64,1500\n\ntcbench aimrepo report \\\n    --aim-repo $REPO\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading-with-dropout | run 1/315 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (3): [32, 64, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19simclr-dropout-and-projection","title":"ucdavis-icdm19/simclr-dropout-and-projection","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/simclr-dropout-and-projection\n\nrm -rf $REPO\n\ntcbench campaign contralearn-and-finetune \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --campaign-id 'simclr-dropout-and-projection' \\\n    --flowpic-dims 32 \\\n    --split-indexes 0,1,2,3,4 \\\n    --augmentations changertt,timeshift \\\n    --cl-projection-layer-dims 30,84 \\\n    --cl-seeds 12345,1,2,3,4 \\\n    --ft-seeds 12345,1,2,3,4 \\\n    --dropout enabled,disabled\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --groupby projection_layer_dim,with_dropout\n</code></pre> <p>Campaign dru run</p> <pre><code>##########\n# campaign_id: simclr-dropout-and-projection | run 1/500 - time to completion 0:00:00\n##########\n\nsplit_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (2): [30, 84]\ndropout                    (2): ['enabled', 'disabled']\nflowpic dims               (1): [32]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19simclr-other-augmentation-pairs","title":"ucdavis-icdm19/simclr-other-augmentation-pairs","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs\n\nrm -rf $REPO\n\nfor AUGMENTATIONS in \\\n    \"packetloss,colorjitter\"\\\n    \"packetloss,rotate\"\\\n    \"colorjitter,rotate\"\\\n    \"changertt,timeshift\"\\\n    \"changertt,rotate\"\\\n    \"changertt,colorjitter\"; do\n\n    tcbench campaign contralearn-and-finetune \\\n            --dry-run \\\n            --aim-repo $REPO \\\n            --artifacts-folder $REPO/artifacts \\\n            --campaign-id 'simclr-other-augmentation-pairs' \\\n            --flowpic-dims 32 \\\n            --split-indexes 0,1,2,3,4 \\\n            --augmentations $AUGMENTATIONS \\\n            --cl-projection-layer-dims 30 \\\n            --cl-seeds 12345,1,2,3,4 \\\n            --ft-seeds 12345,1,2,3,4 \\\n            --dropout disabled\n\ndone\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --groupby augmentations\n</code></pre> <p>Campaign dry run</p> <p>Each of the combination of augmentations has the following grid <pre><code>##########\n# campaign_id: simclr-other-augmentation-pairs | run 1/125 - time to completion 0:00:00\n##########\n\nsplit_indexes              (5): [0, 1, 2, 3, 4]\ncontrastive learning seeds (5): [12345, 1, 2, 3, 4]\nfinetune seeds             (5): [12345, 1, 2, 3, 4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disabled']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19larger-trainsetaugmentation-at-loading","title":"ucdavis-icdm19/larger-trainset/augmentation-at-loading","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/larger-trainset/augmentation-at-loading\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 \\\n    --campaign-id 'augment-at-loading-larger-trainset' \\\n    --split-indexes -1 \\\n    --dataset ucdavis-icdm19 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --augmentations noaug,rotate,horizontalflip,colorjitter,changertt,timeshift,packetloss \\\n    --no-dropout \\\n    --no-test-leftover \\\n\ntcbench aimrepo report --aim-repo $DST\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading-larger-trainset | run 1/140 - time to completion 0:00:00\n##########\n\nsplit_indexes (1): [-1]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'changertt', 'timeshift', 'packetloss']\nflowpic_dims  (1): [32]\nseeds         (20): [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19larger-trainsetsimclr","title":"ucdavis-icdm19/larger-trainset/simclr","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/larger-trainset/simclr\n\nrm -rf $REPO\n\nCONTRALEARN_SEEDS=(32 33 34 35 6 7 8 9 10 11 12 13 14 15 16 17 18 20 43 64)\nFINETUNE_SEEDS=(2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 22 43 64)\n\n\nfor idx in {0..19}; do\n    tcbench campaign contralearn-and-finetune \\\n        --aim-repo $REPO \\\n        --artifacts-folder $REPO/artifacts \\\n        --campaign-id 'simclr-larger-trainset' \\\n        --batch-size 32 \\\n        --flowpic-dims 32 \\\n        --dropout disabled \\\n        --split-indexes -1 \\\n        --cl-projection-layer-dims 30 \\\n        --cl-seeds ${CONTRALEARN_SEEDS[$idx]} \\\n        --ft-seeds ${FINETUNE_SEEDS[$idx]}\ndone\n\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --groupby campaign_id\n</code></pre> <p>Campaign dry run</p> <p>Each of the campaing has only single run with the following grid <pre><code>##########\n# campaign_id: simclr-larger-trainset | run 1/1 - time to completion 0:00:00\n##########\n\nsplit_indexes              (1): [-1]\ncontrastive learning seeds (1): [34]\nfinetune seeds             (1): [4]\nprojection layer dims      (1): [30]\ndropout                    (1): ['disabled']\nflowpic dims               (1): [32]\n</code></pre></p>"},{"location":"papers/imc23/ml_artifacts/#mirage19augmentation-at-loading-no-dropoutminpkts10","title":"mirage19/augmentation-at-loading-no-dropout/minpkts10","text":"<pre><code>REPO=campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --campaign-id 'augment-at-loading' \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --dataset mirage19 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --dataset-minpkts 10 \\\n    --no-dropout\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --metrics acc,f1,precision,recall\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading | run 1/105 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#mirage22augmentation-at-loading-no-dropoutminpkts10","title":"mirage22/augmentation-at-loading-no-dropout/minpkts10","text":"<pre><code>REPO=campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --campaign-id 'augment-at-loading' \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --dataset-minpkts 10 \\\n    --no-dropout\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --metrics acc,f1,precision,recall\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading | run 1/105 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#mirage22augmentation-at-loading-no-dropoutminpkts1000","title":"mirage22/augmentation-at-loading-no-dropout/minpkts1000","text":"<pre><code>REPO=campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --campaign-id 'augment-at-loading' \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --dataset mirage22 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --dataset-minpkts 1000 \\\n    --no-dropout\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --metrics acc,f1,precision,recall\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading | run 1/105 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#utmobilenet21augmentation-at-loading-no-dropoutminpkts10","title":"utmobilenet21/augmentation-at-loading-no-dropout/minpkts10","text":"<pre><code>REPO=campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --campaign-id 'augment-at-loading' \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --dataset utmobilenet21 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32 \\\n    --dataset-minpkts 10 \\\n    --no-dropout\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --metrics acc,f1,precision,recall\n</code></pre> <p>Campaign dry run</p> <pre><code>##########\n# campaign_id: augment-at-loading | run 1/105 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (1): [32]\nseeds         (3): [12345, 42, 666]\n</code></pre>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19augmentation-at-loading-dropout-impact","title":"ucdavis-icdm19/augmentation-at-loading-dropout-impact","text":"<pre><code>REPO=campaigns/ucdavis-icdm19/augmentation-at-loading-dropout-impact\n\nrm -rf $REPO\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --campaign-id 'augment-at-loading-dropout-impact' \\\n    --aim-experiment-name 'augment-at-loading' \\\n    --split-indexes 0,1,2,3,4 \\\n    --dataset ucdavis-icdm19 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32,1500\n\ntcbench campaign augment-at-loading \\\n    --aim-repo $REPO \\\n    --artifacts-folder $REPO/artifacts \\\n    --seeds 12345,42,666 \\\n    --campaign-id 'augment-at-loading-dropout-impact' \\\n    --aim-experiment-name 'augment-at-loading' \\\n    --split-indexes 0,1,2,3,4 \\\n    --dataset ucdavis-icdm19 \\\n    --method monolithic \\\n    --input-repr flowpic \\\n    --flowpic-dims 32,1500 \\\n    --no-dropout\n\ntcbench aimrepo report \\\n    --aim-repo $REPO \\\n    --groupby flowpic_dim,with_dropout,aug_name\n</code></pre> <p>Campaign dry run</p> <p>Each of the two campaign has the following grid <pre><code>##########\n# campaign_id: augment-at-loading-dropout-impact | run 1/210 - time to completion 0:00:00\n##########\n\nsplit_indexes (5): [0, 1, 2, 3, 4]\naugmentations (7): ['noaug', 'rotate', 'horizontalflip', 'colorjitter', 'packetloss', 'changertt', 'timeshift']\nflowpic_dims  (2): [32, 1500]\nseeds         (3): [12345, 42, 666]\n</code></pre></p>"},{"location":"papers/imc23/ml_artifacts/#ucdavis-icdm19-git-repo-forked","title":"ucdavis-icdm19-git-repo-forked","text":"<p>This campaign differ from all the others as it  repeats the experiments of  Rezaei et al. ICDM19 paper. using our version of the <code>ucdavis-icdm19</code> datasets to validate that our preprocessing does not alter the dataset itself.</p> <p>To do so, we did minor modification (stored in this figshare archive)  of the Rezaei et al. code base without changing how to use it.</p> <p>In other words, to generate the result run <pre><code>python dataProcessInMemoryQUIC.py \npython pre-training.py\npython re-training.py\n</code></pre></p> <p>All results are collected in an output folder <code>/artifacts</code> as csv files.</p>"},{"location":"papers/imc23/notebooks/","title":"Tables and Figures Jupyter Notebooks","text":"<p>The tables and figures are created via a set of  Jupyter notebooks.</p> <p>The notebooks are stored on both tcbench github as well as  the in the paper  figshare collection.</p> <p>The pages linked below show the rendered version of the notebooks. If you want to run the notebooks, make sure to</p> <ol> <li> <p>Have installed (or imported) <code>ucdavis-icdm19</code>, <code>mirage19</code>, <code>mirage22</code>, <code>utmobilenet21</code>.  Please check each dataset page for more details.</p> </li> <li> <p>Have installed the ml_artifacts</p> </li> <li> <p>To install modeling artifacts, grab <code>ml_artifacts.tgz</code> and unpack it under the  folder mentioned above. The tarball contains a <code>/campaigns</code> folder so the final structure should be <pre><code>tree notebooks/ -d -L 2\nnotebooks/\n\u251c\u2500\u2500 submission_tables_and_figures\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 campaigns\n\u2514\u2500\u2500 tutorials\n</code></pre></p> </li> <li> <p>To install/import datasets refer to the <code>install</code> and <code>import</code> pages.</p> </li> </ol>"},{"location":"papers/imc23/notebooks/#tables","title":"Tables","text":"<ul> <li> <p>Table 2: Summary of Datasets Properties.   <code>table2_datasets_properties.ipynb</code></p> </li> <li> <p>Table 3:  (G0) Baseline ML performance without augmentation in a supervised setting.  <code>table3_xgboost_baseline.ipynb</code></p> </li> <li> <p>Table 4: Comparing data augmentation functions applied in supervised training.   <code>table4_ucdavis-icdm19_comparing_data_augmentations_functions.ipynb</code></p> </li> <li> <p>Table 5:  Impact of dropout and SimCLR projection layer dimension on fine-tuning.  <code>table5_simclr_dropout_and_projectionlayer.ipynb</code></p> </li> <li> <p>Table 6: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.  <code>table6_simclr_other_augmentation_pairs.ipynb</code></p> </li> <li> <p>Table 7: Accuracy on 32x32 flowpic when enlarging training set (w/o Dropout).  <code>table7_larger_trainset.ipynb</code></p> </li> <li> <p>Table 8: (G3) Data augmentation in supervised setting on other datasets.   <code>table8_augmentation-at-loading_on_other_datasets.ipynb</code></p> </li> <li> <p>Table 9 - appendix: Macro-average Accuracy with different retraining dataset and different sampling methods for Rezaei at al. ICM19.  <code>table9_icdm_finetuning_per_class_metrics_on_human.ipynb</code></p> </li> <li> <p>Table 10 - appendix: Performance comparison across augmentations for different flowpic sizes.  <code>table10_ucdavis-icdm19_tukey.ipynb</code></p> </li> </ul>"},{"location":"papers/imc23/notebooks/#figures","title":"Figures","text":"<ul> <li> <p>Figure 1: Example of a packet time series transformed into a flowpic representation for a randomly selected flow.  <code>figure1_flowpic_example.ipynb</code></p> </li> <li> <p>Figure 3: Average confusion matrixes for the 32x32 resolution across all experiments in Table 4.   <code>figure3_confusion_matrix_supervised_setting.ipynb</code></p> </li> <li> <p>Figure 4: Average 32x32 flowpic for each class across multiple data splits.   <code>figure4_ucdavis_per_class_average_flowpic</code></p> </li> <li> <p>Figure 5: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.  <code>figure5_ucdavis_augmentations_comparison</code></p> </li> <li> <p>Figure 6: Critical distance plot of the accuracy obtained with each augmentation across the four tested datasets.  <code>figure6_augmentations_comparison_across_datasets_critical_distance</code></p> </li> <li> <p>Figure 7: Average rank obtained per augmentation and dataset. Ranks closer to 1 indicate a better performance.  <code>figure7_augmentations_comparison_across_datasets_average_rank</code></p> </li> <li> <p>Figure 8 - appendix: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.  <code>figure8_ucdavis_per_class_average_flowpic</code></p> </li> <li> <p>Figure 10(b) - appendix: Classwise evaluation on <code>human</code>.  <code>figure10b_icdm_finetuning_per_class_metrics_on_human</code></p> </li> <li> <p>Figure 11 - appendix: Accuracy difference w/ and w/o Dropout in supervised learning.  <code>figure11_dropout_impact_supervised_setting.ipynb</code></p> </li> </ul>"},{"location":"papers/imc23/notebooks/#others","title":"Others","text":"<ul> <li>Miscellaneous stats across the paper.  <code>miscellaneous_stats.ipynb</code></li> </ul>"},{"location":"papers/imc23/notebooks/#references","title":"References","text":"<p>Rezaei et al., How to Achieve High Classification Accuracy with Just a Few Labels: A Semi-supervised Approach Using Sampled Packets, ICDM19, </p>"},{"location":"papers/imc23/pytest/","title":"ML unit testing","text":"<p>Multiple tests are available to verify different functionalities for either tcbench and the modeling campaigns created.</p> <p>Tests are not bundled with pypi installation. Rather, you need to follow the procedure described in the artifact page to fetch the source code and install all artifacts and datasets.</p> <p>Tests are coded via <code>pytest</code>  and are available under the <code>/tests</code> folder.</p> <p>Tests trigger model training</p> <p>Most of the test verify that the models train for the campaigns described in the paper are indeed reproducible, i.e., the provide the exact same models obtained for the paper.</p> <p>To do so, the pytest resources fetched from figshare  contains a subset of reference models so the test trigger the modeling for those scenarios and check that what trained matches what created for the paper.</p> <p>So be aware that running these tests might take a while depending on your local environment.</p> <p>To trigger all tests run</p> <pre><code>pytest tests\n</code></pre> <p>Output</p> <pre><code>============================ test session starts ======================================\nplatform linux -- Python 3.10.13, pytest-7.4.2, pluggy-1.3.0\nrootdir: /tmp/tcbench-pip/tcbench\nplugins: anyio-3.7.1, helpers-namespace-2021.12.29\ncollected 101 items\n\ntests/test_augmentations_at_loading.py ...........                               [ 10%]\ntests/test_augmentations_at_loading_xgboost.py .                                 [ 11%]\ntests/test_cli_command_campaign.py ....                                          [ 15%]\ntests/test_cli_command_singlerun.py ............                                 [ 27%]\ntests/test_contrastive_learning_and_finetune.py ..                               [ 29%]\ntests/test_libtcdatasets_datasets_utils.py .................                     [ 46%]\ntests/test_modeling_backbone.py ................                                 [ 62%]\ntests/test_modeling_dataprep.py ..................................               [ 96%]\ntests/test_modeling_methods.py ....                                              [100%]\n============================== 101 passed, 8 warnings in 6523.55s (1:48:43) =========================\n</code></pre>"},{"location":"papers/imc23/notebooks/figure10b_icdm_finetuning_per_class_metrics_on_human/","title":"Figure 10(b): Classwise evaluation on human.","text":"<pre><code>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</code></pre> <pre><code>path = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/IncrementalSampling_Retraining(human-triggered)_20/\"\n)\n\nclass_reps = list(path.glob(\"*class_rep.csv\"))\n\nper_cls = np.stack(\n    [\n        pd.read_csv(file)[:5][[\"Accuracy\", \"precision\", \"recall\", \"f1-score\"]].values\n        for file in class_reps\n    ],\n    axis=0,\n)\n\n\nmeans = np.mean(per_cls, axis=0)\n\ncis = np.zeros([per_cls.shape[1], per_cls.shape[2]])\nfor i in range(per_cls.shape[1]):\n    for j in range(per_cls.shape[2]):\n        cis[i, j] = compute_confidence_intervals(per_cls[:, i, j])\n</code></pre> <pre><code>X = [\"G. Drive\", \"Youtube\", \"G. Doc\", \"G. Search\", \"G. Music\"]\nX_axis = np.arange(len(X))\n\nplt.rcParams.update({'font.size': 16})\n\nfig, ax = plt.subplots(figsize=(7, 6.5))\nax.bar(\n    X_axis - 0.3,\n    means[:, 0],\n    0.2,\n    label=\"Accuracy\",\n    yerr=cis[:, 0],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis - 0.1,\n    means[:, 1],\n    0.2,\n    label=\"Precision\",\n    yerr=cis[:, 1],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis + 0.1,\n    means[:, 2],\n    0.2,\n    label=\"Recall\",\n    yerr=cis[:, 2],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\nax.bar(\n    X_axis + 0.3,\n    means[:, 3],\n    0.2,\n    label=\"F1\",\n    yerr=cis[:, 3],\n    ecolor=\"black\",\n    alpha=0.5,\n    capsize=10,\n)\n\n\nplt.xticks(X_axis, X)\nax.set_xlabel(\"Class\")\nax.set_ylabel(\"Value\")\nax.set_ylim([0, 1])\nplt.legend()\nax.legend(bbox_to_anchor=(1, 1.02))\nplt.grid(axis=\"y\")\n\nplt.savefig(\"icdm19_fig3b_replicate_human_ci.png\", dpi=300, bbox_inches=\"tight\")\n</code></pre>"},{"location":"papers/imc23/notebooks/figure11_dropout_impact_supervised_setting/","title":"Figure11 dropout impact supervised setting","text":""},{"location":"papers/imc23/notebooks/figure11_dropout_impact_supervised_setting/#figure-11-accuracy-difference-w-and-wo-dropout-in-supervised-learning","title":"Figure 11: Accuracy difference w/ and w/o Dropout in supervised learning.","text":"<pre><code>import itertools\nimport pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>RENAME = {\n    \"noaug\": \"No aug.\",\n    \"rotate\": \"Rotate\",\n    \"colorjitter\": \"Color jitter\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"changertt\": \"Change RTT\",\n    \"timeshift\": \"Time shift\",\n    \"packetloss\": \"Packet loss\",\n}\n</code></pre> <pre><code>folder = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19/augmentation-at-loading-dropout-impact/campaign_summary/augment-at-loading-dropout-impact/\"\n)\n</code></pre> <pre><code>df = pd.concat(\n    [\n        pd.read_parquet(folder / f\"runsinfo_flowpic_dim_{flowpic_dim}.parquet\")\n        for flowpic_dim in (32, 1500)\n    ]\n)\n</code></pre> <pre><code>df = df[\n    [\n        \"flowpic_dim\",\n        \"test_split_name\",\n        \"aug_name\",\n        \"seed\",\n        \"split_index\",\n        \"acc\",\n        \"with_dropout\",\n    ]\n]\n</code></pre> <pre><code>df = df[df[\"test_split_name\"] != \"test-train-val-leftover\"]\ndf = df.replace(RENAME)\n</code></pre> <pre><code>df_with_dropout = (\n    df[df[\"with_dropout\"] == True]\n    .drop(\"with_dropout\", axis=1)\n    .rename({\"acc\": \"withdropout_acc\"}, axis=1)\n)\n\ndf_no_dropout = (\n    df[df[\"with_dropout\"] == False]\n    .drop(\"with_dropout\", axis=1)\n    .rename({\"acc\": \"nodropout_acc\"}, axis=1)\n)\n</code></pre> <pre><code>df = pd.merge(\n    df_with_dropout,\n    df_no_dropout,\n    on=[\n        \"flowpic_dim\",\n        \"test_split_name\",\n        \"aug_name\",\n        \"seed\",\n        \"split_index\",\n    ],\n    suffixes=[\"withdropout_\", \"nodropout_\"],\n)\n</code></pre> <pre><code>df = df.iloc[df[\"nodropout_acc\"].dropna().index]\n</code></pre> <pre><code>df[\"acc_diff\"] = df[\"withdropout_acc\"] - df[\"nodropout_acc\"]\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</code></pre> <pre><code>df_merged = df.groupby([\"flowpic_dim\", \"test_split_name\", \"aug_name\"]).agg(\n    {\"acc_diff\": [\"mean\", \"std\", \"count\", \"min\", \"max\", compute_confidence_intervals]}\n)\ndf_merged = df_merged.rename(\n    columns={\"compute_confidence_intervals\": \"confidence_interval\"}\n)\ndf_merged = df_merged.droplevel(0, axis=1)\n</code></pre> <pre><code>df_merged\n</code></pre> mean std count min max confidence_interval flowpic_dim test_split_name aug_name 32 test-human Change RTT 1.606426e-01 2.650081 15 -6.024096 4.819277 1.467566 Color jitter 1.285141e+00 5.100786 15 -8.433735 7.228916 2.824721 Horizontal flip -1.044177e+00 2.727209 15 -4.819277 6.024096 1.510278 No aug. -9.638554e-01 2.584049 15 -4.819277 2.409639 1.430999 Packet loss 2.409639e-01 2.922951 15 -3.614458 6.024096 1.618676 Rotate 1.124498e+00 3.712572 15 -7.228916 6.024096 2.055954 Time shift -4.016064e-01 3.366931 15 -6.024096 6.024096 1.864544 test-script Change RTT 3.555556e-01 0.660287 15 -0.666667 1.333333 0.365655 Color jitter 8.444444e-01 1.413465 15 -1.333333 3.333333 0.782751 Horizontal flip 3.555556e-01 1.330155 15 -2.666667 2.666667 0.736615 No aug. -1.777778e-01 0.815200 15 -2.000000 0.666667 0.451442 Packet loss 7.111111e-01 1.139943 15 -1.333333 2.666667 0.631280 Rotate 2.222222e-01 0.860663 15 -0.666667 2.000000 0.476619 Time shift -2.222222e-01 1.172886 15 -2.000000 2.000000 0.649523 1500 test-human Change RTT -3.212851e-01 2.711959 15 -4.819277 6.024096 1.501833 Color jitter 1.927711e+00 1.866498 15 -1.204819 6.024096 1.033632 Horizontal flip 9.638554e-01 1.942713 15 -2.409639 3.614458 1.075838 No aug. -2.368476e-15 3.643031 15 -4.819277 7.228916 2.017443 Packet loss 1.285141e+00 3.795431 15 -3.614458 9.638554 2.101840 Rotate -8.032129e-02 2.896821 15 -4.819277 4.819277 1.604206 Time shift 6.425703e-01 3.215719 15 -3.614458 7.228916 1.780806 test-script Change RTT 8.888889e-02 1.003697 15 -1.333333 2.666667 0.555829 Color jitter 9.333333e-01 1.980861 15 -3.333333 5.333333 1.096964 Horizontal flip 7.555556e-01 1.094479 15 -1.333333 2.666667 0.606102 No aug. 4.000000e-01 1.848208 15 -2.000000 4.000000 1.023504 Packet loss 1.866667e+00 1.802996 15 0.000000 6.000000 0.998466 Rotate 2.666667e-01 1.609496 15 -2.666667 3.333333 0.891309 Time shift 8.888889e-01 1.811194 15 -0.666667 5.333333 1.003006 <pre><code>plt.rcParams.update({\"font.size\": 20})\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 6.5))\n\nfor ax, (flowpic_dim, test_split_name) in zip(\n    axes.flatten(), itertools.product((32, 1500), (\"test-human\", \"test-script\"))\n):\n    df_tmp = df_merged.loc[(flowpic_dim, test_split_name)]\n    df_tmp = df_tmp.loc[list(RENAME.values())]\n\n    ax.bar(\n        list(df_tmp.index),\n        df_tmp[\"mean\"],\n        yerr=df_tmp[\"confidence_interval\"],\n        align=\"center\",\n        alpha=0.5,\n        ecolor=\"black\",\n        capsize=10,\n    )\n\n    ax.set_title(f\"{test_split_name}\\n({flowpic_dim}x{flowpic_dim})\")\n\n    ax.set_xticklabels(list(df_tmp.index), rotation=90, ha=\"center\")\n    ax.set_ylim(-4.5, 4.5)\n    ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(1))\n    ax.grid(axis=\"y\", which=\"both\", linestyle=\":\")\n\nplt.tight_layout()\nplt.savefig(\"supervised_dropout_std.png\", dpi=300, bbox_inches=\"tight\")\n</code></pre> <pre><code>/tmp/ipykernel_97694/3765097497.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\nax.set_xticklabels(list(df_tmp.index), rotation=90, ha=\"center\")\n/tmp/ipykernel_97694/3765097497.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\nax.set_xticklabels(list(df_tmp.index), rotation=90, ha=\"center\")\n/tmp/ipykernel_97694/3765097497.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\nax.set_xticklabels(list(df_tmp.index), rotation=90, ha=\"center\")\n/tmp/ipykernel_97694/3765097497.py:23: UserWarning: FixedFormatter should only be used together with FixedLocator\nax.set_xticklabels(list(df_tmp.index), rotation=90, ha=\"center\")\n</code></pre>"},{"location":"papers/imc23/notebooks/figure1_flowpic_example/","title":"Figure 1 : Example of a packet time series transformed into a flowpic representation for a randomly selected flow","text":"<pre><code>import numpy as np\nimport tcbench as tcb\nfrom matplotlib.colors import LogNorm, Normalize\nfrom tcbench import dataprep\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import tcbench\n</code></pre> <pre><code># load unfiltered dataset\nFLOWPIC_BLOCK_DURATION = 15\n</code></pre> <pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n</code></pre> <pre><code>df_sample = df.sample(n=1, random_state=12345)\nser = df_sample.iloc[0]\n</code></pre> <pre><code>fig, axes = plt.subplots(\n    nrows=1, ncols=5, figsize=(15, 3), gridspec_kw=dict(width_ratios=[1, 1, 1, 1, 1.1])\n)\n\ndirection = np.where(ser[\"pkts_dir\"] == 0, -1, 1)\ny = ser[\"pkts_size\"] * direction\nx = ser[\"timetofirst\"]\n\nax = axes[0]\nax.stem(\n    np.where(y &gt; 0, x, 0),\n    np.where(y &gt; 0, y, 0),\n    markerfmt=\"\",\n    basefmt=\"lightgray\",\n    label=\"outgoing\",\n    linefmt=\"green\",\n)\nax.stem(\n    np.where(y &lt; 0, x, 0),\n    np.where(y &lt; 0, y, 0),\n    markerfmt=\"\",\n    basefmt=\"lightgray\",\n    label=\"incoming\",\n    linefmt=\"lightgreen\",\n)\nax.legend()\nax.set_ylabel(\"packet size [B]\")\nax.set_xlabel(\"time [s]\")\n\nrect = mpl.patches.Rectangle(\n    (0, -1500), 15, 3000, linewidth=1, edgecolor=\"r\", facecolor=\"none\"\n)\nax.add_patch(rect)\nax.annotate(\"first\\n15s\", (5, 1000))\n\nfor idx, flowpic_dim in enumerate((32, 64, 256, 512), start=1):\n    # create a single sample dataset\n    dset = dataprep.FlowpicDataset(\n        data=df_sample,\n        timetofirst_colname=\"timetofirst\",\n        pkts_size_colname=\"pkts_size\",\n        pkts_dir_colname=\"pkts_dir\",\n        target_colname=\"app\",\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n    )\n\n    # fetch the flowpic representation\n    flowpic, label = dset[0]\n\n    # flattening the representation\n    # to remove zero values (used for finding\n    # min values)\n    flowpic = flowpic.numpy().squeeze()\n    flattened = flowpic.flatten()\n    flattened = flattened[flattened &gt; 0]\n\n    ax = axes[idx]\n\n    sns.heatmap(\n        ax=ax,\n        data=np.where(flowpic == 0, np.nan, flowpic),\n        vmin=flattened.min(),\n        vmax=flattened.max(),\n        cbar=idx == 4,\n        cbar_kws=dict(fraction=0.046, pad=0.01, aspect=20, label=\"Normalized packets count\"),\n        cmap=plt.get_cmap(\"viridis_r\"),\n        square=True,\n        norm=LogNorm(flattened.min(), flattened.max()),\n    )\n    for _, spine in ax.spines.items():\n        spine.set_visible(True)\n        spine.set_linewidth(1)\n    ax.yaxis.set_ticks([], None)\n    ax.xaxis.set_ticks([], None)\n    ax.set_ylabel(f\"packets size (bins of {1500 // flowpic_dim}B)\")\n    ax.set_xlabel(f\"time (bins of {FLOWPIC_BLOCK_DURATION / flowpic_dim * 1000:.1f}ms)\")\n    ax.set_title(f\"{flowpic_dim}x{flowpic_dim}\")\n\nplt.savefig(\"flowpic_example.png\", dpi=300, bbox_inches=\"tight\")\n</code></pre>"},{"location":"papers/imc23/notebooks/figure3_confusion_matrix_supervised_setting/","title":"Figure 3 : Average confusion matrixes for the 32x32 resolution across all experiments in Table 4","text":"<pre><code>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>folder_artifacts = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/artifacts/\"\n)\n</code></pre> <pre><code>filelists = [\n    list(folder_artifacts.glob(\"*/test-human_conf_mtx.csv\")),\n    list(folder_artifacts.glob(\"*/test-script_conf_mtx.csv\")),\n]\n\ntitles = [\"human\", \"script\"]\n\nCLASSES = {\n    \"google-doc\": \"G. Doc\",\n    \"google-drive\": \"G. Drive\",\n    \"google-music\": \"G. Music\",\n    \"google-search\": \"G. Search\",\n    \"youtube\": \"YouTube\",\n}\n</code></pre> <pre><code>plt.rcParams.update({\"font.size\": 14})\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 5))\n# cbar_ax = fig.add_axes([0.93, 0.2, 0.02, 0.6])  # (left, bottom, width, height)\nfor i in range(2):\n    cm_mean = np.mean(\n        np.stack(\n            [pd.read_csv(file)[list(CLASSES.keys())].values for file in filelists[i]]\n        ),\n        axis=0,\n    )\n\n    normed_cm_mean = normalize(cm_mean, axis=1, norm=\"l1\")\n\n    ax = axes[i]\n\n    sns.heatmap(\n        data=normed_cm_mean,\n        ax=ax,\n        square=True,\n        cmap=\"viridis\",\n        annot=True,\n        annot_kws={\"fontsize\": 11},\n        fmt=\".2f\",\n        vmin=0,\n        vmax=1,\n        cbar_kws=dict(fraction=0.046, pad=0.03, aspect=20),\n    )\n\n    ax.set_xticklabels(list(CLASSES.values()), rotation=45, ha=\"right\")\n    ax.set_yticklabels(list(CLASSES.values()), rotation=0)\n\n    ax.set_title(titles[i])\n\n    ax.set_ylabel(\"Ground Truth\")\n    ax.set_xlabel(\"Prediction\")\n\nplt.tight_layout()\nplt.savefig(\"ucdavis_dataset_confusion_matrix.png\", bbox_inches=\"tight\", dpi=150)\n</code></pre>"},{"location":"papers/imc23/notebooks/figure4_ucdavis_per_class_average_flowpic/","title":"Figure 4: Average 32x32 flowpic for each class across multiple data splits.","text":"<pre><code>import itertools\n\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import tcbench as tcb\nfrom tcbench import dataprep\n</code></pre> <pre><code>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</code></pre> <pre><code># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code># load the first train split\ndset_train_split = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19, split=0),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code>def compute_average_flowpic(df):\n    # gather all (precomputed) flowpic\n    # in a single tensor (n x dim x dim)\n    # and do an avereage across the 1st dimension\n    mtx = np.stack(df[\"flowpic\"], axis=0).mean(\n        axis=0, keepdims=True\n    )  # .astype(np.uint8)\n    return mtx\n</code></pre> <pre><code>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\nCLASSES_RENAME = {\n    \"google-doc\": \"G. Doc\",\n    \"google-drive\": \"G. Drive\",\n    \"google-search\": \"G. Search\",\n    \"google-music\": \"G. Music\",\n    \"youtube\": \"YouTube\",\n}\n</code></pre> <pre><code>average_flowpic = dict()\nfor partition_name in [\n    \"pretraining\",\n    \"train-split\",\n    \"retraining-script-triggered\",\n    \"retraining-human-triggered\",\n]:\n    if partition_name != \"train-split\":\n        df_partition = dset.df[dset.df[\"partition\"] == partition_name]\n    else:\n        df_partition = dset_train_split.df\n\n    average_flowpic[partition_name] = dict()\n\n    for target in TARGETS_LABEL:\n        df_app = df_partition[df_partition[\"app\"] == target]\n        mtx = compute_average_flowpic(df_app).squeeze()\n        average_flowpic[partition_name][target] = mtx\n</code></pre> <pre><code>mtx_min = 100\nmtx_max = -100\nfor partition_name, app in itertools.product(\n    [\n        \"pretraining\",\n        \"train-split\",\n        \"retraining-script-triggered\",\n        \"retraining-human-triggered\",\n    ],\n    TARGETS_LABEL,\n):\n    mtx = average_flowpic[partition_name][app]\n    nonzero = mtx.flatten()\n    nonzero = nonzero[nonzero &gt; 0]\n    if nonzero.min() &lt; mtx_min:\n        mtx_min = nonzero.min()\n    if mtx.max() &gt; mtx_max:\n        mtx_max = mtx.max()\n\n# mtx_min, mtx_max\n</code></pre> <pre><code>mpl.rcParams.update({\"font.size\": 13})\n\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(8, 6), sharex=True, sharey=True)\n\ncbar_ax = fig.add_axes([0.97, 0.2, 0.03, 0.6])  # (left, bottom, width, height)\ncbar_ax.set_ylabel(\"packets size normalized frequency\")\n\nfor i, ax, (partition_name, app) in zip(\n    range(len(axes.flatten())),\n    axes.flatten(),\n    itertools.product(\n        [\n            \"pretraining\",\n            \"train-split\",\n            \"retraining-script-triggered\",\n            \"retraining-human-triggered\",\n        ],\n        TARGETS_LABEL,\n    ),\n):\n    mtx = average_flowpic[partition_name][app]\n\n    sns.heatmap(\n        ax=ax,\n        data=np.where(mtx == 0, np.nan, mtx),\n        vmin=mtx_min,\n        vmax=mtx_max,\n        cbar=i == 0,\n        cmap=plt.get_cmap(\"viridis_r\"),\n        square=True,\n        norm=LogNorm(mtx_min, mtx_max),\n        cbar_ax=None if i else cbar_ax,  # &lt;19\n        cbar_kws=dict(label=\"Normalized packets count\"),\n    )\n\n    # ax.set_title(target)\n    for pos in (\"top\", \"bottom\", \"right\", \"left\"):\n        ax.spines[pos].set_color(\"gray\")\n        ax.spines[pos].set_visible(True)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_ticks([], None)\n    ax.set_ylabel(\"\")\n\nfor ax, app in zip(axes[0], TARGETS_LABEL):\n    ax.set_title(CLASSES_RENAME[app], fontsize=13)\n\nfor ax, partition_name in zip(\n    axes[:, 0],\n    [\"Pretraining\\n(all splits)\", \"Pretraining\\n(split 0)\", \"script\", \"human\"],\n):\n    ax.set_ylabel(partition_name, fontsize=13)\n\n### annotations\npatches = [\n    ######### A #########\n    # vertical\n    mpl.patches.ConnectionPatch(\n        xyA=(5, -1),\n        coordsA=axes[0][3].transData,\n        xyB=(5, 28),\n        coordsB=axes[3][3].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(33, -1),\n        coordsA=axes[0][3].transData,\n        xyB=(33, 28),\n        coordsB=axes[3][3].transData,\n    ),\n    # horizontal\n    mpl.patches.ConnectionPatch(\n        xyA=(5, -1),\n        coordsA=axes[0][3].transData,\n        xyB=(33, -1),\n        coordsB=axes[0][3].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(5, 28),\n        coordsA=axes[3][3].transData,\n        xyB=(33, 28),\n        coordsB=axes[3][3].transData,\n    ),\n    ######### B #########\n    # vertical\n    mpl.patches.ConnectionPatch(\n        xyA=(-1, 29),\n        coordsA=axes[3][3].transData,\n        xyB=(-1, 33),\n        coordsB=axes[3][3].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(33, 29),\n        coordsA=axes[3][3].transData,\n        xyB=(33, 33),\n        coordsB=axes[3][3].transData,\n    ),\n    # horizontal\n    mpl.patches.ConnectionPatch(\n        xyA=(-1, 29),\n        coordsA=axes[3][3].transData,\n        xyB=(33, 29),\n        coordsB=axes[3][3].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(-1, 33),\n        coordsA=axes[3][3].transData,\n        xyB=(33, 33),\n        coordsB=axes[3][3].transData,\n    ),\n    ######### C #########\n    # vertical\n    mpl.patches.ConnectionPatch(\n        xyA=(12, -1),\n        coordsA=axes[0][2].transData,\n        xyB=(12, 28),\n        coordsB=axes[3][2].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(29, -1),\n        coordsA=axes[0][2].transData,\n        xyB=(29, 28),\n        coordsB=axes[3][2].transData,\n    ),\n    # horizontal\n    mpl.patches.ConnectionPatch(\n        xyA=(12, -1),\n        coordsA=axes[0][2].transData,\n        xyB=(29, -1),\n        coordsB=axes[0][2].transData,\n    ),\n    mpl.patches.ConnectionPatch(\n        xyA=(12, 28),\n        coordsA=axes[3][2].transData,\n        xyB=(29, 28),\n        coordsB=axes[3][2].transData,\n    ),\n]\n\nfor p in patches:\n    p.set(color=\"red\", linewidth=2)\n    fig.add_artist(p)\n\nfig.text(0.73, 0.47, \"A\", color=\"red\", fontweight=\"bold\", fontsize=30)\nfig.text(0.6, 0.05, \"B\", color=\"red\", fontweight=\"bold\", fontsize=30)\nfig.text(0.525, 0.35, \"C\", color=\"red\", fontweight=\"bold\", fontsize=30)\n\nplt.savefig(\n    \"ucdavis_dataset_different_partition.png\",\n    bbox_inches=\"tight\",\n    pad_inches=0.05,\n    dpi=300,\n)\nplt.tight_layout()\n</code></pre> <pre><code>/tmp/ipykernel_19221/3453343292.py:122: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</code></pre>"},{"location":"papers/imc23/notebooks/figure5_ucdavis_augmentations_comparison/","title":"Figure 5: Critical distance plot of the accuracy obtained with each augmentation for the 32x32 and 64x64 cases.","text":"<pre><code>import pathlib\n\nimport autorank\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>folder_path = pathlib.Path(\n    \"./campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/\"\n)\n</code></pre> <pre><code>df = pd.concat(\n    [\n        pd.read_parquet(folder_path / f\"runsinfo_flowpic_dim_{flowpic_dim}.parquet\")\n        for flowpic_dim in (32, 64, 1500)\n    ]\n)\n</code></pre> <pre><code>def prepare_data(dat, test_split):\n    res = dat.query(\"test_split_name == @test_split &amp; flowpic_dim != 1500\")\n    res = res[[\"aug_name\", \"split_index\", \"flowpic_dim\", \"seed\", \"acc\"]]\n    res[\"id\"] = (\n        \"split_index\"\n        + res[\"split_index\"].astype(str)\n        + \"_seed\"\n        + res[\"seed\"].astype(str)\n        + \"_flowpicdim\"\n        + res[\"flowpic_dim\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"acc\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df, test_split, force_ranks=True):\n    df1 = prepare_data(df, test_split)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement)\n    if force_ranks:\n        return autorank.autorank(new_df, force_mode=\"nonparametric\")\n    else:\n        return autorank.autorank(new_df)\n</code></pre> <pre><code>res_script = get_ranks(df, \"test-script\", force_ranks=True)\nres_human = get_ranks(df, \"test-human\", force_ranks=True)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 4))\nax = axes[0]\nautorank.plot_stats(res_script, ax=ax)\nax.set_title(\"Script\")\nfor obj in ax._children:\n    if isinstance(obj, mpl.text.Text) and obj.get_text() in (\n        \"Change RTT\",\n        \"Time shift\",\n    ):\n        obj.set_fontweight(\"bold\")\n\nax = axes[1]\nautorank.plot_stats(res_human, ax=ax)\nax.set_title(\"Human\")\nfor obj in ax._children:\n    if isinstance(obj, mpl.text.Text) and obj.get_text() in (\n        \"Change RTT\",\n        \"Time shift\",\n    ):\n        obj.set_fontweight(\"bold\")\n\nplt.tight_layout()\nplt.savefig(\"augmentations_rank_comparison.png\", bbox_inches=\"tight\", dpi=300)\n</code></pre> <pre><code>Tests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\nTests for normality and homoscedacity are ignored for test selection, forcing nonparametric tests\n</code></pre> <pre><code>autorank.create_report(res_human)\n</code></pre> <pre><code>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.008). Therefore, we assume that all populations are normal.\nWe applied Bartlett's test for homogeneity and reject the null hypothesis (p=0.030) that thedata is homoscedastic. Thus, we assume that our data is heteroscedastic.\nBecause we have more than two populations and the populations are normal but heteroscedastic, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the mean values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the mean value (M), the standard deviation (SD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.001) of the Friedman test that there is no difference in the central tendency of the populations No augmentation (MD=68.675+-1.313, MAD=1.807, MR=5.300), Color jitter (MD=69.880+-2.339, MAD=3.614, MR=4.567), Horizontal flip (MD=71.084+-1.753, MAD=1.807, MR=4.383), Change RTT (MD=72.289+-1.701, MAD=2.410, MR=3.633), Packet Loss (MD=71.084+-1.326, MAD=1.807, MR=3.500), Time shift (MD=71.084+-1.564, MAD=1.807, MR=3.417), and Rotate (MD=72.289+-1.661, MAD=2.410, MR=3.200). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: No augmentation, Color jitter, and Horizontal flip; Color jitter, Horizontal flip, Change RTT, Packet Loss, Time shift, and Rotate. All other differences are significant.\n</code></pre> <pre><code>autorank.create_report(res_script)\n</code></pre> <pre><code>The statistical analysis was conducted for 7 populations with 30 paired samples.\nThe family-wise significance level of the tests is alpha=0.050.\nWe rejected the null hypothesis that the population is normal for the populations Rotate (p=0.001), Packet Loss (p=0.002), and Time shift (p=0.002). Therefore, we assume that not all populations are normal.\nBecause we have more than two populations and the populations and some of them are not normal, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the median values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the median (MD), the median absolute deviation (MAD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=1.644 of the Nemenyi test.\nWe reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations No augmentation (MD=96.000+-0.333, MAD=0.333, MR=5.883), Horizontal flip (MD=96.000+-1.000, MAD=0.667, MR=5.567), Rotate (MD=96.667+-0.667, MAD=0.667, MR=4.017), Packet Loss (MD=96.667+-1.000, MAD=0.667, MR=3.750), Time shift (MD=96.667+-1.000, MAD=0.667, MR=3.233), Change RTT (MD=97.333+-0.667, MAD=0.667, MR=2.933), and Color jitter (MD=97.333+-1.333, MAD=1.333, MR=2.617). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\nBased on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: No augmentation and Horizontal flip; Horizontal flip and Rotate; Rotate, Packet Loss, Time shift, Change RTT, and Color jitter. All other differences are significant.\n</code></pre>"},{"location":"papers/imc23/notebooks/figure6_augmentations_comparison_across_datasets_critical_distance/","title":"Figure 6 : Critical distance plot of the accuracy obtained withh each augmentationacross the four tested datasets","text":"<pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import autorank\nimport pandas as pd\n</code></pre> <pre><code>dat = {\n    \"mirage19\": pd.read_parquet(\n        \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_10\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_1000\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"utmobile19\": pd.read_parquet(\n        \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n}\n</code></pre> <pre><code>def prepare_data(df):\n    res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\n    res.loc[:, \"id\"] = (\n        \"split_index\"\n        + res.loc[:, \"split_index\"].astype(str)\n        + \"_seed\"\n        + res.loc[:, \"seed\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"f1\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    rankmat = new_df.rank(axis=\"columns\", ascending=False)\n    return rankmat\n\n\ndef get_pivoted_df(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    return new_df\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(4, 2.5))\n\nperfs = pd.concat(\n    [\n        get_pivoted_df(dat[\"mirage19\"]),\n        get_pivoted_df(dat[\"mirage22_10\"]),\n        get_pivoted_df(dat[\"mirage22_1000\"]),\n        get_pivoted_df(dat[\"utmobile19\"]),\n    ]\n)\n\nautorank.plot_stats(autorank.autorank(perfs), ax=ax)\nfor obj in ax._children:\n    if isinstance(obj, mpl.text.Text) and obj.get_text() in (\n        \"Change RTT\",\n        \"Time shift\",\n    ):\n        obj.set_fontweight(\"bold\")\n\nplt.savefig(\"augmentations_rank_comparison_together1.png\", bbox_inches=\"tight\", dpi=300)\n</code></pre> <pre><code>/tmp/ipykernel_13329/2271513213.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_13329/2271513213.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_13329/2271513213.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n/tmp/ipykernel_13329/2271513213.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  res.loc[:, \"id\"] = (\n</code></pre>"},{"location":"papers/imc23/notebooks/figure7_augmentations_comparison_across_datasets_average_rank/","title":"Figure7 augmentations comparison across datasets average rank","text":""},{"location":"papers/imc23/notebooks/figure7_augmentations_comparison_across_datasets_average_rank/#figure-7-average-rank-obtained-per-augmentation-and-dataset","title":"Figure 7: Average rank obtained per augmentation and dataset.","text":"<pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import autorank\nimport pandas as pd\n</code></pre> <pre><code>dat = {\n    \"mirage19\": pd.read_parquet(\n        \"./campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_10\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"mirage22_1000\": pd.read_parquet(\n        \"./campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n    \"utmobile19\": pd.read_parquet(\n        \"./campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/runsinfo_flowpic_dim_32.parquet\"\n    ),\n}\n</code></pre> <pre><code>def prepare_data(df):\n    res = df[[\"hash\", \"aug_name\", \"seed\", \"split_index\", \"f1\"]]\n    res.loc[:, \"id\"] = (\n        \"split_index\"\n        + res.loc[:, \"split_index\"].astype(str)\n        + \"_seed\"\n        + res.loc[:, \"seed\"].astype(str)\n    )\n    res = res[[\"aug_name\", \"id\", \"f1\"]]\n    return res.sort_values([\"aug_name\", \"id\"])\n\n\ndef get_ranks(df):\n    df1 = prepare_data(df)\n    df1 = df1.pivot(columns=\"aug_name\", index=\"id\").reset_index(drop=True)\n    df1.columns = df1.columns.get_level_values(1)\n    new_df = pd.DataFrame(\n        {\n            \"changertt\": df1[\"changertt\"].values,\n            \"colorjitter\": df1[\"colorjitter\"].values,\n            \"horizontalflip\": df1[\"horizontalflip\"].values,\n            \"noaug\": df1[\"noaug\"].values,\n            \"packetloss\": df1[\"packetloss\"].values,\n            \"rotate\": df1[\"rotate\"].values,\n            \"timeshift\": df1[\"timeshift\"].values,\n        }\n    )\n    replacement = {\n        \"noaug\": \"No augmentation\",\n        \"horizontalflip\": \"Horizontal flip\",\n        \"rotate\": \"Rotate\",\n        \"timeshift\": \"Time shift\",\n        \"colorjitter\": \"Color jitter\",\n        \"changertt\": \"Change RTT\",\n        \"packetloss\": \"Packet Loss\",\n    }\n    new_df = new_df.rename(columns=replacement).dropna()\n    rankmat = new_df.rank(axis=\"columns\", ascending=False)\n    return rankmat\n</code></pre> <pre><code>def prepare_ranks_data(dataset):\n    res = get_ranks(dat[dataset])\n    res[\"dataset\"] = dataset\n    return res\n\n\ntogether = pd.concat(\n    [\n        prepare_ranks_data(\"mirage19\"),\n        prepare_ranks_data(\"mirage22_10\"),\n        prepare_ranks_data(\"mirage22_1000\"),\n        prepare_ranks_data(\"utmobile19\"),\n    ]\n)\n\ndf_tmp = pd.melt(\n    together, id_vars=[\"dataset\"], var_name=\"augmentation\", value_name=\"rank\"\n)\n</code></pre> <pre><code>/tmp/ipykernel_13347/159222887.py:3: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nres.loc[:, \"id\"] = (\n/tmp/ipykernel_13347/159222887.py:3: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nres.loc[:, \"id\"] = (\n/tmp/ipykernel_13347/159222887.py:3: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nres.loc[:, \"id\"] = (\n/tmp/ipykernel_13347/159222887.py:3: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nres.loc[:, \"id\"] = (\n</code></pre> <pre><code>df_tmp2\n</code></pre> dataset Mirage19 Mirage22\\n(10ptks) Mirage22\\n(1,000pkts) UTMobilenet21 augmentation Change RTT 1.000000 1.733333 1.666667 3.466667 Time shift 2.733333 2.666667 3.666667 2.133333 No augmentation 3.000000 4.533333 4.933333 4.933333 Horizontal flip 3.333333 3.266667 5.200000 5.266667 Packet Loss 5.333333 3.533333 3.333333 5.933333 Color jitter 5.600000 5.533333 5.800000 3.800000 Rotate 7.000000 6.733333 3.400000 2.466667 <pre><code>df_tmp2 = df_tmp.groupby([\"dataset\", \"augmentation\"])[\"rank\"].mean().reset_index()\ndf_tmp2 = df_tmp2.pivot(index=\"augmentation\", columns=\"dataset\", values=\"rank\")\ndf_tmp2 = df_tmp2.rename(\n    {\n        \"mirage19\": \"Mirage19\",\n        \"mirage22_10\": \"Mirage22\\n(10ptks)\",\n        \"mirage22_1000\": \"Mirage22\\n(1,000pkts)\",\n        \"utmobile19\": \"UTMobilenet21\",\n    },\n    axis=1,\n)\ndf_tmp2 = df_tmp2.sort_values(by=\"Mirage19\", ascending=True)\n\nfig, ax = plt.subplots(figsize=(5, 2.3))\nax = sns.heatmap(\n    ax=ax,\n    data=df_tmp2,\n    annot=True,\n    fmt=\".1f\",\n    cmap=plt.get_cmap(\"magma_r\"),\n    cbar_kws=dict(label=\"Average rank\"),\n)\n\nfor lab in ax.get_yticklabels():\n    if lab.get_text() in (\"Change RTT\", \"Time shift\"):\n        lab.set_fontweight(\"bold\")\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=25, ha=\"right\")\nax.tick_params(axis=\"x\", which=\"major\", pad=0)\nax.set_ylabel(None)\nax.set_xlabel(None)\n\nplt.savefig(\"augmentations_rank_comparison_together2.png\", bbox_inches=\"tight\", dpi=300)\n</code></pre>"},{"location":"papers/imc23/notebooks/figure8_ucdavis_kde_on_pkts_size/","title":"Figure 8: Investigating root cause of G1 discrepancies: Kernel density estimation of the per-class packet size distributions.","text":"<pre><code>import itertools\n\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm, Normalize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>import tcbench as tcb\nfrom tcbench import dataprep\n</code></pre> <pre><code>FLOWPIC_DIM = 32\nFLOWPIC_BLOCK_DURATION = 15\n</code></pre> <pre><code># load unfiltered dataset\ndset = dataprep.FlowpicDataset(\n    data=tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19),\n    timetofirst_colname=\"timetofirst\",\n    pkts_size_colname=\"pkts_size\",\n    pkts_dir_colname=\"pkts_dir\",\n    target_colname=\"app\",\n    flowpic_dim=FLOWPIC_DIM,\n    flowpic_block_duration=FLOWPIC_BLOCK_DURATION,\n)\n</code></pre> <pre><code>REPLACE = {\n    \"google-doc\": \"G. Doc\",\n    \"google-drive\": \"G. Drive\",\n    \"google-music\": \"G. Music\",\n    \"google-search\": \"G. Search\",\n    \"youtube\": \"YouTube\",\n    \"retraining-human-triggered\": \"Human\",\n    \"retraining-script-triggered\": \"Script\",\n}\n\ndset.df = dset.df.assign(\n    app = dset.df[\"app\"].replace(REPLACE),\n    partition = dset.df[\"partition\"].replace(REPLACE)\n)\n</code></pre> <pre><code>TARGETS_LABEL = sorted(dset.df[\"app\"].unique())\nPARTITIONS_NAME = sorted(dset.df[\"partition\"].unique())\n</code></pre> <pre><code>all_pkts_size = dict()\n\nfor partition_name in PARTITIONS_NAME:\n    all_pkts_size[partition_name] = dict()\n\n    for app in TARGETS_LABEL:\n        df_tmp = dset.df[\n            (dset.df[\"partition\"] == partition_name) &amp; (dset.df[\"app\"] == app)\n        ]\n\n        l = []\n        for idx in df_tmp.index:\n            ser = df_tmp.loc[idx]\n            indexes = np.where(ser[\"timetofirst\"] &lt; FLOWPIC_BLOCK_DURATION)[0]\n            pkts_size = ser[\"pkts_size\"][indexes]\n            l.append(pkts_size)\n        all_pkts_size[partition_name][app] = np.concatenate(l)\n</code></pre> <pre><code># WARNING: computing the KDE will take a few minutes\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 5))\n\nline_props = {\n    \"pretraining\": dict(linestyle=\"-\"),\n    \"Script\": dict(\n        linestyle=(0, (1, 1))\n    ), \n    \"Human\": dict(linestyle=(0, (1, 1))),\n}\n\nfor ax, app in zip(axes, TARGETS_LABEL):\n    for partition_name in [\n        \"pretraining\",\n        \"Script\",\n        \"Human\",\n    ]:\n        props = line_props[partition_name]\n        sns.kdeplot(\n            ax=ax,\n            data=all_pkts_size[partition_name][app],\n            linewidth=2,\n            label=partition_name,\n            **props,\n            fill=True,\n            alpha=0.1\n        )\n    ax.legend(bbox_to_anchor=(0.5, 1.5), loc=\"upper center\")\n    ax.set_title(app, fontsize=10)\n    ax.set_xlim((-500, 1800))\n    ax.set_xlabel(\"packet size\")\n    ax.set_ylabel(\"kde\")\n\nplt.tight_layout()\nplt.savefig(\"ucdavid-icdm19_kde_pkts_size.png\", dpi=300, bbox_inches='tight')\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/","title":"Miscellaneous stats","text":""},{"location":"papers/imc23/notebooks/miscellaneous_stats/#miscellaneous-stats-across-the-paper","title":"Miscellaneous stats across the paper","text":"<pre><code>import json\nimport pathlib\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom tcbench.modeling import backbone\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#section-3","title":"Section 3","text":""},{"location":"papers/imc23/notebooks/miscellaneous_stats/#total-number-of-campaigns","title":"total number of campaigns","text":"<pre><code>def find_artifacts_folder(folder):\n    if folder.name == \"artifacts\":\n        return [folder]\n\n    res = []\n    for item in folder.iterdir():\n        if item.is_dir():\n            res += find_artifacts_folder(item)\n    return res\n\n# \"campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/\n</code></pre> <pre><code>folders = find_artifacts_folder(pathlib.Path(\"./campaigns/\"))\nlen(folders)\n</code></pre> <pre><code>13\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#total-number-of-runs","title":"total number of runs","text":"<pre><code>sum([len(list(path.iterdir())) for path in folders])\n</code></pre> <pre><code>2760\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#section-4","title":"Section 4","text":""},{"location":"papers/imc23/notebooks/miscellaneous_stats/#average-depth-of-xgboost-models","title":"Average depth of xgboost models","text":"<pre><code>class Node:\n    def __init__(self, node_id, left=None, right=None):\n        self.node_id = node_id\n        self.left = left\n        self.right = right\n\n    def is_leaf(self):\n        return self.left is None and self.right is None\n</code></pre> <pre><code>def build_graph(tree_data):\n    df_tmp = tree_data.fillna(-1)\n    nodes = {node_id: Node(node_id) for node_id in df_tmp[\"ID\"]}\n    nodes[-1] = None\n\n    for idx, node in zip(range(len(df_tmp)), nodes.values()):\n        left_id, right_id = df_tmp.iloc[idx][[\"Yes\", \"No\"]]\n        node.left = nodes.get(left_id, None)\n        node.right = nodes.get(right_id, None)\n\n    return next(iter(nodes.values()))\n</code></pre> <pre><code>def _graph_max_depth(node, depth=0):\n    if node.is_leaf():\n        return depth\n    return max(\n        _graph_max_depth(node.left, depth + 1), _graph_max_depth(node.right, depth + 1)\n    )\n</code></pre> <pre><code>def _tree_max_depth(df_tree):\n    root = build_graph(df_tree)\n    return _graph_max_depth(root)\n</code></pre> <pre><code>def trees_avg_depth(fname):\n    xgb_model = backbone.xgboost_factory().xgb_model\n    xgb_model.load_model(fname)\n    booster_data = xgb_model.get_booster().trees_to_dataframe()\n    return booster_data.groupby(\"Tree\").apply(_tree_max_depth).mean()\n</code></pre> <pre><code>folder = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries/artifacts/\"\n)\n\nnp.array([trees_avg_depth(fname) for fname in folder.glob(\"*/*.json\")]).mean()\n</code></pre> <pre><code>1.6982666666666666\n</code></pre> <pre><code>folder = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic/artifacts/\"\n)\n\nnp.array([trees_avg_depth(fname) for fname in folder.glob(\"*/*.json\")]).mean()\n</code></pre> <pre><code>1.3896\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#section-4_1","title":"Section 4","text":""},{"location":"papers/imc23/notebooks/miscellaneous_stats/#average-experiment-duration","title":"average experiment duration","text":"<pre><code>folder = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/\"\n)\n</code></pre> <pre><code># all test splits are evaluated at the same time\n# so it's enough to check one of them\nruns_1500 = pd.read_parquet(folder / \"runsinfo_flowpic_dim_1500.parquet\")\nruns_1500[runs_1500[\"test_split_name\"] == \"test-script\"][\"run_duration\"].mean()\n</code></pre> <pre><code>1512.8632845379057\n</code></pre> <pre><code>runs_32 = pd.read_parquet(folder / \"runsinfo_flowpic_dim_32.parquet\")\nruns_32[runs_32[\"test_split_name\"] == \"test-script\"][\"run_duration\"].mean()\n</code></pre> <pre><code>55.191846643175396\n</code></pre> <pre><code>runs_64 = pd.read_parquet(folder / \"runsinfo_flowpic_dim_64.parquet\")\nruns_64[runs_64[\"test_split_name\"] == \"test-script\"][\"run_duration\"].mean()\n</code></pre> <pre><code>70.5957797731672\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#number-of-samples-when-doing-a-8020-trainval-split-based-on-all-samples-available","title":"number of samples when doing a 80/20 train/val split based on all samples available","text":"<pre><code>folder = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/larger-trainset/augmentation-at-loading\"\n)\n</code></pre> <pre><code># this is reported in the logs so we can simply check one run\n# that does not have any augmentation\n\nruns = pd.read_parquet(\n    folder\n    / \"campaign_summary/augment-at-loading-larger-trainset/runsinfo_flowpic_dim_32.parquet\"\n)\n</code></pre> <pre><code>run_hash = runs[runs[\"aug_name\"] == \"noaug\"][\"hash\"].values[0]\n</code></pre> <pre><code>fname_log = folder / \"artifacts\" / run_hash / \"log.txt\"\nfname_log.read_text().splitlines()[:32]\n</code></pre> <pre><code>['',\n'connecting to AIM repo at: /mnt/storage/finamore/imc23-submission/camera-ready/campaigns/ucdavis-icdm19/augment-at-loading_larger-trainset/__staging__/netml05_gpu0',\n'created aim run hash=d0af742e1b0846169452b04a',\n'artifacts folder at: /mnt/storage/finamore/imc23-submission/camera-ready/campaigns/ucdavis-icdm19/augment-at-loading_larger-trainset/__staging__/netml05_gpu0/artifacts/d0af742e1b0846169452b04a',\n'WARNING: the artifact folder is not a subfolder of the AIM repo',\n'--- run hparams ---',\n'flowpic_dim: 32',\n'flowpic_block_duration: 15',\n'split_index: -1',\n'max_samples_per_class: -1',\n'aug_name: noaug',\n'patience_steps: 5',\n'suppress_val_augmentation: False',\n'dataset: ucdavis-icdm19',\n'dataset_minpkts: -1',\n'seed: 25',\n'with_dropout: False',\n'campaign_id: augment-at-loading-larger-trainset',\n'campaign_exp_idx: 20',\n'-------------------',\n'loaded: /opt/anaconda/anaconda3/envs/tcbench/lib/python3.10/site-packages/tcbench/libtcdatasets/datasets/ucdavis-icdm19/preprocessed/ucdavis-icdm19.parquet',\n'no augmentation',\n'no augmentation',\n'dataset samples count',\n'               train  val',\n'app                      ',\n'google-search   1532  383',\n'google-drive    1307  327',\n'google-doc       976  245',\n'youtube          861  216',\n'google-music     473  119',\n'']\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#comparing-simclr-results-between-100-samples-and-larger-training","title":"comparing SimCLR results between 100 samples and larger training","text":"<pre><code>df_100_samples = pd.read_csv(\n    \"campaigns/ucdavis-icdm19/simclr-dropout-and-projection/campaign_summary/simclr-dropout-and-projection/summary_flowpic_dim_32.csv\",\n    header = [0, 1],\n    index_col = [0, 1, 2]\n)\n</code></pre> <pre><code>ser_100samples = df_100_samples[\"acc\"].xs(30, level=1, axis=0).xs(False, level=1, axis=0)[\"mean\"]\nser_100samples\n</code></pre> <pre><code>test-human     74.690909\ntest-script    92.184000\nName: mean, dtype: float64\n</code></pre> <pre><code>df_largerdataset = pd.read_csv(\n    \"campaigns/ucdavis-icdm19/larger-trainset/simclr/campaign_summary/simclr-larger-trainset/summary_flowpic_dim_32.csv\",\n    header = [0, 1],\n    index_col = [0, 1]\n)\n</code></pre> <pre><code>ser_largerdataset = df_largerdataset[\"acc\"][\"mean\"].droplevel(1, axis=0)\n</code></pre> <pre><code>ser_largerdataset\n</code></pre> <pre><code>test-human     80.454545\ntest-script    93.900000\nName: mean, dtype: float64\n</code></pre> <pre><code>(ser_largerdataset - ser_100samples).round(2)\n</code></pre> <pre><code>test-human     5.76\ntest-script    1.72\nName: mean, dtype: float64\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#min-and-max-from-table-3","title":"min and max from Table 3","text":"<pre><code>df_script = pd.read_csv(\n    \"table3_ucdavis-icdm19_comparing_data_augmentations_functions_test_on_script.csv\",\n    header=[0, 1, 2],\n    index_col=[0],\n)\n\ndf_human = pd.read_csv(\n    \"table3_ucdavis-icdm19_comparing_data_augmentations_functions_test_on_human.csv\",\n    header=[0, 1, 2],\n    index_col=[0],\n)\n</code></pre> <pre><code>ser_script = df_script[\"ours\"][\"32\"][\"mean\"].drop(\"mean_diff\", axis=0)\nser_script.name = \"script\"\n\nser_human = df_human[\"ours\"][\"32\"][\"mean\"].drop(\"mean_diff\", axis=0)\nser_human.name = \"human\"\n\ndf_tmp = pd.concat((ser_script, ser_human), axis=1)\ndf_tmp.max() - df_tmp.min()\n</code></pre> <pre><code>script    2.09\nhuman     3.22\ndtype: float64\n</code></pre>"},{"location":"papers/imc23/notebooks/miscellaneous_stats/#min-and-max-from-table-8","title":"min and max from Table 8","text":"<pre><code>df_others = pd.read_csv(\n    \"table8_augmentation-at-loading_on_other_datasets.csv\", header=[0, 1], index_col=[0]\n)\ndf_tmp = df_others.xs(\"mean\", level=1, axis=1)\ndf_tmp.max() - df_tmp.min()\n</code></pre> <pre><code>mirage22 - minpkts10          5.50\nmirage22 - minpkts1000       10.08\nutmobilenet21 - minpkts10     9.84\nmirage19 - minpkts10         13.93\ndtype: float64\n</code></pre>"},{"location":"papers/imc23/notebooks/table10_ucdavis-icdm19_tukey/","title":"Table10 ucdavis icdm19 tukey","text":""},{"location":"papers/imc23/notebooks/table10_ucdavis-icdm19_tukey/#table-10-performance-comparison-across-augmentations-for-different-flowpic-sizes","title":"Table 10: Performance comparison across augmentations for different flowpic sizes.","text":"<pre><code>import pathlib\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import tukey_hsd\n</code></pre> <pre><code>folder = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout\"\n)\ndf = pd.concat(\n    (\n        pd.read_parquet(folder / \"runsinfo_flowpic_dim_1500.parquet\"),\n        pd.read_parquet(folder / \"runsinfo_flowpic_dim_64.parquet\"),\n        pd.read_parquet(folder / \"runsinfo_flowpic_dim_32.parquet\"),\n    )\n)\n</code></pre> <pre><code># df = pd.read_parquet('campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/1684447037/merged_runsinfo.parquet')\n</code></pre> <pre><code>df_script = df[df[\"test_split_name\"] == \"test-script\"]\n\nacc_32 = df_script[df_script[\"flowpic_dim\"] == 32][\"acc\"].values.tolist()\nacc_64 = df_script[df_script[\"flowpic_dim\"] == 64][\"acc\"].values.tolist()\nacc_1500 = df_script[df_script[\"flowpic_dim\"] == 1500][\"acc\"].values.tolist()\n</code></pre> <pre><code>res = tukey_hsd(acc_32, acc_64, acc_1500)\n</code></pre> <pre><code>df = pd.DataFrame(\n    np.array([res.pvalue[0, 1], res.pvalue[0, 2], res.pvalue[1, 2]]).reshape(-1, 1),\n    columns=[\"pvalue\"],\n    index=pd.MultiIndex.from_arrays(\n        [(\"32x32\", \"32x32\", \"64x64\"), (\"64x64\", \"1500x1500\", \"1500x1500\")]\n    ),\n)\ndf = df.assign(is_different=df[\"pvalue\"] &lt; 0.05)\n</code></pre> <pre><code>df\n</code></pre> pvalue is_different 32x32 64x64 5.772842e-01 False 1500x1500 1.936038e-06 True 64x64 1500x1500 1.044272e-08 True"},{"location":"papers/imc23/notebooks/table2_datasets_properties/","title":"Table2 datasets properties","text":""},{"location":"papers/imc23/notebooks/table2_datasets_properties/#table-2-datasets-properties","title":"Table 2 : Datasets properties","text":"<pre><code>import pandas as pd\nimport tcbench as tcb\n</code></pre>"},{"location":"papers/imc23/notebooks/table2_datasets_properties/#ucdavis-icdm19","title":"ucdavis-icdm19","text":"<pre><code>df = tcb.load_parquet(tcb.DATASETS.UCDAVISICDM19)\n\n# add number of packets\ndf = df.assign(packets=df[\"pkts_size\"].apply(len))\n\n# number of samples\ndf_tmp = pd.DataFrame(\n    df.groupby([\"partition\", \"app\"])[\"app\"].value_counts()\n).reset_index()\ndf_tmp = df_tmp.pivot(index=\"partition\", columns=\"app\", values=\"count\")\ndf_tmp = df_tmp.assign(\n    count=df_tmp.sum(axis=1),\n    flows_min=df_tmp.min(axis=1),\n    flows_max=df_tmp.max(axis=1),\n    rho=(df_tmp.max(axis=1) / df_tmp.min(axis=1)).round(1),\n    classes=len(df[\"app\"].cat.categories),\n)\n\n# mean pkts per flow\nmean_pkts = df.groupby(\"partition\")[\"packets\"].mean().round(0)\nmean_pkts.name = \"mean_pkts\"\nflows_all = df.groupby(\"partition\")[\"partition\"].count()\nflows_all.name = \"flows_all\"\n\n# combining everything together\ndf_tmp = pd.concat((df_tmp, mean_pkts, flows_all), axis=1)\ndf_tmp = df_tmp[[\"classes\", \"flows_all\", \"flows_min\", \"flows_max\", \"rho\", \"mean_pkts\"]]\ndisplay(df_tmp)\n\nstats_ucdavis19 = df_tmp\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts partition pretraining 5 6439 592 1915 3.2 6653.0 retraining-human-triggered 5 83 15 20 1.3 7666.0 retraining-script-triggered 5 150 30 30 1.0 7131.0"},{"location":"papers/imc23/notebooks/table2_datasets_properties/#mirage19","title":"mirage19","text":"<p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</code></pre> <pre><code># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE19, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=10\"],\n)\n</code></pre> <pre><code>df_tmp = pd.concat((df_unfiltered, df_minpkts10), axis=0)\ndisplay(df_tmp)\nstats_mirage19 = df_tmp\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 21 122007 1986 11737 5.9 23.0 min_pkts=10 20 64172 1013 7505 7.4 17.0"},{"location":"papers/imc23/notebooks/table2_datasets_properties/#mirage22","title":"mirage22","text":"<p>The unfiltered version of the dataset has an extra class, which corresponds to <code>\"background\"</code> traffic</p> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</code></pre> <pre><code># min_pkts = 10\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=10\"],\n)\n</code></pre> <pre><code># min_pkts = 1000\ndf = tcb.load_parquet(tcb.DATASETS.MIRAGE22, min_pkts=1000)\n\nser = df[\"app\"].value_counts()\ndf_minpkts1000 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"min_pkts=1000\"],\n)\n</code></pre> <pre><code>df_tmp = pd.concat((df_unfiltered, df_minpkts10, df_minpkts1000), axis=0)\ndisplay(df_tmp)\nstats_mirage22 = df_tmp\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 10 59071 2252 18882 8.4 3068.0 min_pkts=10 9 26773 970 4437 4.6 6598.0 min_pkts=1000 9 4569 190 2220 11.7 38321.0"},{"location":"papers/imc23/notebooks/table2_datasets_properties/#utmobilenet21","title":"utmobilenet21","text":"<pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21)\n\nser = df[\"app\"].value_counts()\ndf_unfiltered = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"unfiltered\"],\n)\n</code></pre> <pre><code># unfiltered\ndf = tcb.load_parquet(tcb.DATASETS.UTMOBILENET21, min_pkts=10)\n\nser = df[\"app\"].value_counts()\ndf_minpkts10 = pd.DataFrame(\n    [\n        dict(\n            classes=len(ser),\n            flows_all=ser.sum(),\n            flows_min=ser.min(),\n            flows_max=ser.max(),\n            rho=(ser.max() / ser.min()).round(1),\n            mean_pkts=df[\"packets\"].mean().round(0),\n        )\n    ],\n    index=[\"minpkts=10\"],\n)\n</code></pre> <pre><code>df_tmp = pd.concat((df_unfiltered, df_minpkts10), axis=0)\ndisplay(df_tmp)\nstats_utmobilenet21 = df_tmp\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts unfiltered 17 34378 159 5591 35.2 664.0 minpkts=10 14 9460 130 2496 19.2 2366.0"},{"location":"papers/imc23/notebooks/table2_datasets_properties/#alltogether","title":"alltogether","text":"<pre><code>df_tmp = pd.concat(\n    (\n        (stats_ucdavis19.assign(dataset=\"ucdavis-icdm19\")).set_index(\n            [\"dataset\", stats_ucdavis19.index]\n        ),\n        (stats_mirage19.assign(dataset=\"mirage19\")).set_index(\n            [\"dataset\", stats_mirage19.index]\n        ),\n        (stats_mirage22.assign(dataset=\"mirage22\")).set_index(\n            [\"dataset\", stats_mirage22.index]\n        ),\n        (stats_utmobilenet21.assign(dataset=\"utmobilenet21\")).set_index(\n            [\"dataset\", stats_utmobilenet21.index]\n        ),\n    )\n).rename(\n    {\n        \"retraining-human-triggered\": \"human\",\n        \"retraining-script-triggered\": \"script\",\n    },\n    axis=0,\n)\ndisplay(df_tmp)\ndf_tmp.to_csv(\"table2_datasets_properties.csv\")\n</code></pre> classes flows_all flows_min flows_max rho mean_pkts dataset ucdavis-icdm19 pretraining 5 6439 592 1915 3.2 6653.0 human 5 83 15 20 1.3 7666.0 script 5 150 30 30 1.0 7131.0 mirage19 unfiltered 21 122007 1986 11737 5.9 23.0 min_pkts=10 20 64172 1013 7505 7.4 17.0 mirage22 unfiltered 10 59071 2252 18882 8.4 3068.0 min_pkts=10 9 26773 970 4437 4.6 6598.0 min_pkts=1000 9 4569 190 2220 11.7 38321.0 utmobilenet21 unfiltered 17 34378 159 5591 35.2 664.0 minpkts=10 14 9460 130 2496 19.2 2366.0"},{"location":"papers/imc23/notebooks/table3_xgboost_baseline/","title":"Table3 xgboost baseline","text":""},{"location":"papers/imc23/notebooks/table3_xgboost_baseline/#table-3-g0-baseline-ml-performance-without-augmentation-in-a-supervised-setting","title":"Table 3: (G0) Baseline ML performance without augmentation in a supervised setting.","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>df = pd.read_csv(\n    \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-flowpic/campaign_summary/noaugmentation-flowpic/summary_flowpic_dim_32.csv\",\n    header=[0, 1],\n    index_col=[0, 1],\n)\n</code></pre> <pre><code># reformatting\ndf_tmp = df[\"acc\"][[\"mean\", \"ci95\"]].round(2)\ndf_tmp.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\n</code></pre> mean ci95 test-script 96.80 0.37 test-human 73.65 2.14 <pre><code>df = pd.read_csv(\n    \"./campaigns/ucdavis-icdm19/xgboost/noaugmentation-timeseries/campaign_summary/noaugmentation-timeseries/summary_max_n_pkts_10.csv\",\n    header=[0, 1],\n    index_col=[0, 1],\n)\n</code></pre> <pre><code># reformatting\ndf_tmp = df[\"acc\"][[\"mean\", \"ci95\"]].round(2)\ndf_tmp.loc[[\"test-script\", \"test-human\"]].droplevel(1, axis=0).astype(float).round(2)\n</code></pre> mean ci95 test-script 94.53 0.56 test-human 66.91 1.40"},{"location":"papers/imc23/notebooks/table4_ucdavis-icdm19_comparing_data_augmentations_functions/","title":"Table4 ucdavis icdm19 comparing data augmentations functions","text":""},{"location":"papers/imc23/notebooks/table4_ucdavis-icdm19_comparing_data_augmentations_functions/#table-4-comparing-data-augmentation-functions-applied-in-supervised-training","title":"Table 4: Comparing data augmentation functions applied in supervised training.","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport statsmodels.stats.api as sms\n</code></pre> <pre><code>import itertools\nimport pathlib\nimport tempfile\n</code></pre> <pre><code>def compute_ci95(ser):\n    low, high = sms.DescrStatsW(ser.values).tconfint_mean(alpha=0.05)\n    mean = ser.mean()\n    ci = high - mean\n    return ci\n</code></pre> <pre><code>folder_campaign_summary = pathlib.Path(\n    \"campaigns/ucdavis-icdm19/augmentation-at-loading-with-dropout/campaign_summary/augment-at-loading-with-dropout/\"\n)\n</code></pre> <pre><code># load results\ndf = pd.concat(\n    [\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_32.parquet\"),\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_64.parquet\"),\n        pd.read_parquet(folder_campaign_summary / \"runsinfo_flowpic_dim_1500.parquet\"),\n    ]\n)\n</code></pre> <pre><code>df_agg_dict = dict()\nfor flowpic_dim in (32, 64, 1500):\n    df_tmp = df[df[\"flowpic_dim\"] == flowpic_dim]\n    df_agg = df_tmp.groupby([\"test_split_name\", \"aug_name\"]).agg(\n        {\"acc\": [\"count\", \"mean\", \"std\", compute_ci95]}\n    )\n    df_agg = df_agg.droplevel(0, axis=1).rename({\"compute_ci95\": \"ci95\"}, axis=1)\n    fname = folder_campaign_summary / f\"summary_flowpic_dim_{flowpic_dim}.csv\"\n    df_agg_dict[flowpic_dim] = df_agg\n</code></pre> <pre><code># loading imc22-paper results\n# (there are oviously copied)\n\nIMC22_TABLE_TEST_SCRIPT = \"\"\"\naug_name,32,64,1500\nNo augmentation,98.67,99.1,96.22\nRotate,98.6,98.87,94.89\nHorizontal flip,98.93,99.27,97.33\nColor jitter,96.73,96.4,94.0\nPacket loss,98.73,99.6,96.22\nTime shift,99.13,99.53,97.56\nChange rtt,99.4,100.0,98.44\n\"\"\"\n\nIMC22_TABLE_TEST_HUMAN = \"\"\"\naug_name,32,64,1500\nNo augmentation,92.4,85.6,73.3\nRotate,93.73,87.07,77.3\nHorizontal flip,94.67,79.33,87.9\nColor jitter,82.93,74.93,68.0\nPacket loss,90.93,85.6,84.0\nTime shift,92.8,87.33,77.3\nChange rtt,96.4,88.6,90.7\n\"\"\"\n\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\n    f_tmp.write(IMC22_TABLE_TEST_SCRIPT)\n    f_tmp.seek(0)\n    df_imc22_table_test_script = pd.read_csv(f_tmp.name)\n    df_imc22_table_test_script = df_imc22_table_test_script.set_index(\"aug_name\")\n    df_imc22_table_test_script.columns = pd.MultiIndex.from_product(\n        [[\"imc22-paper\"], df_imc22_table_test_script.columns, [\"mean\"]]\n    )\n\nwith tempfile.NamedTemporaryFile(\"w\") as f_tmp:\n    f_tmp.write(IMC22_TABLE_TEST_HUMAN)\n    f_tmp.seek(0)\n    df_imc22_table_test_human = pd.read_csv(f_tmp.name)\n    df_imc22_table_test_human = df_imc22_table_test_human.set_index(\"aug_name\")\n    df_imc22_table_test_human.columns = pd.MultiIndex.from_product(\n        [[\"imc22-paper\"], df_imc22_table_test_human.columns, [\"mean\"]]\n    )\n</code></pre> <pre><code>RENAMING = {\n    \"test-human\": \"human\",\n    \"test-script\": \"script\",\n    \"test-train-val-leftover\": \"leftover\",\n    \"noaug\": \"No augmentation\",\n    \"changertt\": \"Change rtt\",\n    \"colorjitter\": \"Color jitter\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"packetloss\": \"Packet loss\",\n    \"rotate\": \"Rotate\",\n    \"timeshift\": \"Time shift\",\n}\n\nAUG_NAME_ORDER = [\n    \"No augmentation\",\n    \"Rotate\",\n    \"Horizontal flip\",\n    \"Color jitter\",\n    \"Packet loss\",\n    \"Time shift\",\n    \"Change rtt\",\n]\n\npartial_dfs = {\n    \"human\": dict(),\n    \"script\": dict(),\n    \"leftover\": dict(),\n}\nfor flowpic_dim in (32, 64, 1500):\n    df_tmp = df_agg_dict[flowpic_dim][[\"mean\", \"ci95\"]].round(2).reset_index()\n    df_tmp = df_tmp.assign(\n        test_split_name=df_tmp[\"test_split_name\"].replace(RENAMING),\n        aug_name=df_tmp[\"aug_name\"].replace(RENAMING),\n    )\n    df_tmp = df_tmp.set_index(\"test_split_name\", drop=True)\n    for split_name in (\"script\", \"human\", \"leftover\"):\n        df_partial = df_tmp.loc[split_name].copy()\n        df_partial = df_partial.set_index(\"aug_name\", drop=True)\n        df_partial = df_partial.loc[AUG_NAME_ORDER]\n        partial_dfs[split_name][flowpic_dim] = df_partial\n</code></pre> <pre><code>df_ours_script = pd.concat(partial_dfs[\"script\"], axis=1)\ndf_ours_script.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_script.columns.levels]\n)\n\ndf_ours_human = pd.concat(partial_dfs[\"human\"], axis=1)\ndf_ours_human.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_human.columns.levels]\n)\n\ndf_ours_leftover = pd.concat(partial_dfs[\"leftover\"], axis=1)\ndf_ours_leftover.columns = pd.MultiIndex.from_product(\n    [[\"ours\"], *df_ours_leftover.columns.levels]\n)\n</code></pre> <pre><code>print(\"=== test on script ===\")\ndf_tmp = pd.concat((df_imc22_table_test_script, df_ours_script), axis=1)\n\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n    (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n    (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n    (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndisplay(df_tmp.fillna(\"\"))\ndf_tmp.fillna(\"\").to_csv(\n    \"table3_ucdavis-icdm19_comparing_data_augmentations_functions_test_on_script.csv\"\n)\n</code></pre> <pre><code>=== test on script ===\n</code></pre> imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 98.67 99.1 96.22 95.64 0.37 95.87 0.29 94.93 0.72 Rotate 98.6 98.87 94.89 96.31 0.44 96.93 0.46 95.69 0.39 Horizontal flip 98.93 99.27 97.33 95.47 0.45 96.00 0.59 94.89 0.79 Color jitter 96.73 96.4 94.0 97.56 0.55 97.16 0.62 94.93 0.68 Packet loss 98.73 99.6 96.22 96.89 0.52 96.84 0.63 95.96 0.51 Time shift 99.13 99.53 97.56 96.71 0.6 97.16 0.49 96.89 0.27 Change rtt 99.4 100.0 98.44 97.29 0.35 97.02 0.46 96.93 0.31 mean_diff -2.05 -2.26 -0.63 <pre><code>print(\"=== test on human ===\")\ndf_tmp = pd.concat((df_imc22_table_test_human, df_ours_human), axis=1)\n\ndf_tmp.loc[\"mean_diff\", :] = np.nan\ndf_tmp.loc[\"mean_diff\", (\"ours\", 32, \"mean\")] = (\n    (df_tmp[(\"ours\", 32, \"mean\")] - df_tmp[(\"imc22-paper\", \"32\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 64, \"mean\")] = (\n    (df_tmp[(\"ours\", 64, \"mean\")] - df_tmp[(\"imc22-paper\", \"64\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndf_tmp.loc[\"mean_diff\", (\"ours\", 1500, \"mean\")] = (\n    (df_tmp[(\"ours\", 1500, \"mean\")] - df_tmp[(\"imc22-paper\", \"1500\", \"mean\")])\n    .mean()\n    .round(2)\n)\ndisplay(df_tmp.fillna(\"\"))\ndf_tmp.fillna(\"\").to_csv(\n    \"table3_ucdavis-icdm19_comparing_data_augmentations_functions_test_on_human.csv\"\n)\n</code></pre> <pre><code>=== test on human ===\n</code></pre> imc22-paper ours 32 64 1500 32 64 1500 mean mean mean mean ci95 mean ci95 mean ci95 aug_name No augmentation 92.4 85.6 73.3 68.84 1.45 69.08 1.35 69.32 1.63 Rotate 93.73 87.07 77.3 71.65 1.98 71.08 1.51 68.19 0.97 Horizontal flip 94.67 79.33 87.9 69.40 1.63 70.52 2.03 73.90 1.06 Color jitter 82.93 74.93 68.0 68.43 2.82 70.20 1.99 69.08 1.72 Packet loss 90.93 85.6 84.0 70.68 1.35 71.33 1.45 71.08 1.13 Time shift 92.8 87.33 77.3 70.36 1.63 71.89 1.59 71.08 1.33 Change rtt 96.4 88.6 90.7 70.76 1.99 71.49 1.59 71.97 1.08 mean_diff -21.96 -13.27 -9.13 <pre><code>print(\"=== test on leftover ===\")\ndisplay(df_ours_leftover)\ndf_ours_leftover.to_csv(\n    \"table3_ucdavis-icdm19_comparing_data_augmentations_functions_test_on_leftover.csv\"\n)\n</code></pre> <pre><code>=== test on leftover ===\n</code></pre> ours 32 64 1500 mean ci95 mean ci95 mean ci95 aug_name No augmentation 95.78 0.29 96.09 0.38 95.79 0.51 Rotate 96.74 0.35 97.00 0.38 95.79 0.31 Horizontal flip 95.68 0.40 96.32 0.59 95.97 0.80 Color jitter 96.93 0.56 96.46 0.46 95.47 0.49 Packet loss 96.99 0.39 97.25 0.39 96.84 0.49 Time shift 97.02 0.50 97.51 0.46 97.67 0.29 Change rtt 98.38 0.18 97.97 0.39 98.19 0.22"},{"location":"papers/imc23/notebooks/table5_simclr_dropout_and_projectionlayer/","title":"Table5 simclr dropout and projectionlayer","text":""},{"location":"papers/imc23/notebooks/table5_simclr_dropout_and_projectionlayer/#table-5-impact-of-dropout-and-simclr-projection-layer-dimension-on-fine-tuning","title":"Table 5: Impact of dropout and SimCLR projection layer dimension on fine-tuning.","text":"<pre><code>import itertools\n\nimport pandas as pd\n</code></pre> <pre><code>df = pd.read_csv(\n    \"campaigns/ucdavis-icdm19/simclr-dropout-and-projection/campaign_summary/simclr-dropout-and-projection/summary_flowpic_dim_32.csv\",\n    header=[0, 1],\n    index_col=[0, 1, 2],\n)\n\ndf = df[\"acc\"][[\"mean\", \"ci95\"]]\ndf = df.T\ndf.columns.set_names(\"test_split_name\", level=0, inplace=True)\ndf.columns.set_names(\"projection_layer_dim\", level=1, inplace=True)\ndf.columns.set_names(\"with_dropout\", level=2, inplace=True)\ndf = df.reorder_levels(\n    [\"test_split_name\", \"with_dropout\", \"projection_layer_dim\"], axis=1\n)\n\ndf = df[list(itertools.product([\"test-script\", \"test-human\"], [True, False], [30, 84]))]\ndf = df.round(2)\n\ndf.to_csv(\"table5_simclr_dropout_and_projectionlayer.csv\")\ndf\n</code></pre> test_split_name test-script test-human with_dropout True False True False projection_layer_dim 30 84 30 84 30 84 30 84 mean 91.81 92.02 92.18 92.54 72.12 73.31 74.69 74.35 ci95 0.38 0.36 0.31 0.33 1.37 1.04 1.13 1.38"},{"location":"papers/imc23/notebooks/table6_simclr_other_augmentation_pairs/","title":"Table6 simclr other augmentation pairs","text":""},{"location":"papers/imc23/notebooks/table6_simclr_other_augmentation_pairs/#table-6-comparing-the-fine-tuning-performance-when-using-different-pairs-of-augmentation-for-pretraining","title":"Table 6: Comparing the fine-tuning performance when using different pairs of augmentation for pretraining.","text":"<pre><code>import itertools\n\nimport pandas as pd\n</code></pre> <pre><code>RENAME = {\n    \"colorjitter\": \"Color jitter\",\n    \"timeshift\": \"Time shift\",\n    \"changertt\": \"Change RTT\",\n    \"rotate\": \"Rotate\",\n    \"packetloss\": \"Packet loss\",\n}\n</code></pre> <pre><code>df = pd.read_csv(\n    \"./campaigns/ucdavis-icdm19/simclr-other-augmentation-pairs/campaign_summary/simclr-other-augmentation-pairs/summary_flowpic_dim_32.csv\",\n    header=[0, 1],\n    index_col=[0, 1],\n)\n\ndf = df[\"acc\"][[\"mean\", \"ci95\"]].round(2)\ndf = df.reset_index()\ndf = df.assign(\n    aug1=df[\"level_1\"].apply(eval).str[0],\n    aug2=df[\"level_1\"].apply(eval).str[1],\n)\ndf = df.drop(\"level_1\", axis=1)\ndf = df.rename({\"level_0\": \"test_split_name\"}, axis=1)\ndf = df.replace(RENAME)\ndf = df.pivot(index=\"test_split_name\", columns=[\"aug1\", \"aug2\"])\ndf.columns.set_names([\"stat\", \"aug1\", \"aug2\"], inplace=True)\ndf = df.reorder_levels([\"aug1\", \"aug2\", \"stat\"], axis=1)\ndf.columns.set_names([\"\", \"\", \"\"], inplace=True)\ndf.index.name = None\n\ndf = df[\n    list(itertools.product([\"Change RTT\"], [\"Time shift\"], [\"mean\", \"ci95\"]))\n    + list(\n        itertools.product([\"Packet loss\"], [\"Color jitter\", \"Rotate\"], [\"mean\", \"ci95\"])\n    )\n    + list(\n        itertools.product([\"Change RTT\"], [\"Color jitter\", \"Rotate\"], [\"mean\", \"ci95\"])\n    )\n    + list(itertools.product([\"Color jitter\"], [\"Rotate\"], [\"mean\", \"ci95\"]))\n]\ndf = df.loc[[\"test-script\", \"test-human\"]]\n\ndf.to_csv(\"table5_simclr_other_augmentation_pairs.csv\")\ndf\n</code></pre> Change RTT Packet loss Change RTT Color jitter Time shift Color jitter Rotate Color jitter Rotate Rotate mean ci95 mean ci95 mean ci95 mean ci95 mean ci95 mean ci95 test-script 92.18 0.31 90.17 0.41 91.94 0.3 91.72 0.36 92.38 0.32 91.79 0.34 test-human 74.69 1.13 73.67 1.24 71.22 1.2 75.56 1.23 74.33 1.26 71.64 1.23"},{"location":"papers/imc23/notebooks/table7_larger_trainset/","title":"Table7 larger trainset","text":""},{"location":"papers/imc23/notebooks/table7_larger_trainset/#table-7-accuracy-on-32x32-flowpic-when-enlarging-the-training-set-wo-dropout","title":"Table 7: Accuracy on 32x32 flowpic when enlarging the training set (w/o Dropout)","text":"<pre><code>import itertools\nimport pathlib\n\nimport pandas as pd\n</code></pre> <pre><code>RENAME = {\n    \"noaug\": \"No augmentation\",\n    \"rotate\": \"Rotate\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"colorjitter\": \"Color jitter\",\n    \"packetloss\": \"Packet loss\",\n    \"timeshift\": \"Time shift\",\n    \"changertt\": \"Change RTT\",\n}\n</code></pre> <pre><code>folder = pathlib.Path(\"campaigns/ucdavis-icdm19/larger-trainset/\")\n</code></pre> <pre><code>df_sup = pd.read_csv(\n    folder\n    / \"augmentation-at-loading/campaign_summary/augment-at-loading-larger-trainset/summary_flowpic_dim_32.csv\",\n    header=[0, 1],\n    index_col=[0, 1],\n)\ndf_sup = df_sup[\"acc\"][[\"mean\", \"ci95\"]]\ndf_sup.index.set_names([\"test_split_name\", \"aug_name\"], inplace=True)\ndf_sup = df_sup.reset_index().pivot(\n    columns=[\"test_split_name\"], index=\"aug_name\", values=[\"mean\", \"ci95\"]\n)\ndf_sup.columns.set_names([\"stat\", \"test_split_name\"], inplace=True)\ndf_sup = df_sup.reorder_levels([\"test_split_name\", \"stat\"], axis=1)\ndf_sup = df_sup[\n    list(itertools.product([\"test-script\", \"test-human\"], [\"mean\", \"ci95\"]))\n]\ndf_sup = df_sup.rename(RENAME, axis=0).rename(RENAME, axis=1)\ndf_sup.index.set_names([\"\"], inplace=True)\ndf_sup.columns.set_names([\"\", \"\"], inplace=True)\ndf_sup = df_sup.round(2)\ndf_sup = df_sup.loc[list(RENAME.values())]\ndf_sup.to_csv(\"table7_larger-trainset_augment-at-loading.csv\")\ndf_sup\n</code></pre> test-script test-human mean ci95 mean ci95 No augmentation 98.37 0.19 72.95 0.96 Rotate 98.47 0.25 73.73 1.09 Horizontal flip 98.20 0.15 74.58 1.16 Color jitter 98.63 0.21 72.47 1.02 Packet loss 98.63 0.19 73.43 1.25 Time shift 98.60 0.22 73.25 1.17 Change RTT 98.33 0.16 72.47 1.04 <pre><code>df_cl = pd.read_csv(\n    folder\n    / \"simclr/campaign_summary/simclr-larger-trainset/summary_flowpic_dim_32.csv\",\n    header=[0, 1],\n    index_col=[0, 1],\n)\ndf_cl = df_cl[\"acc\"][[\"mean\", \"ci95\"]]\ndf_cl = df_cl.droplevel(1, axis=0).round(2)\ndf_cl = df_cl.loc[[\"test-script\", \"test-human\"]]\ndf_cl.to_csv(\"table7_larger-trainset_simclr.csv\")\ndf_cl\n</code></pre> mean ci95 test-script 93.90 0.74 test-human 80.45 2.37"},{"location":"papers/imc23/notebooks/table8_augmentation-at-loading_on_other_datasets/","title":"Table8 augmentation at loading on other datasets","text":""},{"location":"papers/imc23/notebooks/table8_augmentation-at-loading_on_other_datasets/#table-8-g3-data-augmentation-in-supervised-setting-on-other-datasets","title":"Table 8 (G3) Data augmentation in supervised setting on other datasets.","text":"<pre><code>import pathlib\n\nimport pandas as pd\n\nAUGMENTATIONS_ORDER = [\n    \"noaug\",\n    \"rotate\",\n    \"horizontalflip\",\n    \"colorjitter\",\n    \"packetloss\",\n    \"timeshift\",\n    \"changertt\",\n]\n\nRENAME = {\n    \"noaug\": \"No augmentation\",\n    \"changertt\": \"Change RTT\",\n    \"horizontalflip\": \"Horizontal flip\",\n    \"colorjitter\": \"Color jitter\",\n    \"packetloss\": \"Packet loss\",\n    \"rotate\": \"Rotate\",\n    \"timeshift\": \"Time shift\",\n}\n</code></pre> <pre><code>def load_summary_report(fname, level0):\n    df = pd.read_csv(fname, header=[0, 1], index_col=[0, 1]).droplevel(0, axis=0)\n    df = df[\"f1\"]\n    df = df[[\"mean\", \"ci95\"]]\n    df = df.loc[AUGMENTATIONS_ORDER].rename(RENAME)\n    df.columns = pd.MultiIndex.from_arrays([[level0, level0], df.columns])\n\n    return df\n</code></pre> <pre><code>df = pd.concat(\n    (\n        load_summary_report(\n            \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/summary_flowpic_dim_32.csv\",\n            \"mirage22 - minpkts10\",\n        ),\n        load_summary_report(\n            \"campaigns/mirage22/augmentation-at-loading-no-dropout/minpkts1000/campaign_summary/augment-at-loading/summary_flowpic_dim_32.csv\",\n            \"mirage22 - minpkts1000\",\n        ),\n        load_summary_report(\n            \"campaigns/utmobilenet21/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/summary_flowpic_dim_32.csv\",\n            \"utmobilenet21 - minpkts10\",\n        ),\n        load_summary_report(\n            \"campaigns/mirage19/augmentation-at-loading-no-dropout/minpkts10/campaign_summary/augment-at-loading/summary_flowpic_dim_32.csv\",\n            \"mirage19 - minpkts10\",\n        ),\n    ),\n    axis=1,\n)\ndf = (df * 100).round(2)\ndisplay(df)\ndf.to_csv(\"table8_augmentation-at-loading_on_other_datasets.csv\")\n</code></pre> mirage22 - minpkts10 mirage22 - minpkts1000 utmobilenet21 - minpkts10 mirage19 - minpkts10 mean ci95 mean ci95 mean ci95 mean ci95 No augmentation 90.97 1.15 83.35 3.13 79.82 1.53 69.91 1.57 Rotate 88.25 1.20 87.32 2.24 79.45 1.28 60.35 1.17 Horizontal flip 91.90 0.84 83.82 2.26 80.03 1.33 69.78 1.28 Color jitter 89.77 1.16 81.40 3.62 78.68 2.14 67.00 1.11 Packet loss 92.34 1.10 87.19 2.52 72.07 1.73 67.55 1.46 Time shift 92.80 1.21 86.73 3.88 81.91 2.12 70.33 1.26 Change RTT 93.75 0.83 91.48 2.12 81.32 1.54 74.28 1.22"},{"location":"papers/imc23/notebooks/table9_icdm_finetuning_per_class_metrics_on_human/","title":"Table9 icdm finetuning per class metrics on human","text":""},{"location":"papers/imc23/notebooks/table9_icdm_finetuning_per_class_metrics_on_human/#table-9-macro-average-accuracy-with-different-retraining-dataset-and-different-sampling-methods","title":"Table 9: Macro-average Accuracy with different retraining dataset and different sampling methods","text":"<pre><code>import pathlib\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.stats.api as sms\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n</code></pre> <pre><code>def compute_confidence_intervals(array, alpha=0.05):\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</code></pre> <pre><code>path = pathlib.Path(\"./campaigns/ucdavis-icdm19-git-repo-forked/artifacts/\")\n\nclass_repss = list(path.glob(\"*10/\"))\n</code></pre> <pre><code>data = dict()\n\nfor path in class_repss:\n    if \"script\" in str(path):\n        class_reps = list(path.glob(\"*class_rep.csv\"))\n        accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\n\n        augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\n        data[augmentation_name] = (\n            np.mean(accs) * 100,\n            compute_confidence_intervals(accs),\n        )\n\ndf_script = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_script.columns = pd.MultiIndex.from_arrays([[\"script\", \"script\"], df_script.columns])\n# df_script\n</code></pre> <pre><code>data = dict()\nfor path in class_repss:\n    if \"human\" in str(path):\n        class_reps = list(path.glob(\"*class_rep.csv\"))\n        accs = [pd.read_csv(file).iloc[6].values[2] for file in class_reps]\n\n        augmentation_name = path.name.split(\"_\")[0].replace(\"Sampling\", \"\")\n        data[augmentation_name] = (\n            np.mean(accs) * 100,\n            compute_confidence_intervals(accs),\n        )\n\ndf_human = pd.DataFrame(data, index=[\"mean\", \"ci95\"]).T.round(2)\ndf_human.columns = pd.MultiIndex.from_arrays([[\"human\", \"human\"], df_human.columns])\n</code></pre> <pre><code>df_tmp = pd.concat((df_script, df_human), axis=1).T\ndisplay(df_tmp)\ndf_tmp.to_csv(\"icdm_finetuning_per_class_metrics_on_human.csv\")\n</code></pre> FixedStep Random Incremental script mean 87.11 94.63 96.22 ci95 0.09 0.02 0.01 human mean 82.60 87.29 92.56 ci95 0.03 0.04 0.03"},{"location":"tcbench/","title":"The tcbench framework","text":"<p>tcbench is a ML/DL framework specific for Traffic Classification (TC) created as research project by the AI4NET team of the Huawei Technologies research center in Paris, France.</p> <p>What is Traffic Classification?</p> <p>Nodes within a computer network operate by exchanging  information, namely packets, which is regulated according to standardized protocols (e.g., HTTP for the web). So to understand  the network health it is required to constantly monitor this information flow and react accordingly. For instance, one might want to prioritize certain traffic (e.g., video meeting) or block it (e.g., social media in working environment).</p> <p>Traffic classification is the the act of labeling an exchange of packets  based on the Internet application which generated it.</p> <p>The academic literature is ripe with methods and proposals for TC. Yet, it is scarce of code artifacts and public datasets  do not offer common conventions of use.</p> <p>We designed tcbench with the following goals in mind:</p> Goal State of the art tcbench  Data curation There are a few public datasets for TC, yet no common format/schema, cleaning process, or standard train/val/test folds. An (opinionated) curation of datasets to create easy to use parquet files with associated train/val/test fold.  Code TC literature has no reference code base for ML/DL modeling tcbench is  open source with an easy to use CLI based on  click  Model tracking Most of ML framework requires integration with cloud environments and subscription services tcbench uses aimstack to save on local servers metrics during training which can be later explored via its web UI or aggregated in report summaries using tcbench"},{"location":"tcbench/#features-and-roadmap","title":"Features and roadmap","text":"<p>tcbench is still under development, but (as suggested by its name) ultimately aims to be a reference framework for benchmarking multiple ML/DL solutions  related to TC.</p> <p>At the current stage, tcbench offers</p> <ul> <li> <p>Integration with 4 datasets, namely <code>ucdavis-icdm19</code>, <code>mirage19</code>, <code>mirage22</code> and <code>utmobilenet21</code>. You can use these datasets and their curated version independently from tcbench. Check out the dataset install process and dataset loading tutorial.</p> </li> <li> <p>Good support for flowpic input representation and minimal support for 1d time series (based on network packets properties) input representation.</p> </li> <li> <p>Data augmentation functionality for flowpic input representation.</p> </li> <li> <p>Modeling via XGBoost, vanilla DL supervision and contrastive learning (via SimCLR or SupCon).</p> </li> </ul> <p>Most of the above functionalities described relate to our  IMC23 paper.</p> <p>More exiting features including more datasets and algorithms will come in the next months. </p> <p>Stay tuned !</p>"},{"location":"tcbench/cli_intro/","title":"CLI Introduction","text":"<p>tcbench can be used for as SDK and from the command line.</p> <p>When installing tcbench you install also a <code>tcbench</code> command line script created via  click  and  rich.</p> <p>For instance <pre><code>tcbench --help\n</code></pre></p> <p>Output</p> <pre><code> Usage: tcbench [OPTIONS] COMMAND [ARGS]...\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version      Show tcbench version and exit.                                            \u2502\n\u2502 --help         Show this message and exit.                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 aimrepo         Investigate AIM repository content.                                      \u2502\n\u2502 campaign        Triggers a modeling campaign.                                            \u2502\n\u2502 datasets        Install/Remove traffic classification datasets.                          \u2502\n\u2502 run             Triggers a modeling run.                                                 \u2502\n\u2502 tree            show the command tree of your CLI.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The commands are organized in a nested structure which you can visualize using</p> <pre><code>tcbench tree\n</code></pre> <p>Output</p> <pre><code>main\n\u251c\u2500\u2500 aimrepo - Investigate AIM repository content.\n\u2502   \u251c\u2500\u2500 ls - List a subset of properties of each run.\n\u2502   \u251c\u2500\u2500 merge - Coalesce different AIM repos into a single new repo.\n\u2502   \u251c\u2500\u2500 properties - List properties across all runs.\n\u2502   \u2514\u2500\u2500 report - Summarize runs performance metrics.\n\u251c\u2500\u2500 campaign - Triggers a modeling campaign.\n\u2502   \u251c\u2500\u2500 augment-at-loading - Modeling by applying data augmentation when loading the training set.\n\u2502   \u2514\u2500\u2500 contralearn-and-finetune - Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\n\u251c\u2500\u2500 datasets - Install/Remove traffic classification datasets.\n\u2502   \u251c\u2500\u2500 delete - Delete a dataset.\n\u2502   \u251c\u2500\u2500 import - Import datasets.\n\u2502   \u251c\u2500\u2500 info - Show the meta-data related to supported datasets.\n\u2502   \u251c\u2500\u2500 install - Install a dataset.\n\u2502   \u251c\u2500\u2500 lsparquet - Tree view of the datasets parquet files.\n\u2502   \u251c\u2500\u2500 samples-count - Show report on number of samples per class.\n\u2502   \u2514\u2500\u2500 schema - Show datasets schemas\n\u251c\u2500\u2500 run - Triggers a modeling run.\n\u2502   \u251c\u2500\u2500 augment-at-loading - Modeling by applying data augmentation when loading the training set.\n\u2502   \u2514\u2500\u2500 contralearn-and-finetune - Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\n\u2514\u2500\u2500 tree - show the command tree of your CLI\n</code></pre>"},{"location":"tcbench/install/","title":"Install","text":"<p>First prepare a python virtual environment, for example via  conda <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> <p>tcbench is availabe on pypi so you install it via pip <pre><code>python -m pip install tcbench\n</code></pre></p> <p>All dependecies are automatically pulled.</p> <p>Verify the installation was successful by running <pre><code>tcbench --version\n</code></pre></p> <p>Output</p> <pre><code>version: 0.0.21\n</code></pre>"},{"location":"tcbench/install/#developer","title":"Developer","text":"<p>For developing your own projects or contributing to tcbench fork/clone the official repository and install the developer version.</p> <pre><code>python -m pip install .[dev]\n</code></pre> <p>The only difference with respect to the base version is the installation of extra dependencies.</p>"},{"location":"tcbench/internals/","title":"Interal structure","text":"<p><code>tcbench</code> is structured into three collections of modules:</p> <ul> <li> <p><code>cli</code> contains the logic for handling the command line.</p> </li> <li> <p><code>libtcdatasets</code> contains the logic for handling datasets curation.</p> </li> <li> <p><code>modeling</code> contains the logic for handling ML/DL models.</p> </li> </ul>"},{"location":"tcbench/internals/#cli","title":"<code>cli</code>","text":"<p>The entry point of <code>tcbench</code> is the script <code>tcbench/cli/main.py</code>: This is the root of the click prompt.</p> <p>The different sub-commands are organized in separate modules</p> <ul> <li><code>cli.command_aimrepo</code> for the <code>aimrepo</code> sub-command.</li> <li><code>cli.command_campaign</code> for the <code>campaign</code> sub-command.</li> <li><code>cli.command_datasets</code> for the <code>datasets</code> sub-command.</li> <li><code>cli.command_singlerun</code> for the <code>run</code> sub-command.</li> </ul> <p>while <code>cli.clickutils</code> and <code>cli.richutils</code> collects some utility functions used for formatting the CLI and its output.</p>"},{"location":"tcbench/internals/#libtcdatasets","title":"<code>libtcdatasets</code>","text":"<p>A module <code>datasets_utils</code> contains general utility function used across the other modules in this group.</p> <p>The remaining are pairs of modules each associated to  a different datasets.</p> <ul> <li> <p><code>XYZ_to_parquet</code> modules convert the original raw data into the curated format </p> </li> <li> <p><code>XYZ_generate_splits</code> modules split the data into train/val/test splits.</p> </li> </ul> <p>For instance, for <code>ucdavis-icdm19</code> the two modules are <code>ucdavis_icdm19_csv_to_parquet</code> and <code>ucdavis_icdm19_generate_splits</code>. Please refer to datasets install for more details about this pre-processing steps.</p> <p>These module pairs are designed to be also usable from the command line. As a matter of fact, the curation triggered from the <code>tcbench</code> CLI is just a wrapper around the lower level command line parser.</p> <p>For instance <pre><code>cd code_artifacts_paper132\npython tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py --help\n</code></pre></p> <p>Output</p> <pre><code>usage: ucdavis_icdm19_csv_to_parquet.py [-h] --input-folder INPUT_FOLDER\n                                        [--output-folder OUTPUT_FOLDER]\n                                        [--num-workers NUM_WORKERS]\n\noptions:\n  -h, --help            show this help message and exit\n  --input-folder INPUT_FOLDER, -i INPUT_FOLDER\n                        Root folder of UCDavis dataset\n  --output-folder OUTPUT_FOLDER, -o OUTPUT_FOLDER\n                        Folder where to save output parquet files (default:\n                        datasets/ucdavis-icdm19)\n  --num-workers NUM_WORKERS, -w NUM_WORKERS\n                        Number of workers for parallel loading (default: 4)\n</code></pre> <p>Important</p> <p>We discourage the direct use of these lower level modules in favor of the global <code>tcbench</code> CLI which automatically handles configurations so to have a uniform installation across datasets.</p>"},{"location":"tcbench/internals/#modeling","title":"<code>modeling</code>","text":"<p>These modules in this group handle DL/ML model training.</p> <ul> <li><code>modeling.aimutils</code> collects utility functions related to AIM repositories.</li> <li><code>modeling.augmentation</code> collects classes and functions related to data augmentation.</li> <li><code>modeling.backbone</code> collects classes and functions for DL architectures.</li> <li><code>modeling.dataprep</code> collects classes and functions related to data loading and preparation.</li> <li><code>modeling.losses</code> collectsion functions for SimCLR losses.</li> <li><code>modeling.methods</code> collects classes handling training.</li> <li><code>modeling.utils</code> collects complementary utility functions.</li> </ul> <p>These modules are \"glued\" together into two sub-group utilities</p> <ul> <li><code>run_&lt;XYZ&gt;</code> are modules triggering runs</li> <li><code>run_campaign_&lt;XYZ&gt;</code> are modules tringgering campaigns</li> </ul> <p>Both these module types work also as script and can be invoked on the command line.</p> <p>For instance with the following we can trigger individual run to investigate the role of different augmentations. This is equivalent to use <code>tcbench run augment-at-loading</code>.</p> <pre><code>python tcbench/modeling/run_augmentations_at_loading.py --help\n</code></pre> <p>Output</p> <pre><code>usage: run_augmentations_at_loading.py [-h] [--artifacts-folder ARTIFACTS_FOLDER]\n                                       [--workers WORKERS] [--gpu-index GPU_INDEX]\n                                       [--aim-repo AIM_REPO]\n                                       [--aim-experiment-name AIM_EXPERIMENT_NAME]\n                                       [--final] [--flowpic-dim {32,64,1500}]\n                                       [--flowpic-block-duration FLOWPIC_BLOCK_DURATION]\n                                       [--dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}]\n                                       [--dataset-minpkts {-1,10,100,1000}]\n                                       [--split-index SPLIT_INDEX]\n                                       [--max-samples-per-class MAX_SAMPLES_PER_CLASS]\n                                       [--train-val-split-ratio TRAIN_VAL_SPLIT_RATIO]\n                                       [--aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}]\n                                       [--suppress-val-augmentation] [--seed SEED]\n                                       [--batch-size BATCH_SIZE]\n                                       [--patience-steps PATIENCE_STEPS]\n                                       [--learning-rate LEARNING_RATE] [--epochs EPOCHS]\n                                       [--suppress-test-train-val-leftover]\n                                       [--suppress-dropout]\n\noptions:\n  -h, --help            show this help message and exit\n  --artifacts-folder ARTIFACTS_FOLDER\n                        Artifact folder (default: aim-repo/artifacts)\n  --workers WORKERS     Number of parallel worker for loading the data (default: 20)\n  --gpu-index GPU_INDEX\n                        The GPU id to use (default: 0)\n  --aim-repo AIM_REPO   Local aim folder or URL of AIM remote server (default: aim-repo)\n  --aim-experiment-name AIM_EXPERIMENT_NAME\n                        The name of the experiment for AIM tracking (default: augmentation-\n                        at-loading)\n  --final\n  --flowpic-dim {32,64,1500}\n                        Flowpic dimension (default: 32)\n  --flowpic-block-duration FLOWPIC_BLOCK_DURATION\n                        Flowpic block duration (in seconds) (default: 15)\n  --dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}\n                        Dataset to use for modeling (default: ucdavis-icdm19)\n  --dataset-minpkts {-1,10,100,1000}\n                        When used in combination with --dataset can refine the dataset and\n                        split to use for modeling (default: -1)\n  --split-index SPLIT_INDEX\n                        Datasplit index (default: 0)\n  --max-samples-per-class MAX_SAMPLES_PER_CLASS\n                        Activated when --split-index is -1 to define how many samples to\n                        select for train+val (with a 80/20 split between train and val\n                        (default: -1)\n  --train-val-split-ratio TRAIN_VAL_SPLIT_RATIO\n                        Training train/val split (default: 0.8)\n  --aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}\n                        Augmentation policy (default: noaug)\n  --suppress-val-augmentation\n                        Do not augment validation set (default: False)\n  --seed SEED           Random seed (default: 12345)\n  --batch-size BATCH_SIZE\n                        Training batch size (default: 64)\n  --patience-steps PATIENCE_STEPS\n  --learning-rate LEARNING_RATE\n                        Traning learning rate (default: 0.001)\n  --epochs EPOCHS       Number of epochs for training (default: 50)\n  --suppress-test-train-val-leftover\n                        Skip test on leftover split (default: False)\n  --suppress-dropout    Mask dropout layers with Identity (default: False)\n</code></pre>"},{"location":"tcbench/overview/","title":"Overview","text":""},{"location":"tcbench/overview/#install-tcbench","title":"Install <code>tcbench</code>","text":"<p>First prepare a python virtual environment, for example via  conda <pre><code>conda create -n tcbench python=3.10 pip\nconda activate tcbench\n</code></pre></p> <p>Grab the latest <code>code_artifacts_paper132.tgz</code>  from  figshare and unpack it. It contains a folder <code>/code_artifacts_paper132</code> from which you can trigger the installation.</p> <pre><code>cd code_artifacts_paper132\npython -m pip install .\n</code></pre> <p>All dependecies are automatically installed.</p> <pre><code>tcbench --version\n</code></pre> <p>Output</p> <pre><code>version: 0.0.16\n</code></pre>"},{"location":"tcbench/overview/#tcbench-internals","title":"<code>tcbench</code> internals","text":"<p><code>tcbench</code> is composed by 3 collections of modules:</p> <ul> <li> <p><code>cli</code> contains modules for composing the command line interfaces using click and  rich.</p> </li> <li> <p><code>libtcdatasets</code> contains modules for  datasets curation.</p> </li> <li> <p><code>modeling</code> contains modules for DL/ML modeling.</p> </li> </ul>"},{"location":"tcbench/overview/#cli","title":"<code>cli</code>","text":"<p>The entry point of <code>tcbench</code> is the script <code>tcbench/cli/main.py</code>: This is the root of the click prompt.</p> <p>The different sub-commands are organized in separate modules</p> <ul> <li><code>cli.command_aimrepo</code> for the <code>aimrepo</code> sub-command.</li> <li><code>cli.command_campaign</code> for the <code>campaign</code> sub-command.</li> <li><code>cli.command_datasets</code> for the <code>datasets</code> sub-command.</li> <li><code>cli.command_singlerun</code> for the <code>run</code> sub-command.</li> </ul> <p>while <code>cli.clickutils</code> and <code>cli.richutils</code> collects some utility functions used for formatting the CLI and its output.</p>"},{"location":"tcbench/overview/#libtcdatasets","title":"<code>libtcdatasets</code>","text":"<p>A module <code>datasets_utils</code> contains general utility function used across the other modules in this group.</p> <p>The remaining are pairs of modules each associated to  a different datasets.</p> <ul> <li> <p><code>XYZ_to_parquet</code> modules convert the original raw data into the curated format </p> </li> <li> <p><code>XYZ_generate_splits</code> modules split the data into train/val/test splits.</p> </li> </ul> <p>For instance, for <code>ucdavis-icdm19</code> the two modules are <code>ucdavis_icdm19_csv_to_parquet</code> and <code>ucdavis_icdm19_generate_splits</code>. Please refer to datasets install for more details about this pre-processing steps.</p> <p>These module pairs are designed to be also usable from the command line. As a matter of fact, the curation triggered from the <code>tcbench</code> CLI is just a wrapper around the lower level command line parser.</p> <p>For instance <pre><code>cd code_artifacts_paper132\npython tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py --help\n</code></pre></p> <p>Output</p> <pre><code>usage: ucdavis_icdm19_csv_to_parquet.py [-h] --input-folder INPUT_FOLDER\n                                        [--output-folder OUTPUT_FOLDER]\n                                        [--num-workers NUM_WORKERS]\n\noptions:\n  -h, --help            show this help message and exit\n  --input-folder INPUT_FOLDER, -i INPUT_FOLDER\n                        Root folder of UCDavis dataset\n  --output-folder OUTPUT_FOLDER, -o OUTPUT_FOLDER\n                        Folder where to save output parquet files (default:\n                        datasets/ucdavis-icdm19)\n  --num-workers NUM_WORKERS, -w NUM_WORKERS\n                        Number of workers for parallel loading (default: 4)\n</code></pre> <p>Important</p> <p>We discourage the direct use of these lower level modules in favor of the global <code>tcbench</code> CLI which automatically handles configurations so to have a uniform installation across datasets.</p>"},{"location":"tcbench/overview/#modeling","title":"<code>modeling</code>","text":"<p>These modules in this group handle DL/ML model training.</p> <ul> <li><code>modeling.aimutils</code> collects utility functions related to AIM repositories.</li> <li><code>modeling.augmentation</code> collects classes and functions related to data augmentation.</li> <li><code>modeling.backbone</code> collects classes and functions for DL architectures.</li> <li><code>modeling.dataprep</code> collects classes and functions related to data loading and preparation.</li> <li><code>modeling.losses</code> collectsion functions for SimCLR losses.</li> <li><code>modeling.methods</code> collects classes handling training.</li> <li><code>modeling.utils</code> collects complementary utility functions.</li> </ul> <p>These modules are \"glued\" together into two sub-group utilities</p> <ul> <li><code>run_&lt;XYZ&gt;</code> are modules triggering runs</li> <li><code>run_campaign_&lt;XYZ&gt;</code> are modules tringgering campaigns</li> </ul> <p>Both these module types work also as script and can be invoked on the command line.</p> <p>For instance with the following we can trigger individual run to investigate the role of different augmentations. This is equivalent to use <code>tcbench run augment-at-loading</code>.</p> <pre><code>python tcbench/modeling/run_augmentations_at_loading.py --help\n</code></pre> <p>Output</p> <pre><code>usage: run_augmentations_at_loading.py [-h] [--artifacts-folder ARTIFACTS_FOLDER]\n                                       [--workers WORKERS] [--gpu-index GPU_INDEX]\n                                       [--aim-repo AIM_REPO]\n                                       [--aim-experiment-name AIM_EXPERIMENT_NAME]\n                                       [--final] [--flowpic-dim {32,64,1500}]\n                                       [--flowpic-block-duration FLOWPIC_BLOCK_DURATION]\n                                       [--dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}]\n                                       [--dataset-minpkts {-1,10,100,1000}]\n                                       [--split-index SPLIT_INDEX]\n                                       [--max-samples-per-class MAX_SAMPLES_PER_CLASS]\n                                       [--train-val-split-ratio TRAIN_VAL_SPLIT_RATIO]\n                                       [--aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}]\n                                       [--suppress-val-augmentation] [--seed SEED]\n                                       [--batch-size BATCH_SIZE]\n                                       [--patience-steps PATIENCE_STEPS]\n                                       [--learning-rate LEARNING_RATE] [--epochs EPOCHS]\n                                       [--suppress-test-train-val-leftover]\n                                       [--suppress-dropout]\n\noptions:\n  -h, --help            show this help message and exit\n  --artifacts-folder ARTIFACTS_FOLDER\n                        Artifact folder (default: aim-repo/artifacts)\n  --workers WORKERS     Number of parallel worker for loading the data (default: 20)\n  --gpu-index GPU_INDEX\n                        The GPU id to use (default: 0)\n  --aim-repo AIM_REPO   Local aim folder or URL of AIM remote server (default: aim-repo)\n  --aim-experiment-name AIM_EXPERIMENT_NAME\n                        The name of the experiment for AIM tracking (default: augmentation-\n                        at-loading)\n  --final\n  --flowpic-dim {32,64,1500}\n                        Flowpic dimension (default: 32)\n  --flowpic-block-duration FLOWPIC_BLOCK_DURATION\n                        Flowpic block duration (in seconds) (default: 15)\n  --dataset {ucdavis-icdm19,utmobilenet21,mirage19,mirage22}\n                        Dataset to use for modeling (default: ucdavis-icdm19)\n  --dataset-minpkts {-1,10,100,1000}\n                        When used in combination with --dataset can refine the dataset and\n                        split to use for modeling (default: -1)\n  --split-index SPLIT_INDEX\n                        Datasplit index (default: 0)\n  --max-samples-per-class MAX_SAMPLES_PER_CLASS\n                        Activated when --split-index is -1 to define how many samples to\n                        select for train+val (with a 80/20 split between train and val\n                        (default: -1)\n  --train-val-split-ratio TRAIN_VAL_SPLIT_RATIO\n                        Training train/val split (default: 0.8)\n  --aug-name {noaug,rotate,horizontalflip,colorjitter,packetloss,timeshift,changertt}\n                        Augmentation policy (default: noaug)\n  --suppress-val-augmentation\n                        Do not augment validation set (default: False)\n  --seed SEED           Random seed (default: 12345)\n  --batch-size BATCH_SIZE\n                        Training batch size (default: 64)\n  --patience-steps PATIENCE_STEPS\n  --learning-rate LEARNING_RATE\n                        Traning learning rate (default: 0.001)\n  --epochs EPOCHS       Number of epochs for training (default: 50)\n  --suppress-test-train-val-leftover\n                        Skip test on leftover split (default: False)\n  --suppress-dropout    Mask dropout layers with Identity (default: False)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/","title":"Tcbench cli clickutils","text":""},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.compose_help_string_from_list","title":"<code>compose_help_string_from_list(items)</code>","text":"<p>Compose a string from a list</p> Source code in <code>src/tcbench/cli/clickutils.py</code> <pre><code>def compose_help_string_from_list(items:List[str]) -&gt; str:\n    \"\"\"Compose a string from a list\"\"\"\n    return \"\\[\" + f'{\"|\".join(items)}' + \"].\"\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.convert_params_dict_to_list","title":"<code>convert_params_dict_to_list(params, skip_params=None)</code>","text":"<p>Convert a dictionary of parameters (name,value) pairs into a list of \"-- Source code in <code>src/tcbench/cli/clickutils.py</code> <pre><code>def convert_params_dict_to_list(params:Dict[str,Any], skip_params:List[str]=None) -&gt; List[str]:\n    \"\"\"Convert a dictionary of parameters (name,value) pairs into a list of \"--&lt;param-name&gt; &lt;param-value&gt;\"\"\"\n    if skip_params is None:\n        skip_params = set()\n\n    l = []\n    for par_name, par_value in params.items():\n        if par_name in skip_params or par_value == False or par_value is None:\n            continue\n        par_name = par_name.replace(\"_\", \"-\")\n        if par_value == True:\n            l.append(f\"--{par_name}\")\n        else:\n            l.append(f\"--{par_name} {str(par_value)}\")\n\n    return l\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_clickutils/#tcbench.cli.clickutils.help_append_choices","title":"<code>help_append_choices(help_string, values)</code>","text":"<p>Append to an help string a styled version of a list of values</p> Source code in <code>src/tcbench/cli/clickutils.py</code> <pre><code>def help_append_choices(help_string:str, values:List[str]) -&gt; str:\n    \"\"\"Append to an help string a styled version of a list of values\"\"\"\n    text = \"|\".join([f\"[bold]{text}[/bold]\" for text in values])\n    return f\"{help_string} [yellow]Choices: [{text}][/yellow]\"\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/","title":"Tcbench cli command aimrepo","text":""},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.aimrepo","title":"<code>aimrepo(ctx)</code>","text":"<p>Investigate AIM repository content.</p> Source code in <code>src/tcbench/cli/command_aimrepo.py</code> <pre><code>@click.group(\"aimrepo\")\n@click.pass_context\ndef aimrepo(ctx):\n    \"\"\"Investigate AIM repository content.\"\"\"\n    pass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.ls","title":"<code>ls(ctx, aim_repo_folder)</code>","text":"<p>List a subset of properties of each run.</p> Source code in <code>src/tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"ls\")\n@click.pass_context\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\ndef ls(ctx, aim_repo_folder):\n    \"\"\"List a subset of properties of each run.\"\"\"\n    from tcbench.modeling import aimutils\n\n    if not aim_repo_folder.exists():\n        raise RuntimeError(f'Not found {aim_repo_folder}')\n    if not (aim_repo_folder / '.aim'):\n        raise RuntimeError(f'The input {aim_repo_folder} is not an AIM repository')\n\n    repo = aim.Repo(str(aim_repo_folder))\n    prop = aimutils.get_repo_properties(repo)\n    df = prop['df_run']\n    cols = ['hash', 'creation_time', 'end_time']\n    if 'campaign_id' in df.columns:\n        cols.insert(1, 'campaign_id')\n    df = df[cols]\n    df = df.astype(str)\n\n    table = Table(box=None)\n    for col in df.columns:\n        table.add_column(col)\n    for idx in range(len(df)):\n        table.add_row(*df.iloc[idx].values)\n    console.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.properties","title":"<code>properties(ctx, aim_repo_folder)</code>","text":"<p>List properties across all runs.</p> Source code in <code>src/tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"properties\")\n@click.pass_context\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\ndef properties(ctx, aim_repo_folder):\n    \"\"\"List properties across all runs.\"\"\"\n    from tcbench.modeling import aimutils\n\n    def format_duration(timedelta):\n        parts = timedelta.components\n        text = ''\n        if parts.days &gt; 0:\n            text += f'{parts.days}d '\n        if parts.hours &gt; 0:\n            text += f'{parts.hours}h'\n        text += f'{parts.minutes}m{parts.seconds}s'\n        return text\n\n    repo = aim.Repo(str(aim_repo_folder))\n    prop = aimutils.get_repo_properties(repo)\n\n    table = Table(box=box.ROUNDED)\n    table.add_column('Name', overflow='fold')\n    table.add_column('No. unique', overflow='fold', justify='right')\n    table.add_column('Value', overflow='fold')\n\n    table.add_row('runs', '-', str(len(prop['df_run']))) \n    duration_mean, duration_std = prop['run_duration']\n    duration_mean = format_duration(duration_mean)\n    duration_std = format_duration(duration_std)\n    table.add_row(f'run duration (mean {PLUSMINUS} std)', '-', f'{duration_mean} {PLUSMINUS} {duration_std}')\n    table.add_row('metrics', str(len(prop['metrics'])), str(prop['metrics']))\n    table.add_row('contexts', str(len(prop['contexts'])), str(prop['contexts']))\n\n    table.add_section()\n    for hparam_name in sorted([name for name in prop if name.startswith('hparam')] + ['experiment']):\n        values = prop[hparam_name]\n        table.add_row(\n            hparam_name.replace('hparams.',''),\n            f'{len(values)}',\n            str(values),\n        )\n    console.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.report","title":"<code>report(ctx, **kwargs)</code>","text":"<p>Summarize runs performance metrics.</p> Source code in <code>src/tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"report\")\n@click.pass_context\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n    \"--campaign-id\",\n    \"campaign_id\",\n    default=None,\n    show_default=True,\n    help=\"Campaign ID to select. By default consider the latest registered campaign.\",\n)\n@click.option(\n    \"--metrics\",\n    \"metrics\",\n    default=\"acc\",\n    show_default=True,\n    help=\"Coma separated list of metrics to consider.\",\n)\n@click.option(\n    \"--contexts\",\n    \"splits\",\n    default=None,\n    show_default=True,\n    help=\"Coma separated list of test split names to consider. By default consider only split which name starts with 'test'\",\n)\n@click.option(\n    \"--groupby\",\n    \"groupby_params\",\n    default=None,\n    show_default=True,\n    help=\"Coma separated list of parameters to aggregate campaign results. By default, try to guess the list from the properties with more than one value.\"\n)\n@click.option(\n    \"--output-folder\",\n    \"output_folder\",\n    default=None,\n    show_default=True,\n    type=str,\n    help=\"Folder where to store output reports.\",\n)\n@click.option(\n    \"--precision\",\n    \"float_precision\",\n    default=2,\n    show_default=2,\n    type=int,\n    help='Number of floating point digits in console report.',\n)\ndef report(ctx, **kwargs):\n    \"\"\"Summarize runs performance metrics.\"\"\"\n    import pandas as pd\n    from tcbench.modeling import aimutils, utils\n\n    repo = aim.Repo(str(kwargs['aim_repo_folder']))\n\n    prop = aimutils.get_repo_properties(repo)\n    metrics = kwargs['metrics'].split(',')\n    #if \"duration\" not in metrics:\n    #    metrics.append('duration')\n\n    contexts = kwargs['splits']\n    if contexts is None:\n        contexts = [\n            context_name\n            for context_name in prop['contexts']\n            if context_name.startswith('test')\n        ]\n    else:\n        contexts = contexts.split(',')\n\n    groupby_params = kwargs['groupby_params']\n    if groupby_params is None:\n        groupby_params = []\n        for prop_name, prop_values in prop.items():\n            if prop_name.startswith('hparams') and \\\n               prop_name not in {\n                'hparams.campaign_exp_idx', \n                'hparams.seed', \n                'hparams.split_index'\n                } and \\\n               len(prop_values) &gt; 1:\n                groupby_params.append(prop_name)\n        if len(groupby_params) == 0:\n            groupby_params = ['hparams.campaign_id']\n    else:\n        l = []\n        for param_name in groupby_params.split(','):\n            if param_name in prop:\n                l.append(param_name)\n            elif f'hparams.{param_name}' in prop:\n                l.append(f'hparams.{param_name}')\n            else:\n                console.print(f'[yellow]WARNING: parameter {param_name} unknown and will be skipped')\n        groupby_params = l\n\n    if 'test_split_name' not in groupby_params:\n        groupby_params.insert(0, 'test_split_name')\n\n\n    if any(metric_name not in prop['metrics'] for metric_name in metrics):\n        raise RuntimeError(f'Metric {metrics_name} not available: possible values are {prop[\"metrics\"]}')\n    if any(context_name not in prop['contexts'] for context_name in contexts):\n        raise RuntimeError(f'Split {context_name} not available: possible values are {prop[\"contexts\"]}')\n\n    metrics2 = metrics[:]\n    #if \"duration\" not in metrics2:\n    #    metrics2.append(\"duration\")\n    df = aimutils.metrics_to_pandas(repo, prop['df_run'], metrics2, contexts)\n    df = df.assign(run_duration=df[\"end_time\"] - df[\"creation_time\"])\n\n\n    for campaign_id in prop['hparams.campaign_id']:\n        df_campaign = df[df['hparams.campaign_id'] == campaign_id]\n        if len(df_campaign) == 0:\n            console.print(f'WARNING: missing campaign_id:{campaign_id}')\n            continue\n\n        console.print(f'campaign_id: {campaign_id}')\n        console.print(f'runs: {df_campaign[\"hash\"].nunique()}')\n\n        columns = groupby_params + metrics + ['run_duration', 'hash']\n        #if \"duration\" not in columns:\n        #    columns.append(\"duration\")\n\n        df_tmp = df_campaign[columns]\n        df_tmp = df_tmp.astype({mtr:float for mtr in metrics})\n        agg_funcs = {\n            mtr_name: [\"count\", \"mean\", \"std\", ('ci95', utils.compute_confidence_intervals)]\n            for mtr_name in metrics\n        }\n        agg_funcs[\"run_duration\"] = [\"mean\", \"std\", ('ci95', utils.compute_confidence_intervals)]\n        agg_funcs['hash'] = [('runs', 'count')]\n        df_agg = (\n            df_tmp\n                .groupby(by=groupby_params)\n                .agg(agg_funcs)\n        )\n\n        ## table to console\n        _report_rich_table((df_agg\n            .round(kwargs['float_precision'])\n            .drop([tpl for tpl in df_agg.columns if tpl[1] == 'count'], axis=1)\n        ))\n\n\n\n        ## dump to file\n        df_agg.index.names = [name.replace('hparams.', '') for name in df_agg.index.names]\n        df_agg = df_agg.drop([('hash', 'runs')], axis=1)\n\n\n        folder = kwargs['output_folder'] \n        if folder is None:\n            folder = pathlib.Path(kwargs['aim_repo_folder']) / 'campaign_summary' / campaign_id\n        else:\n            folder = pathlib.Path(folder) / 'campaign_summary' / campaign_id\n        if not folder.exists():\n            folder.mkdir(parents=True)\n\n        is_xgboost_on_pktseries = (\n            'hparams.flow_representation' in df.columns and\n            (df['hparams.flow_representation'].unique() == ['pktseries']).all()\n        )\n        hparam_name = 'flowpic_dim'\n        if is_xgboost_on_pktseries:\n            hparam_name = 'max_n_pkts'\n\n        for hparam_value in df_campaign[f'hparams.{hparam_name}'].unique():\n            # run info\n            df_tmp = df_campaign[df_campaign[f'hparams.{hparam_name}'] == hparam_value]\n            df_tmp = df_tmp.reset_index(drop=True)\n            df_tmp.columns = [col.replace('hparams.','') for col in df_tmp.columns]\n            fname = folder / f'runsinfo_{hparam_name}_{hparam_value}.parquet'\n            console.print(f'saving: {fname}')\n            df_tmp.to_parquet(fname)\n\n            # metrics report\n            df_tmp = df_agg.reset_index()\n            if (hparam_name, '') in df_tmp.columns:\n                df_tmp = df_tmp[df_tmp[hparam_name] == hparam_value]\n                df_tmp = df_tmp.drop([(hparam_name, '')], axis=1)\n            fname = folder / f'summary_{hparam_name}_{hparam_value}.csv'\n            console.print(f'saving: {fname}')\n            df_tmp.to_csv(fname, index=None)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_aimrepo/#tcbench.cli.command_aimrepo.merge","title":"<code>merge(ctx, src_paths, dst_path)</code>","text":"<p>Coalesce different AIM repos into a single new repo.</p> Source code in <code>src/tcbench/cli/command_aimrepo.py</code> <pre><code>@aimrepo.command(\"merge\")\n@click.pass_context\n@click.option(\n    \"--src\",\n    \"src_paths\",\n    type=pathlib.Path,\n    multiple=True,\n    metavar=\"PATH\",\n    required=True,\n    help=\"AIM repository to merge.\",\n)\n@click.option(\n    \"--dst\",\n    \"dst_path\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"New AIM repository to create.\",\n)\ndef merge(ctx, src_paths, dst_path):\n    \"\"\"Coalesce different AIM repos into a single new repo.\"\"\"\n    from tcbench.modeling import aimutils\n\n    if dst_path.exists():\n        shutil.rmtree(dst_path)\n\n    aimutils.init_repository(dst_path)\n    dst_repo = aim.Repo(str(dst_path))\n\n    for path in src_paths:\n        src_repo = aim.Repo(str(path))\n        src_runs = aimutils.list_repo(src_repo)\n        hashes = src_runs['hash'].values.tolist()\n        src_repo.copy_runs(hashes, dst_repo)\n\n        if (path / 'artifacts').exists():\n            dst_path_artifacts = dst_path / 'artifacts'\n            if not dst_path_artifacts.exists():\n                dst_path_artifacts.mkdir(parents=True)\n            for run_hash in hashes:\n                shutil.copytree((path / 'artifacts' / run_hash), dst_path / 'artifacts' / run_hash)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/","title":"Tcbench cli command campaign","text":""},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.campaign","title":"<code>campaign(ctx)</code>","text":"<p>Triggers a modeling campaign.</p> Source code in <code>src/tcbench/cli/command_campaign.py</code> <pre><code>@click.group(\"campaign\")\n@click.pass_context\ndef campaign(ctx):\n    \"\"\"Triggers a modeling campaign.\"\"\"\n    pass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.augment_at_loading","title":"<code>augment_at_loading(ctx, **kwargs)</code>","text":"<p>Modeling by applying data augmentation when loading the training set.</p> Source code in <code>src/tcbench/cli/command_campaign.py</code> <pre><code>@campaign.command(\"augment-at-loading\")\n@click.pass_context\n@click.option(\n    \"--artifacts-folder\",\n    \"artifacts_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_ARTIFACTS_FOLDER,\n    show_default=True,\n    help=\"Artifacts folder.\",\n)\n@click.option(\n    \"--workers\",\n    \"workers\",\n    type=int,\n    default=20,\n    show_default=True,\n    help=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n    \"--gpu-index\",\n    \"gpu_index\",\n    type=str,\n    default=\"0\",\n    show_default=True,\n    help=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n    \"--aim-experiment-name\",\n    \"aim_experiment_name\",\n    default=\"augmentations-at-loading\",\n    show_default=True,\n    help=\"The name of the experiment for AIM tracking.\",\n)\n###############\n# flowpic\n###############\n@click.option(\n    \"--flowpic-dims\",\n    \"flowpic_dims\",\n    type=str,\n    default=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)),\n    show_default=True,\n    help=\"Coma separated list of flowpic dimensions for experiments.\",\n)\n# @click.option(\n#    \"--flowpic-block-duration\",\n#    \"flowpic_block_duration\",\n#    type=int,\n#    default=15,\n#    show_default=True,\n#    help=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n# )\n###############\n# data\n###############\n@click.option(\n    \"--dataset\",\n    \"dataset\",\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=str(DATASETS.UCDAVISICDM19),\n    show_default=True,\n    help=\"Dataset to use for modeling.\",\n)\n@click.option(\n    \"--dataset-minpkts\",\n    type=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\n    default=\"-1\",\n    callback=CLICK_CALLBACK_TOINT,\n    show_default=True,\n    help=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n)\n@click.option(\n    \"--split-indexes\",\n    \"split_indexes\",\n    type=str,\n    default=None,\n    help=\"Coma separted list of split indexes (by default all splits are used).\",\n)\n@click.option(\n    \"--max-samples-per-class\",\n    type=int,\n    default=-1,\n    help=\"Activated when --split-indexes is -1 to define how many samples to select for train+val (with a 80/20 split between train and val).\",\n)\n###############\n# training\n###############\n# @click.option(\n#    \"--train-val-split-ratio\",\n#    \"train_val_split_ratio\",\n#    type=float,\n#    default=0.8,\n#    show_default=True,\n#    help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n# )\n@click.option(\n    \"--augmentations\",\n    type=str,\n    default=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS)),\n    show_default=True,\n    help=\"Coma separated list of augmentations for experiments. Choices: \"\n    + VALID_AUGMENTATIONS_FOR_AUGATLOAD,\n)\n@click.option(\n    \"--seeds\",\n    \"seeds\",\n    type=str,\n    default=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_SEEDS)),\n    show_default=True,\n    help=\"Coma separated list of seed for experiments.\",\n)\n@click.option(\n    \"--batch-size\",\n    \"batch_size\",\n    type=int,\n    default=32,\n    show_default=True,\n    help=\"Training batch size.\",\n)\n@click.option(\n    \"--patience-steps\",\n    \"patience_steps\",\n    default=5,\n    type=int,\n    show_default=True,\n    help=\"Max. number of epochs without improvement before stopping training.\",\n)\n@click.option(\n    \"--learning-rate\",\n    \"learning_rate\",\n    type=float,\n    default=0.001,\n    show_default=True,\n    help=\"Training learning rate.\",\n)\n@click.option(\n    \"--epochs\",\n    \"epochs\",\n    type=int,\n    default=50,\n    show_default=True,\n    help=\"Number of epochs for training.\",\n)\n@click.option(\n    \"--no-test-leftover\",\n    \"suppress_test_train_val_leftover\",\n    default=False,\n    is_flag=True,\n    help=\"Skip test on leftover split (specific for ucdavis-icdm19, and default enabled for all other datasets).\",\n)\n@click.option(\n    \"--no-dropout\",\n    \"suppress_dropout\",\n    default=False,\n    is_flag=True,\n    help=\"Mask dropout layers with Identity layers.\",\n)\n@click.option(\n    \"--method\",\n    \"method\",\n    type=click.Choice(\n        (str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n    ),\n    default=str(MODELING_METHOD_TYPE.MONOLITHIC),\n    show_default=True,\n    help=\"Method to use for training.\",\n)\n@click.option(\n    \"--input-repr\",\n    \"flow_representation\",\n    type=CLICK_TYPE_INPUT_REPR,\n    callback=CLICK_CALLBACK_INPUT_REPR,\n    default=str(MODELING_INPUT_REPR_TYPE.PKTSERIES),\n    show_default=True,\n    metavar=\"TEXT\",\n    help=\"Input representation.\",\n)\n@click.option(\n    \"--pktseries-len\",\n    \"max_n_pkts\",\n    default=\"10,30\",\n    show_default=True,\n    metavar=\"INTEGER\",\n    help=\"Number of packets (when using time series as input).\",\n)\n@click.option(\n    \"--campaign-id\",\n    \"campaign_id\",\n    type=str,\n    default=None,\n    help=\"A campaign id to mark all experiments.\",\n)\n@click.option(\n    \"--dry-run\",\n    \"dry_run\",\n    default=False,\n    is_flag=True,\n    help=\"Show the number of experiments and then quit.\",\n)\n@click.option(\n    \"--max-train-splits\",\n    \"max_train_splits\",\n    type=int,\n    default=-1,\n    show_default=True,\n    help=\"The maximum number of training splits to experiment with. If -1, use all available.\",\n)\ndef augment_at_loading(ctx, **kwargs):\n    \"\"\"Modeling by applying data augmentation when loading the training set.\"\"\"\n    method = kwargs[\"method\"]\n\n    if method == str(MODELING_METHOD_TYPE.MONOLITHIC):\n        from tcbench.modeling import (\n            run_campaign_augmentations_at_loading as entry_point,\n        )\n\n        if str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\n            kwargs[\"suppress_test_train_val_leftover\"] = True\n\n        params = clickutils.convert_params_dict_to_list(\n            kwargs, skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n        )\n    else:\n        from tcbench.modeling import (\n            run_campaign_augmentations_at_loading_xgboost as entry_point,\n        )\n\n        params = clickutils.convert_params_dict_to_list(\n            kwargs,\n            skip_params=[\n                \"method\",\n                \"gpu_index\",\n                \"augmentations\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"patience_steps\",\n                \"epochs\",\n                \"suppress_dropout\",\n                \"dataset_minpkts\",\n                # \"flowpic_dims\",\n                \"dataset\",\n                \"workers\",\n            ],\n        )\n\n    parser = entry_point.cli_parser()\n    args = parser.parse_args((\" \".join(params)).split())\n    args.method = method\n    entry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_campaign/#tcbench.cli.command_campaign.contrastivelearning_and_finetune","title":"<code>contrastivelearning_and_finetune(ctx, **kwargs)</code>","text":"<p>Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.</p> Source code in <code>src/tcbench/cli/command_campaign.py</code> <pre><code>@campaign.command(\"contralearn-and-finetune\")\n@click.pass_context\n@click.option(\n    \"--campaign-id\",\n    \"campaign_id\",\n    type=str,\n    default=None,\n    help=\"A campaign id to mark all experiments.\",\n)\n@click.option(\n    \"--workers\",\n    \"workers\",\n    type=int,\n    default=50,\n    show_default=True,\n    help=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n    \"--gpu-index\",\n    \"gpu_index\",\n    type=str,\n    default=\"0\",\n    show_default=True,\n    help=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n    \"--aim-experiment-name\",\n    \"aim_experiment_name\",\n    default=\"contrastive-learning-and-finetune\",\n    show_default=True,\n    help=\"The name of the experiment for AIM tracking.\",\n)\n@click.option(\n    \"--artifacts-folder\",\n    \"artifacts_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_ARTIFACTS_FOLDER,\n    show_default=True,\n    help=\"Artifacts folder.\",\n)\n@click.option(\n    \"--dry-run\",\n    \"dry_run\",\n    default=False,\n    is_flag=True,\n    help=\"Show the number of experiments and then quit.\",\n)\n@click.option(\n    \"--max-train-splits\",\n    \"max_train_splits\",\n    type=int,\n    default=-1,\n    show_default=True,\n    help=\"The maximum number of training splits to experiment with. If -1, use all available.\",\n)\n@click.option(\n    \"--augmentations\",\n    type=str,\n    default=DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_AUGMENTATIONS,\n    show_default=True,\n    help=\"Coma separated list of augmentations. Choices: \"\n    + VALID_AUGMENTATIONS_FOR_CONTRALEARN,\n)\n@click.option(\n   \"--train-val-split-ratio\",\n   \"train_val_split_ratio\",\n   type=float,\n   default=0.8,\n   show_default=True,\n   help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n)\n@click.option(\n    \"--flowpic-dims\",\n    \"flowpic_dim\",\n    type=str,\n    default=\"32\",\n    show_default=True,\n    help=\"Coma separated list of flowpic dimensions for experiments.\",\n)\n@click.option(\n    \"--cl-seeds\",\n    \"contrastive_learning_seeds\",\n    default=\",\".join(\n        map(\n            str,\n            DEFAULT_CAMPAING_CONTRALEARNANDFINETUNE_SEEDS_CONTRALEARN,\n        )\n    ),\n    show_default=True,\n    help=\"Coma separated list of seeds to use for contrastive learning pretraining.\",\n)\n@click.option(\n    \"--ft-seeds\",\n    \"finetune_seeds\",\n    default=\",\".join(map(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_SEEDS_FINETUNE)),\n    show_default=True,\n    help=\"Coma separated list of seeds to use for finetune training.\",\n)\n@click.option(\n    \"--dropout\",\n    \"dropout\",\n    default=\"disabled\",\n    show_default=True,\n    help=\"Coma separated list. Choices:\"\n    + clickutils.compose_help_string_from_list((\"enabled\", \"disabled\")),\n)\n@click.option(\n    \"--cl-projection-layer-dims\",\n    \"projection_layer_dims\",\n    type=str,\n    default=\"30\",\n    help=\"Coma separate list of contrastive learning projection layer dimensions.\",\n    show_default=True,\n)\n#    parser.add_argument(\n#        \"--finetune-augmentation\", default=\"none\",\n#        choices=(\"none\", \"only-views\", \"views-and-original\"),\n#        help=utils.compose_cli_help_string(\"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\")\n#    )\n@click.option(\n    \"--batch-size\",\n    \"batch_size\",\n    type=int,\n    default=32,\n    show_default=True,\n    help=\"Training batch size.\",\n)\n@click.option(\n    \"--split-indexes\",\n    \"split_indexes\",\n    type=str,\n    default=None,\n    help=\"Coma separted list of split indexes (by default all splits are used).\",\n)\ndef contrastivelearning_and_finetune(ctx, **kwargs):\n    \"\"\"Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\"\"\"\n    from tcbench.modeling import (\n        run_campaign_contrastive_learning_and_finetune as entry_point,\n    )\n\n    kwargs[\"dataset\"] = DATASETS.UCDAVISICDM19\n\n    if str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\n        kwargs[\"suppress_test_train_val_leftover\"] = True\n\n    params = clickutils.convert_params_dict_to_list(\n        kwargs,\n        skip_params=[\"dataset\"],\n    )\n    parser = entry_point.cli_parser()\n    args = parser.parse_args((\" \".join(params)).split())\n    entry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/","title":"Tcbench cli command datasets","text":""},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.datasets","title":"<code>datasets(ctx)</code>","text":"<p>Install/Remove traffic classification datasets.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@click.group()\n@click.pass_context\ndef datasets(ctx):\n    \"\"\"Install/Remove traffic classification datasets.\"\"\"\n    pass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.info","title":"<code>info(ctx, dataset_name)</code>","text":"<p>Show the meta-data related to supported datasets.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"info\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=False,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    help=\"Dataset to install.\",\n    default=None,\n)\ndef info(ctx, dataset_name):\n    \"\"\"Show the meta-data related to supported datasets.\"\"\"\n    rich_obj = datasets_utils.get_rich_tree_datasets_properties(dataset_name)\n    cli.console.print(rich_obj)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.install","title":"<code>install(ctx, dataset_name, input_folder, num_workers)</code>","text":"<p>Install a dataset.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"install\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=True,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    help=\"Dataset to install.\",\n)\n@click.option(\n    \"--input-folder\",\n    \"-i\",\n    \"input_folder\",\n    required=False,\n    type=pathlib.Path,\n    default=None,\n    help=\"Folder where to find pre-downloaded tarballs.\",\n)\n@click.option(\n    \"--num-workers\",\n    \"-w\",\n    required=False,\n    type=int,\n    default=20,\n    show_default=True,\n    help=\"Number of parallel workers to use when processing the data.\",\n)\ndef install(ctx, dataset_name, input_folder, num_workers):\n    \"\"\"Install a dataset.\"\"\"\n    if (\n        dataset_name\n        in (\n            datasets_utils.DATASETS.UCDAVISICDM19,\n            datasets_utils.DATASETS.UTMOBILENET21,\n        )\n        and input_folder is None\n    ):\n        raise RuntimeError(\n            f\"Cannot automatically download {dataset_name}. Please download it separately and retry install using the --input-folder option\"\n        )\n    datasets_utils.install(dataset_name, input_folder, num_workers=num_workers)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.lsfiles","title":"<code>lsfiles(ctx, dataset_name)</code>","text":"<p>Tree view of the datasets parquet files.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"lsfiles\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=False,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=None,\n    help=\"Dataset name.\",\n)\ndef lsfiles(ctx, dataset_name):\n    \"\"\"Tree view of the datasets parquet files.\"\"\"\n    _ls_files(dataset_name)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.schema","title":"<code>schema(ctx, dataset_name, schema_type)</code>","text":"<p>Show datasets schemas</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"schema\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=False,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=None,\n    help=\"Dataset to install.\",\n)\n@click.option(\n    \"--type\",\n    \"-t\",\n    \"schema_type\",\n    required=False,\n    type=click.Choice((\"unfiltered\", \"filtered\", \"splits\")),\n    default=\"unfiltered\",\n    show_default=True,\n    help=\"Schema type (unfiltered: original raw data; filtered: curated data; splits: train/val/test splits).\",\n)\ndef schema(ctx, dataset_name, schema_type):\n    \"\"\"Show datasets schemas\"\"\"\n    rich_obj = datasets_utils.get_rich_dataset_schema(dataset_name, schema_type)\n    cli.console.print(rich_obj)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.report_samples_count","title":"<code>report_samples_count(ctx, dataset_name, min_pkts, split)</code>","text":"<p>Show report on number of samples per class.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"samples-count\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=False,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=None,\n    help=\"Dataset to install.\",\n)\n@click.option(\n    \"--min-pkts\",\n    \"min_pkts\",\n    required=False,\n    type=click.Choice((\"-1\", \"10\", \"1000\")),\n    default=\"-1\",\n    show_default=True,\n    help=\"\",\n)\n@click.option(\n    \"--split\",\n    \"split\",\n    required=False,\n    type=click.Choice((\"human\", \"script\", \"0\", \"1\", \"2\", \"3\", \"4\")),\n    default=None,\n    help=\"\",\n)\ndef report_samples_count(ctx, dataset_name, min_pkts, split):\n    \"\"\"Show report on number of samples per class.\"\"\"\n    with cli.console.status(\"Computing...\", spinner=\"dots\"):\n        min_pkts = int(min_pkts)\n        if min_pkts == -1 and split is not None:\n            if dataset_name != datasets_utils.DATASETS.UCDAVISICDM19:\n                min_pkts = 10\n\n        df_split = None\n        if dataset_name == datasets_utils.DATASETS.UCDAVISICDM19 or split is None:\n            df = datasets_utils.load_parquet(dataset_name, min_pkts, split)\n        else:\n            df = datasets_utils.load_parquet(dataset_name, min_pkts, split=None)\n            df_split = datasets_utils.load_parquet(dataset_name, min_pkts, split=split)\n\n    title = \"unfiltered\"\n    if dataset_name == datasets_utils.DATASETS.UCDAVISICDM19:\n        if split is not None:\n            title = f\"filtered, split: {split}\"\n    else:\n        title = []\n        if min_pkts != -1:\n            title.append(f\"min_pkts: {min_pkts}\")\n        if split:\n            title.append(f\"split: {split}\")\n        if title:\n            title = \", \".join(title)\n        else:\n            title = \"unfiltered\"\n\n    if df_split is None:\n        if (\n            dataset_name == datasets_utils.DATASETS.UCDAVISICDM19\n            and min_pkts == -1\n            and split is None\n        ):\n            ser = df.groupby([\"partition\", \"app\"])[\"app\"].count()\n        else:\n            ser = df[\"app\"].value_counts()\n\n        richutils.rich_samples_count_report(ser, title=title)\n    else:\n        richutils.rich_splits_report(df, df_split, split_index=split, title=title)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.import_datasets","title":"<code>import_datasets(ctx, dataset_name, path_archive)</code>","text":"<p>Fetch and install the curated version of the dataset.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"import\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=True,\n    type=click.Choice([DATASETS.UCDAVISICDM19.value, DATASETS.UTMOBILENET21.value], case_sensitive=False),\n    default=None,\n    help=\"Dataset name.\",\n)\n@click.option(\n    \"--archive\",\n    \"path_archive\",\n    required=False,\n    type=pathlib.Path,\n    default=None,\n    help=\"Path of an already downloaded curated archive.\",\n)\ndef import_datasets(ctx, dataset_name, path_archive):\n    \"\"\"Fetch and install the curated version of the dataset.\"\"\"\n    datasets_utils.import_dataset(dataset_name, path_archive)\n    cli.console.print()\n    cli.console.print(\"Files installed\")\n    _ls_files(dataset_name)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_datasets/#tcbench.cli.command_datasets.delete_dataset","title":"<code>delete_dataset(ctx, dataset_name)</code>","text":"<p>Delete a dataset.</p> Source code in <code>src/tcbench/cli/command_datasets.py</code> <pre><code>@datasets.command(name=\"delete\")\n@click.pass_context\n@click.option(\n    \"--name\",\n    \"-n\",\n    \"dataset_name\",\n    required=False,\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=None,\n    help=\"Dataset to delete.\",\n)\ndef delete_dataset(ctx, dataset_name):\n    \"\"\"Delete a dataset.\"\"\"\n    folder = datasets_utils.get_dataset_folder(dataset_name)\n    if not folder.exists():\n        cli.console.print(f\"[red]Dataset {dataset_name} is not installed[/red]\")\n    else:\n        with cli.console.status(f\"Deleting {dataset_name}...\", spinner=\"dots\"):\n            shutil.rmtree(str(folder))\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/","title":"Tcbench cli command singlerun","text":""},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.singlerun","title":"<code>singlerun(ctx)</code>","text":"<p>Triggers a modeling run.</p> Source code in <code>src/tcbench/cli/command_singlerun.py</code> <pre><code>@click.group(\"run\")\n@click.pass_context\ndef singlerun(ctx):\n    \"\"\"Triggers a modeling run.\"\"\"\n    pass\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.augment_at_loading","title":"<code>augment_at_loading(ctx, **kwargs)</code>","text":"<p>Modeling by applying data augmentation when loading the training set.</p> Source code in <code>src/tcbench/cli/command_singlerun.py</code> <pre><code>@singlerun.command(\"augment-at-loading\")\n@click.pass_context\n@click.option(\n    \"--artifacts-folder\",\n    \"artifacts_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_ARTIFACTS_FOLDER,\n    show_default=True,\n    help=\"Artifacts folder.\",\n)\n@click.option(\n    \"--workers\",\n    \"workers\",\n    type=int,\n    default=20,\n    show_default=True,\n    help=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n    \"--gpu-index\",\n    \"gpu_index\",\n    type=str,\n    default=\"0\",\n    show_default=True,\n    help=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n    \"--aim-experiment-name\",\n    \"aim_experiment_name\",\n    default=\"augmentation-at-loading\",\n    show_default=True,\n    help=\"The name of the experiment for AIM tracking.\",\n)\n###############\n# flowpic\n###############\n@click.option(\n    \"--flowpic-dim\",\n    \"flowpic_dim\",\n    type=click.Choice((\"32\", \"64\", \"1500\")),\n    callback=CLICK_CALLBACK_TOINT,\n    default=\"32\",\n    show_default=True,\n    help=\"Flowpic dimension.\",\n)\n@click.option(\n    \"--flowpic-block-duration\",\n    \"flowpic_block_duration\",\n    type=int,\n    default=15,\n    show_default=True,\n    help=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n)\n###############\n# data\n###############\n@click.option(\n    \"--dataset\",\n    \"dataset\",\n    type=CLICK_TYPE_DATASET_NAME,\n    callback=CLICK_CALLBACK_DATASET_NAME,\n    default=str(DATASETS.UCDAVISICDM19),\n    show_default=True,\n    help=\"Dataset to use for modeling.\",\n)\n@click.option(\n    \"--dataset-minpkts\",\n    type=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\n    default=\"-1\",\n    callback=CLICK_CALLBACK_TOINT,\n    show_default=True,\n    help=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n)\n@click.option(\n    \"--split-index\",\n    \"split_index\",\n    type=int,\n    default=0,\n    show_default=True,\n    help=\"Data split index.\",\n)\n#    parser.add_argument(\n#        \"--max-samples-per-class\",\n#        type=int,\n#        default=-1,\n#        help=utils.compose_cli_help_string(\"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\")\n#    )\n###############\n# training\n###############\n@click.option(\n    \"--train-val-split-ratio\",\n    \"train_val_split_ratio\",\n    type=float,\n    default=0.8,\n    show_default=True,\n    help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n)\n@click.option(\n    \"--aug-name\",\n    \"aug_name\",\n    type=click.Choice(\n        (\n            \"noaug\",\n            \"rotate\",\n            \"horizontalflip\",\n            \"colorjitter\",\n            \"packetloss\",\n            \"timeshift\",\n            \"changertt\",\n        )\n    ),\n    default=\"noaug\",\n    show_default=True,\n    help=\"Name of the augmentation to use.\",\n)\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\n@click.option(\n    \"--seed\",\n    \"seed\",\n    type=int,\n    default=12345,\n    show_default=True,\n    help=\"Seed to initialize random generators.\",\n)\n@click.option(\n    \"--batch-size\",\n    \"batch_size\",\n    type=int,\n    default=32,\n    show_default=True,\n    help=\"Training batch size\",\n)\n@click.option(\n    \"--patience-steps\",\n    \"patience_steps\",\n    default=5,\n    type=int,\n    show_default=True,\n    help=\"Max. number of epochs without improvement before stopping training.\",\n)\n@click.option(\n    \"--learning-rate\",\n    \"learning_rate\",\n    type=float,\n    default=0.001,\n    show_default=True,\n    help=\"Training learning rate.\",\n)\n@click.option(\n    \"--epochs\",\n    \"epochs\",\n    type=int,\n    default=50,\n    show_default=True,\n    help=\"Number of epochs for training.\",\n)\n@click.option(\n    \"--no-test-leftover\",\n    \"suppress_test_train_val_leftover\",\n    default=False,\n    is_flag=True,\n    help=\"Skip test on leftover split (specific for ucdavis-icdm19, and default enabled for all other datasets).\",\n)\n@click.option(\n    \"--no-dropout\",\n    \"suppress_dropout\",\n    default=False,\n    is_flag=True,\n    help=\"Mask dropout layers with Identity layers.\",\n)\n@click.option(\n    \"--method\",\n    \"method\",\n    type=click.Choice(\n        (str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n    ),\n    default=str(MODELING_METHOD_TYPE.MONOLITHIC),\n    show_default=True,\n    help=\"Method to use for training.\",\n)\n@click.option(\n    \"--input-repr\",\n    \"flow_representation\",\n    type=CLICK_TYPE_INPUT_REPR,\n    callback=CLICK_CALLBACK_INPUT_REPR,\n    default=str(MODELING_INPUT_REPR_TYPE.PKTSERIES),\n    show_default=True,\n    #metavar=\"TEXT\",\n    help=\"Input representation.\",\n)\n@click.option(\n    \"--pktseries-len\",\n    \"max_n_pkts\",\n    type=click.Choice((\"10\", \"30\")),\n    default=\"10\",\n    show_default=True,\n    metavar=\"INTEGER\",\n    help=\"Number of packets (when using time series as input).\",\n)\ndef augment_at_loading(ctx, **kwargs):\n    \"\"\"Modeling by applying data augmentation when loading the training set.\"\"\"\n    method = kwargs[\"method\"]\n\n    if method == str(MODELING_METHOD_TYPE.MONOLITHIC):\n        from tcbench.modeling import run_augmentations_at_loading as entry_point\n\n        if str(kwargs[\"dataset\"]) != str(DATASETS.UCDAVISICDM19):\n            kwargs[\"suppress_test_train_val_leftover\"] = True\n\n        params = clickutils.convert_params_dict_to_list(\n            kwargs, skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n        )\n    else:\n        from tcbench.modeling import run_augmentations_at_loading_xgboost as entry_point\n\n        params = clickutils.convert_params_dict_to_list(\n            kwargs,\n            skip_params=[\n                \"method\",\n                \"gpu_index\",\n                \"aug_name\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"patience_steps\",\n                \"epochs\",\n                \"suppress_dropout\",\n                \"dataset_minpkts\",\n            ],\n        )\n\n    parser = entry_point.cli_parser()\n    args = parser.parse_args((\" \".join(params)).split())\n    args.method = method\n    entry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_command_singlerun/#tcbench.cli.command_singlerun.contrastivelearning_and_finetune","title":"<code>contrastivelearning_and_finetune(ctx, **kwargs)</code>","text":"<p>Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.</p> Source code in <code>src/tcbench/cli/command_singlerun.py</code> <pre><code>@singlerun.command(\"contralearn-and-finetune\")\n@click.pass_context\n@click.option(\n    \"--artifacts-folder\",\n    \"artifacts_folder\",\n    type=pathlib.Path,\n    default=DEFAULT_ARTIFACTS_FOLDER,\n    show_default=True,\n    help=\"Artifacts folder.\",\n)\n@click.option(\n    \"--workers\",\n    \"workers\",\n    type=int,\n    default=20,\n    show_default=True,\n    help=\"Number of parallel worker for loading the data.\",\n)\n@click.option(\n    \"--gpu-index\",\n    \"gpu_index\",\n    type=str,\n    default=\"0\",\n    show_default=True,\n    help=\"The id of the GPU to use (if training with deep learning).\",\n)\n@click.option(\n    \"--aim-repo\",\n    \"aim_repo\",\n    type=pathlib.Path,\n    default=DEFAULT_AIM_REPO,\n    show_default=True,\n    help=\"AIM repository location (local folder or URL).\",\n)\n@click.option(\n    \"--aim-experiment-name\",\n    \"aim_experiment_name\",\n    default=\"contrastive-learning-and-finetune\",\n    show_default=True,\n    help=\"The name of the experiment for AIM tracking.\",\n)\n#    parser.add_argument(\"--final\", action=\"store_true\", default=False)\n###############\n# flowpic\n###############\n@click.option(\n    \"--flowpic-dim\",\n    \"flowpic_dim\",\n    type=click.Choice((\"32\",)),  # \"64\", \"1500\")),\n    callback=CLICK_CALLBACK_TOINT,\n    default=\"32\",\n    show_default=True,\n    help=\"Flowpic dimension.\",\n)\n@click.option(\n    \"--flowpic-block-duration\",\n    \"flowpic_block_duration\",\n    type=int,\n    default=15,\n    show_default=True,\n    help=\"Number of seconds for the head of a flow (i.e., block) to use for a flowpic.\",\n)\n###############\n# data\n###############\n@click.option(\n    \"--dataset\",\n    \"dataset\",\n    type=click.Choice((str(DATASETS.UCDAVISICDM19),)),  # CLICK_TYPE_DATASET_NAME,\n    # callback=CLICK_CALLBACK_DATASET_NAME,\n    default=str(DATASETS.UCDAVISICDM19),\n    show_default=True,\n    help=\"Dataset to use for modeling.\",\n)\n# @click.option(\n#    \"--dataset-minpkts\",\n#    type=click.Choice((\"-1\", \"10\", \"100\", \"1000\")),\n#    default=\"-1\",\n#    callback=CLICK_CALLBACK_TOINT,\n#    show_default=True,\n#    help=\"In combination with --dataset, refines preprocessed and split dataset to use.\",\n# )\n@click.option(\n    \"--split-index\",\n    \"split_index\",\n    type=int,\n    default=0,\n    show_default=True,\n    help=\"Data split index.\",\n)\n###############\n# training\n###############\n# @click.option(\n#    \"--train-val-split-ratio\",\n#    \"train_val_split_ratio\",\n#    type=float,\n#    default=0.8,\n#    show_default=True,\n#    help=\"If not predefined by the selected split, the ratio data to use for training (rest is for validation).\",\n# )\n# @click.option(\n#    \"--aug-name\",\n#    \"aug_name\",\n#    type=click.Choice(\n#        (\n#            \"noaug\",\n#            \"rotate\",\n#            \"horizontalflip\",\n#            \"colorjitter\",\n#            \"packetloss\",\n#            \"timeshift\",\n#            \"changertt\",\n#        )\n#    ),\n#    default=\"noaug\",\n#    show_default=True,\n#    help=\"Name of the augmentation to use.\",\n# )\n#    parser.add_argument(\n#        \"--suppress-val-augmentation\",\n#        action='store_true',\n#        default=False,\n#        help=utils.compose_cli_help_string('Do not augment validation set')\n#    )\n@click.option(\n    \"--batch-size\",\n    \"batch_size\",\n    type=int,\n    default=32,\n    show_default=True,\n    help=\"Training batch size\",\n)\n@click.option(\n    \"--no-dropout\",\n    \"suppress_dropout\",\n    default=False,\n    is_flag=True,\n    help=\"Mask dropout layers with Identity layers.\",\n)\n# @click.option(\n#    \"--method\",\n#    \"method\",\n#    type=click.Choice(\n#        (str(MODELING_METHOD_TYPE.MONOLITHIC), str(MODELING_METHOD_TYPE.XGBOOST))\n#    ),\n#    default=str(MODELING_METHOD_TYPE.MONOLITHIC),\n#    show_default=True,\n#    help=\"Method to use for training.\",\n# )\n@click.option(\n    \"--cl-aug-names\",\n    \"augmentations\",\n    default=\"changertt,timeshift\",\n    show_default=True,\n    help=\"Coma separated list of augmentations pool for contrastive learning.\",\n)\n@click.option(\n    \"--cl-projection-layer-dim\",\n    \"projection_layer_dim\",\n    type=int,\n    default=30,\n    help=\"The number of units in the contrastive learning projection layer.\",\n    show_default=True,\n)\n@click.option(\n    \"--cl-learning-rate\",\n    \"contrastive_learning_lr\",\n    type=float,\n    default=0.001,\n    show_default=True,\n    help=\"Learning rate for pretraining.\",\n)\n@click.option(\n    \"--cl-seed\",\n    \"contrastive_learning_seed\",\n    type=int,\n    default=12345,\n    show_default=True,\n    help=\"Seed for contrastive learning pretraining.\",\n)\n@click.option(\n    \"--cl-patience-steps\",\n    \"contrastive_learning_patience_steps\",\n    type=int,\n    default=3,\n    help=\"Max steps to wait before stopping training if the top5 validation accuracy does not improve.\",\n    show_default=True,\n)\n@click.option(\n    \"--cl-temperature\",\n    \"contrastive_learning_temperature\",\n    type=float,\n    default=0.07,\n    help=\"Temperature for InfoNCE loss.\",\n    show_default=True,\n)\n@click.option(\n    \"--cl-epochs\",\n    \"contrastive_learning_epochs\",\n    type=int,\n    default=50,\n    show_default=True,\n    help=\"Epochs for contrastive learning pretraining.\",\n)\n####################################\n# finetune configs\n####################################\n@click.option(\n    \"--ft-learning-rate\",\n    \"finetune_lr\",\n    type=float,\n    default=0.01,\n    help=\"Learning rate for finetune.\",\n    show_default=True,\n)\n@click.option(\n    \"--ft-patience-steps\",\n    \"finetune_patience_steps\",\n    type=int,\n    default=5,\n    show_default=True,\n    help=\"Max steps to wait before stopping finetune training loss does not improve.\",\n)\n@click.option(\n    \"--ft-patience-min-delta\",\n    \"finetune_patience_min_delta\",\n    type=float,\n    default=0.001,\n    show_default=True,\n    help=\"Minimum decrease of training loss to be considered as improvement.\",\n)\n@click.option(\n    \"--ft-train-samples\",\n    \"finetune_train_samples\",\n    type=int,\n    default=10,\n    show_default=True,\n    help=\"Number of samples per-class for finetune training.\",\n)\n@click.option(\n    \"--ft-epochs\",\n    \"finetune_epochs\",\n    type=int,\n    default=50,\n    show_default=True,\n    help=\"Epochs for finetune training.\",\n)\n@click.option(\n    \"--ft-seed\",\n    \"finetune_seed\",\n    type=int,\n    default=12345,\n    show_default=True,\n    help=\"Seed for finetune training.\",\n)\ndef contrastivelearning_and_finetune(ctx, **kwargs):\n    \"\"\"Modeling by pre-training via constrative learning and then finetune the final classifier from the pre-trained model.\"\"\"\n\n    import tcbench.modeling.run_contrastive_learning_and_finetune as entry_point\n\n    params = clickutils.convert_params_dict_to_list(\n        kwargs,  # skip_params=[\"method\", \"flow_representation\", \"max_n_pkts\"]\n    )\n\n    parser = entry_point.cli_parser()\n    args = parser.parse_args((\" \".join(params)).split())\n    args.method = \"simclr\"\n    args.augmentations = kwargs[\"augmentations\"].split(\",\")\n    entry_point.main(args)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/","title":"Tcbench cli richutils","text":""},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_series","title":"<code>_rich_table_from_series(ser, columns, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas Series</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_series(ser:pd.Series, columns:List[str], with_total:bool=False) -&gt; rich.table.Table:\n    \"\"\"Compose a rich Table from a pandas Series\"\"\"\n    table = Table()\n    table.add_column(columns[0])\n    table.add_column(columns[1], justify=\"right\")\n    for index, value in zip(ser.index, ser.values):\n        table.add_row(str(index), str(value))\n    if with_total:\n        table.add_section()\n        table.add_row(\"__total__\", str(ser.values.sum()))\n    return table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_series_multiindex","title":"<code>_rich_table_from_series_multiindex(ser, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas Series with MultiIndex</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_series_multiindex(ser:pd.Series, with_total:bool=False) -&gt; rich.table.Table:\n    \"\"\"Compose a rich Table from a pandas Series with MultiIndex\"\"\"\n    table = Table()\n    for col in ser.index.names:\n        table.add_column(col)\n    table.add_column(\"samples\", justify=\"right\")\n\n    for level0 in ser.index.levels[0]:\n        tmp = ser.loc[level0]\n        for level1, value in zip(tmp.index, tmp.values):\n            table.add_row(str(level0), str(level1), str(value))\n            level0 = \"\"\n        if with_total:\n            table.add_row(\"\", \"__total__\", str(tmp.values.sum()))\n        table.add_section()\n    return table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils._rich_table_from_dataframe","title":"<code>_rich_table_from_dataframe(df, with_total=False)</code>","text":"<p>Compose a rich Table from a pandas DataFrame</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def _rich_table_from_dataframe(df:pd.DataFrame, with_total:bool=False) -&gt; rich.table.Table:\n    \"\"\"Compose a rich Table from a pandas DataFrame\"\"\"\n    table = Table()\n    table.add_column(df.index.name)\n    for col in df.columns:\n        table.add_column(col, justify=\"right\")\n    for idx in range(df.shape[0]):\n        table.add_row(df.index[idx], *list(df.iloc[idx].values.astype(str)))\n    if with_total:\n        table.add_section()\n        table.add_row(\"__total__\", *list(map(str, df.sum().values)))\n    return table\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_samples_count_report","title":"<code>rich_samples_count_report(obj, columns=None, title=None, with_total=True)</code>","text":"<p>Compute and format into a table the per-class samples count</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def rich_samples_count_report(obj:pd.Series | pd.DataFrame, columns:List[str]=None, title:str=None, with_total:bool=True) -&gt; rich.table.Table:\n    \"\"\"Compute and format into a table the per-class samples count\"\"\"\n    if columns is None:\n        columns = [\"app\", \"samples\"]\n    if isinstance(obj, pd.Series):\n        if isinstance(obj.index, pd.MultiIndex):\n            table = _rich_table_from_series_multiindex(obj, with_total=with_total)\n        else:\n            table = _rich_table_from_series(obj, columns, with_total=with_total)\n    else:\n        table = _rich_table_from_dataframe(obj, with_total=with_total)\n\n    if title is not None:\n        console.print(title)\n    console.print(table)\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_packets_report","title":"<code>rich_packets_report(df, packets_colname='packets', title=None)</code>","text":"<p>Compute and reports stats for number of packets per flow</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def rich_packets_report(df:pd.DataFrame, packets_colname:str=\"packets\", title:str=None) -&gt; rich.table.Table:\n    \"\"\"Compute and reports stats for number of packets per flow\"\"\"\n    ser = df[packets_colname].describe().round(2)\n    rich_samples_count_report(\n        ser, columns=[\"stat\", \"value\"], title=title, with_total=False\n    )\n</code></pre>"},{"location":"tcbench/api/tcbench_cli_richutils/#tcbench.cli.richutils.rich_label","title":"<code>rich_label(text, extra_new_line=False)</code>","text":"<p>Output on the console a formatted label</p> Source code in <code>src/tcbench/cli/richutils.py</code> <pre><code>def rich_label(text:str, extra_new_line:bool=False) -&gt; None:\n    \"\"\"Output on the console a formatted label\"\"\"\n    if extra_new_line:\n        console.print()\n    console.print(Panel(text, box=box.ROUNDED, expand=False, padding=0))\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets/","title":"Tcbench libtcdatasets","text":""},{"location":"tcbench/api/tcbench_libtcdatasets/#generating-trainvaltest-splits","title":"Generating train/val/test splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/","title":"Tcbench libtcdatasets datasets utils","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_yaml","title":"<code>load_yaml(fname)</code>","text":"<p>Load an input YAML file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the YAML filename to load</p> required Return <p>The YAML object loaded</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_yaml(fname: pathlib.Path) -&gt; Dict[Any, Any]:\n    \"\"\"Load an input YAML file\n\n    Arguments:\n        fname: the YAML filename to load\n\n    Return:\n        The YAML object loaded\n    \"\"\"\n    with open(fname) as fin:\n        return yaml.safe_load(fin)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_config","title":"<code>load_config(fname)</code>","text":"<p>Load the configuration file of the framework</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the YAML config file to load</p> required Return <p>The loaded config file</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_config(fname: pathlib.Path) -&gt; Dict:\n    \"\"\"Load the configuration file of the framework\n\n    Arguments:\n        fname: the YAML config file to load\n\n    Return:\n        The loaded config file\n    \"\"\"\n    return load_yaml(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.download_url","title":"<code>download_url(url, save_to, verify=True)</code>","text":"<p>Download a dataset tarball.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>the object URL</p> required <code>save_to</code> <code>Path</code> <p>an optional destination folder. If dst is None, the tarball is placed at the root_folder specified by the archive (or downloadutils.DEFAULT_DOWNLOAD_FOLDER when no root_folder is specified). Note that if dst != root_folder, the internal metadata will be adjusted using install()</p> required <p>Returns:</p> Type Description <code>Path</code> <p>the path of the downloaded file</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def download_url(url: str, save_to: pathlib.Path, verify:bool =True) -&gt; pathlib.Path:\n    \"\"\"Download a dataset tarball.\n\n    Args:\n        url: the object URL\n        save_to: an optional destination folder. If dst is None, the tarball is placed at\n            the root_folder specified by the archive (or downloadutils.DEFAULT_DOWNLOAD_FOLDER\n            when no root_folder is specified). Note that if dst != root_folder,\n            the internal metadata will be adjusted using install()\n\n    Returns:\n        the path of the downloaded file\n    \"\"\"\n    save_to = pathlib.Path(save_to)\n\n    fname = pathlib.Path(url).name\n    save_as = save_to / fname\n\n    resp = requests.get(url, stream=True, verify=verify)\n    totalbytes = int(resp.headers.get(\"content-length\", 0))\n\n    if not save_as.parent.exists():\n        save_as.parent.mkdir(parents=True)\n\n    with open(str(save_as), \"wb\") as fout, richprogress.Progress(\n        richprogress.TextColumn(\"[progress.description]{task.description}\"),\n        richprogress.BarColumn(),\n        richprogress.FileSizeColumn(),\n        richprogress.TextColumn(\"/\"),\n        richprogress.TotalFileSizeColumn(),\n        richprogress.TextColumn(\"eta\"),\n        richprogress.TimeRemainingColumn(),\n        console=console,\n    ) as progressbar:\n        task_id = progressbar.add_task(\"Downloading...\", total=totalbytes)\n        for data in resp.iter_content(chunk_size=64 * 1024):\n            size = fout.write(data)\n            progressbar.advance(task_id, advance=size)\n\n    return save_as\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_datasets_root_folder","title":"<code>get_datasets_root_folder()</code>","text":"<p>Returns the path where datasets all datasets are installed</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_datasets_root_folder() -&gt; pathlib.Path:\n    \"\"\"Returns the path where datasets all datasets are installed\"\"\"\n    return _get_module_folder() / FOLDER_DATASETS\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_dataset_folder","title":"<code>get_dataset_folder(dataset_name)</code>","text":"<p>Returns the path where a specific datasets in installed</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_dataset_folder(dataset_name: str | DATASETS) -&gt; pathlib.Path:\n    \"\"\"Returns the path where a specific datasets in installed\"\"\"\n    if dataset_name:\n        dataset_name = str(dataset_name)\n    return _get_module_folder() / FOLDER_DATASETS / dataset_name\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.get_dataset_parquet_filename","title":"<code>get_dataset_parquet_filename(dataset_name, min_pkts=-1, split=None, animation=False)</code>","text":"<p>Returns the path of a dataset parquet file</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str | DATASETS</code> <p>The name of the dataset</p> required <code>min_pkts</code> <code>int</code> <p>the filtering rule applied when curating the datasets. If -1, load the unfiltered dataset</p> <code>-1</code> <code>split</code> <code>str</code> <p>if min_pkts!=-1, is used to request the loading of the split file. For DATASETS.UCDAVISICDM19 values can be \"human\", \"script\" or a number between 0 and 4. For all other dataset split can be anything which is not None (e.g., True)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>The pathlib.Path of a dataset parquet file</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def get_dataset_parquet_filename(\n    dataset_name: str | DATASETS, min_pkts: int = -1, split: str = None, animation=False\n) -&gt; pathlib.Path:\n    \"\"\"Returns the path of a dataset parquet file\n\n    Arguments:\n        dataset_name: The name of the dataset\n        min_pkts: the filtering rule applied when curating the datasets.\n            If -1, load the unfiltered dataset\n        split: if min_pkts!=-1, is used to request the loading of\n            the split file. For DATASETS.UCDAVISICDM19\n            values can be \"human\", \"script\" or a number\n            between 0 and 4.\n            For all other dataset split can be anything\n            which is not None (e.g., True)\n\n    Returns:\n        The pathlib.Path of a dataset parquet file\n    \"\"\"\n    dataset_folder = get_dataset_folder(dataset_name) / \"preprocessed\"\n    path = dataset_folder / f\"{dataset_name}.parquet\"\n\n    if isinstance(split, int) and split &lt; 0:\n        split = None\n\n    #    if isinstance(split, bool):\n    #        split = 0\n    #    elif isinstance(split, int):\n    #        split = str(split)\n    #\n    #    if min_pkts == -1 and (split is None or int(split) &lt; 0):\n    #        return path\n    #\n    if isinstance(dataset_name, str):\n        dataset_name = DATASETS.from_str(dataset_name)\n\n    if dataset_name != DATASETS.UCDAVISICDM19:\n        if min_pkts != -1:\n            dataset_folder /= \"imc23\"\n            if split is None:\n                path = (\n                    dataset_folder\n                    / f\"{dataset_name}_filtered_minpkts{min_pkts}.parquet\"\n                )\n            else:\n                path = (\n                    dataset_folder\n                    / f\"{dataset_name}_filtered_minpkts{min_pkts}_splits.parquet\"\n                )\n    else:\n        #        if split is None:\n        #            raise RuntimeError('split cannot be None for ucdavis-icdm19')\n        #        dataset_folder /= 'imc23'\n        #        if split in ('human', 'script'):\n        #            path = dataset_folder / f'test_split_{split}.parquet'\n        #        else:\n        #            if split == 'train':\n        #                split = 0\n        #            path = dataset_folder / f'train_split_{split}.parquet'\n\n        if split is not None:\n            dataset_folder /= \"imc23\"\n            if split in (\"human\", \"script\"):\n                path = dataset_folder / f\"test_split_{split}.parquet\"\n            else:\n                if split == \"train\":\n                    split = 0\n                path = dataset_folder / f\"train_split_{split}.parquet\"\n\n    return path\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_datasets_utils/#tcbench.libtcdatasets.datasets_utils.load_parquet","title":"<code>load_parquet(dataset_name, min_pkts=-1, split=None, columns=None, animation=False)</code>","text":"<p>Load and returns a dataset parquet file</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str | DATASETS</code> <p>The name of the dataset</p> required <code>min_pkts</code> <code>int</code> <p>the filtering rule applied when curating the datasets. If -1, load the unfiltered dataset</p> <code>-1</code> <code>split</code> <code>str</code> <p>if min_pkts!=-1, is used to request the loading of the split file. For DATASETS.UCDAVISICDM19 values can be \"human\", \"script\" or a number between 0 and 4. For all other dataset split can be anything which is not None (e.g., True)</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>A list of columns to load (if None, load all columns)</p> <code>None</code> <code>animation</code> <code>bool</code> <p>if True, create a loading animation on the console</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas dataframe and the related parquet file used to load the dataframe</p> Source code in <code>src/tcbench/libtcdatasets/datasets_utils.py</code> <pre><code>def load_parquet(\n    dataset_name: str | DATASETS,\n    min_pkts: int = -1,\n    split: str = None,\n    columns: List[str] = None,\n    animation: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Load and returns a dataset parquet file\n\n    Arguments:\n        dataset_name: The name of the dataset\n        min_pkts: the filtering rule applied when curating the datasets.\n            If -1, load the unfiltered dataset\n        split: if min_pkts!=-1, is used to request the loading of\n            the split file. For DATASETS.UCDAVISICDM19\n            values can be \"human\", \"script\" or a number\n            between 0 and 4.\n            For all other dataset split can be anything\n            which is not None (e.g., True)\n        columns: A list of columns to load (if None, load all columns)\n        animation: if True, create a loading animation on the console\n\n    Returns:\n        A pandas dataframe and the related parquet file used to load the dataframe\n    \"\"\"\n    path = get_dataset_parquet_filename(dataset_name, min_pkts, split)\n\n    import pandas as pd\n    from tcbench import cli\n\n    if animation:\n        with cli.console.status(f\"loading: {path}...\", spinner=\"dots\"):\n            return pd.read_parquet(path, columns=columns)\n    return pd.read_parquet(path, columns=columns)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/","title":"Tcbench libtcdatasets mirage19 json to parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.scan_for_strings","title":"<code>scan_for_strings(raw_packets, n_packets, min_len=5)</code>","text":"<p>Extract ASCII strings by processing packets payload</p> <p>Parameters:</p> Name Type Description Default <code>raw_packets</code> <code>List</code> <p>a list of list of raw packets bytes. The outer list represent a packet, the inner list the individual bytes (as integer values) of the payload</p> required <code>n_packets</code> <code>int</code> <p>maximum number of packets to process</p> required <code>min_len</code> <code>int</code> <p>minimum lenght of the strings to return</p> <code>5</code> Return <p>An array containing the identified string</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def scan_for_strings(raw_packets: List, n_packets: int, min_len: int = 5) -&gt; List[str]:\n    \"\"\"Extract ASCII strings by processing packets payload\n\n    Arguments:\n        raw_packets: a list of list of raw packets bytes. The outer list\n            represent a packet, the inner list the individual bytes (as integer values)\n            of the payload\n        n_packets: maximum number of packets to process\n        min_len: minimum lenght of the strings to return\n\n    Return:\n        An array containing the identified string\n    \"\"\"\n    strings = []\n    for pkt in raw_packets[:n_packets]:\n        if not pkt:\n            continue\n        text = \"\".join(char if char in VALIDCHARS else \"#\" for char in map(chr, pkt))\n        for string in re.findall(r\"[^#]+\", text):\n            if \"http:\" in string or (\n                len(string) &gt;= min_len and (\".\" in string or \" \" in string)\n            ):\n                strings.append(string)\n    return strings\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.flatten_dict","title":"<code>flatten_dict(data, parent_name=None)</code>","text":"<p>Helper function to flatten a nested dictionary. For example, the structure {\"a\":{\"b\":{\"c\":1, \"d\":2}}} is transformed into [(\"a_b_c\":1), (\"a_b_d\":2)]</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>the dictionary to explore</p> required <code>parent_name</code> <code>str</code> <p>the key associated the data currently processed</p> <code>None</code> Return <p>A list of (str, value) pairs where the str is the flattened key corresponding the the chaining of the nested key dictionary</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def flatten_dict(data: Dict, parent_name: str = None) -&gt; List:\n    \"\"\"Helper function to flatten a nested dictionary.\n    For example, the structure {\"a\":{\"b\":{\"c\":1, \"d\":2}}}\n    is transformed into [(\"a_b_c\":1), (\"a_b_d\":2)]\n\n    Arguments:\n        data: the dictionary to explore\n        parent_name: the key associated the data\n            currently processed\n\n    Return:\n        A list of (str, value) pairs where\n        the str is the flattened key corresponding\n        the the chaining of the nested key dictionary\n    \"\"\"\n    if not isinstance(data, dict):\n        return [(parent_name, data)]\n\n    new_items = []\n    for key, value in data.items():\n        new_key = key if not parent_name else f\"{parent_name}_{key}\"\n        new_items.extend(flatten_dict(value, new_key))\n    return new_items\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.convert_to_dataframe","title":"<code>convert_to_dataframe(data)</code>","text":"<p>Convert a nested dictionary into a pandas DataFrame</p> Argument <p>data: a (possibly) nested dictionary of key-value pairs</p> Return <p>A pandas DataFrame collecting the flattened keys of the nested dictionary and the related value</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def convert_to_dataframe(data: Dict) -&gt; pd.DataFrame:\n    \"\"\"Convert a nested dictionary into a pandas DataFrame\n\n    Argument:\n        data: a (possibly) nested dictionary of key-value pairs\n\n    Return:\n        A pandas DataFrame collecting the flattened keys\n        of the nested dictionary and the related value\n    \"\"\"\n    new_data = dict()\n    for net_tuple, value in data.items():\n        new_data[net_tuple] = dict(flatten_dict(value))\n\n    df = pd.DataFrame(new_data).T\n    df.columns = [col.lower() for col in df.columns]\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.worker","title":"<code>worker(fname, save_to=None)</code>","text":"<p>A helper function to transform a JSON MIRAGE input file into a pandas DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the JSON file to process</p> required <code>save_to</code> <code>str</code> <p>an optional file name where to store the transformed data as parquet</p> <code>None</code> Return <p>A pandas DataFrame with the loaded JSON data</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def worker(fname: str, save_to: str = None) -&gt; pd.DataFrame:\n    \"\"\"A helper function to transform a JSON MIRAGE input\n    file into a pandas DataFrame\n\n    Arguments:\n        fname: the JSON file to process\n        save_to: an optional file name where to store the\n            transformed data as parquet\n\n    Return:\n        A pandas DataFrame with the loaded JSON data\n    \"\"\"\n    with open(str(fname), \"r\") as fin:\n        data = json.load(fin)\n\n    for net_tuple in data:\n        payload_bytes = data[net_tuple][\"packet_data\"][\"L4_raw_payload\"]\n        data[net_tuple][\"strings\"] = scan_for_strings(payload_bytes, 10, 5)\n\n    df = convert_to_dataframe(data)\n\n    # extract name from filename\n    android_name = \"None\"\n    if \"None\" not in fname.name:\n        _1, android_name, *_ = fname.name.split(\"_\")\n    df = df.assign(\n        android_name=android_name,\n        device_name=fname.parent.name,\n    )\n\n    #    if progress:\n    #        print(f\".\", end=\"\", flush=True)\n\n    if save_to:\n        out_fname = save_to / fname.parent.name / fname.name\n        if not out_fname.parent.exists():\n            out_fname.parent.mkdir(parents=True)\n        out_fname = out_fname.with_suffix(\".parquet\")\n\n    if out_fname.exists():\n        raise RuntimeError(f\"file {out_fname} already exists!\")\n\n    df.to_parquet(out_fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.postprocess","title":"<code>postprocess(df)</code>","text":"<p>Process a the data loaded from MIRAGE JSON to (1) identify a background class; (2) remove invalid IPs (e.g., 127.0.0.1, 255.255.255.255); (3) add a unique \"row_id\" row; (4) add an \"app\" column with the label represented as category</p> Argument <p>df: the pandas DataFrame to process</p> Return <p>The modified version of the input data</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def postprocess(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process a the data loaded from MIRAGE JSON to\n    (1) identify a background class;\n    (2) remove invalid IPs (e.g., 127.0.0.1, 255.255.255.255);\n    (3) add a unique \"row_id\" row;\n    (4) add an \"app\" column with the label represented as category\n\n    Argument:\n        df: the pandas DataFrame to process\n\n    Return:\n        The modified version of the input data\n    \"\"\"\n    # create a background class\n    df = df.assign(\n        app=np.where(\n            df[\"android_name\"] == df[\"flow_metadata_bf_label\"],\n            df[\"android_name\"],\n            \"background\",\n        )\n    )\n\n    # split connection index to recover network tuple info\n    df = df.reset_index().rename({\"index\": \"conn_id\"}, axis=1)\n    df = df.assign(_tmp_col=df[\"conn_id\"].str.split(\",\"))\n    df = df.assign(\n        src_ip=df[\"_tmp_col\"].str[0],\n        src_port=df[\"_tmp_col\"].str[1],\n        dst_ip=df[\"_tmp_col\"].str[2],\n        dst_port=df[\"_tmp_col\"].str[3],\n        proto=df[\"_tmp_col\"].str[4],\n    ).drop(\"_tmp_col\", axis=1)\n\n    # drop invalid IPs\n    # df = df[(~df[\"src_ip\"].isin(INVALID_IPS)) &amp; (~df[\"dst_ip\"].isin(INVALID_IPS))]\n\n    # add a unique row_id\n    df = df.reset_index().rename({\"index\": \"row_id\"}, axis=1)\n\n    # enforce app to be categorical\n    df = df.assign(\n        app=df[\"app\"].astype(\"category\"),\n        packets=df[\"packet_data_l4_payload_bytes\"].apply(len),\n    )\n\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage19_json_to_parquet/#tcbench.libtcdatasets.mirage19_json_to_parquet.main","title":"<code>main(input_folder, save_as=None, workers=30)</code>","text":"<p>The main processing loop</p> Argument <p>input_folder: the folder where the MIRAGE JSON files are contained save_as: the output filename where to store the parquet     after loading the data workers: number of parallel workers to use for processing</p> Source code in <code>src/tcbench/libtcdatasets/mirage19_json_to_parquet.py</code> <pre><code>def main(\n    input_folder: str, save_as: pathlib.Path = None, workers: int = 30\n) -&gt; pd.DataFrame:\n    \"\"\"The main processing loop\n\n    Argument:\n        input_folder: the folder where the MIRAGE JSON files are contained\n        save_as: the output filename where to store the parquet\n            after loading the data\n        workers: number of parallel workers to use for processing\n    \"\"\"\n    input_folder = pathlib.Path(input_folder)\n    if (input_folder / \"MIRAGE-2019_traffic_dataset_downloadable\").exists():\n        input_folder /= \"MIRAGE-2019_traffic_dataset_downloadable\"\n\n    # creating a temporary folder\n    with tempfile.TemporaryDirectory() as tmp_folder:\n        tmp_folder = pathlib.Path(tmp_folder)\n\n        files = list(input_folder.glob(\"*/*.json\"))\n        if len(files) == 0:\n            raise RuntimeError(\n                f\"Did not find any .json file for input folder {input_folder} ! Make sure the input folder support a */*.json glob search\"\n            )\n\n        print(f\"found {len(files)} JSON files to load\")\n\n        func_worker = functools.partial(worker, save_to=tmp_folder)\n        # params = []\n        # for path in files:\n        #    params.append(\n        #        (\n        #            path,\n        #            True,\n        #            tmp_folder,\n        #        )\n        #    )\n\n        with richprogress.Progress(\n            richprogress.TextColumn(\"[progress.description]{task.description}\"),\n            richprogress.BarColumn(),\n            richprogress.MofNCompleteColumn(),\n            richprogress.TimeElapsedColumn(),\n            console=console,\n        ) as progressbar:\n            task_id = progressbar.add_task(\"Converting JSONs...\", total=len(files))\n            with Pool(workers) as pool:\n                # note: order does not matter because the worker\n                #   save output to file\n                for _ in pool.imap_unordered(func_worker, files):\n                    progressbar.advance(task_id)\n\n        print(\"merging files...\")\n        l = list(map(pd.read_parquet, tmp_folder.glob(\"*/*.parquet\")))\n        df = pd.concat(l)\n        df = postprocess(df)\n\n        if save_as is not None:\n            if not save_as.parent.exists():\n                save_as.parent.mkdir(parents=True)\n\n            print(f\"saving: {save_as}\")\n            df.to_parquet(save_as)\n\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_mirage22_json_to_parquet/","title":"Tcbench libtcdatasets mirage22 json to parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_mirage22_json_to_parquet/#tcbench.libtcdatasets.mirage22_json_to_parquet.postprocess","title":"<code>postprocess(df)</code>","text":"<p>Process the loaded MIRAGE JSON by (1) adding a background class; (2) adding an \"app\" column with label information, and encoding it as pandas category</p> Source code in <code>src/tcbench/libtcdatasets/mirage22_json_to_parquet.py</code> <pre><code>def postprocess(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process the loaded MIRAGE JSON by\n    (1) adding a background class;\n    (2) adding an \"app\" column with label information, and encoding it as pandas category\n    \"\"\"\n    df = df.assign(\n        app=np.where(\n            df[\"android_name\"] == df[\"flow_metadata_bf_label\"],\n            df[\"android_name\"],\n            \"background\",\n        )\n    )\n    df = df.assign(\n        app=np.where(\n            df[\"flow_metadata_bf_activity\"] == \"Unknown\", \"background\", df[\"app\"]\n        )\n    )\n    df = df.assign(\n        app=df[\"app\"].astype(\"category\"),\n        packets=df[\"packet_data_l4_payload_bytes\"].apply(len),\n    )\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_mirage19_generate_splits/","title":"Tcbench libtcdatasets tcbench mirage19 generate splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_mirage22_generate_splits/","title":"Tcbench libtcdatasets tcbench mirage22 generate splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/","title":"Tcbench libtcdatasets tcbench ucdavis icdm19 generate splits","text":"<p>This module is taking the monolithic parquet files generated using  ucdavis-icdm19_csv-to-parquet.py and create random \"splits\" for training.</p> <p>According to the logic of the paper those splits contains 100 samples from the /pretraining while two test splits are generated using the original /Retraining(human-triggered) and  /Retraining(script-triggered) partitions</p>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/#tcbench.libtcdatasets.ucdavis_icdm19_generate_splits.generate_train_splits","title":"<code>generate_train_splits(path, n_splits=5, seed=12345)</code>","text":"<p>Extract n_splits of 100 samples per classes</p> Source code in <code>src/tcbench/libtcdatasets/ucdavis_icdm19_generate_splits.py</code> <pre><code>def generate_train_splits(\n    path: pathlib.Path, n_splits: int = 5, seed=12345\n) -&gt; List[pd.DataFrame]:\n    \"\"\"Extract n_splits of 100 samples per classes\"\"\"\n    path = pathlib.Path(path)\n    save_to = path.parent\n\n    df = pd.read_parquet(path).reset_index(drop=True)\n\n    # Quote from Sec.3.1\n    # \"for training set we use only 100 \"triggered by script\" flows per class\"\n    #\n    # We interpret this as the training data is selected from the\n    # /pretraining folder of the original dataset\n    # as the other two subfolders have &lt; 100 samples\n    partition = \"pretraining\"\n    apps = df[\"app\"].unique()\n    n_samples = 100\n    rng = np.random.default_rng(seed)\n\n    df_tmp = df[df[\"partition\"] == partition]\n    train_splits = [[] for _ in range(n_splits)]\n    for app in apps:\n        indexes = df_tmp[df_tmp[\"app\"] == app].index.values\n        rng.shuffle(indexes)\n        indexes = indexes[: n_samples * n_splits]\n        for split_indexes, l_split in zip(np.split(indexes, n_splits), train_splits):\n            l_split.append(df_tmp.loc[split_indexes])\n\n    samples_expected = len(apps) * n_samples\n    for idx, l_split in enumerate(train_splits):\n        split = pd.concat(l_split)\n        samples_found = split.shape[0]\n        assert (\n            samples_found == samples_expected\n        ), f\"generated a split with {samples_found} samples rather than {samples_expected}\"\n\n        fname = path.parent / \"imc23\" / f\"train_split_{idx}.parquet\"\n        if not fname.parent.exists():\n            fname.parent.mkdir(parents=True)\n        print(f\"saving: {fname}\")\n        split.to_parquet(fname)\n        train_splits[idx] = split\n\n    split = pd.read_parquet(path.parent / \"imc23\" / f\"train_split_0.parquet\")\n    ser = split[\"app\"].value_counts()\n    rich_samples_count_report(\n        ser, title=f\"samples count : train_split = 0 to {n_splits-1}\"\n    )\n\n    return train_splits\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_ucdavis_icdm19_generate_splits/#tcbench.libtcdatasets.ucdavis_icdm19_generate_splits.generate_test_splits","title":"<code>generate_test_splits(path)</code>","text":"<p>Extract the predefined test splits from the monolithic parquet file</p> Source code in <code>src/tcbench/libtcdatasets/ucdavis_icdm19_generate_splits.py</code> <pre><code>def generate_test_splits(path: pathlib.Path) -&gt; List[pd.DataFrame]:\n    \"\"\"Extract the predefined test splits from the monolithic parquet file\"\"\"\n    path = pathlib.Path(path)\n    # print(f\"loading: {path}\")\n    df = pd.read_parquet(path).reset_index(drop=True)\n\n    df_test_human = df[df[\"partition\"] == \"retraining-human-triggered\"]\n    df_test_script = df[df[\"partition\"] == \"retraining-script-triggered\"]\n\n    for df_tmp, name in zip((df_test_human, df_test_script), (\"human\", \"script\")):\n        fname = path.parent / \"imc23\" / f\"test_split_{name}.parquet\"\n        if not fname.parent.exists():\n            fname.parent.mkdir(parents=True)\n        print(f\"\\nsaving: {fname}\")\n        df_tmp.to_parquet(fname)\n\n        ser = df_tmp[\"app\"].value_counts()\n        rich_samples_count_report(ser, title=f\"samples count : {fname.stem}\")\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/","title":"Tcbench libtcdatasets tcbench utmobilenet21 generate splits","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/#tcbench.libtcdatasets.utmobilenet21_generate_splits.filter_dataset","title":"<code>filter_dataset(df, min_pkts=10, min_samples_per_class=100)</code>","text":"<p>Remove flows with less than 10 packets and classes with less than 100 samples</p> Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_generate_splits.py</code> <pre><code>def filter_dataset(\n    df: pd.DataFrame, min_pkts: int = 10, min_samples_per_class: int = 100\n) -&gt; pd.DataFrame:\n    \"\"\"Remove flows with less than 10 packets and classes with less than 100 samples\"\"\"\n\n    # filtering out flows with less the 10 packets\n    df = df[df[\"packets\"] &gt; min_pkts]\n    filtered_samples_count = df[\"app\"].value_counts()\n\n    # removing classes with less than 100 samples\n    valid_classes = filtered_samples_count[\n        filtered_samples_count &gt; min_samples_per_class\n    ].index.tolist()\n    df = df[df[\"app\"].isin(valid_classes)]\n    final_samples_count = df[\"app\"].value_counts()\n    final_samples_count = final_samples_count[final_samples_count &gt; 0]\n    final_samples_count.name = \"expected_samples\"\n\n    df = df.drop(\"row_id\", axis=1)\n    df = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\n\n    df = df.set_index(\"row_id\", drop=False)\n    df.index.name = None\n\n    df = df.assign(app=df[\"app\"].astype(str).astype(\"category\"))\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_tcbench_utmobilenet21_generate_splits/#tcbench.libtcdatasets.utmobilenet21_generate_splits._verify_splits","title":"<code>_verify_splits(df, df_splits)</code>","text":"<p>Double check that by pulling the samples based on train/val/test indexes we obtain the per-class samples count</p> Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_generate_splits.py</code> <pre><code>def _verify_splits(df, df_splits):\n    \"\"\"Double check that by pulling the samples based\n    on train/val/test indexes we obtain the per-class samples count\n    \"\"\"\n\n    expected_samples_count = df[\"app\"].value_counts()\n    expected_samples_count.name = \"expected_samples\"\n\n    ser = df_splits.iloc[0]\n    train_indexes = ser[\"train_indexes\"]\n    val_indexes = ser[\"val_indexes\"]\n    test_indexes = ser[\"test_indexes\"]\n\n    df_tmp = pd.DataFrame(\n        (\n            df.iloc[train_indexes][\"app\"].value_counts(),\n            df.iloc[val_indexes][\"app\"].value_counts(),\n            df.iloc[test_indexes][\"app\"].value_counts(),\n        ),\n        index=[\"train_samples\", \"val_samples\", \"test_samples\"],\n    ).T\n    df_tmp = df_tmp.assign(total=df_tmp.sum(axis=1))\n    df_tmp = pd.concat((df_tmp, expected_samples_count), axis=1)\n\n    assert (df_tmp[\"total\"] == df_tmp[\"expected_samples\"]).all()\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/","title":"Tcbench libtcdatasets ucdavis icdm19 csv to parquet","text":"<p>This module is preprocessing the original CSV files of the ucdavis-icdm19 paper  (one csv file for each differen flow) to generate a single parquet file where each  row encodes the per-flow timeseries and  numpy arrays</p> <p>The input dataset is composed of three set  of files in 3 subfolders: /pretraining, /Retraining(human-triggered) and /Retraining(script-triggered) ---we call these partitions.</p> <p>Within each partition, there are 5 subfolders, one for each application.</p> <p>For each application, a different file represents a different flow (the flowid is the filename itself). Each flow reports 4 columns corresponding to \"absolute time\", \"relative time to first packet\", \"packet size\" and \"direction\" of each individual packet of a flow.</p> <p>The aim of this module is load all CSVs (across partitions and apps) into a single parquet file where each row corresponds to a different flow described by the following properties - row_id: a unique row index - app: one of ['google-doc', 'google-drive', 'google-music', 'google-search', 'youtube'] - flow_id: the original file name without extension - partition: one of ['pretraining', 'retraining-human-triggered', 'retraining-script-triggered'] - num_pkts: number of packets in the flow   - duration: duration of the flow - bytes: number of bytes of the flow - pkts_unixtime: np.array with the absolute time of each packet - timetofirst: np.array with relative time of each packet with respect to the first packet - pkts_size: np.array with each packet size - pkts_dir: np.array with each packet direction (0 or 1) - pkts_iat: np.array with inter packet time</p>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/#tcbench.libtcdatasets.ucdavis_icdm19_csv_to_parquet.worker","title":"<code>worker(fname)</code>","text":"<p>Helper function to load an individual CSV file into a pandas DataFrame</p> Source code in <code>src/tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py</code> <pre><code>def worker(fname: pathlib.Path) -&gt; pd.DataFrame:\n    \"\"\"Helper function to load an individual CSV file\n    into a pandas DataFrame\n    \"\"\"\n    fname = pathlib.Path(fname)\n    app = fname.parts[-2]\n    partition = fname.parts[-3]\n\n    df = pd.read_csv(\n        fname,\n        sep=\"\\t\",\n        names=[\"unixtime\", \"timetofirst\", \"pkts_size\", \"pkts_dir\"],\n        dtype=dict(\n            unixtime=np.float64,\n            timetofirst=np.float64,\n            pkts_size=np.int16,\n            pkts_dir=np.int8,\n        ),\n    )\n\n    df_new = pd.DataFrame(\n        [\n            [\n                app.lower().replace(\" \", \"-\"),  # app\n                fname.stem,  # flowid\n                partition.lower().replace(\"(\", \"-\").replace(\")\", \"\"),  # partition\n                df.shape[0],  # num_pkts\n                df[\"timetofirst\"].values[-1],  # duration\n                df[\"pkts_size\"].sum(),  # bytes\n                df[\"unixtime\"].values,  # unixtime\n                df[\"timetofirst\"].values,  # timetofirst\n                df[\"pkts_size\"].values,  # pkts_size\n                df[\"pkts_dir\"].values,  # pkts_dir\n                df[\"timetofirst\"].diff().fillna(0).values,  # pkts_iat\n            ]\n        ],\n        columns=[\n            \"app\",\n            \"flow_id\",\n            \"partition\",\n            \"num_pkts\",\n            \"duration\",\n            \"bytes\",\n            \"unixtime\",\n            \"timetofirst\",\n            \"pkts_size\",\n            \"pkts_dir\",\n            \"pkts_iat\",\n        ],\n    )\n\n    return df_new\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_ucdavis_icdm19_csv_to_parquet/#tcbench.libtcdatasets.ucdavis_icdm19_csv_to_parquet.load_files","title":"<code>load_files(folder, n_workers=20)</code>","text":"<p>Load all CSVs (across the 3 dataset partitions) using multiprocessing and concatenate them into a single DataFrame</p> Source code in <code>src/tcbench/libtcdatasets/ucdavis_icdm19_csv_to_parquet.py</code> <pre><code>def load_files(folder: pathlib.Path, n_workers=20) -&gt; pd.DataFrame:\n    \"\"\"Load all CSVs (across the 3 dataset partitions)\n    using multiprocessing and concatenate them into a single DataFrame\n    \"\"\"\n    partitions = [\n        \"pretraining\",\n        \"Retraining(human-triggered)\",\n        \"Retraining(script-triggered)\",\n    ]\n    app_names = [\n        \"Google Doc\",\n        \"Google Drive\",\n        \"Google Music\",\n        \"Google Search\",\n        \"Youtube\",\n    ]\n\n    # check that we have all folder\n    subfolders = list(itertools.product(partitions, app_names))\n    files = []\n    for partition, app in subfolders:\n        path = folder / partition / app\n        if not path.exists():\n            raise RuntimeError(f\"missing {path}\")\n        files.extend(list(path.glob(\"*.txt\")))\n\n    files = sorted(files)\n    print(f\"found {len(files)} CSV files to load\")\n\n    from tcbench.cli import get_rich_console\n\n    with Progress(console=get_rich_console()) as progress:\n        task_id = progress.add_task(\"Converting CSVs...\", total=len(files))\n        l = []\n        with multiprocessing.Pool(n_workers) as pool:\n            for item in pool.imap(worker, files):\n                l.append(item)\n                progress.advance(task_id)\n    print(f\"concatenating files\")\n\n    # sorting to make sure that\n    # multiprocessing does not (unintentionally)\n    # breaks ordering (for replicability)\n    df = pd.concat(l, axis=0).sort_values(by=[\"partition\", \"flow_id\"])\n\n    # adding a unique row\n    df = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\n    df = df.assign(app=df[\"app\"].astype(\"category\"))\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/","title":"Tcbench libtcdatasets utmobilenet21 csv to parquet","text":""},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.create_connection_id","title":"<code>create_connection_id(ser)</code>","text":"<p>Associate a 5-tuple connection id to a flow</p> <p>Parameters:</p> Name Type Description Default <code>ser</code> <code>Series</code> <p>a pandas Series with per-flow data</p> required Return <p>A string encoding the 5-tuple connection id</p> Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def create_connection_id(ser: pd.Series) -&gt; str:\n    \"\"\"Associate a 5-tuple connection id to a flow\n\n    Arguments:\n        ser: a pandas Series with per-flow data\n\n    Return:\n        A string encoding the 5-tuple connection id\n    \"\"\"\n    proto = ser[\"ip_proto\"]\n    src_ip, dst_ip = ser[[\"ip_src\", \"ip_dst\"]].values\n    if proto == 6:\n        src_port, dst_port = ser[[\"tcp_srcport\", \"tcp_dstport\"]].values\n    else:\n        src_port, dst_port = ser[[\"udp_srcport\", \"udp_dstport\"]].values\n\n    if src_ip &gt; dst_ip:\n        src_ip, src_port, dst_ip, dst_port = dst_ip, dst_port, src_ip, src_port\n    conn_id = f\"{src_ip}_{src_port}_{dst_ip}_{dst_port}_{proto}\"\n    return conn_id\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.worker","title":"<code>worker(fname, tmp_folder)</code>","text":"<p>Helper function responsible for processing a utmobilenet21 CSV and save it into a temporary folder</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the input CSV to process</p> required <code>tmp_folder</code> <code>str</code> <p>the temporary folder where to store intermediate output</p> required Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def worker(fname: str, tmp_folder: str) -&gt; None:\n    \"\"\"Helper function responsible for processing a\n    utmobilenet21 CSV and save it into a temporary folder\n\n    Arguments:\n        fname: the input CSV to process\n        tmp_folder: the temporary folder where to store\n            intermediate output\n    \"\"\"\n    # load everything as string (type casting moved later)\n    df = pd.read_csv(fname, dtype=str)\n    df = df.assign(\n        fname=fname.name,\n        partition=fname.parent.name,\n    )\n\n    # reformat columns name\n    df.columns = [col.replace(\".\", \"_\") for col in df.columns]\n\n    if \"location\" not in df.columns:\n        df = df.assign(location=\"\")\n\n    # drop columns not needed\n    df = df[COLUMNS_TO_KEEP]\n\n    # keep only TCP and UDP\n    df = df[\n        (df[\"ip_proto\"].isin({\"6\", \"17\", \"6.0\", \"17.0\"}))\n        &amp; (~df[\"ip_src\"].isna())\n        &amp; (~df[\"ip_dst\"].isna())\n        &amp; (~df[\"ip_src\"].isin(INVALID_IPS))\n        &amp; (~df[\"ip_dst\"].isin(INVALID_IPS))\n    ]\n\n    # extract app label\n    func_parse_timestamp = functools.partial(\n        dateutil.parser.parse, tzinfos={\"CDT\": -5 * 3600}\n    )\n    df = df.assign(\n        app=df[\"fname\"].apply(lambda text: text.split(\" \", 1)[0]),\n        frame_time=df[\"frame_time\"]\n        .apply(lambda text: float(func_parse_timestamp(text).strftime(\"%s.%f\")))\n        .astype(float),\n        conn_id=df.apply(create_connection_id, axis=1).astype(str),\n    ).to_parquet(tmp_folder / fname.with_suffix(\".parquet\").name)\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.create_time_series","title":"<code>create_time_series(df)</code>","text":"<p>Compose flow time series by grouping packets the same flow into numpy arrays</p> Argument <p>df: the input pandas DataFrame where each entry corresponds to a different packet</p> Return <p>The generate per-flow pandas DataFrame containing the following columns (\"src_ip\", \"src_port\", \"dst_ip\", \"dst_port\", \"ip_proto\", \"first\", \"last\", \"duration\", \"packets\", \"bytes\", \"timetofirst\", \"pkts_size\", \"pkts_dir\", \"partition\", \"location\", \"fname\", \"app\" )</p> Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def create_time_series(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compose flow time series by grouping packets the same flow into\n    numpy arrays\n\n    Argument:\n        df: the input pandas DataFrame where each entry corresponds to a different packet\n\n    Return:\n        The generate per-flow pandas DataFrame containing the following\n        columns (\"src_ip\", \"src_port\", \"dst_ip\", \"dst_port\", \"ip_proto\",\n        \"first\", \"last\", \"duration\", \"packets\", \"bytes\", \"timetofirst\",\n        \"pkts_size\", \"pkts_dir\", \"partition\", \"location\", \"fname\", \"app\"\n        )\n    \"\"\"\n    if df.shape[0] == 0:\n        return pd.DataFrame(\n            columns=[\n                \"src_ip\",\n                \"src_port\",\n                \"dst_ip\",\n                \"dst_port\",\n                \"ip_proto\",\n                \"first\",\n                \"last\",\n                \"duration\",\n                \"packets\",\n                \"bytes\",\n                \"timetofirst\",\n                \"pkts_size\",\n                \"pkts_dir\",\n                \"partition\",\n                \"location\",\n                \"fname\",\n                \"app\",\n            ]\n        )\n\n    df_tmp = df.sort_values(by=\"frame_time\")\n\n    first_pkt = df_tmp.iloc[0]\n\n    src_ip, dst_ip = first_pkt[\"ip_src\"], first_pkt[\"ip_dst\"]\n    first = first_pkt[\"frame_time\"]\n    last = df_tmp.iloc[-1][\"frame_time\"]\n    duration = last - first\n    packets = df_tmp.shape[0]\n    ip_proto = first_pkt[\"ip_proto\"]\n    partition = first_pkt[\"partition\"]\n    location = first_pkt[\"location\"]\n    fname = first_pkt[\"fname\"]\n    app = fname.split(\"_\", 1)[0]\n\n    if df_tmp[\"ip_proto\"].values[0] == 6:\n        pkts_size = df_tmp[\"tcp_len\"].fillna(0).values\n        src_port, dst_port = first_pkt[\"tcp_srcport\"], first_pkt[\"tcp_dstport\"]\n    else:\n        pkts_size = df_tmp[\"udp_length\"].fillna(0).values\n        src_port, dst_port = first_pkt[\"udp_srcport\"], first_pkt[\"udp_dstport\"]\n    _bytes = pkts_size.sum()\n    timetofirst = df_tmp[\"frame_time\"].diff().fillna(0).values\n\n    pkts_dir = (df_tmp[\"ip_src\"] == first_pkt[\"ip_src\"]).astype(int).values\n\n    df_res = pd.DataFrame(\n        [\n            [\n                src_ip,\n                src_port,\n                dst_ip,\n                dst_port,\n                ip_proto,\n                first,\n                last,\n                duration,\n                packets,\n                _bytes,\n                timetofirst,\n                pkts_size,\n                pkts_dir,\n                partition,\n                location,\n                fname,\n                app,\n            ]\n        ],\n        columns=[\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"ip_proto\",\n            \"first\",\n            \"last\",\n            \"duration\",\n            \"packets\",\n            \"bytes\",\n            \"timetofirst\",\n            \"pkts_size\",\n            \"pkts_dir\",\n            \"partition\",\n            \"location\",\n            \"fname\",\n            \"app\",\n        ],\n    )\n    return df_res\n</code></pre>"},{"location":"tcbench/api/tcbench_libtcdatasets_utmobilenet21_csv_to_parquet/#tcbench.libtcdatasets.utmobilenet21_csv_to_parquet.main","title":"<code>main(args)</code>","text":"<p>Main function loading CSVs and converting them into a monolithic parquet file</p> Source code in <code>src/tcbench/libtcdatasets/utmobilenet21_csv_to_parquet.py</code> <pre><code>def main(args: argparse.Namespace):\n    \"\"\"Main function loading CSVs and converting them into\n    a monolithic parquet file\n    \"\"\"\n    if (args.input_folder / \"csvs\").exists():\n        args.input_folder /= \"csvs\"\n\n    with Client(n_workers=args.num_workers) as client:\n        staging_folder = pathlib.Path(args.tmp_staging_folder)\n        output_folder = pathlib.Path(args.output_folder)\n\n        extra_line = False\n        for partition in args.input_folder.iterdir():\n            if extra_line:\n                print()\n            print(f\"processing: {partition}\")\n            extra_line = True\n\n            files = list(partition.glob(\"*.csv\"))\n\n            partition_name = partition.name.lower().replace(\" \", \"_\")\n            curr_staging_folder = staging_folder / partition_name\n\n            if curr_staging_folder.exists():\n                shutil.rmtree(str(curr_staging_folder))\n            curr_staging_folder.mkdir(parents=True)\n\n            ##############\n            # stage1: convert csv to parquet + some cleaning\n            ##############\n            (curr_staging_folder / \"stage1\").mkdir(parents=True)\n            func = functools.partial(worker, tmp_folder=curr_staging_folder / \"stage1\")\n            print(f\"found {len(files)} files\")\n            with richprogress.Progress(\n                richprogress.TextColumn(\"[progress.description]{task.description}\"),\n                richprogress.BarColumn(),\n                richprogress.MofNCompleteColumn(),\n                richprogress.TimeElapsedColumn(),\n                console=console,\n            ) as progressbar:\n                task_id = progressbar.add_task(\"Converting CSVs...\", total=len(files))\n                with multiprocessing.Pool(min(len(files), 30)) as pool:\n                    for item in pool.imap_unordered(func, files):\n                        progressbar.advance(task_id)\n            print(\"stage1 completed\")\n\n            #########################\n            # stage2: repartition (if needed)\n            #########################\n            ddf = dd.read_parquet(curr_staging_folder / \"stage1\")\n            if len(files) &gt; 1000:\n                ddf = ddf.reset_index(drop=True).repartition(50)\n            ddf.reset_index(drop=True).persist()\n            progress(ddf)\n            ddf.to_parquet(curr_staging_folder / \"stage2\")\n            print(\"stage2 completed\")\n\n            #########################\n            # stage3: minor types conversion\n            #########################\n            ddf = dd.read_parquet(curr_staging_folder / \"stage2\").reset_index(drop=True)\n            # convert all to float (because some numbers are float?)\n            # then convert to int64 (which supports nan)\n            ddf1 = ddf.astype(\n                {\n                    \"frame_time\": float,\n                    \"ip_proto\": float,\n                    \"tcp_len\": float,\n                    \"tcp_srcport\": float,\n                    \"tcp_dstport\": float,\n                    \"udp_srcport\": float,\n                    \"udp_dstport\": float,\n                    \"udp_length\": float,\n                }\n            ).persist()\n            progress(ddf1)\n            ddf1.to_parquet(curr_staging_folder / \"stage3\")  # , schema=pa_schema)\n            print(\"stage3 completed\")\n\n            #########################\n            # last: time series creation\n            #########################\n            ddf = dd.read_parquet(curr_staging_folder / \"stage3\").reset_index(drop=True)\n\n            meta = pd.DataFrame(\n                dtype=object,\n                columns=[\n                    \"src_ip\",\n                    \"src_port\",\n                    \"dst_ip\",\n                    \"dst_port\",\n                    \"ip_proto\",\n                    \"first\",\n                    \"last\",\n                    \"duration\",\n                    \"packets\",\n                    \"bytes\",\n                    \"timetofirst\",\n                    \"pkts_size\",\n                    \"pkts_dir\",\n                    \"partition\",\n                    \"location\",\n                    \"fname\",\n                    \"app\",\n                ],\n            )\n            meta = meta.astype(\n                {\n                    \"src_ip\": str,\n                    \"dst_ip\": str,\n                    \"src_port\": int,\n                    \"dst_port\": int,\n                    \"ip_proto\": int,\n                    \"first\": float,\n                    \"last\": float,\n                    \"duration\": float,\n                    \"packets\": int,\n                    \"bytes\": int,\n                    \"partition\": str,\n                    \"location\": str,\n                    \"fname\": str,\n                    \"app\": str,\n                }\n            )\n            schema = dict(meta.dtypes)\n            pa_schema = pa.schema(\n                [\n                    (\n                        name.lower(),\n                        pa.string()\n                        if dtype == np.dtype(object)\n                        else pa.from_numpy_dtype(dtype),\n                    )\n                    for name, dtype in schema.items()\n                    if name not in (\"timetofirst\", \"pkts_size\", \"pkts_dir\")\n                ]\n            )\n            pa_schema = pa_schema.append(pa.field(\"pkts_size\", pa.list_(pa.int64())))\n            pa_schema = pa_schema.append(pa.field(\"pkts_dir\", pa.list_(pa.int64())))\n            pa_schema = pa_schema.append(\n                pa.field(\"timetofirst\", pa.list_(pa.float64()))\n            )\n\n            ddf2 = ddf.groupby(\"conn_id\").apply(create_time_series, meta=meta).persist()\n            progress(ddf2)\n            ddf2.reset_index(drop=True).to_parquet(\n                curr_staging_folder / \"stage4\", schema=pa_schema\n            )\n            print(\"stage4 completed\")\n\n            ### we can finally pack everything together\n            df = (\n                dd.read_parquet(curr_staging_folder / \"stage4\")\n                .reset_index(drop=True)\n                .compute()\n            )\n            fname = (staging_folder / partition_name).with_suffix(\".parquet\")\n            if not fname.parent.exists():\n                fname.parent.mkdir(parents=True)\n            print(f\"saving: {fname}\")\n            df.to_parquet(fname)\n\n        print(\"merging all partitions\")\n        if not output_folder.exists():\n            output_folder.mkdir(parents=True)\n        df = dd.read_parquet(staging_folder / \"*.parquet\").compute()\n        df = df.reset_index(drop=True).reset_index().rename({\"index\": \"row_id\"}, axis=1)\n        df = df.assign(app=df[\"app\"].astype(\"category\"))\n\n        fname = output_folder / \"utmobilenet21.parquet\"\n        print(f\"saving: {fname}\")\n        df.to_parquet(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/","title":"Tcbench modeling aimutils","text":"<p>This module contains a collection  of utility functions used to interact with a AIM repository</p>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.list_repo","title":"<code>list_repo(repo)</code>","text":"<p>List all runs in the repository as pandas DataFrame</p> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def list_repo(repo: aim.Repo) -&gt; pd.DataFrame:\n    \"\"\"List all runs in the repository as pandas DataFrame\"\"\"\n    #return pd.concat(run.dataframe() for run in repo.iter_runs())\n    return pd.concat(\n        item.run.dataframe()\n        for item in repo.query_runs('', report_mode=0).iter_runs()\n    )\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.get_latest_campaign_id","title":"<code>get_latest_campaign_id(repo, experiment_name)</code>","text":"<p>Extract the latest campaign id (defined when launching a modeling campaign) from a AIM repo</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>the AIM repository to use</p> required <code>experiment_name</code> <code>str</code> <p>the experiment name from which extract the last campaign</p> required Return <p>The campaign id found</p> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def get_latest_campaign_id(repo: aim.Repo, experiment_name: str) -&gt; str:\n    \"\"\"Extract the latest campaign id (defined when launching\n    a modeling campaign) from a AIM repo\n\n    Arguments:\n        repo: the AIM repository to use\n        experiment_name: the experiment name from which extract the last campaign\n\n    Return:\n        The campaign id found\n    \"\"\"\n    query = f\"run.experiment == '{experiment_name}'\"\n    df = pd.concat(\n        [\n            entry.run.dataframe()\n            for entry in repo.query_runs(query, report_mode=0).iter_runs()\n        ]\n    )\n    campaign_ids = df[\"hparams.campaign_id\"].replace({np.nan: \"0\"})\n    return campaign_ids.unique().max()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.load_campaign","title":"<code>load_campaign(repo, campaign_id=None, experiment_name=None)</code>","text":"<p>Load the latest campaign of experiment into a pandas DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>the AIM repository to query</p> required <code>campaign_id</code> <code>str</code> <p>the campaing_id of the runs to select (if None, it will search for the latest campaign_id)</p> <code>None</code> <code>experiment_name</code> <code>str</code> <p>the experiment_name for the runs to select</p> <code>None</code> Return <p>A tuple with two objects: a DataFrame collecting the informatio for all the runs associated to the campaign, and a list of run object</p> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def load_campaign(\n    repo: aim.Repo, \n    campaign_id: str = None,\n    experiment_name: str = None\n) -&gt; Tuple[pd.DataFrame, List[aim.Run]]:\n    \"\"\"Load the latest campaign of experiment into a pandas DataFrame\n\n    Arguments:\n        repo: the AIM repository to query\n        campaign_id: the campaing_id of the runs to select (if None,\n            it will search for the latest campaign_id)\n        experiment_name: the experiment_name for the runs to select\n\n    Return:\n        A tuple with two objects: a DataFrame collecting the informatio\n        for all the runs associated to the campaign, and a list\n        of run object\n    \"\"\"\n    if campaign_id is None:\n        campaign_id = get_latest_campaign_id(repo, args.experiment_name)\n        print(f\"latest campaign_id: {campaign_id}\")\n\n    query = f\"\"\"\n    run.hparams[\"campaign_id\"] == '{campaign_id}'\n    \"\"\"\n    if experiment_name != '':\n        query += f\"and run.experiment == '{experiment_name}'\"\n\n    runs = []\n    l = []\n    for entry in repo.query_runs(query, report_mode=0).iter_runs():\n        l.append(entry.run.dataframe())\n        runs.append(entry.run)\n\n    df = pd.concat(l)\n\n    ## remove __system columns\n    cols_to_drop = [col for col in df.columns if col.startswith(\"__system\")]\n    if cols_to_drop:\n        df = df.drop(cols_to_drop, axis=1)\n\n    ## rename hparams.blablabla\n    rename_cols = {col: col.replace(\"hparams.\", \"\") for col in df.columns}\n    if rename_cols:\n        df = df.rename(rename_cols, axis=1)\n\n    return df, runs\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.query_metric","title":"<code>query_metric(repo, run_hashes, metric, context)</code>","text":"<p>Collect all metrics associated to a list of runs</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>the AIM repo to query</p> required <code>run_hashes</code> <code>List[str]</code> <p>a list of run hash values to search</p> required <code>metric</code> <code>str</code> <p>the metric to extract (e.g., \"acc\")</p> required <code>context</code> <code>str</code> <p>the split on which the metric was computed (e.g., \"test\")</p> required Return <p>A dictionary where keys maps to run hashes, and values to the related metric found</p> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def query_metric(\n    repo: aim.Repo,\n    run_hashes: List[str],\n    metric: str,\n    context: str,\n) -&gt; Dict[str, Any]:\n    \"\"\"Collect all metrics associated to a list of runs\n\n    Arguments:\n        repo: the AIM repo to query\n        run_hashes: a list of run hash values to search\n        metric: the metric to extract (e.g., \"acc\")\n        context: the split on which the metric was computed (e.g., \"test\")\n\n    Return:\n        A dictionary where keys maps to run hashes, and values\n        to the related metric found\n    \"\"\"\n    query = \"\"\"\n    metric.name == '{metric}' and \n    metric.context['subset'] == '{context}'\n    \"\"\".format(\n        metric=metric, context=context\n    )\n    query = f\"run.hash in {run_hashes} and {query}\"\n\n    metrics = {\n        item.run.hash: item.values.last()[1]\n        for item in repo.query_metrics(query, report_mode=0)\n    }\n\n    if len(metrics) != len(run_hashes):\n        missing_hashes = set(run_hashes) - set(metrics.keys())\n        print(\n            f\"WARNING: found {len(metrics)} metrics for metric={metric} context={context}\"\n        )\n        for run_hash in missing_hashes:\n            if run_hash not in metrics:\n                metrics[run_hash] = np.nan\n\n    return metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.metrics_to_pandas","title":"<code>metrics_to_pandas(repo, df_run, metrics, contexts)</code>","text":"<p>Loads a set of metrics across multiple test splits into a dataframe already containing campaign related information</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>the AIM repo to query</p> required <code>df_run</code> <code>DataFrame</code> <p>a dataframe obtained invoking .load_latest_campaign()</p> required <code>metrics</code> <code>List[str]</code> <p>the list of metrics to load</p> required <code>contexts</code> <code>List[str]</code> <p>the set of split names to query metrics from</p> required Return <p>Expands df_campaign by adding columns related to the loaded metric values</p> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def metrics_to_pandas(\n    repo: aim.Repo,\n    df_run: pd.DataFrame,\n    metrics: List[str],\n    contexts: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"Loads a set of metrics across multiple test splits into\n    a dataframe already containing campaign related information\n\n    Arguments:\n        repo: the AIM repo to query\n        df_run: a dataframe obtained invoking .load_latest_campaign()\n        metrics: the list of metrics to load\n        contexts: the set of split names to query metrics from\n\n    Return:\n        Expands df_campaign by adding columns related to the loaded metric values\n    \"\"\"\n\n    df_campaign = df_run.copy()\n\n    ## add (empty) metrics columns\n    for mtr in metrics:\n        df_campaign.loc[:, mtr] = np.nan\n\n    df_campaign = df_campaign.assign(test_split_name=None)\n\n    l = []\n    for ctx in contexts: \n        with console.status(f\"collecting metrics {ctx}...\", spinner=\"dots\"):\n            df_new = df_campaign.copy()\n\n            df_new = df_new.assign(test_split_name=ctx)\n            for mtr in metrics:\n                df_new.loc[:, mtr] = df_new[\"hash\"].copy()\n                metric_dict = query_metric(\n                    repo=repo,\n                    run_hashes=set(df_campaign[\"hash\"].values),\n                    metric=mtr,\n                    context=ctx,\n                )\n                df_new.loc[:, mtr] = df_new[mtr].replace(metric_dict)\n            l.append(df_new)\n\n    df_new = pd.concat(l)\n\n    return df_new\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_aimutils/#tcbench.modeling.aimutils.track_metrics","title":"<code>track_metrics(tracker, metrics, context, epoch=None)</code>","text":"<p>Save into a run metrics information</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Run</code> <p>the AIM run where to save metrics</p> required <code>metrics</code> <code>Dict[str, Any]</code> <p>a dictionary of key-value pairs to save</p> required <code>context</code> <code>str</code> <p>the context related to the metrics (e.g., train/val/test)</p> required <code>epoch</code> <code>int</code> <p>the epoch when the metrics where collected</p> <code>None</code> Source code in <code>src/tcbench/modeling/aimutils.py</code> <pre><code>def track_metrics(\n    tracker: aim.Run, metrics: Dict[str, Any], context: str, epoch: int = None\n) -&gt; None:\n    \"\"\"Save into a run metrics information\n\n    Arguments:\n        tracker: the AIM run where to save metrics\n        metrics: a dictionary of key-value pairs to save\n        context: the context related to the metrics (e.g., train/val/test)\n        epoch: the epoch when the metrics where collected\n    \"\"\"\n    for name, value in metrics.items():\n        tracker.track(value, name, epoch=epoch, context=dict(subset=context))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/","title":"Tcbench modeling augmentation","text":"<p>This module contains the logic to generate Flowpic data representations and apply augmentations on either flowpic or raw time series.</p> <p>Each augmentation is handled as a subclass of Augmentation which is a callable object.</p> <p>Moreover, each augmentation is designed to have its own  random generator and a set of hyperparameters which which  generate the parameters of an augmentation. Differently from pytorch APIs, this enables visibility on the set of params used for an augmentation (use .get_params())</p>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.Augmentation","title":"<code>Augmentation</code>","text":"<p>Base class for augmentation functions</p> <p>Attributes:</p> Name Type Description <code>rng</code> <p>the numpy random generator used for sampling parameters</p> <code>hyper_params</code> <p>a dictionary of hypter parameters to set up the sampling of the augmentation parameters</p> <code>paramgs</code> <p>a dictionary with the latest parameters generated for a transformation</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class Augmentation:\n    \"\"\"\n    Base class for augmentation functions\n\n    Attributes:\n        rng: the numpy random generator used for sampling parameters\n        hyper_params: a dictionary of hypter parameters to set up the\n            sampling of the augmentation parameters\n        paramgs: a dictionary with the latest parameters generated\n            for a transformation\n    \"\"\"\n\n    def __init__(\n        self,\n        rng: np.random.Generator,\n        randomize_at_every_call: bool = True,\n        **hyper_params: Dict[str, Any],\n    ):\n        \"\"\"\n        Arguments:\n            rng: a numpy random number generator\n            randomize_at_every_call: if True, the parameters for the augmentation\n                are generated at each call\n        \"\"\"\n        self.rng = rng\n        self.hyper_params = hyper_params\n        self.randomize_at_every_call = randomize_at_every_call\n        self.is_first_call = True\n        self.params = {}\n\n    def update_params(self) -&gt; None:\n        pass\n\n    def get_params(self) -&gt; Dict[str, Any]:\n        return self.params\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.Augmentation.__init__","title":"<code>__init__(rng, randomize_at_every_call=True, **hyper_params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>a numpy random number generator</p> required <code>randomize_at_every_call</code> <code>bool</code> <p>if True, the parameters for the augmentation are generated at each call</p> <code>True</code> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def __init__(\n    self,\n    rng: np.random.Generator,\n    randomize_at_every_call: bool = True,\n    **hyper_params: Dict[str, Any],\n):\n    \"\"\"\n    Arguments:\n        rng: a numpy random number generator\n        randomize_at_every_call: if True, the parameters for the augmentation\n            are generated at each call\n    \"\"\"\n    self.rng = rng\n    self.hyper_params = hyper_params\n    self.randomize_at_every_call = randomize_at_every_call\n    self.is_first_call = True\n    self.params = {}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationRotate","title":"<code>AugmentationRotate</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for random rotation</p> <p>The rotation can be configured passing a \"min_degree\" and \"max_degree\" as hyper parameters (-10, 10) by default.</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationRotate(Augmentation):\n    \"\"\"\n    An augmentation for random rotation\n\n    The rotation can be configured passing a \"min_degree\" and \"max_degree\"\n    as hyper parameters (-10, 10) by default.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.hyper_params.setdefault(\"min_degree\", -10)\n        self.hyper_params.setdefault(\"max_degree\", 10)\n        self.params = dict(angle=0)\n\n    def update_params(self) -&gt; None:\n        self.params[\"angle\"] = self.rng.uniform(\n            self.hyper_params[\"min_degree\"], self.hyper_params[\"max_degree\"]\n        )\n\n    def __call__(self, mtx: np.array) -&gt; np.array:\n        if self.is_first_call or self.randomize_at_every_call:\n            self.update_params()\n        self.is_first_call = False\n        tensor = numpy_to_tensor(mtx)\n\n        # .rotate() is counter-clockwise (which\n        # is counter intuitive, so we change the sign\n        tensor = T.functional.rotate(tensor, -self.params[\"angle\"])\n        return tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationHorizontalFlip","title":"<code>AugmentationHorizontalFlip</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for static horizontal flip</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationHorizontalFlip(Augmentation):\n    \"\"\"\n    An augmentation for static horizontal flip\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(rng=None)\n\n    def __call__(self, mtx: np.array) -&gt; np.array:\n        tensor = numpy_to_tensor(mtx)\n        tensor = T.functional.hflip(tensor)\n        return tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationColorJitter","title":"<code>AugmentationColorJitter</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for applying random modification of brightness, saturation, contrast and hue</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationColorJitter(Augmentation):\n    \"\"\"\n    An augmentation for applying random modification of\n    brightness, saturation, contrast and hue\n    \"\"\"\n\n    # Code inspired by the default logic of torchvision.transformations.ColorJitter\n    # https://pytorch.org/vision/main/_modules/torchvision/transforms/transforms.html#ColorJitter\n    def __init__(\n        self,\n        rng,\n        randomize_at_every_call=True,\n        brightness=0.8,\n        saturation=0.8,\n        contrast=0.8,\n        hue=0.2,\n    ):\n        super().__init__(\n            rng,\n            randomize_at_every_call,\n            brightness=brightness,\n            saturation=saturation,\n            contrast=contrast,\n            hue=hue,\n        )\n        (\n            self.hyper_params[\"min_brightness\"],\n            self.hyper_params[\"max_brightness\"],\n        ) = self._check_input(self.hyper_params[\"brightness\"], \"brightness\")\n        (\n            self.hyper_params[\"min_saturation\"],\n            self.hyper_params[\"max_saturation\"],\n        ) = self._check_input(self.hyper_params[\"saturation\"], \"saturation\")\n        (\n            self.hyper_params[\"min_contrast\"],\n            self.hyper_params[\"max_contrast\"],\n        ) = self._check_input(self.hyper_params[\"contrast\"], \"contrast\")\n        self.hyper_params[\"min_hue\"], self.hyper_params[\"max_hue\"] = self._check_input(\n            self.hyper_params[\"hue\"],\n            \"hue\",\n            center=0,\n            bound=(-0.5, 0.5),\n            clip_first_on_zero=False,\n        )\n        self.params = dict()\n\n    def _check_input(\n        self, value, name, center=1, bound=(0, float(\"inf\")), clip_first_on_zero=True\n    ):\n        if isinstance(value, numbers.Number):\n            value = [center - float(value), center + float(value)]\n            if clip_first_on_zero:\n                value[0] = max(value[0], 0.0)\n        elif isinstance(value, (tuple, list)) and len(value) == 2:\n            value = [float(value[0]), float(value[1])]\n        if not bound[0] &lt;= value[0] &lt;= value[1] &lt;= bound[1]:\n            raise ValueError(\n                f\"{name} values should be between {bound}, but got {value}.\"\n            )\n\n        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n        # or (0., 0.) for hue, do nothing\n        if value[0] == value[1] == center:\n            return None\n        else:\n            return tuple(value)\n\n    def update_params(self) -&gt; None:\n        self._order = self.rng.permutation(4)\n        self.params[\"brightness\"] = self.rng.uniform(\n            self.hyper_params[\"min_brightness\"], self.hyper_params[\"max_brightness\"]\n        )\n        self.params[\"saturation\"] = self.rng.uniform(\n            self.hyper_params[\"min_saturation\"], self.hyper_params[\"max_saturation\"]\n        )\n        self.params[\"contrast\"] = self.rng.uniform(\n            self.hyper_params[\"min_contrast\"], self.hyper_params[\"max_contrast\"]\n        )\n        self.params[\"hue\"] = self.rng.uniform(\n            self.hyper_params[\"min_hue\"], self.hyper_params[\"max_hue\"]\n        )\n\n    def __call__(self, mtx: np.array) -&gt; np.array:\n        if self.is_first_call or self.randomize_at_every_call:\n            self.update_params()\n\n        tensor = numpy_to_tensor(mtx)\n        for idx in self._order:\n            if idx == 0:\n                tensor = T.functional.adjust_brightness(\n                    tensor, self.params[\"brightness\"]\n                )\n            elif idx == 1:\n                tensor = T.functional.adjust_saturation(\n                    tensor, self.params[\"saturation\"]\n                )\n            elif idx == 2:\n                tensor = T.functional.adjust_contrast(tensor, self.params[\"contrast\"])\n            else:\n                tensor = T.functional.adjust_hue(tensor, self.params[\"hue\"])\n        self.is_first_call = False\n        return tensor_to_numpy(tensor)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationPacketLoss","title":"<code>AugmentationPacketLoss</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for applying packet loss (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationPacketLoss(Augmentation):\n    \"\"\"\n    An augmentation for applying packet loss (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\n\n    def __init__(self, rng, randomize_at_every_call=True, delta_time=0.1):\n        super().__init__(rng, randomize_at_every_call, delta_time=delta_time)\n\n    # Note: differently from the other augmentations,\n    # we intentionally sample a new t\n    # at every call as to shape the augmentation\n    # in function of the input. This is due to\n    # possible padding in the flowpic: if the\n    # traffic is occurring only at the beginning\n    # of the flow, sampling uniformly t from a large\n    # window (as from above quote) unlikely alter\n    # the input data\n    def __call__(\n        self, timetofirst: np.array, pkts_size: np.array\n    ) -&gt; Tuple[np.array, np.array, np.array]:\n        session_time = timetofirst[-1]\n        random_t_in_session = self.rng.uniform(low=0.0, high=session_time)\n        min_ts = random_t_in_session - self.hyper_params[\"delta_time\"]\n        max_ts = random_t_in_session + self.hyper_params[\"delta_time\"]\n        self.params[\"min_ts\"] = min_ts\n        self.params[\"max_ts\"] = max_ts\n\n        indexes_to_drop = np.where((timetofirst &gt;= min_ts) &amp; (timetofirst &lt;= max_ts))[0]\n        new_timetofirst = _copy_and_delete_from_numpy_array(\n            timetofirst, indexes_to_drop\n        )\n        new_pkts_size = _copy_and_delete_from_numpy_array(pkts_size, indexes_to_drop)\n        return new_timetofirst, new_pkts_size, indexes_to_drop\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationTimeShift","title":"<code>AugmentationTimeShift</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for applying time shift (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationTimeShift(Augmentation):\n    \"\"\"\n    An augmentation for applying time shift (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\n\n    def __init__(self, rng, randomize_at_every_call=True, delta_time=1):\n        super().__init__(rng, randomize_at_every_call, delta_time=delta_time)\n\n    def update_params(self):\n        self.params[\"shift\"] = self.rng.uniform(\n            -self.hyper_params[\"delta_time\"], self.hyper_params[\"delta_time\"]\n        )\n\n    def __call__(\n        self, timetofirst: np.array, pkts_size: np.array\n    ) -&gt; Tuple[np.array, np.array, np.array]:\n        if self.is_first_call or self.randomize_at_every_call:\n            self.update_params()\n        new_timetofirst = np.copy(timetofirst) + self.params[\"shift\"]\n        indexes = np.where(new_timetofirst &lt; 0)[0]\n        new_pkts_size = pkts_size\n        if len(indexes) &gt; 0:\n            new_timetofirst = new_timetofirst[len(indexes) :]\n            new_pkts_size = np.copy(new_pkts_size)[len(indexes) :]\n        return new_timetofirst, new_pkts_size, indexes\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.AugmentationChangeRTT","title":"<code>AugmentationChangeRTT</code>","text":"<p>             Bases: <code>Augmentation</code></p> <p>An augmentation for applying change rtt (according to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic Augmentations\"</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>class AugmentationChangeRTT(Augmentation):\n    \"\"\"\n    An augmentation for applying change rtt (according\n    to the logic of IMC22 paper \"A Few Shots Traffic Classification with mini-FlowPic\n    Augmentations\"\n    \"\"\"\n\n    def __init__(self, rng, randomize_at_every_call=True, min_alpha=0.5, max_alpha=1.5):\n        super().__init__(\n            rng, randomize_at_every_call, min_alpha=min_alpha, max_alpha=max_alpha\n        )\n        self.params = dict()\n\n    def update_params(self):\n        self.params[\"alpha\"] = self.rng.uniform(\n            self.hyper_params[\"min_alpha\"], self.hyper_params[\"max_alpha\"]\n        )\n\n    def __call__(\n        self, timetofirst: np.array, pkts_size: np.array\n    ) -&gt; Tuple[np.array, np.array, np.array]:\n        if self.is_first_call or self.randomize_at_every_call:\n            self.update_params()\n        new_timetofirst = np.copy(timetofirst) * self.params[\"alpha\"]\n        return new_timetofirst, pkts_size, None\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.get_flowpic","title":"<code>get_flowpic(timetofirst, pkts_size, dim=32, max_block_duration=15)</code>","text":"<p>Generate a Flowpic from time series</p> <p>Parameters:</p> Name Type Description Default <code>timetofirst</code> <code>NDArray</code> <p>time series (in seconds) of the intertime between a packet and the first packet of the flow</p> required <code>pkts_size</code> <code>NDArray</code> <p>time series of the packets size</p> required <code>dim</code> <code>int</code> <p>pixels size of the output representation</p> <code>32</code> <code>max_block_duration</code> <code>int</code> <p>how many seconds of the input time series to process</p> <code>15</code> Return <p>a 2d numpy array encoding a flowpic</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def get_flowpic(\n    timetofirst: NDArray,\n    pkts_size: NDArray,\n    dim: int = 32,\n    max_block_duration: int = 15,\n) -&gt; NDArray:\n    \"\"\"Generate a Flowpic from time series\n\n    Arguments:\n        timetofirst: time series (in seconds) of the intertime between a packet and the first packet of the flow\n        pkts_size: time series of the packets size\n        dim: pixels size of the output representation\n        max_block_duration: how many seconds of the input time series to process\n\n    Return:\n        a 2d numpy array encoding a flowpic\n    \"\"\"\n    indexes = np.where(timetofirst &lt; max_block_duration)[0]\n\n    timetofirst = timetofirst[indexes]\n    pkts_size = np.clip(pkts_size[indexes], a_min=0, a_max=MAX_PACKET_SIZE)\n\n    timetofirst_norm = (timetofirst / max_block_duration) * dim\n    pkts_size_norm = (pkts_size / MAX_PACKET_SIZE) * dim\n    bins = range(dim + 1)\n    mtx, _, _ = np.histogram2d(x=pkts_size_norm, y=timetofirst_norm, bins=(bins, bins))\n\n    # Quote from Sec.2.1 of the IMC22 paper\n    # &gt; If more than max value (255) packets of\n    # &gt; a certain size arrive in a time slot,\n    # &gt; we set the pixel value to max value\n    mtx = np.clip(mtx, a_min=0, a_max=255).astype(\"uint8\")\n    return mtx\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.numpy_to_tensor","title":"<code>numpy_to_tensor(mtx)</code>","text":"<p>Transforms a 2d numpy array into a 3d Tensor adding an extra dimension</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def numpy_to_tensor(mtx: np.array) -&gt; torch.Tensor:\n    \"\"\"Transforms a 2d numpy array into a 3d Tensor adding an extra dimension\"\"\"\n    if len(mtx.shape) == 2:\n        return torch.from_numpy(np.expand_dims(mtx, 0))\n    return torch.from_numpy(mtx)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.tensor_to_numpy","title":"<code>tensor_to_numpy(tensor)</code>","text":"<p>Transforms a 3d tensor into a 2d numpy array removing the first dimension</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def tensor_to_numpy(tensor: torch.Tensor) -&gt; np.array:\n    \"\"\"Transforms a 3d tensor into a 2d numpy array removing the first dimension\"\"\"\n    mtx = tensor.numpy()\n    if mtx.ndim &gt; 2 and mtx.shape[0] == 1:\n        return mtx.squeeze()\n    return mtx\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation._copy_and_delete_from_numpy_array","title":"<code>_copy_and_delete_from_numpy_array(array, indexes)</code>","text":"<p>Helper function to remove elements from a numpy array</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def _copy_and_delete_from_numpy_array(array: np.array, indexes: np.array) -&gt; np.array:\n    \"\"\"Helper function to remove elements from a numpy array\"\"\"\n    return np.delete(np.copy(array), indexes)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.augmentation_factory","title":"<code>augmentation_factory(aug_name, rng, hyper_params)</code>","text":"<p>A factory method to create instances of augmentation classes</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def augmentation_factory(\n    aug_name: str, rng: np.random.Generator, hyper_params: Dict[str, Any]\n) -&gt; Augmentation:\n    \"\"\"A factory method to create instances of augmentation classes\"\"\"\n    if aug_name not in AUGMENTATION_CLASSES:\n        return None\n\n    if hyper_params is None or len(hyper_params) == 0:\n        hyper_params = AUGMENTATION_DEFAULT_HPARAMS.get(aug_name, {}).copy()\n\n    return AUGMENTATION_CLASSES[aug_name](rng, **hyper_params)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_augmentation/#tcbench.modeling.augmentation.apply_augmentation","title":"<code>apply_augmentation(aug_name, aug, **kwargs)</code>","text":"<p>Applies a transformation to the input features</p> Source code in <code>src/tcbench/modeling/augmentation.py</code> <pre><code>def apply_augmentation(\n    aug_name: str, aug: Augmentation, **kwargs: Dict[str, Any]\n) -&gt; Dict[str, NDArray]:\n    \"\"\"Applies a transformation to the input features\"\"\"\n    if aug_name in {\"rotate\", \"horizontalflip\", \"colorjitter\"}:\n        kwargs[\"flowpic\"] = aug(kwargs[\"flowpic\"])\n    elif aug_name in {\"timeshift\", \"packetloss\", \"changertt\"}:\n        new_timetofirst, new_pkts_size, _ = aug(\n            kwargs[\"timetofirst\"], kwargs[\"pkts_size\"]\n        )\n        kwargs[\"timetofirst\"] = new_timetofirst\n        kwargs[\"pkts_size\"] = new_pkts_size\n        kwargs[\"flowpic\"] = get_flowpic(new_timetofirst, new_pkts_size)\n    return kwargs\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/","title":"Tcbench modeling backbone","text":"<p>This module collects all network architectures.</p> <p>A few convetion are used for networks</p> <ol> <li> <p>All networks are expected to inherity from  the archetype class called BaseNet</p> </li> <li> <p>BaseNet is composing layers by means of two attribytes: .features is used for feature extraction while .classifier  corresponds to the final model head</p> </li> </ol>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet","title":"<code>BaseNet</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>class BaseNet(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self._init_args = args\n        self._init_kwargs = kwargs\n        self.features = None\n        self.classifier = None\n\n    def forward(self, x):\n        if self.features is None:\n            raise RuntimeError(\"self.features in None, i.e., no architecture defined\")\n        x = self.features(x)\n        if self.classifier:\n            x = self.classifier(x)\n        return x\n\n    def get_copy(self) -&gt; Self:\n        \"\"\"Get weights from the model\"\"\"\n        return deepcopy(self.state_dict())\n\n    def set_state_dict(self, state_dict) -&gt; Self:\n        \"\"\"Set weights into the model\"\"\"\n        self.load_state_dict(deepcopy(state_dict))\n        return self\n\n    def load_weights(self, fname: pathlib.Path, drop_classifier: bool = False) -&gt; Self:\n        \"\"\"Load into the network the weights stored into a file\n\n        Argument:\n            fname: the file storing the weights\n            drop_classifier: if the network needs need to remove the classifier\n        \"\"\"\n        state_dict = torch.load(fname)\n        if drop_classifier:\n            keys_to_drop = [key for key in state_dict if key.startswith(\"classifier\")]\n            for key in keys_to_drop:\n                del state_dict[key]\n        self.set_state_dict(state_dict)\n        return self\n\n    def save_weights(self, fname: pathlib.Path) -&gt; None:\n        \"\"\"Store to file the weights of the network\n\n        Arguments:\n            fname: the file where to store the weights\n        \"\"\"\n        fname = pathlib.Path(fname)\n        if not fname.parent.exists():\n            fname.parent.mkdir(parents=True)\n        torch.save(self.state_dict(), fname)\n\n    def _find_index_of_last_linear_layer(self) -&gt; int:\n        \"\"\"Introspect the network architecture to identify\n        the index of the last linear layer\"\"\"\n        last_block = self.features[-1]\n        for idx in range(len(last_block) - 1, -1, -1):\n            layer = last_block[idx]\n            if isinstance(layer, nn.Identity):\n                continue\n            elif isinstance(layer, nn.Linear):\n                return idx\n        raise RuntimeError(f\"Didn't find any linear layer\")\n\n    def latent_space_dim(self) -&gt; int:\n        \"\"\"Returns the number of units in the latent space\n        of the model (i.e., the shape of the output generated\n        be .features)\n        \"\"\"\n        idx = self._find_index_of_last_linear_layer()\n        layer = self.features[-1][idx]\n        return list(layer.parameters())[-1].shape[0]\n\n    def reset_classifier(self, num_classes) -&gt; Self:\n        \"\"\"Recreate the classifier of the network\n\n        Attributes:\n            num_classes: the number of units for the new classifier layer\n        \"\"\"\n        self.classifier = nn.Linear(self.latent_space_dim(), num_classes)\n        self.initialize_weights(self.classifier)\n        return self\n\n    @property\n    def num_classes(self) -&gt; int:\n        \"\"\"The number of units for the classifier of the network\"\"\"\n        if self.classifier is None or isinstance(self.classifier, nn.Identity):\n            return None\n        return list(self.classifier.parameters())[0].shape[0]\n\n    def is_equal_to(self, other_net: BaseNet) -&gt; bool:\n        \"\"\"Returns True if other_net is identical\n        (architecture and weights) to the current\n        network\"\"\"\n        return are_equal(self, other_net)\n\n    def initialize_weights(self, m: nn.Module) -&gt; None:\n        \"\"\"Initialize the weights of the network using Kaiming He\"\"\"\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.kaiming_uniform_(m.weight.data)\n            nn.init.constant_(m.bias.data, 0)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>The number of units for the classifier of the network</p>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.get_copy","title":"<code>get_copy()</code>","text":"<p>Get weights from the model</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def get_copy(self) -&gt; Self:\n    \"\"\"Get weights from the model\"\"\"\n    return deepcopy(self.state_dict())\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.set_state_dict","title":"<code>set_state_dict(state_dict)</code>","text":"<p>Set weights into the model</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def set_state_dict(self, state_dict) -&gt; Self:\n    \"\"\"Set weights into the model\"\"\"\n    self.load_state_dict(deepcopy(state_dict))\n    return self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.load_weights","title":"<code>load_weights(fname, drop_classifier=False)</code>","text":"<p>Load into the network the weights stored into a file</p> Argument <p>fname: the file storing the weights drop_classifier: if the network needs need to remove the classifier</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def load_weights(self, fname: pathlib.Path, drop_classifier: bool = False) -&gt; Self:\n    \"\"\"Load into the network the weights stored into a file\n\n    Argument:\n        fname: the file storing the weights\n        drop_classifier: if the network needs need to remove the classifier\n    \"\"\"\n    state_dict = torch.load(fname)\n    if drop_classifier:\n        keys_to_drop = [key for key in state_dict if key.startswith(\"classifier\")]\n        for key in keys_to_drop:\n            del state_dict[key]\n    self.set_state_dict(state_dict)\n    return self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.save_weights","title":"<code>save_weights(fname)</code>","text":"<p>Store to file the weights of the network</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the file where to store the weights</p> required Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def save_weights(self, fname: pathlib.Path) -&gt; None:\n    \"\"\"Store to file the weights of the network\n\n    Arguments:\n        fname: the file where to store the weights\n    \"\"\"\n    fname = pathlib.Path(fname)\n    if not fname.parent.exists():\n        fname.parent.mkdir(parents=True)\n    torch.save(self.state_dict(), fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet._find_index_of_last_linear_layer","title":"<code>_find_index_of_last_linear_layer()</code>","text":"<p>Introspect the network architecture to identify the index of the last linear layer</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def _find_index_of_last_linear_layer(self) -&gt; int:\n    \"\"\"Introspect the network architecture to identify\n    the index of the last linear layer\"\"\"\n    last_block = self.features[-1]\n    for idx in range(len(last_block) - 1, -1, -1):\n        layer = last_block[idx]\n        if isinstance(layer, nn.Identity):\n            continue\n        elif isinstance(layer, nn.Linear):\n            return idx\n    raise RuntimeError(f\"Didn't find any linear layer\")\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.latent_space_dim","title":"<code>latent_space_dim()</code>","text":"<p>Returns the number of units in the latent space of the model (i.e., the shape of the output generated be .features)</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def latent_space_dim(self) -&gt; int:\n    \"\"\"Returns the number of units in the latent space\n    of the model (i.e., the shape of the output generated\n    be .features)\n    \"\"\"\n    idx = self._find_index_of_last_linear_layer()\n    layer = self.features[-1][idx]\n    return list(layer.parameters())[-1].shape[0]\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.reset_classifier","title":"<code>reset_classifier(num_classes)</code>","text":"<p>Recreate the classifier of the network</p> <p>Attributes:</p> Name Type Description <code>num_classes</code> <p>the number of units for the new classifier layer</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def reset_classifier(self, num_classes) -&gt; Self:\n    \"\"\"Recreate the classifier of the network\n\n    Attributes:\n        num_classes: the number of units for the new classifier layer\n    \"\"\"\n    self.classifier = nn.Linear(self.latent_space_dim(), num_classes)\n    self.initialize_weights(self.classifier)\n    return self\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.is_equal_to","title":"<code>is_equal_to(other_net)</code>","text":"<p>Returns True if other_net is identical (architecture and weights) to the current network</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def is_equal_to(self, other_net: BaseNet) -&gt; bool:\n    \"\"\"Returns True if other_net is identical\n    (architecture and weights) to the current\n    network\"\"\"\n    return are_equal(self, other_net)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.BaseNet.initialize_weights","title":"<code>initialize_weights(m)</code>","text":"<p>Initialize the weights of the network using Kaiming He</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def initialize_weights(self, m: nn.Module) -&gt; None:\n    \"\"\"Initialize the weights of the network using Kaiming He\"\"\"\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\n        if m.bias is not None:\n            nn.init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight.data)\n        nn.init.constant_(m.bias.data, 0)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.have_same_layers_and_types","title":"<code>have_same_layers_and_types(net1, net2)</code>","text":"<p>Compares to networks based on architecture</p> <p>Parameters:</p> Name Type Description Default <code>net1</code> <code>BaseNet</code> <p>a network</p> required <code>net2</code> <code>BaseNet</code> <p>another netwokr</p> required Return <p>True if the two network have the same number of layers each with the same type</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def have_same_layers_and_types(net1: BaseNet, net2: BaseNet) -&gt; bool:\n    \"\"\"Compares to networks based on architecture\n\n    Arguments:\n        net1: a network\n        net2: another netwokr\n\n    Return:\n        True if the two network have the same\n        number of layers each with the same type\n    \"\"\"\n    if len(net1.features) != len(net2.features):\n        return False\n\n    for block1, block2 in zip(net1.features, net2.features):\n        if len(block1) != len(block2):\n            return False\n        for layer1, layer2 in zip(block1, block2):\n            if type(layer1) != type(layer2):\n                return False\n\n    has_classifier1 = net1.classifier is None\n    has_classifier2 = net2.classifier is None\n    has_classifier = has_classifier1 + has_classifier2\n\n    if has_classifier == 1:\n        return False\n    elif has_classifier == 0:\n        return True\n    return type(net1.classifier) == type(net2.classifier)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.are_equal","title":"<code>are_equal(net1, net2)</code>","text":"<p>Compare two networks considering both architecture and weights</p> <p>Parameters:</p> Name Type Description Default <code>net1</code> <code>BaseNet</code> <p>a network</p> required <code>net2</code> <code>BaseNet</code> <p>another netwokr</p> required Return <p>True if the two network have the same number of layers each with the same type and they have the same weights</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def are_equal(net1: BaseNet, net2: BaseNet) -&gt; bool:\n    \"\"\"Compare two networks considering both architecture and weights\n\n    Arguments:\n        net1: a network\n        net2: another netwokr\n\n    Return:\n        True if the two network have the same\n        number of layers each with the same type\n        and they have the same weights\n    \"\"\"\n    if not have_same_layers_and_types(net1, net2):\n        return False\n    for (name1, weights1), (name2, weights2) in zip(\n        net1.to(\"cpu\").state_dict().items(), net2.to(\"cpu\").state_dict().items()\n    ):\n        if not (weights1 == weights2).all():\n            return False\n    return True\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.compute_features_size","title":"<code>compute_features_size(input_size, modules)</code>","text":"<p>Compute the number of units at the end of a chain of modules</p> <p>Attributes:</p> Name Type Description <code>input_size</code> <p>the shape of the input tensor</p> <code>modules</code> <p>a list of modules processing the input in sequence</p> Return <p>The number of units in the ouput generated processing a input of the specified shape through the list of modules</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def compute_features_size(input_size: Tuple[int], modules: List[nn.Module]) -&gt; int:\n    \"\"\"Compute the number of units at the end of a chain of modules\n\n    Attributes:\n        input_size: the shape of the input tensor\n        modules: a list of modules processing the input in sequence\n\n    Return:\n        The number of units in the ouput generated processing\n        a input of the specified shape through the list of modules\n    \"\"\"\n    x_dummy = torch.autograd.Variable(torch.ones(1, *input_size))\n    for m in modules:\n        x_dummy = m(x_dummy)\n    return int(np.prod(x_dummy.shape))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.has_dropout_layer","title":"<code>has_dropout_layer(module)</code>","text":"<p>Detect if the input module is a dropout layer or is a network containing a dropout layer</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def has_dropout_layer(module: nn.Module) -&gt; bool:\n    \"\"\"Detect if the input module is a dropout layer\n    or is a network containing a dropout layer\n    \"\"\"\n    if isinstance(module, (nn.Dropout, nn.Dropout1d, nn.Dropout2d)):\n        return True\n    return any(map(has_dropout_layer, module.children()))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.clone_net","title":"<code>clone_net(net)</code>","text":"<p>An utility function to clone a network</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to clone</p> required Return <p>A new instance of the same network passed as input initialized with the same weights</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def clone_net(net: BaseNet) -&gt; BaseNet:\n    \"\"\"An utility function to clone a network\n\n    Arguments:\n        net: the network to clone\n\n    Return:\n        A new instance of the same network passed\n        as input initialized with the same weights\n    \"\"\"\n    curr_module = sys.modules[__name__]\n    net_class = getattr(curr_module, net.__class__.__name__)\n    new_net = net_class(*net._init_args, **net._init_kwargs)\n\n    # the classifier might have been added after instanciation\n    if net.classifier:\n        new_net.reset_classifier(num_classes=net.num_classes)\n\n    for idx in range(len(net.features)):\n        new_block = new_net.features[idx]\n        old_block = net.features[idx]\n        for idx2 in range(len(new_block)):\n            new_layer = new_block[idx2]\n            old_layer = old_block[idx2]\n            if isinstance(old_layer, nn.Identity) and not isinstance(\n                new_layer, nn.Identity\n            ):\n                new_block[idx2] = nn.Identity()\n    if isinstance(net.classifier, nn.Identity):\n        new_net.classifier = nn.Identity()\n\n    new_net.set_state_dict(net.get_copy())\n    return new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_backbone/#tcbench.modeling.backbone.net_factory","title":"<code>net_factory(num_classes=5, flowpic_dim=32, with_dropout=True, projection_layer_dim=None)</code>","text":"<p>An utilify function to create Flowpic-related networks</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>the number of classes for the classifier</p> <code>5</code> <code>flowpic_dim</code> <code>int</code> <p>the resolution of the flowpic representation</p> <code>32</code> <code>with_dropout</code> <code>bool</code> <p>if False, the network use nn.Identity to mask out dropout layers</p> <code>True</code> <code>projection_layer_dim</code> <code>int</code> <p>the number of units for the SimCLR projection layer</p> <code>None</code> Return <p>the instanciated network</p> Source code in <code>src/tcbench/modeling/backbone.py</code> <pre><code>def net_factory(\n    num_classes: int = 5,\n    flowpic_dim: int = 32,\n    with_dropout: bool = True,\n    projection_layer_dim: int = None,\n) -&gt; BaseNet:\n    \"\"\"An utilify function to create Flowpic-related networks\n\n    Arguments:\n        num_classes: the number of classes for the classifier\n        flowpic_dim: the resolution of the flowpic representation\n        with_dropout: if False, the network use nn.Identity to mask out dropout layers\n        projection_layer_dim: the number of units for the SimCLR projection layer\n\n    Return:\n        the instanciated network\n    \"\"\"\n    kwargs = dict(\n        num_classes=num_classes,\n        flowpic_dim=flowpic_dim,\n        with_dropout=with_dropout,\n        projection_layer_dim=projection_layer_dim,\n    )\n    if flowpic_dim in (32, 64):\n        return LeNet5FlowpicIMC22_Mini(**kwargs)\n    return LeNet5FlowpicIMC22_Full(**kwargs)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/","title":"Tcbench modeling dataprep","text":"<p>This modules contains the a hierarchy of classes for composing Datasets and a variety of  function to load those objects from file</p> <p>All datasets are inherited from an archetype class named FlowpicDataset. This is a wrapper around a pandas DataFrame and offer functionality to create flowpic representation based on raw time series.</p> <p>Two other classes are then created to apply transformation functions</p> <p>Specifically: - AugmentWhenLoadingDataset: this class applies      transformations when instanciated.     This is useful when performing supervised training</p> <ul> <li>MultiViewDataset: this class applies     multi-view transformation on-the-fly.     This is useful when performing contrastive learning training</li> </ul>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset","title":"<code>FlowpicDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>class FlowpicDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        data: str | pd.DataFrame,\n        timetofirst_colname: str,\n        pkts_size_colname: str,\n        pkts_dir_colname: str,\n        target_colname: str,\n        flowpic_dim: int = 32,\n        flowpic_block_duration: int = 15,\n        quiet: bool = False,\n        logger: logging.Logger = None,\n        n_workers: int = 10,\n        flow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,  # str='flowpic',\n        max_n_pkts: int = 10,\n    ):\n        \"\"\"\n        Arguments:\n            data: if a string, it corresponds to a parquet file from\n                where to load the raw data; if a pandas DataFrame,\n                the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            pkts_dir_colname: the column name mapping to the\n                packet direction time series\n            target_colname: the column name where the labels are stored\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            quiet: if False, no output on the console is generated when loading\n            logger: the logger to use\n            n_workers: how many processes to spawn when processing the data\n            flow_representation: flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction\n            max_n_pkts: timeseries length in case of flow_representation==\"pktseries\"\n        \"\"\"\n        self.scaler = None\n        self.flow_representation = flow_representation\n        self.max_n_pkts = max_n_pkts\n        self.logger = logger\n        self.timetofirst_colname = timetofirst_colname\n        self.pkts_size_colname = pkts_size_colname\n        self.pkts_dir_colname = pkts_dir_colname\n        self.transform = torchvision.transforms.Compose(\n            [\n                torchvision.transforms.ToTensor(),\n            ]\n        )\n        self.n_workers = n_workers\n\n        self.df = data\n        if isinstance(data, (str, pathlib.Path)):\n            if not quiet:\n                self.log_msg(f\"loading: {data}\")\n            self.df = pd.read_parquet(data)\n        if \"flowpic\" not in self.df:\n            self.df = self.add_flowpic(\n                self.df,\n                timetofirst_colname,\n                pkts_size_colname,\n                flowpic_dim,\n                flowpic_block_duration,\n                n_workers,\n            )\n\n        self.data = self.df[\"flowpic\"].values\n        self.target = self.df[target_colname].cat.codes.astype(\"int64\").values\n        self.target_colname = target_colname\n        self.num_classes = self.df[target_colname].nunique()\n\n    def log_msg(self, msg: str) -&gt; None:\n        \"\"\"An utility function to log messages\"\"\"\n        utils.log_msg(msg, self.logger)\n\n    def set_scaler(self, scaler):\n        self.scaler = scaler\n\n    @classmethod\n    def add_flowpic(\n        cls,\n        df: pd.DataFrame,\n        timetofirst_colname: str,\n        pkts_size_colname: str,\n        flowpic_dim: int = 32,\n        flowpic_block_duration: int = 15,\n        n_workers: int = 1,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process a raw dataframe to create flowpic representation\n\n        Arguments:\n            df: a pandas DataFrame, the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            n_workers: how many processes to spawn when processing the data\n        \"\"\"\n        func = functools.partial(\n            augmentation.get_flowpic,\n            dim=flowpic_dim,\n            max_block_duration=flowpic_block_duration,\n        )\n\n        params = []\n        for idx in range(df.shape[0]):\n            ser = df.iloc[idx]\n            params.append((ser[timetofirst_colname], ser[pkts_size_colname]))\n\n        if n_workers &gt; 1:\n            with multiprocessing.Pool(n_workers) as pool:\n                flowpic_l = pool.starmap(func, params)\n        else:\n            flowpic_l = [func(*pars) for pars in params]\n\n        return df.assign(flowpic=flowpic_l)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns how many samples are in the dataset\"\"\"\n        return len(self.target)\n\n    def __getitem__(self, index:int) -&gt; Any:\n        \"\"\"\n        Arguments:\n            index: the index of the sample\n\n        Return:\n            A tuple with the flowpic representation (as tensor)\n            and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")\n        \"\"\"\n        if self.flow_representation == MODELING_INPUT_REPR_TYPE.FLOWPIC:  #'flowpic':\n            return (\n                self.transform(np.expand_dims(self.data[index], 2)).double(),\n                self.target[index],\n            )\n        ser = self.df.iloc[[index]]\n        normalize_df = _create_df_to_normalize_pkt_series(\n            ser,\n            self.timetofirst_colname,\n            self.pkts_size_colname,\n            self.pkts_dir_colname,\n            self.max_n_pkts,\n        )\n        normalized = self.scaler.transform(normalize_df)\n        return (normalized, self.target[index])\n\n    #        df = _create_df_to_normalize_pkt_series(ser,\n    #                                        self.timetofirst_colname,\n    #                                        self.pkts_size_colname,\n    #                                        self.pkts_dir_colname,\n    #                                        self.max_n_pkts)\n    #\n    #        if self.scaler:\n    #            df = self.scaler.transform(df)\n    #        return (df, self.target[index])\n\n    def num_classes(self):\n        \"\"\"Returns the number of classes in the dataset\"\"\"\n        return self.df[self.target_colname].nunique()\n\n    def samples_count(self) -&gt; pd.Series:\n        \"\"\"\n        Return:\n            a pd.Series with the frequency count of\n            number of samples per class in the dataset\n        \"\"\"\n        return self.df[self.target_colname].value_counts()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__init__","title":"<code>__init__(data, timetofirst_colname, pkts_size_colname, pkts_dir_colname, target_colname, flowpic_dim=32, flowpic_block_duration=15, quiet=False, logger=None, n_workers=10, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>if a string, it corresponds to a parquet file from where to load the raw data; if a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>pkts_dir_colname</code> <code>str</code> <p>the column name mapping to the packet direction time series</p> required <code>target_colname</code> <code>str</code> <p>the column name where the labels are stored</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>quiet</code> <code>bool</code> <p>if False, no output on the console is generated when loading</p> <code>False</code> <code>logger</code> <code>Logger</code> <p>the logger to use</p> <code>None</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>10</code> <code>flow_representation</code> <code>MODELING_INPUT_REPR_TYPE</code> <p>flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction</p> <code>FLOWPIC</code> <code>max_n_pkts</code> <code>int</code> <p>timeseries length in case of flow_representation==\"pktseries\"</p> <code>10</code> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    timetofirst_colname: str,\n    pkts_size_colname: str,\n    pkts_dir_colname: str,\n    target_colname: str,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    quiet: bool = False,\n    logger: logging.Logger = None,\n    n_workers: int = 10,\n    flow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,  # str='flowpic',\n    max_n_pkts: int = 10,\n):\n    \"\"\"\n    Arguments:\n        data: if a string, it corresponds to a parquet file from\n            where to load the raw data; if a pandas DataFrame,\n            the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        pkts_dir_colname: the column name mapping to the\n            packet direction time series\n        target_colname: the column name where the labels are stored\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        quiet: if False, no output on the console is generated when loading\n        logger: the logger to use\n        n_workers: how many processes to spawn when processing the data\n        flow_representation: flow is represented either by \"flowpic\" or \"pktseries\", i.e. three series with pkts_size, interarrival time (derived from timetofirst) and pkt direction\n        max_n_pkts: timeseries length in case of flow_representation==\"pktseries\"\n    \"\"\"\n    self.scaler = None\n    self.flow_representation = flow_representation\n    self.max_n_pkts = max_n_pkts\n    self.logger = logger\n    self.timetofirst_colname = timetofirst_colname\n    self.pkts_size_colname = pkts_size_colname\n    self.pkts_dir_colname = pkts_dir_colname\n    self.transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    )\n    self.n_workers = n_workers\n\n    self.df = data\n    if isinstance(data, (str, pathlib.Path)):\n        if not quiet:\n            self.log_msg(f\"loading: {data}\")\n        self.df = pd.read_parquet(data)\n    if \"flowpic\" not in self.df:\n        self.df = self.add_flowpic(\n            self.df,\n            timetofirst_colname,\n            pkts_size_colname,\n            flowpic_dim,\n            flowpic_block_duration,\n            n_workers,\n        )\n\n    self.data = self.df[\"flowpic\"].values\n    self.target = self.df[target_colname].cat.codes.astype(\"int64\").values\n    self.target_colname = target_colname\n    self.num_classes = self.df[target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>An utility function to log messages</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n    \"\"\"An utility function to log messages\"\"\"\n    utils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.add_flowpic","title":"<code>add_flowpic(df, timetofirst_colname, pkts_size_colname, flowpic_dim=32, flowpic_block_duration=15, n_workers=1)</code>  <code>classmethod</code>","text":"<p>Process a raw dataframe to create flowpic representation</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>1</code> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>@classmethod\ndef add_flowpic(\n    cls,\n    df: pd.DataFrame,\n    timetofirst_colname: str,\n    pkts_size_colname: str,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    n_workers: int = 1,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Process a raw dataframe to create flowpic representation\n\n    Arguments:\n        df: a pandas DataFrame, the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        n_workers: how many processes to spawn when processing the data\n    \"\"\"\n    func = functools.partial(\n        augmentation.get_flowpic,\n        dim=flowpic_dim,\n        max_block_duration=flowpic_block_duration,\n    )\n\n    params = []\n    for idx in range(df.shape[0]):\n        ser = df.iloc[idx]\n        params.append((ser[timetofirst_colname], ser[pkts_size_colname]))\n\n    if n_workers &gt; 1:\n        with multiprocessing.Pool(n_workers) as pool:\n            flowpic_l = pool.starmap(func, params)\n    else:\n        flowpic_l = [func(*pars) for pars in params]\n\n    return df.assign(flowpic=flowpic_l)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns how many samples are in the dataset</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns how many samples are in the dataset\"\"\"\n    return len(self.target)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>the index of the sample</p> required Return <p>A tuple with the flowpic representation (as tensor) and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def __getitem__(self, index:int) -&gt; Any:\n    \"\"\"\n    Arguments:\n        index: the index of the sample\n\n    Return:\n        A tuple with the flowpic representation (as tensor)\n        and the associated label (in case flow_representation==\"flowpic\") or flattened, normalized timeseries (in case flow_representation==\"pktseries\")\n    \"\"\"\n    if self.flow_representation == MODELING_INPUT_REPR_TYPE.FLOWPIC:  #'flowpic':\n        return (\n            self.transform(np.expand_dims(self.data[index], 2)).double(),\n            self.target[index],\n        )\n    ser = self.df.iloc[[index]]\n    normalize_df = _create_df_to_normalize_pkt_series(\n        ser,\n        self.timetofirst_colname,\n        self.pkts_size_colname,\n        self.pkts_dir_colname,\n        self.max_n_pkts,\n    )\n    normalized = self.scaler.transform(normalize_df)\n    return (normalized, self.target[index])\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.num_classes","title":"<code>num_classes()</code>","text":"<p>Returns the number of classes in the dataset</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def num_classes(self):\n    \"\"\"Returns the number of classes in the dataset\"\"\"\n    return self.df[self.target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.FlowpicDataset.samples_count","title":"<code>samples_count()</code>","text":"Return <p>a pd.Series with the frequency count of number of samples per class in the dataset</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def samples_count(self) -&gt; pd.Series:\n    \"\"\"\n    Return:\n        a pd.Series with the frequency count of\n        number of samples per class in the dataset\n    \"\"\"\n    return self.df[self.target_colname].value_counts()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset","title":"<code>AugmentWhenLoadingDataset</code>","text":"<p>             Bases: <code>FlowpicDataset</code></p> <p>Wrapper around FlowpicDataset to enable creation of augmented samples when instanciating the class</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>class AugmentWhenLoadingDataset(FlowpicDataset):\n    \"\"\"Wrapper around FlowpicDataset to enable creation\n    of augmented samples when instanciating the class\n    \"\"\"\n\n    def __init__(\n        self,\n        data: str | pd.DataFrame,\n        timetofirst_colname: str,\n        pkts_size_colname: str,\n        pkts_dir_colname: str,\n        target_colname: str,\n        flowpic_dim: int=32,\n        flowpic_block_duration: int=15,\n        aug_name:str =\"noaug\",\n        aug_hparams: Dict[str, Any]=None,\n        aug_samples: int=10,\n        quiet: bool=False,\n        logger: logging.Logger =None,\n        n_workers: int=10,\n        seed: int=12345,\n        flow_representation:MODELING_INPUT_REPR_TYPE =MODELING_INPUT_REPR_TYPE.FLOWPIC, \n        max_n_pkts:int=10,\n    ):\n        \"\"\"\n        Arguments:\n            data: if a string, it corresponds to a parquet file from\n                where to load the raw data; if a pandas DataFrame,\n                the data to use for the dataset\n            timetofirst_colname: the column name mapping to the\n                packet timeseries of containing timestamps\n                (relative to the first packet of the time series)\n            pkts_size_colname: the column name mapping to the\n                packet size time series\n            target_colname: the column name where the labels are stored\n            flowpic_dim: the flowpic resolution to use\n            flowpic_block_duration: how many seconds of the\n                input data need to be used to generate a flowpic\n            aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n                \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            aug_samples: how many samples to create for each\n                already existing sample\n            quiet: if False, no output on the console is generated when loading\n            logger: the logger to use\n            n_workers: how many processes to spawn when processing the data\n            seed: random seed\n            flow_representation: a MODELING_INPUT_REPR_TYPE value\n            max_n_pkts: packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)\n        \"\"\"\n        super().__init__(\n            data,\n            timetofirst_colname=timetofirst_colname,\n            pkts_size_colname=pkts_size_colname,\n            pkts_dir_colname=pkts_dir_colname,\n            target_colname=target_colname,\n            flowpic_dim=flowpic_dim,\n            flowpic_block_duration=flowpic_block_duration,\n            quiet=quiet,\n            logger=logger,\n            n_workers=n_workers,\n            flow_representation=flow_representation,\n            max_n_pkts=max_n_pkts,\n        )\n        self.aug_samples = aug_samples\n        self.seed = seed\n\n        self.df = self.samples_augmentation(\n            aug_name=aug_name,\n            aug_hparams=aug_hparams,\n            samples=aug_samples,\n            flowpic_dim=flowpic_dim,\n            flowpic_block_duration=flowpic_block_duration,\n            seed=seed,\n        )\n\n        self.data = self.df[\"flowpic\"].values\n        self.target = self.df[target_colname].cat.codes.astype(\"int64\").values\n        self.target_colname = target_colname\n        self.num_classes = self.df[target_colname].nunique()\n\n    def regenerate_flowpic(\n        self, dim: int = 32, block_duration: int = 15\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Overwrite the existing flowpic by creating new ones\n\n        Arguments:\n            dim: the flowpic resolution\n            block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n\n        Return:\n            A new dataframe with a \"flowpic\" column reporting the flowpic\n            for each available sample\n        \"\"\"\n        self.df = self.add_flowpic(\n            self.df,\n            self.timetofirst_colname,\n            self.pkts_size_colname,\n            dim,\n            block_duration,\n            self.n_workers,\n        )\n        return self.df\n\n    def _worker_aug_torch(\n        self,\n        df: pd.DataFrame,\n        aug_class: augmentation.Augmentation,\n        samples: int = 9,\n        *args: List[Any],\n        **kwargs: Dict[str, Any],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Helper function for a multi-processing worker handling\n        augmentations related to flowpic (i.e., working on Tensor data)\n\n        Arguments:\n            df: a batch of samples to process\n            aug_class: an instance of an augmentation class (see augmentation module)\n            samples: how many samples to create for each existings sample\n\n        Return:\n            An expanded version of the input dataframe containing\n            the new samples. IMPORTANT: the new DataFrame is performing\n            a shallow copy of the original and act only of a subset of\n            columns. Hence, the returned version might have semantical\n            incosistencies\n        \"\"\"\n        new_flowpic = []\n        new_aug_params = []\n        df = pd.concat([df.copy() for _ in range(samples)]).assign(\n            is_augmented=True, aug_params={}\n        )\n        dtypes = dict(self.df.dtypes)\n        for idx in range(df.shape[0]):\n            ser = df.iloc[idx]\n            new_sample = ser.copy()\n            new_flowpic.append(aug_class(new_sample[\"flowpic\"]))\n            new_aug_params.append(aug_class.get_params().copy())\n        print(\".\", end=\"\", flush=True)\n        df = df.assign(flowpic=new_flowpic, aug_params=new_aug_params)\n        return df\n\n    def _worker_aug_numpy(\n        self,\n        df: pd.DataFrame,\n        aug_class: augmentation.Augmentation,\n        samples: int = 9,\n        flowpic_dim: int = 32,\n        flowpic_block_duration: int = 15,\n    ):\n        \"\"\"\n        Helper function for a multi-processing worker handling\n        augmentations related to time series (i.e., working with numpy arrays)\n\n        Arguments:\n            df: a batch of samples to process\n            aug_class: an instance of an augmentation class (see augmentation module)\n            samples: how many samples to create for each existings sample\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n\n        Return:\n            An expanded version of the input dataframe containing\n            the new samples. IMPORTANT: the new DataFrame is performing\n            a shallow copy of the original and act only of a subset of\n            columns. Hence, the returned version might have semantical\n            incosistencies\n        \"\"\"\n        new_timetofirst = []\n        new_pkts_size = []\n        new_flowpic = []\n        new_aug_params = []\n        df = pd.concat([df.copy() for _ in range(samples)]).assign(\n            is_augmented=True, aug_params={}\n        )\n        for idx in range(df.shape[0]):\n            ser = df.iloc[idx]\n            _timetofirst, _pkts_size, indexes = aug_class(\n                ser[self.timetofirst_colname], ser[self.pkts_size_colname]\n            )\n            new_aug_params.append(aug_class.get_params().copy())\n            new_timetofirst.append(_timetofirst)\n            new_pkts_size.append(_pkts_size)\n            new_flowpic.append(\n                augmentation.get_flowpic(\n                    _timetofirst, _pkts_size, flowpic_dim, flowpic_block_duration\n                )\n            )\n        df = df.assign(\n            timetofirst=new_timetofirst,\n            pkts_size=new_pkts_size,\n            flowpic=new_flowpic,\n            aug_params=new_aug_params,\n        )\n        print(\".\", end=\"\", flush=True)\n        return df\n\n    def _samples_augmentation_loop(\n        self,\n        aug_name: str,\n        aug_hparams: Dict[str, Any],\n        samples: int,\n        worker_func: Callable,\n        seed: int = 12345,\n        flowpic_dim: int = 32,\n        flowpic_block_duration: int = 15,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Helper function handling the main loop for samples augmentation\n        by means of multiprocessing\n\n        Arguments:\n            aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n                \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            samples: how many samples to create for each\n                already existing sample\n            worker_func: the callback to use for augmentation\n            seed: the seed to use for augmentation\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n\n        Return:\n            A pandas DataFrame with all the original samples plus\n            the augmented ones\n        \"\"\"\n        if aug_hparams is None:\n            aug_hparams = dict()\n        params = []\n        indexes = self.df.index.values\n        partition_size = indexes.shape[0] // self.n_workers\n        for idx in range(0, len(indexes), partition_size):\n            rng = np.random.default_rng(seed + idx)\n            # aug_class = augmentation.augmentation_factory(aug_name, rng, **aug_hparams)\n            aug_class = augmentation.augmentation_factory(aug_name, rng, aug_hparams)\n            partition_indexes = indexes[idx : idx + partition_size]\n            params.append(\n                (\n                    self.df.loc[partition_indexes],\n                    aug_class,\n                    samples,\n                    flowpic_dim,\n                    flowpic_block_duration,\n                )\n            )\n\n        # Note: this is a very dirty trick\n        #\n        # We experienced deadlocks similar to what reported here\n        # https://github.com/pytorch/pytorch/issues/3492\n        # when using torchvision.transforms (with both functional APIs\n        # and instanciating classes). But the logic in the\n        # .augmentation module works fine in single process\n        #\n        # Relying on torch.multiprocessing\n        # https://github.com/pytorch/pytorch/issues/3492\n        # and setting torch.multiprogressing.set_start_method('spawn')\n        # (in if __name__ == '__main__') fixed the issue\n        #\n        # But this requires invoking the .Pool() differently\n        # depending on the underlining augmentation (pytorch or numpy)\n        if worker_func.__name__ == \"_worker_aug_torch\":\n            with torch.multiprocessing.Pool(self.n_workers) as pool:\n                augmented_l = pool.starmap(worker_func, params)\n        else:\n            with multiprocessing.Pool(self.n_workers) as pool:\n                augmented_l = pool.starmap(worker_func, params)\n\n        self.df = pd.concat([self.df] + augmented_l).reset_index()\n        return self.df\n\n    def samples_augmentation(\n        self,\n        aug_name:str=\"noaug\",\n        aug_hparams:Dict[str, Any]=None,\n        samples:int=None,\n        seed:int=12345,\n        flowpic_dim:int=32,\n        flowpic_block_duration:int=15,\n    ):\n        \"\"\"Applies samples augmentation\n\n        Arguments:\n            aug_name: one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}\n            aug_hparams: the augmentation parameters (see the augmentation module)\n            samples: final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)\n            seed: random number generator seed\n            flowpic_dim: the flowpic resolution\n            flowpic_block_duration: the max number of seconds for each sample time series\n                to consider when computing a flowpic\n\n        Return:\n            A pandas DataFrame with all original samples and the augmented ones\n        \"\"\"\n        if samples is None:\n            samples = self.aug_samples\n\n        if aug_name not in augmentation.AUGMENTATION_CLASSES:\n            self.log_msg(\"no augmentation\")\n            return self.df.assign(is_augmented=False)\n\n        self.df = self.df.assign(is_augmented=False)\n        samples -= 1\n        if aug_name == \"horizontalflip\":\n            samples = 1\n        worker_func = self._worker_aug_numpy\n        if aug_name in (\"rotate\", \"horizontalflip\", \"colorjitter\"):\n            worker_func = self._worker_aug_torch\n\n        self.log_msg(f\"data augmentation ({aug_name})\")\n        self._samples_augmentation_loop(\n            aug_name=aug_name,\n            aug_hparams=aug_hparams,\n            samples=samples,\n            worker_func=worker_func,\n            seed=seed,\n            flowpic_dim=flowpic_dim,\n            flowpic_block_duration=flowpic_block_duration,\n        )\n        print()\n        return self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.__init__","title":"<code>__init__(data, timetofirst_colname, pkts_size_colname, pkts_dir_colname, target_colname, flowpic_dim=32, flowpic_block_duration=15, aug_name='noaug', aug_hparams=None, aug_samples=10, quiet=False, logger=None, n_workers=10, seed=12345, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>if a string, it corresponds to a parquet file from where to load the raw data; if a pandas DataFrame, the data to use for the dataset</p> required <code>timetofirst_colname</code> <code>str</code> <p>the column name mapping to the packet timeseries of containing timestamps (relative to the first packet of the time series)</p> required <code>pkts_size_colname</code> <code>str</code> <p>the column name mapping to the packet size time series</p> required <code>target_colname</code> <code>str</code> <p>the column name where the labels are stored</p> required <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution to use</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>how many seconds of the input data need to be used to generate a flowpic</p> <code>15</code> <code>aug_name</code> <code>str</code> <p>one of {\"noaug\", \"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"</p> <code>'noaug'</code> <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> <code>None</code> <code>aug_samples</code> <code>int</code> <p>how many samples to create for each already existing sample</p> <code>10</code> <code>quiet</code> <code>bool</code> <p>if False, no output on the console is generated when loading</p> <code>False</code> <code>logger</code> <code>Logger</code> <p>the logger to use</p> <code>None</code> <code>n_workers</code> <code>int</code> <p>how many processes to spawn when processing the data</p> <code>10</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>12345</code> <code>flow_representation</code> <code>MODELING_INPUT_REPR_TYPE</code> <p>a MODELING_INPUT_REPR_TYPE value</p> <code>FLOWPIC</code> <code>max_n_pkts</code> <code>int</code> <p>packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)</p> <code>10</code> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    timetofirst_colname: str,\n    pkts_size_colname: str,\n    pkts_dir_colname: str,\n    target_colname: str,\n    flowpic_dim: int=32,\n    flowpic_block_duration: int=15,\n    aug_name:str =\"noaug\",\n    aug_hparams: Dict[str, Any]=None,\n    aug_samples: int=10,\n    quiet: bool=False,\n    logger: logging.Logger =None,\n    n_workers: int=10,\n    seed: int=12345,\n    flow_representation:MODELING_INPUT_REPR_TYPE =MODELING_INPUT_REPR_TYPE.FLOWPIC, \n    max_n_pkts:int=10,\n):\n    \"\"\"\n    Arguments:\n        data: if a string, it corresponds to a parquet file from\n            where to load the raw data; if a pandas DataFrame,\n            the data to use for the dataset\n        timetofirst_colname: the column name mapping to the\n            packet timeseries of containing timestamps\n            (relative to the first packet of the time series)\n        pkts_size_colname: the column name mapping to the\n            packet size time series\n        target_colname: the column name where the labels are stored\n        flowpic_dim: the flowpic resolution to use\n        flowpic_block_duration: how many seconds of the\n            input data need to be used to generate a flowpic\n        aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n            \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        aug_samples: how many samples to create for each\n            already existing sample\n        quiet: if False, no output on the console is generated when loading\n        logger: the logger to use\n        n_workers: how many processes to spawn when processing the data\n        seed: random seed\n        flow_representation: a MODELING_INPUT_REPR_TYPE value\n        max_n_pkts: packet series len (if flow_representation == MODELING_INPUT_REPR_TYPE.PKTSERIES)\n    \"\"\"\n    super().__init__(\n        data,\n        timetofirst_colname=timetofirst_colname,\n        pkts_size_colname=pkts_size_colname,\n        pkts_dir_colname=pkts_dir_colname,\n        target_colname=target_colname,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        quiet=quiet,\n        logger=logger,\n        n_workers=n_workers,\n        flow_representation=flow_representation,\n        max_n_pkts=max_n_pkts,\n    )\n    self.aug_samples = aug_samples\n    self.seed = seed\n\n    self.df = self.samples_augmentation(\n        aug_name=aug_name,\n        aug_hparams=aug_hparams,\n        samples=aug_samples,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        seed=seed,\n    )\n\n    self.data = self.df[\"flowpic\"].values\n    self.target = self.df[target_colname].cat.codes.astype(\"int64\").values\n    self.target_colname = target_colname\n    self.num_classes = self.df[target_colname].nunique()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.regenerate_flowpic","title":"<code>regenerate_flowpic(dim=32, block_duration=15)</code>","text":"<p>Overwrite the existing flowpic by creating new ones</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A new dataframe with a \"flowpic\" column reporting the flowpic for each available sample</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def regenerate_flowpic(\n    self, dim: int = 32, block_duration: int = 15\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Overwrite the existing flowpic by creating new ones\n\n    Arguments:\n        dim: the flowpic resolution\n        block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n\n    Return:\n        A new dataframe with a \"flowpic\" column reporting the flowpic\n        for each available sample\n    \"\"\"\n    self.df = self.add_flowpic(\n        self.df,\n        self.timetofirst_colname,\n        self.pkts_size_colname,\n        dim,\n        block_duration,\n        self.n_workers,\n    )\n    return self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._worker_aug_torch","title":"<code>_worker_aug_torch(df, aug_class, samples=9, *args, **kwargs)</code>","text":"<p>Helper function for a multi-processing worker handling augmentations related to flowpic (i.e., working on Tensor data)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>a batch of samples to process</p> required <code>aug_class</code> <code>Augmentation</code> <p>an instance of an augmentation class (see augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each existings sample</p> <code>9</code> Return <p>An expanded version of the input dataframe containing the new samples. IMPORTANT: the new DataFrame is performing a shallow copy of the original and act only of a subset of columns. Hence, the returned version might have semantical incosistencies</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def _worker_aug_torch(\n    self,\n    df: pd.DataFrame,\n    aug_class: augmentation.Augmentation,\n    samples: int = 9,\n    *args: List[Any],\n    **kwargs: Dict[str, Any],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Helper function for a multi-processing worker handling\n    augmentations related to flowpic (i.e., working on Tensor data)\n\n    Arguments:\n        df: a batch of samples to process\n        aug_class: an instance of an augmentation class (see augmentation module)\n        samples: how many samples to create for each existings sample\n\n    Return:\n        An expanded version of the input dataframe containing\n        the new samples. IMPORTANT: the new DataFrame is performing\n        a shallow copy of the original and act only of a subset of\n        columns. Hence, the returned version might have semantical\n        incosistencies\n    \"\"\"\n    new_flowpic = []\n    new_aug_params = []\n    df = pd.concat([df.copy() for _ in range(samples)]).assign(\n        is_augmented=True, aug_params={}\n    )\n    dtypes = dict(self.df.dtypes)\n    for idx in range(df.shape[0]):\n        ser = df.iloc[idx]\n        new_sample = ser.copy()\n        new_flowpic.append(aug_class(new_sample[\"flowpic\"]))\n        new_aug_params.append(aug_class.get_params().copy())\n    print(\".\", end=\"\", flush=True)\n    df = df.assign(flowpic=new_flowpic, aug_params=new_aug_params)\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._worker_aug_numpy","title":"<code>_worker_aug_numpy(df, aug_class, samples=9, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Helper function for a multi-processing worker handling augmentations related to time series (i.e., working with numpy arrays)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>a batch of samples to process</p> required <code>aug_class</code> <code>Augmentation</code> <p>an instance of an augmentation class (see augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each existings sample</p> <code>9</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>An expanded version of the input dataframe containing the new samples. IMPORTANT: the new DataFrame is performing a shallow copy of the original and act only of a subset of columns. Hence, the returned version might have semantical incosistencies</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def _worker_aug_numpy(\n    self,\n    df: pd.DataFrame,\n    aug_class: augmentation.Augmentation,\n    samples: int = 9,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n):\n    \"\"\"\n    Helper function for a multi-processing worker handling\n    augmentations related to time series (i.e., working with numpy arrays)\n\n    Arguments:\n        df: a batch of samples to process\n        aug_class: an instance of an augmentation class (see augmentation module)\n        samples: how many samples to create for each existings sample\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n\n    Return:\n        An expanded version of the input dataframe containing\n        the new samples. IMPORTANT: the new DataFrame is performing\n        a shallow copy of the original and act only of a subset of\n        columns. Hence, the returned version might have semantical\n        incosistencies\n    \"\"\"\n    new_timetofirst = []\n    new_pkts_size = []\n    new_flowpic = []\n    new_aug_params = []\n    df = pd.concat([df.copy() for _ in range(samples)]).assign(\n        is_augmented=True, aug_params={}\n    )\n    for idx in range(df.shape[0]):\n        ser = df.iloc[idx]\n        _timetofirst, _pkts_size, indexes = aug_class(\n            ser[self.timetofirst_colname], ser[self.pkts_size_colname]\n        )\n        new_aug_params.append(aug_class.get_params().copy())\n        new_timetofirst.append(_timetofirst)\n        new_pkts_size.append(_pkts_size)\n        new_flowpic.append(\n            augmentation.get_flowpic(\n                _timetofirst, _pkts_size, flowpic_dim, flowpic_block_duration\n            )\n        )\n    df = df.assign(\n        timetofirst=new_timetofirst,\n        pkts_size=new_pkts_size,\n        flowpic=new_flowpic,\n        aug_params=new_aug_params,\n    )\n    print(\".\", end=\"\", flush=True)\n    return df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset._samples_augmentation_loop","title":"<code>_samples_augmentation_loop(aug_name, aug_hparams, samples, worker_func, seed=12345, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Helper function handling the main loop for samples augmentation by means of multiprocessing</p> <p>Parameters:</p> Name Type Description Default <code>aug_name</code> <code>str</code> <p>one of {\"noaug\", \"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"</p> required <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> required <code>samples</code> <code>int</code> <p>how many samples to create for each already existing sample</p> required <code>worker_func</code> <code>Callable</code> <p>the callback to use for augmentation</p> required <code>seed</code> <code>int</code> <p>the seed to use for augmentation</p> <code>12345</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A pandas DataFrame with all the original samples plus the augmented ones</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def _samples_augmentation_loop(\n    self,\n    aug_name: str,\n    aug_hparams: Dict[str, Any],\n    samples: int,\n    worker_func: Callable,\n    seed: int = 12345,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Helper function handling the main loop for samples augmentation\n    by means of multiprocessing\n\n    Arguments:\n        aug_name: one of {\"noaug\", \"rotate\", \"horizontalflip\",\n            \"colorjitter\", \"packetloss\", \"changertt\", \"timeshift\"\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        samples: how many samples to create for each\n            already existing sample\n        worker_func: the callback to use for augmentation\n        seed: the seed to use for augmentation\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n\n    Return:\n        A pandas DataFrame with all the original samples plus\n        the augmented ones\n    \"\"\"\n    if aug_hparams is None:\n        aug_hparams = dict()\n    params = []\n    indexes = self.df.index.values\n    partition_size = indexes.shape[0] // self.n_workers\n    for idx in range(0, len(indexes), partition_size):\n        rng = np.random.default_rng(seed + idx)\n        # aug_class = augmentation.augmentation_factory(aug_name, rng, **aug_hparams)\n        aug_class = augmentation.augmentation_factory(aug_name, rng, aug_hparams)\n        partition_indexes = indexes[idx : idx + partition_size]\n        params.append(\n            (\n                self.df.loc[partition_indexes],\n                aug_class,\n                samples,\n                flowpic_dim,\n                flowpic_block_duration,\n            )\n        )\n\n    # Note: this is a very dirty trick\n    #\n    # We experienced deadlocks similar to what reported here\n    # https://github.com/pytorch/pytorch/issues/3492\n    # when using torchvision.transforms (with both functional APIs\n    # and instanciating classes). But the logic in the\n    # .augmentation module works fine in single process\n    #\n    # Relying on torch.multiprocessing\n    # https://github.com/pytorch/pytorch/issues/3492\n    # and setting torch.multiprogressing.set_start_method('spawn')\n    # (in if __name__ == '__main__') fixed the issue\n    #\n    # But this requires invoking the .Pool() differently\n    # depending on the underlining augmentation (pytorch or numpy)\n    if worker_func.__name__ == \"_worker_aug_torch\":\n        with torch.multiprocessing.Pool(self.n_workers) as pool:\n            augmented_l = pool.starmap(worker_func, params)\n    else:\n        with multiprocessing.Pool(self.n_workers) as pool:\n            augmented_l = pool.starmap(worker_func, params)\n\n    self.df = pd.concat([self.df] + augmented_l).reset_index()\n    return self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_dataprep/#tcbench.modeling.dataprep.AugmentWhenLoadingDataset.samples_augmentation","title":"<code>samples_augmentation(aug_name='noaug', aug_hparams=None, samples=None, seed=12345, flowpic_dim=32, flowpic_block_duration=15)</code>","text":"<p>Applies samples augmentation</p> <p>Parameters:</p> Name Type Description Default <code>aug_name</code> <code>str</code> <p>one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}</p> <code>'noaug'</code> <code>aug_hparams</code> <code>Dict[str, Any]</code> <p>the augmentation parameters (see the augmentation module)</p> <code>None</code> <code>samples</code> <code>int</code> <p>final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)</p> <code>None</code> <code>seed</code> <code>int</code> <p>random number generator seed</p> <code>12345</code> <code>flowpic_dim</code> <code>int</code> <p>the flowpic resolution</p> <code>32</code> <code>flowpic_block_duration</code> <code>int</code> <p>the max number of seconds for each sample time series to consider when computing a flowpic</p> <code>15</code> Return <p>A pandas DataFrame with all original samples and the augmented ones</p> Source code in <code>src/tcbench/modeling/dataprep.py</code> <pre><code>def samples_augmentation(\n    self,\n    aug_name:str=\"noaug\",\n    aug_hparams:Dict[str, Any]=None,\n    samples:int=None,\n    seed:int=12345,\n    flowpic_dim:int=32,\n    flowpic_block_duration:int=15,\n):\n    \"\"\"Applies samples augmentation\n\n    Arguments:\n        aug_name: one of {\"rotate\", \"horizontalflip\", \"colorjitter\", \"packetloss\", \"timeshift\", \"changertt\" or \"noaug\"}\n        aug_hparams: the augmentation parameters (see the augmentation module)\n        samples: final number of samples for each individual sample (e.g., samples=10 means the original sample and 9 augmented versions)\n        seed: random number generator seed\n        flowpic_dim: the flowpic resolution\n        flowpic_block_duration: the max number of seconds for each sample time series\n            to consider when computing a flowpic\n\n    Return:\n        A pandas DataFrame with all original samples and the augmented ones\n    \"\"\"\n    if samples is None:\n        samples = self.aug_samples\n\n    if aug_name not in augmentation.AUGMENTATION_CLASSES:\n        self.log_msg(\"no augmentation\")\n        return self.df.assign(is_augmented=False)\n\n    self.df = self.df.assign(is_augmented=False)\n    samples -= 1\n    if aug_name == \"horizontalflip\":\n        samples = 1\n    worker_func = self._worker_aug_numpy\n    if aug_name in (\"rotate\", \"horizontalflip\", \"colorjitter\"):\n        worker_func = self._worker_aug_torch\n\n    self.log_msg(f\"data augmentation ({aug_name})\")\n    self._samples_augmentation_loop(\n        aug_name=aug_name,\n        aug_hparams=aug_hparams,\n        samples=samples,\n        worker_func=worker_func,\n        seed=seed,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n    )\n    print()\n    return self.df\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_losses/","title":"Tcbench modeling losses","text":"<p>Code borrowed from https://github.com/HobbitLong/SupContrast</p>"},{"location":"tcbench/api/tcbench_modeling_losses/#tcbench.modeling.losses.SupConLoss","title":"<code>SupConLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf. It also supports the unsupervised contrastive loss in SimCLR</p> Source code in <code>src/tcbench/modeling/losses.py</code> <pre><code>class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n\n    def __init__(\n        self,\n        temperature=0.07,\n        contrast_mode=\"all\",\n        base_temperature=0.07,\n        topn_acc_rank=5,\n    ):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n        self.topn_acc_rank = topn_acc_rank\n\n    def forward(\n        self,\n        features: torch.Tensor,\n        labels: torch.Tensor = None,\n        mask: torch.Tensor = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n        it degenerates to SimCLR unsupervised loss:\n        https://arxiv.org/pdf/2002.05709.pdf\n\n        Arguments:\n            features: hidden vector of shape [bsz, n_views, ...].\n            labels: ground truth of shape [bsz].\n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n                has the same class as sample i. Can be asymmetric.\n        Return:\n            A dictionary with key \"loss\" corresponding to the computed\n            loss; \"acc_top_1\" with the top-1 accuracy computed;\n            \"acc_top_n\" with the top-n accuracy computed\n        \"\"\"\n        device = torch.device(\"cuda\") if features.is_cuda else torch.device(\"cpu\")\n\n        if len(features.shape) &lt; 3:\n            raise ValueError(\n                \"`features` needs to be [bsz, n_views, ...],\"\n                \"at least 3 dimensions are required\"\n            )\n        if len(features.shape) &gt; 3:\n            features = features.view(features.shape[0], features.shape[1], -1)\n\n        batch_size = features.shape[0]\n        if labels is not None and mask is not None:\n            raise ValueError(\"Cannot define both `labels` and `mask`\")\n        elif labels is None and mask is None:\n            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n        elif labels is not None:\n            labels = labels.contiguous().view(-1, 1)\n            if labels.shape[0] != batch_size:\n                raise ValueError(\"Num of labels does not match num of features\")\n            mask = torch.eq(labels, labels.T).float().to(device)\n        else:\n            mask = mask.float().to(device)\n\n        contrast_count = features.shape[1]\n        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n        if self.contrast_mode == \"one\":\n            anchor_feature = features[:, 0]\n            anchor_count = 1\n        elif self.contrast_mode == \"all\":\n            anchor_feature = contrast_feature\n            anchor_count = contrast_count\n        else:\n            raise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n\n        # compute logits\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T), self.temperature\n        )\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # tile mask\n        # meaning, repeating the mask horizontally and vertically\n        # this is likely becoming a diagonal matrix\n        # with also the position of the positives\n        mask = mask.repeat(anchor_count, contrast_count)\n\n        # mask-out self-contrast cases\n        # meaning, a matrix of 1s without the main diagonal\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n            0,\n        )\n\n        # now the mask is selecting only the positives\n        mask = mask * logits_mask\n\n        # compute log_prob\n        exp_logits = torch.exp(logits) * logits_mask\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n\n        # compute mean of log-likelihood over positive\n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n\n        # loss\n        loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.view(anchor_count, batch_size).mean()\n\n        #########################\n        # compute top n accuracy\n        #########################\n        diagonal = torch.eye(exp_logits.shape[0], dtype=bool).to(device)\n        pos_mask = mask.bool()\n        exp_logits_noself = exp_logits.masked_fill(diagonal, -9e15)\n\n        comb_exp_logits_noself = torch.cat(\n            [\n                exp_logits_noself[pos_mask][:, None],\n                exp_logits_noself.masked_fill(pos_mask, -9e15),\n            ],\n            dim=-1,\n        )\n        sim_argsort = comb_exp_logits_noself.argsort(dim=-1, descending=True).argmin(\n            dim=-1\n        )\n        acc_top_1 = (sim_argsort == 0).float().mean()\n        acc_top_n = (sim_argsort &lt; self.topn_acc_rank).float().mean()\n\n        return {\n            \"loss\": loss,\n            \"acc_top_1\": acc_top_1,\n            f\"acc_top_{self.topn_acc_rank}\": acc_top_n,\n        }\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_losses/#tcbench.modeling.losses.SupConLoss.forward","title":"<code>forward(features, labels=None, mask=None)</code>","text":"<p>Compute loss for model. If both <code>labels</code> and <code>mask</code> are None, it degenerates to SimCLR unsupervised loss: https://arxiv.org/pdf/2002.05709.pdf</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>hidden vector of shape [bsz, n_views, ...].</p> required <code>labels</code> <code>Tensor</code> <p>ground truth of shape [bsz].</p> <code>None</code> <code>mask</code> <code>Tensor</code> <p>contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j has the same class as sample i. Can be asymmetric.</p> <code>None</code> <p>Return:     A dictionary with key \"loss\" corresponding to the computed     loss; \"acc_top_1\" with the top-1 accuracy computed;     \"acc_top_n\" with the top-n accuracy computed</p> Source code in <code>src/tcbench/modeling/losses.py</code> <pre><code>def forward(\n    self,\n    features: torch.Tensor,\n    labels: torch.Tensor = None,\n    mask: torch.Tensor = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n    it degenerates to SimCLR unsupervised loss:\n    https://arxiv.org/pdf/2002.05709.pdf\n\n    Arguments:\n        features: hidden vector of shape [bsz, n_views, ...].\n        labels: ground truth of shape [bsz].\n        mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n            has the same class as sample i. Can be asymmetric.\n    Return:\n        A dictionary with key \"loss\" corresponding to the computed\n        loss; \"acc_top_1\" with the top-1 accuracy computed;\n        \"acc_top_n\" with the top-n accuracy computed\n    \"\"\"\n    device = torch.device(\"cuda\") if features.is_cuda else torch.device(\"cpu\")\n\n    if len(features.shape) &lt; 3:\n        raise ValueError(\n            \"`features` needs to be [bsz, n_views, ...],\"\n            \"at least 3 dimensions are required\"\n        )\n    if len(features.shape) &gt; 3:\n        features = features.view(features.shape[0], features.shape[1], -1)\n\n    batch_size = features.shape[0]\n    if labels is not None and mask is not None:\n        raise ValueError(\"Cannot define both `labels` and `mask`\")\n    elif labels is None and mask is None:\n        mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n    elif labels is not None:\n        labels = labels.contiguous().view(-1, 1)\n        if labels.shape[0] != batch_size:\n            raise ValueError(\"Num of labels does not match num of features\")\n        mask = torch.eq(labels, labels.T).float().to(device)\n    else:\n        mask = mask.float().to(device)\n\n    contrast_count = features.shape[1]\n    contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n    if self.contrast_mode == \"one\":\n        anchor_feature = features[:, 0]\n        anchor_count = 1\n    elif self.contrast_mode == \"all\":\n        anchor_feature = contrast_feature\n        anchor_count = contrast_count\n    else:\n        raise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n\n    # compute logits\n    anchor_dot_contrast = torch.div(\n        torch.matmul(anchor_feature, contrast_feature.T), self.temperature\n    )\n    # for numerical stability\n    logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n    logits = anchor_dot_contrast - logits_max.detach()\n\n    # tile mask\n    # meaning, repeating the mask horizontally and vertically\n    # this is likely becoming a diagonal matrix\n    # with also the position of the positives\n    mask = mask.repeat(anchor_count, contrast_count)\n\n    # mask-out self-contrast cases\n    # meaning, a matrix of 1s without the main diagonal\n    logits_mask = torch.scatter(\n        torch.ones_like(mask),\n        1,\n        torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n        0,\n    )\n\n    # now the mask is selecting only the positives\n    mask = mask * logits_mask\n\n    # compute log_prob\n    exp_logits = torch.exp(logits) * logits_mask\n    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n\n    # compute mean of log-likelihood over positive\n    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n\n    # loss\n    loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos\n    loss = loss.view(anchor_count, batch_size).mean()\n\n    #########################\n    # compute top n accuracy\n    #########################\n    diagonal = torch.eye(exp_logits.shape[0], dtype=bool).to(device)\n    pos_mask = mask.bool()\n    exp_logits_noself = exp_logits.masked_fill(diagonal, -9e15)\n\n    comb_exp_logits_noself = torch.cat(\n        [\n            exp_logits_noself[pos_mask][:, None],\n            exp_logits_noself.masked_fill(pos_mask, -9e15),\n        ],\n        dim=-1,\n    )\n    sim_argsort = comb_exp_logits_noself.argsort(dim=-1, descending=True).argmin(\n        dim=-1\n    )\n    acc_top_1 = (sim_argsort == 0).float().mean()\n    acc_top_n = (sim_argsort &lt; self.topn_acc_rank).float().mean()\n\n    return {\n        \"loss\": loss,\n        \"acc_top_1\": acc_top_1,\n        f\"acc_top_{self.topn_acc_rank}\": acc_top_n,\n    }\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/","title":"Tcbench modeling methods","text":"<p>This modules contains a set of classes  used to handle training and inference, namely Trainer objects</p>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss","title":"<code>PatienceMonitorLoss</code>","text":"<p>A callable class implementing monitoring of a loss metric</p> <p>Attributes:</p> Name Type Description <code>steps</code> <p>the maximum patience</p> <code>steps_left</code> <p>steps left before patience expires</p> <code>min_delta</code> <p>the minimum difference against the best loss observed so far to be considered as an improvement</p> <code>best_loss</code> <p>the best loss observed so far</p> <code>best_epoch</code> <p>teh epoch when the best loss was observed</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class PatienceMonitorLoss:\n    \"\"\"A callable class implementing monitoring of\n    a loss metric\n\n    Attributes:\n        steps: the maximum patience\n        steps_left: steps left before patience expires\n        min_delta: the minimum difference against\n            the best loss observed so far to\n            be considered as an improvement\n        best_loss: the best loss observed so far\n        best_epoch: teh epoch when the best loss was observed\n    \"\"\"\n\n    def __init__(self, steps: int = 5, min_delta: float = 0.001):\n        \"\"\"\n        Arguments:\n            steps: the maximum patience\n            min_delta: the minimum difference against\n        \"\"\"\n        self.steps = steps\n        self.steps_left = steps\n        self.min_delta = min_delta\n        self.best_loss = np.inf\n        self.best_epoch = -1\n\n    def is_improved(self, loss: float) -&gt; bool:\n        \"\"\"Returns true if input loss differ\n        from the best observed loss by at least\n        min_delta\"\"\"\n        diff = self.best_loss - loss\n        return diff &gt; self.min_delta\n\n    def is_expired(self) -&gt; bool:\n        \"\"\"Returns True if steps_left == 0\"\"\"\n        return self.steps_left == 0\n\n    def get_best_metrics(self) -&gt; Dict[str, float]:\n        \"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\n        return dict(best_loss=self.best_loss, best_epoch=self.best_epoch)\n\n    def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n        \"\"\"The input metrics is a dictionary expected to have\n        a \"loss\" key, and the related value is compared against\n        the best loss observed so far. Returns True if the\n        input loss is improved wrt the best loss observed\"\"\"\n        loss = metrics[\"loss\"]\n        if self.is_improved(loss):\n            self.best_loss = loss\n            self.steps_left = self.steps\n            self.best_epoch = idx_epoch\n            return True\n        self.steps_left -= 1\n        return False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.__init__","title":"<code>__init__(steps=5, min_delta=0.001)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>the maximum patience</p> <code>5</code> <code>min_delta</code> <code>float</code> <p>the minimum difference against</p> <code>0.001</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(self, steps: int = 5, min_delta: float = 0.001):\n    \"\"\"\n    Arguments:\n        steps: the maximum patience\n        min_delta: the minimum difference against\n    \"\"\"\n    self.steps = steps\n    self.steps_left = steps\n    self.min_delta = min_delta\n    self.best_loss = np.inf\n    self.best_epoch = -1\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.is_improved","title":"<code>is_improved(loss)</code>","text":"<p>Returns true if input loss differ from the best observed loss by at least min_delta</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def is_improved(self, loss: float) -&gt; bool:\n    \"\"\"Returns true if input loss differ\n    from the best observed loss by at least\n    min_delta\"\"\"\n    diff = self.best_loss - loss\n    return diff &gt; self.min_delta\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.is_expired","title":"<code>is_expired()</code>","text":"<p>Returns True if steps_left == 0</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def is_expired(self) -&gt; bool:\n    \"\"\"Returns True if steps_left == 0\"\"\"\n    return self.steps_left == 0\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.get_best_metrics","title":"<code>get_best_metrics()</code>","text":"<p>Returns a dictionary with best loss and epoch observed</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def get_best_metrics(self) -&gt; Dict[str, float]:\n    \"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\n    return dict(best_loss=self.best_loss, best_epoch=self.best_epoch)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorLoss.__call__","title":"<code>__call__(metrics, idx_epoch)</code>","text":"<p>The input metrics is a dictionary expected to have a \"loss\" key, and the related value is compared against the best loss observed so far. Returns True if the input loss is improved wrt the best loss observed</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n    \"\"\"The input metrics is a dictionary expected to have\n    a \"loss\" key, and the related value is compared against\n    the best loss observed so far. Returns True if the\n    input loss is improved wrt the best loss observed\"\"\"\n    loss = metrics[\"loss\"]\n    if self.is_improved(loss):\n        self.best_loss = loss\n        self.steps_left = self.steps\n        self.best_epoch = idx_epoch\n        return True\n    self.steps_left -= 1\n    return False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy","title":"<code>PatienceMonitorAccuracy</code>","text":"<p>A callable class implementing monitoring of a performance metric</p> <p>Attributes     steps: the maximum patience     steps_left: steps left before patience expires     best_acc: the best loss observed so far     best_epoch: the epoch when the best loss was observed     acc_name: the name of the performance metric</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class PatienceMonitorAccuracy:\n    \"\"\"A callable class implementing monitoring of\n    a performance metric\n\n    Attributes\n        steps: the maximum patience\n        steps_left: steps left before patience expires\n        best_acc: the best loss observed so far\n        best_epoch: the epoch when the best loss was observed\n        acc_name: the name of the performance metric\n    \"\"\"\n\n    def __init__(self, name: str = \"acc\", steps: int = 3):\n        \"\"\"\n        Arguments:\n            name: the name of the performance metric\n            steps: the maximum patience\n        \"\"\"\n        self.steps = steps\n        self.steps_left = steps\n        self.best_acc = -np.inf\n        self.best_epoch = -1\n        self.acc_name = name\n\n    def is_improved(self, acc: float) -&gt; bool:\n        \"\"\"Return True if the input accuracy is\n        higher than the best observed so far\"\"\"\n        return acc &gt; self.best_acc\n\n    def is_expired(self) -&gt; bool:\n        \"\"\"Returns True if steps_left == 0\"\"\"\n        return self.steps_left == 0\n\n    def get_best_metrics(self) -&gt; Dict[str, float]:\n        \"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\n        name = f\"best_{self.acc_name}\"\n        return {name: self.best_acc, \"best_epoch\": self.best_epoch}\n\n    def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n        \"\"\"The input metrics is a dictionary expected to have\n        a key with the same name provided when instanciating\n        the class. The related value is compared against\n        the best loss observed so far. Returns True if the\n        performance metric is improved wrt the best performance observed\"\"\"\n        acc = metrics[self.acc_name]\n        if self.is_improved(acc):\n            self.best_acc = acc\n            self.steps_left = self.steps\n            self.best_epoch = idx_epoch\n            return True\n        self.steps_left -= 1\n        return False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.__init__","title":"<code>__init__(name='acc', steps=3)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the performance metric</p> <code>'acc'</code> <code>steps</code> <code>int</code> <p>the maximum patience</p> <code>3</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(self, name: str = \"acc\", steps: int = 3):\n    \"\"\"\n    Arguments:\n        name: the name of the performance metric\n        steps: the maximum patience\n    \"\"\"\n    self.steps = steps\n    self.steps_left = steps\n    self.best_acc = -np.inf\n    self.best_epoch = -1\n    self.acc_name = name\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.is_improved","title":"<code>is_improved(acc)</code>","text":"<p>Return True if the input accuracy is higher than the best observed so far</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def is_improved(self, acc: float) -&gt; bool:\n    \"\"\"Return True if the input accuracy is\n    higher than the best observed so far\"\"\"\n    return acc &gt; self.best_acc\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.is_expired","title":"<code>is_expired()</code>","text":"<p>Returns True if steps_left == 0</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def is_expired(self) -&gt; bool:\n    \"\"\"Returns True if steps_left == 0\"\"\"\n    return self.steps_left == 0\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.get_best_metrics","title":"<code>get_best_metrics()</code>","text":"<p>Returns a dictionary with best loss and epoch observed</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def get_best_metrics(self) -&gt; Dict[str, float]:\n    \"\"\"Returns a dictionary with best loss and epoch observed\"\"\"\n    name = f\"best_{self.acc_name}\"\n    return {name: self.best_acc, \"best_epoch\": self.best_epoch}\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.PatienceMonitorAccuracy.__call__","title":"<code>__call__(metrics, idx_epoch)</code>","text":"<p>The input metrics is a dictionary expected to have a key with the same name provided when instanciating the class. The related value is compared against the best loss observed so far. Returns True if the performance metric is improved wrt the best performance observed</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __call__(self, metrics: Dict[str, float], idx_epoch: int) -&gt; bool:\n    \"\"\"The input metrics is a dictionary expected to have\n    a key with the same name provided when instanciating\n    the class. The related value is compared against\n    the best loss observed so far. Returns True if the\n    performance metric is improved wrt the best performance observed\"\"\"\n    acc = metrics[self.acc_name]\n    if self.is_improved(acc):\n        self.best_acc = acc\n        self.steps_left = self.steps\n        self.best_epoch = idx_epoch\n        return True\n    self.steps_left -= 1\n    return False\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer","title":"<code>SimpleTrainer</code>","text":"<p>A base class offering functionality for training and testing a supervised model</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class SimpleTrainer:\n    \"\"\"A base class offering functionality for\n    training and testing a supervised model\n    \"\"\"\n\n    def __init__(\n        self,\n        net: backbone.BaseNet,\n        optimizer: pytorch.optim.Optimizer,\n        criterion: torch.nn.Module,\n        device: str = \"cuda:0\",\n        deterministic: bool = True,\n        tracker: aim.Run = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"\n        Arguments:\n            net: the architecture to use\n            optimizer: the optimizer to use\n            criterion: the instance of the loss to use\n            device: the device to use\n            deterministic: see _make_deterministic()\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n        \"\"\"\n        if deterministic:\n            _make_deterministic()\n\n        self.device = device\n        self.optimizer = optimizer\n        self.logger = logger\n\n        self.net = net\n        if self.net:\n            self.net = net.double().to(device)\n        self.criterion = criterion\n        self.tracker = tracker\n        self._is_training = False\n        self._reset_metrics()\n\n    def log_msg(self, msg: str) -&gt; None:\n        \"\"\"Register a message to file and echoes it\n        to the console\"\"\"\n        utils.log_msg(msg, self.logger)\n\n    def _reset_metrics(self) -&gt; None:\n        \"\"\"Helper method to clean (before training)\n        internal objects used for tracking metrics\n        \"\"\"\n        self.best_model = None\n        self.metrics = defaultdict(list)\n\n    def _track_metrics(\n        self, metrics: Dict[str, float], context: str, epoch: int = None\n    ) -&gt; None:\n        \"\"\"Helper method invoked during training, validation\n        and testing to track loss and performance metrics\"\"\"\n        for name, value in metrics.items():\n            self.metrics[f\"{context}_{name}\"].append(value)\n            if self.tracker:\n                self.tracker.track(\n                    value, name, epoch=epoch, context=dict(subset=context)\n                )\n\n    def _do_epoch(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: int,\n        context: str,\n        track_preds_targets: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n            track_preds_targets: if True, return predicted and target labels\n\n        Returns:\n            A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n            If track_preds_targets is True, the dictionary contains also the keys\n           \"preds\" and \"targets\", each mapped to a list of integer labels\n        \"\"\"\n        cum_loss = 0\n        correct = 0\n        samples = 0\n        preds = []\n        targets = []\n        num_batches = int(np.ceil(data_loader.dataset.df.shape[0] / data_loader.batch_size))\n\n        for batch_idx, (x, y) in progress.track(enumerate(data_loader), description='', total=num_batches, transient=True):\n            # x can be a list if the data loader\n            # is associated to a multi-view dataset\n            # but in this context we are not using contrastive\n            # learning, hence we concat the views\n            if isinstance(x, list):\n                y = y.repeat(len(x))\n                x = torch.cat(x, dim=0)\n            x = x.to(self.device)\n            y = y.to(self.device)\n\n            scores = self.net.forward(x)\n            loss = self.criterion(scores, y)\n            cum_loss += loss.item()\n\n            if self._is_training:\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n            y_pred = scores.argmax(dim=1)\n            correct += (y_pred == y).sum().item()\n            samples += x.shape[0]\n\n            if track_preds_targets:\n                preds.extend(y_pred.cpu().numpy().tolist())\n                targets.extend(y.cpu().numpy().tolist())\n\n            #print(\".\", end=\"\", flush=True)\n\n        #print(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\n        #print(\"\\r\", end=\"\", flush=True)\n        metrics = dict(\n            loss=cum_loss / (batch_idx+1),\n            acc=100 * correct / samples,\n        )\n        self._track_metrics(metrics, epoch=idx_epoch, context=context)\n        if track_preds_targets:\n            metrics[\"preds\"] = preds\n            metrics[\"targets\"] = targets\n        return metrics\n\n    def train_one_epoch(\n        self, train_loader: torch.utils.data.DataLoader, idx_epoch: int, context: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n        self.net.train()\n        self._is_training = True\n        out = self._do_epoch(train_loader, idx_epoch, context)\n        self._is_training = False\n        return out\n\n    def train_loop(\n        self,\n        epochs: int,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader = None,\n        patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n        quiet: bool = False,\n        context: str = None,\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        The entry point for triggering training\n\n        Arguments:\n            epochs: number of epochs to run\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Returns:\n            The best model obtained during training\n        \"\"\"\n        if context is None:\n            context = \"\"\n        else:\n            context += \"_\"\n\n        if self.optimizer is None:\n            raise RuntimeError(\"optimizer cannot be None when training\")\n\n        self.net = self.net.to(self.device)\n        self._reset_metrics()\n\n        if backbone.has_dropout_layer(self.net):\n            utils.log_msg(\n                \"---\\nWARNING: Detected Dropout layer!\\nWARNING: During supervised training, the monitored train_acc will be inaccurate\\n---\",\n                self.logger,\n            )\n\n        for idx_epoch in range(epochs):\n            if patience_monitor and patience_monitor.is_expired():\n                break\n            train_metrics = self.train_one_epoch(\n                train_loader, idx_epoch, context=f\"{context}train\"\n            )\n            msg = f\"epoch: {idx_epoch:3d} | \"\n            msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\n            for metric_name, metric_value in train_metrics.items():\n                if \"acc\" not in metric_name:\n                    continue\n                metric_name = f\"train_{metric_name}\"\n                msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n            if val_loader:\n                val_metrics, _ = self.test_loop(\n                    val_loader, idx_epoch, with_reports=False, context=f\"{context}val\"\n                )\n                msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\n                for metric_name, metric_value in val_metrics.items():\n                    if \"acc\" not in metric_name:\n                        continue\n                    metric_name = f\"val_{metric_name}\"\n                    msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n                if patience_monitor and patience_monitor(val_metrics, idx_epoch):\n                    self.best_model = self.net.get_copy()\n                    metrics = patience_monitor.get_best_metrics()\n                    self._track_metrics(metrics, context=f\"{context}val\")\n                    msg += \" | *\"\n            else:\n                if patience_monitor and patience_monitor(train_metrics, idx_epoch):\n                    self.best_model = self.net.get_copy()\n                    metrics = patience_monitor.get_best_metrics()\n                    self._track_metrics(metrics, context=f\"{context}train\")\n                    msg += \" | *\"\n                else:\n                    self.best_model = self.net.get_copy()\n\n            if not quiet:\n                self.log_msg(msg)\n\n        if not quiet:\n            if patience_monitor and patience_monitor.is_expired():\n                self.log_msg(\"run out of patience\")\n            else:\n                self.log_msg(\"reached max epochs\")\n\n        self.net.set_state_dict(self.best_model)\n        return self.net\n\n    def test_loop(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: int = None,\n        with_reports: bool = False,\n        context: str = None,\n    ) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n        \"\"\"\n        Run inference on a model (for testing or validation)\n\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\n        self.net.eval()\n        if context is None:\n            context = \"val\" if self._is_training else \"test\"\n        with torch.no_grad():\n            metrics = self._do_epoch(\n                data_loader, idx_epoch, track_preds_targets=True, context=context\n            )\n\n        preds = metrics[\"preds\"]\n        targets = metrics[\"targets\"]\n        del metrics[\"preds\"]\n        del metrics[\"targets\"]\n\n        self._track_metrics(metrics, epoch=idx_epoch, context=context)\n\n        reports = {}\n        if with_reports:\n            reports = dict(\n                class_rep=pd.DataFrame(\n                    classification_report(targets, preds, output_dict=True)\n                ).T,\n                conf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\n                preds=preds,\n            )\n\n        return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.__init__","title":"<code>__init__(net, optimizer, criterion, device='cuda:0', deterministic=True, tracker=None, logger=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the architecture to use</p> required <code>optimizer</code> <code>Optimizer</code> <p>the optimizer to use</p> required <code>criterion</code> <code>Module</code> <p>the instance of the loss to use</p> required <code>device</code> <code>str</code> <p>the device to use</p> <code>'cuda:0'</code> <code>deterministic</code> <code>bool</code> <p>see _make_deterministic()</p> <code>True</code> <code>tracker</code> <code>Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>the logging reference</p> <code>None</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(\n    self,\n    net: backbone.BaseNet,\n    optimizer: pytorch.optim.Optimizer,\n    criterion: torch.nn.Module,\n    device: str = \"cuda:0\",\n    deterministic: bool = True,\n    tracker: aim.Run = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"\n    Arguments:\n        net: the architecture to use\n        optimizer: the optimizer to use\n        criterion: the instance of the loss to use\n        device: the device to use\n        deterministic: see _make_deterministic()\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n    \"\"\"\n    if deterministic:\n        _make_deterministic()\n\n    self.device = device\n    self.optimizer = optimizer\n    self.logger = logger\n\n    self.net = net\n    if self.net:\n        self.net = net.double().to(device)\n    self.criterion = criterion\n    self.tracker = tracker\n    self._is_training = False\n    self._reset_metrics()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>Register a message to file and echoes it to the console</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n    \"\"\"Register a message to file and echoes it\n    to the console\"\"\"\n    utils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._reset_metrics","title":"<code>_reset_metrics()</code>","text":"<p>Helper method to clean (before training) internal objects used for tracking metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _reset_metrics(self) -&gt; None:\n    \"\"\"Helper method to clean (before training)\n    internal objects used for tracking metrics\n    \"\"\"\n    self.best_model = None\n    self.metrics = defaultdict(list)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._track_metrics","title":"<code>_track_metrics(metrics, context, epoch=None)</code>","text":"<p>Helper method invoked during training, validation and testing to track loss and performance metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _track_metrics(\n    self, metrics: Dict[str, float], context: str, epoch: int = None\n) -&gt; None:\n    \"\"\"Helper method invoked during training, validation\n    and testing to track loss and performance metrics\"\"\"\n    for name, value in metrics.items():\n        self.metrics[f\"{context}_{name}\"].append(value)\n        if self.tracker:\n            self.tracker.track(\n                value, name, epoch=epoch, context=dict(subset=context)\n            )\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer._do_epoch","title":"<code>_do_epoch(data_loader, idx_epoch, context, track_preds_targets=False)</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> required <code>track_preds_targets</code> <code>bool</code> <p>if True, return predicted and target labels</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of metrics containing the measured average \"loss\" and \"acc\".</p> <code>Dict[str, Any]</code> <p>If track_preds_targets is True, the dictionary contains also the keys</p> <p>\"preds\" and \"targets\", each mapped to a list of integer labels</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: int,\n    context: str,\n    track_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n        track_preds_targets: if True, return predicted and target labels\n\n    Returns:\n        A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n        If track_preds_targets is True, the dictionary contains also the keys\n       \"preds\" and \"targets\", each mapped to a list of integer labels\n    \"\"\"\n    cum_loss = 0\n    correct = 0\n    samples = 0\n    preds = []\n    targets = []\n    num_batches = int(np.ceil(data_loader.dataset.df.shape[0] / data_loader.batch_size))\n\n    for batch_idx, (x, y) in progress.track(enumerate(data_loader), description='', total=num_batches, transient=True):\n        # x can be a list if the data loader\n        # is associated to a multi-view dataset\n        # but in this context we are not using contrastive\n        # learning, hence we concat the views\n        if isinstance(x, list):\n            y = y.repeat(len(x))\n            x = torch.cat(x, dim=0)\n        x = x.to(self.device)\n        y = y.to(self.device)\n\n        scores = self.net.forward(x)\n        loss = self.criterion(scores, y)\n        cum_loss += loss.item()\n\n        if self._is_training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        y_pred = scores.argmax(dim=1)\n        correct += (y_pred == y).sum().item()\n        samples += x.shape[0]\n\n        if track_preds_targets:\n            preds.extend(y_pred.cpu().numpy().tolist())\n            targets.extend(y.cpu().numpy().tolist())\n\n        #print(\".\", end=\"\", flush=True)\n\n    #print(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\n    #print(\"\\r\", end=\"\", flush=True)\n    metrics = dict(\n        loss=cum_loss / (batch_idx+1),\n        acc=100 * correct / samples,\n    )\n    self._track_metrics(metrics, epoch=idx_epoch, context=context)\n    if track_preds_targets:\n        metrics[\"preds\"] = preds\n        metrics[\"targets\"] = targets\n    return metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, idx_epoch, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(\n    self, train_loader: torch.utils.data.DataLoader, idx_epoch: int, context: str\n) -&gt; Dict[str, Any]:\n    \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n    self.net.train()\n    self._is_training = True\n    out = self._do_epoch(train_loader, idx_epoch, context)\n    self._is_training = False\n    return out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.train_loop","title":"<code>train_loop(epochs, train_loader, val_loader=None, patience_monitor=None, quiet=False, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>number of epochs to run</p> required <code>train_loader</code> <code>DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseNet</code> <p>The best model obtained during training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_loop(\n    self,\n    epochs: int,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader = None,\n    patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n    quiet: bool = False,\n    context: str = None,\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    The entry point for triggering training\n\n    Arguments:\n        epochs: number of epochs to run\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Returns:\n        The best model obtained during training\n    \"\"\"\n    if context is None:\n        context = \"\"\n    else:\n        context += \"_\"\n\n    if self.optimizer is None:\n        raise RuntimeError(\"optimizer cannot be None when training\")\n\n    self.net = self.net.to(self.device)\n    self._reset_metrics()\n\n    if backbone.has_dropout_layer(self.net):\n        utils.log_msg(\n            \"---\\nWARNING: Detected Dropout layer!\\nWARNING: During supervised training, the monitored train_acc will be inaccurate\\n---\",\n            self.logger,\n        )\n\n    for idx_epoch in range(epochs):\n        if patience_monitor and patience_monitor.is_expired():\n            break\n        train_metrics = self.train_one_epoch(\n            train_loader, idx_epoch, context=f\"{context}train\"\n        )\n        msg = f\"epoch: {idx_epoch:3d} | \"\n        msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\n        for metric_name, metric_value in train_metrics.items():\n            if \"acc\" not in metric_name:\n                continue\n            metric_name = f\"train_{metric_name}\"\n            msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n        if val_loader:\n            val_metrics, _ = self.test_loop(\n                val_loader, idx_epoch, with_reports=False, context=f\"{context}val\"\n            )\n            msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\n            for metric_name, metric_value in val_metrics.items():\n                if \"acc\" not in metric_name:\n                    continue\n                metric_name = f\"val_{metric_name}\"\n                msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n            if patience_monitor and patience_monitor(val_metrics, idx_epoch):\n                self.best_model = self.net.get_copy()\n                metrics = patience_monitor.get_best_metrics()\n                self._track_metrics(metrics, context=f\"{context}val\")\n                msg += \" | *\"\n        else:\n            if patience_monitor and patience_monitor(train_metrics, idx_epoch):\n                self.best_model = self.net.get_copy()\n                metrics = patience_monitor.get_best_metrics()\n                self._track_metrics(metrics, context=f\"{context}train\")\n                msg += \" | *\"\n            else:\n                self.best_model = self.net.get_copy()\n\n        if not quiet:\n            self.log_msg(msg)\n\n    if not quiet:\n        if patience_monitor and patience_monitor.is_expired():\n            self.log_msg(\"run out of patience\")\n        else:\n            self.log_msg(\"reached max epochs\")\n\n    self.net.set_state_dict(self.best_model)\n    return self.net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimpleTrainer.test_loop","title":"<code>test_loop(data_loader, idx_epoch=None, with_reports=False, context=None)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def test_loop(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: int = None,\n    with_reports: bool = False,\n    context: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Run inference on a model (for testing or validation)\n\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\n    self.net.eval()\n    if context is None:\n        context = \"val\" if self._is_training else \"test\"\n    with torch.no_grad():\n        metrics = self._do_epoch(\n            data_loader, idx_epoch, track_preds_targets=True, context=context\n        )\n\n    preds = metrics[\"preds\"]\n    targets = metrics[\"targets\"]\n    del metrics[\"preds\"]\n    del metrics[\"targets\"]\n\n    self._track_metrics(metrics, epoch=idx_epoch, context=context)\n\n    reports = {}\n    if with_reports:\n        reports = dict(\n            class_rep=pd.DataFrame(\n                classification_report(targets, preds, output_dict=True)\n            ).T,\n            conf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\n            preds=preds,\n        )\n\n    return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer","title":"<code>XGboostTrainer</code>","text":"<p>A base class offering functionality for training and testing a supervised model</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class XGboostTrainer:\n    \"\"\"A base class offering functionality for\n    training and testing a supervised model\n    \"\"\"\n\n    def __init__(\n        self,\n        xgboost_model:Any,\n        net:Any=None,\n        device:Any=None,\n        tracker: aim.Run = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"\n        Arguments:\n            xgboost_model: XGboost model\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n        \"\"\"\n        self.xgboost_model = xgboost_model\n        self.logger = logger\n        self.tracker = tracker\n        self._is_training = False\n        self._reset_metrics()\n\n    def log_msg(self, msg: str) -&gt; None:\n        \"\"\"Register a message to file and echoes it\n        to the console\"\"\"\n        utils.log_msg(msg, self.logger)\n\n    def _reset_metrics(self) -&gt; None:\n        \"\"\"Helper method to clean (before training)\n        internal objects used for tracking metrics\n        \"\"\"\n        self.best_model = None\n        self.metrics = defaultdict(list)\n\n    def _track_metrics(self, metrics: Dict[str, float], context: str) -&gt; None:\n        \"\"\"Helper method invoked during training, validation\n        and testing to track loss and performance metrics\"\"\"\n        for name, value in metrics.items():\n            self.metrics[f\"{context}_{name}\"].append(value)\n            if self.tracker:\n                self.tracker.track(value, name, context=dict(subset=context))\n\n    def _do_epoch(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        context: str,\n        track_preds_targets: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n            track_preds_targets: if True, return predicted and target labels\n\n        Returns:\n            A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n            If track_preds_targets is True, the dictionary contains also the keys\n           \"preds\" and \"targets\", each mapped to a list of integer labels\n        \"\"\"\n        cum_loss = 0\n        correct = 0\n        samples = 0\n        preds = []\n        targets = []\n        x_all = []\n        y_all = []\n\n        for batch_idx, (x, y) in enumerate(data_loader):\n            # x can be a list if the data loader\n            # is associated to a multi-view dataset\n            # but in this context we are not using contrastive\n            # learning, hence we concat the views\n            # if isinstance(x, list):\n            #    y = y.repeat(len(x))\n            #    x = torch.cat(x, dim=0)\n\n            # scores = self.net.forward(x)\n            # loss = self.criterion(scores, y)\n            # cum_loss += loss.item()\n            # print(x.reshape(x.shape[0], -1).shape)\n            # x = x.cpu().numpy()\n            # y = y.cpu().numpy()\n            x_all.append(x.reshape(x.shape[0], -1))\n            # y_all.append(y.reshape(y.shape[0], -1))\n            y_all.append(y)\n\n        x_all = np.concatenate(x_all, axis=0)\n        # x_all = xgb.DMatrix(x_all)\n        y_all = np.concatenate(y_all, axis=0)\n        if self._is_training:\n            self.xgboost_model.fit(x_all, y_all)\n\n        y_pred = self.xgboost_model.predict(x_all)\n        correct += (y_pred == y_all).sum().item()\n        samples += x_all.shape[0]\n\n        if track_preds_targets:\n            preds.extend(y_pred.tolist())\n            targets.extend(y_all.tolist())\n\n        print(\".\", end=\"\", flush=True)\n\n        print(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\n        print(\"\\r\", end=\"\", flush=True)\n        metrics = dict(\n            # loss=cum_loss / batch_idx,\n            acc=100\n            * correct\n            / samples,\n        )\n        self._track_metrics(metrics, context=context)\n        if track_preds_targets:\n            metrics[\"preds\"] = preds\n            metrics[\"targets\"] = targets\n        return metrics\n\n    def train_one_epoch(\n        self, train_loader: torch.utils.data.DataLoader, context: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n        self._is_training = True\n        out = self._do_epoch(train_loader, context)\n        self._is_training = False\n        return out\n\n    def train_loop(\n        self,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader = None,\n        patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n        quiet: bool = False,\n        context: str = None,\n    ) -&gt; Any:\n        \"\"\"\n        The entry point for triggering training\n\n        Arguments:\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Returns:\n            The best model obtained during training\n        \"\"\"\n        if context is None:\n            context = \"\"\n        else:\n            context += \"_\"\n\n        self._reset_metrics()\n\n        t1 = time.perf_counter_ns()\n        # for idx_epoch in range(epochs):\n        train_metrics = self.train_one_epoch(train_loader, context=f\"{context}train\")\n        t2 = time.perf_counter_ns()\n        if self.tracker:\n            self.tracker.track((t2 - t1) / 1E9, 'duration', context=dict(subset=f\"{context}train\"))\n\n        # msg = f\"epoch: {idx_epoch:3d} | \"\n        # msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\n        msg = \"\"\n        for metric_name, metric_value in train_metrics.items():\n            if \"acc\" not in metric_name:\n                continue\n            metric_name = f\"train_{metric_name}\"\n            msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n        if val_loader:\n            val_metrics, _ = self.test_loop(\n                val_loader, with_reports=False, context=f\"{context}val\"\n            )\n            # msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\n            for metric_name, metric_value in val_metrics.items():\n                if \"acc\" not in metric_name:\n                    continue\n                metric_name = f\"val_{metric_name}\"\n                msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n            if patience_monitor and patience_monitor(val_metrics):\n                self.best_model = self.net.get_copy()\n                metrics = patience_monitor.get_best_metrics()\n                self._track_metrics(metrics, context=f\"{context}val\")\n                msg += \" | *\"\n        else:\n            if patience_monitor and patience_monitor(train_metrics):\n                self.best_model = self.net.get_copy()\n                metrics = patience_monitor.get_best_metrics()\n                self._track_metrics(metrics, context=f\"{context}train\")\n                msg += \" | *\"\n            else:\n                pass\n\n        if not quiet:\n            self.log_msg(msg)\n\n        if not quiet:\n            self.log_msg(\"done\")\n\n        # self.net.set_state_dict(self.best_model)\n        return self.xgboost_model\n\n    def test_loop(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        with_reports: bool = False,\n        context: str = None,\n    ) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n        \"\"\"\n        Run inference on a model (for testing or validation)\n\n        Arguments:\n            data_loader: the data to use\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\n        if context is None:\n            context = \"val\" if self._is_training else \"test\"\n\n        t1 = time.perf_counter_ns()\n        metrics = self._do_epoch(data_loader, track_preds_targets=True, context=context)\n        t2 = time.perf_counter_ns()\n        metrics['duration'] = (t2 - t1) / 1E9\n\n        preds = metrics[\"preds\"]\n        targets = metrics[\"targets\"]\n        del metrics[\"preds\"]\n        del metrics[\"targets\"]\n\n        self._track_metrics(metrics, context=context)\n\n        reports = {}\n        if with_reports:\n            reports = dict(\n                class_rep=pd.DataFrame(\n                    classification_report(targets, preds, output_dict=True)\n                ).T,\n                conf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\n                preds=preds,\n            )\n\n        return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.__init__","title":"<code>__init__(xgboost_model, net=None, device=None, tracker=None, logger=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>xgboost_model</code> <code>Any</code> <p>XGboost model</p> required <code>tracker</code> <code>Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>the logging reference</p> <code>None</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(\n    self,\n    xgboost_model:Any,\n    net:Any=None,\n    device:Any=None,\n    tracker: aim.Run = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"\n    Arguments:\n        xgboost_model: XGboost model\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n    \"\"\"\n    self.xgboost_model = xgboost_model\n    self.logger = logger\n    self.tracker = tracker\n    self._is_training = False\n    self._reset_metrics()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.log_msg","title":"<code>log_msg(msg)</code>","text":"<p>Register a message to file and echoes it to the console</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def log_msg(self, msg: str) -&gt; None:\n    \"\"\"Register a message to file and echoes it\n    to the console\"\"\"\n    utils.log_msg(msg, self.logger)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._reset_metrics","title":"<code>_reset_metrics()</code>","text":"<p>Helper method to clean (before training) internal objects used for tracking metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _reset_metrics(self) -&gt; None:\n    \"\"\"Helper method to clean (before training)\n    internal objects used for tracking metrics\n    \"\"\"\n    self.best_model = None\n    self.metrics = defaultdict(list)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._track_metrics","title":"<code>_track_metrics(metrics, context)</code>","text":"<p>Helper method invoked during training, validation and testing to track loss and performance metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _track_metrics(self, metrics: Dict[str, float], context: str) -&gt; None:\n    \"\"\"Helper method invoked during training, validation\n    and testing to track loss and performance metrics\"\"\"\n    for name, value in metrics.items():\n        self.metrics[f\"{context}_{name}\"].append(value)\n        if self.tracker:\n            self.tracker.track(value, name, context=dict(subset=context))\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer._do_epoch","title":"<code>_do_epoch(data_loader, context, track_preds_targets=False)</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> required <code>track_preds_targets</code> <code>bool</code> <p>if True, return predicted and target labels</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of metrics containing the measured average \"loss\" and \"acc\".</p> <code>Dict[str, Any]</code> <p>If track_preds_targets is True, the dictionary contains also the keys</p> <p>\"preds\" and \"targets\", each mapped to a list of integer labels</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    context: str,\n    track_preds_targets: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n        track_preds_targets: if True, return predicted and target labels\n\n    Returns:\n        A dictionary of metrics containing the measured average \"loss\" and \"acc\".\n        If track_preds_targets is True, the dictionary contains also the keys\n       \"preds\" and \"targets\", each mapped to a list of integer labels\n    \"\"\"\n    cum_loss = 0\n    correct = 0\n    samples = 0\n    preds = []\n    targets = []\n    x_all = []\n    y_all = []\n\n    for batch_idx, (x, y) in enumerate(data_loader):\n        # x can be a list if the data loader\n        # is associated to a multi-view dataset\n        # but in this context we are not using contrastive\n        # learning, hence we concat the views\n        # if isinstance(x, list):\n        #    y = y.repeat(len(x))\n        #    x = torch.cat(x, dim=0)\n\n        # scores = self.net.forward(x)\n        # loss = self.criterion(scores, y)\n        # cum_loss += loss.item()\n        # print(x.reshape(x.shape[0], -1).shape)\n        # x = x.cpu().numpy()\n        # y = y.cpu().numpy()\n        x_all.append(x.reshape(x.shape[0], -1))\n        # y_all.append(y.reshape(y.shape[0], -1))\n        y_all.append(y)\n\n    x_all = np.concatenate(x_all, axis=0)\n    # x_all = xgb.DMatrix(x_all)\n    y_all = np.concatenate(y_all, axis=0)\n    if self._is_training:\n        self.xgboost_model.fit(x_all, y_all)\n\n    y_pred = self.xgboost_model.predict(x_all)\n    correct += (y_pred == y_all).sum().item()\n    samples += x_all.shape[0]\n\n    if track_preds_targets:\n        preds.extend(y_pred.tolist())\n        targets.extend(y_all.tolist())\n\n    print(\".\", end=\"\", flush=True)\n\n    print(\"\\r\" + \" \" * (batch_idx + 1), end=\"\", flush=True)\n    print(\"\\r\", end=\"\", flush=True)\n    metrics = dict(\n        # loss=cum_loss / batch_idx,\n        acc=100\n        * correct\n        / samples,\n    )\n    self._track_metrics(metrics, context=context)\n    if track_preds_targets:\n        metrics[\"preds\"] = preds\n        metrics[\"targets\"] = targets\n    return metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(\n    self, train_loader: torch.utils.data.DataLoader, context: str\n) -&gt; Dict[str, Any]:\n    \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n    self._is_training = True\n    out = self._do_epoch(train_loader, context)\n    self._is_training = False\n    return out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.train_loop","title":"<code>train_loop(train_loader, val_loader=None, patience_monitor=None, quiet=False, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The best model obtained during training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_loop(\n    self,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader = None,\n    patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n    quiet: bool = False,\n    context: str = None,\n) -&gt; Any:\n    \"\"\"\n    The entry point for triggering training\n\n    Arguments:\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Returns:\n        The best model obtained during training\n    \"\"\"\n    if context is None:\n        context = \"\"\n    else:\n        context += \"_\"\n\n    self._reset_metrics()\n\n    t1 = time.perf_counter_ns()\n    # for idx_epoch in range(epochs):\n    train_metrics = self.train_one_epoch(train_loader, context=f\"{context}train\")\n    t2 = time.perf_counter_ns()\n    if self.tracker:\n        self.tracker.track((t2 - t1) / 1E9, 'duration', context=dict(subset=f\"{context}train\"))\n\n    # msg = f\"epoch: {idx_epoch:3d} | \"\n    # msg += \"train_loss: {loss:.6f}\".format(loss=train_metrics[\"loss\"])\n    msg = \"\"\n    for metric_name, metric_value in train_metrics.items():\n        if \"acc\" not in metric_name:\n            continue\n        metric_name = f\"train_{metric_name}\"\n        msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n    if val_loader:\n        val_metrics, _ = self.test_loop(\n            val_loader, with_reports=False, context=f\"{context}val\"\n        )\n        # msg += \" | val_loss: {loss:.6f}\".format(loss=val_metrics[\"loss\"])\n        for metric_name, metric_value in val_metrics.items():\n            if \"acc\" not in metric_name:\n                continue\n            metric_name = f\"val_{metric_name}\"\n            msg += f\" | {metric_name}: {metric_value:5.1f}%\"\n\n        if patience_monitor and patience_monitor(val_metrics):\n            self.best_model = self.net.get_copy()\n            metrics = patience_monitor.get_best_metrics()\n            self._track_metrics(metrics, context=f\"{context}val\")\n            msg += \" | *\"\n    else:\n        if patience_monitor and patience_monitor(train_metrics):\n            self.best_model = self.net.get_copy()\n            metrics = patience_monitor.get_best_metrics()\n            self._track_metrics(metrics, context=f\"{context}train\")\n            msg += \" | *\"\n        else:\n            pass\n\n    if not quiet:\n        self.log_msg(msg)\n\n    if not quiet:\n        self.log_msg(\"done\")\n\n    # self.net.set_state_dict(self.best_model)\n    return self.xgboost_model\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.XGboostTrainer.test_loop","title":"<code>test_loop(data_loader, with_reports=False, context=None)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>the data to use</p> required <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def test_loop(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    with_reports: bool = False,\n    context: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Run inference on a model (for testing or validation)\n\n    Arguments:\n        data_loader: the data to use\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\n    if context is None:\n        context = \"val\" if self._is_training else \"test\"\n\n    t1 = time.perf_counter_ns()\n    metrics = self._do_epoch(data_loader, track_preds_targets=True, context=context)\n    t2 = time.perf_counter_ns()\n    metrics['duration'] = (t2 - t1) / 1E9\n\n    preds = metrics[\"preds\"]\n    targets = metrics[\"targets\"]\n    del metrics[\"preds\"]\n    del metrics[\"targets\"]\n\n    self._track_metrics(metrics, context=context)\n\n    reports = {}\n    if with_reports:\n        reports = dict(\n            class_rep=pd.DataFrame(\n                classification_report(targets, preds, output_dict=True)\n            ).T,\n            conf_mtx=pd.DataFrame(confusion_matrix(targets, preds)),\n            preds=preds,\n        )\n\n    return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer","title":"<code>ContrastiveLearningTrainer</code>","text":"<p>             Bases: <code>SimpleTrainer</code></p> <p>A trainer designed for contrastive learning</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class ContrastiveLearningTrainer(SimpleTrainer):\n    \"\"\"A trainer designed for contrastive learning\"\"\"\n\n    def __init__(\n        self,\n        net: backbone.BaseNet,\n        optimizer: torch.optim.Optimizer,\n        criterion: torch.nn.Module,\n        device: str = \"cuda:0\",\n        deterministic: bool = True,\n        tracker: aim.Run = None,\n        logger: logging.Logger = None,\n    ):\n        super().__init__(\n            net=net,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            deterministic=deterministic,\n            tracker=tracker,\n            logger=logger,\n        )\n\n    @classmethod\n    def prepare_net_for_train(\n        cls, net: backbone.BaseNet, fname_weights: pathlib.Path = None\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity)\n        its .classifier and the last activation function of .features\n\n        Arguments:\n            net: the network to modify\n            fname_weights: if provided, the weights are loaded into\n                the network after the modification\n\n        Return:\n            A new instance of the input network with architecture\n            modification required to run training contrastive learning\n            training\n        \"\"\"\n        new_net = backbone.clone_net(net)\n        if not hasattr(net, \"prepare_for_contrastivelearning\"):\n            raise RuntimeError(\n                \"Did not find a .prepare_for_contrativelearning() method in the network. Cannot adapt the network for training\"\n            )\n        new_net.prepare_for_contrastivelearning(fname_weights)\n        new_net = new_net.double()\n        return new_net\n\n    @classmethod\n    def init_train(\n        cls,\n        net: backbone.BaseNet,\n        optimizer: torch.optim.Optimizer = None,\n        fname_weights: pathlib.Path = None,\n    ) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n        \"\"\"\n        Clones the input network and prepares it for contrastive learning,\n        and instanciate a new optimized bounding it to the new network weights\n\n        Arguments:\n            net: the network to use\n            optimizer: the optimizer to tuse\n            fname_weights: if provided, the weights are loaded\n                into the new network before returning it\n\n        Return:\n            A tuple with the new updated network and the related optimizer\n        \"\"\"\n        new_net = cls.prepare_net_for_train(net, fname_weights)\n        new_optimizer = None\n        if optimizer:\n            new_optimizer = _reset_optimizer(optimizer, new_net.parameters())\n        return new_net, new_optimizer\n\n    def _do_epoch(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: int,\n        context: str = \"train\",\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Helper method invoked during training, validation\n        and testing to perform forward (and backward) propagation\n\n        Arguments:\n            data_loader: an instance of a pytorch DataLoader\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A dictionary of metrics collected\n        \"\"\"\n        cum_metrics = defaultdict(float)\n        num_batches = int(np.ceil(data_loader.dataset.df.shape[0] / data_loader.batch_size))\n\n        #for batch_idx, (x, y) in enumerate(data_loader):\n        for batch_idx, (x, y) in progress.track(enumerate(data_loader), description='', total=num_batches, transient=True):\n            # x is a list with the multiple views\n            x = torch.cat(x, dim=0).to(self.device)\n            y = y.to(self.device)\n            bsz = y.shape[0]\n\n            # forward pass\n            features = self.net(x)\n            # apply L2 normalization\n            features = torch.nn.functional.normalize(features, dim=1)\n\n            # compute loss\n            f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n            metrics = self.criterion(features)\n            loss = metrics[\"loss\"]\n\n            if self._is_training:\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n            #print(\".\", end=\"\", flush=True)\n\n            for name, value in metrics.items():\n                cum_metrics[name] += value.item()\n\n        #print(\"\\r\" + \" \" * batch_idx + \" \", end=\"\", flush=True)\n        #print(\"\\r\", end=\"\", flush=True)\n        metrics = {}\n        for name, value in cum_metrics.items():\n            value = value / (batch_idx + 1)\n            if name.startswith(\"acc\"):\n                value *= 100\n            metrics[name] = value\n        self._track_metrics(metrics, epoch=idx_epoch, context=context)\n\n        return metrics\n\n    def train_one_epoch(self, train_loader, idx_epoch, context):\n        \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n        self.net.train()\n        self._is_training = True\n        out = self._do_epoch(train_loader, idx_epoch, context)\n        self._is_training = False\n        return out\n\n    def train_loop(\n        self,\n        epochs: int,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoaer = None,\n        patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n        quiet: bool = False,\n        run_init_train: bool = True,\n        context: str = None,\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        The entry point for triggering training\n\n        Arguments:\n            epochs: number of epochs to run\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            quiet: if False, no message on the console is reported\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            The best model obtained during training\n        \"\"\"\n        assert context is not None\n\n        if run_init_train:\n            self.net, self.optimizer = self.init_train(self.net, self.optimizer)\n            self.net = self.net.to(self.device)\n            x, y = next(iter(train_loader))\n            self.log_msg(\n                \"\\n======= net adapted for contrastive learning training =========\",\n                self.logger\n            )\n            #torchsummary.summary(self.net.float(), tuple(x[0][0].shape))\n            utils.log_torchsummary(self.net.float(), tuple(x[0][0].shape), self.logger)\n            self.net.double()\n\n        t1 = time.perf_counter_ns()\n        res = super().train_loop(\n            epochs=epochs,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            patience_monitor=patience_monitor,\n            quiet=quiet,\n            context=context,\n        )\n        t2 = time.perf_counter_ns()\n        if self.tracker:\n            self.tracker.track((t2 - t1) / 1E9, 'duration', context=dict(subset=f\"{context}train\"))\n\n        return res\n\n    def test_loop(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: ind = None,\n        context: str = None,\n        *args,\n        **kwargs,\n    ) -&gt; Tuple[Dict[str, Any], Any]:\n        \"\"\"\n        Run inference on a model (for testing or validation)\n\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second is an empty dictionary\n        \"\"\"\n        self.net.eval()\n\n        if context is None:\n            context = \"val\" if self._is_training else \"test\"\n\n        with torch.no_grad():\n            t1 = time.perf_counter_ns()\n            metrics = self._do_epoch(data_loader, idx_epoch, context=context)\n            t2 = time.perf_counter_ns()\n            metrics['duration'] = (t2 - t1) / 1E9\n\n        # we return an empty report just to have\n        # consistency with the return types\n        # of SimpleTrainer\n        reports = {}\n        return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.prepare_net_for_train","title":"<code>prepare_net_for_train(net, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity) its .classifier and the last activation function of .features</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to modify</p> required <code>fname_weights</code> <code>Path</code> <p>if provided, the weights are loaded into the network after the modification</p> <code>None</code> Return <p>A new instance of the input network with architecture modification required to run training contrastive learning training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef prepare_net_for_train(\n    cls, net: backbone.BaseNet, fname_weights: pathlib.Path = None\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    Clone a backbone.BaseNet a modifies to mask (via torch.nn.Identity)\n    its .classifier and the last activation function of .features\n\n    Arguments:\n        net: the network to modify\n        fname_weights: if provided, the weights are loaded into\n            the network after the modification\n\n    Return:\n        A new instance of the input network with architecture\n        modification required to run training contrastive learning\n        training\n    \"\"\"\n    new_net = backbone.clone_net(net)\n    if not hasattr(net, \"prepare_for_contrastivelearning\"):\n        raise RuntimeError(\n            \"Did not find a .prepare_for_contrativelearning() method in the network. Cannot adapt the network for training\"\n        )\n    new_net.prepare_for_contrastivelearning(fname_weights)\n    new_net = new_net.double()\n    return new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.init_train","title":"<code>init_train(net, optimizer=None, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clones the input network and prepares it for contrastive learning, and instanciate a new optimized bounding it to the new network weights</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to use</p> required <code>optimizer</code> <code>Optimizer</code> <p>the optimizer to tuse</p> <code>None</code> <code>fname_weights</code> <code>Path</code> <p>if provided, the weights are loaded into the new network before returning it</p> <code>None</code> Return <p>A tuple with the new updated network and the related optimizer</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_train(\n    cls,\n    net: backbone.BaseNet,\n    optimizer: torch.optim.Optimizer = None,\n    fname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n    \"\"\"\n    Clones the input network and prepares it for contrastive learning,\n    and instanciate a new optimized bounding it to the new network weights\n\n    Arguments:\n        net: the network to use\n        optimizer: the optimizer to tuse\n        fname_weights: if provided, the weights are loaded\n            into the new network before returning it\n\n    Return:\n        A tuple with the new updated network and the related optimizer\n    \"\"\"\n    new_net = cls.prepare_net_for_train(net, fname_weights)\n    new_optimizer = None\n    if optimizer:\n        new_optimizer = _reset_optimizer(optimizer, new_net.parameters())\n    return new_net, new_optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer._do_epoch","title":"<code>_do_epoch(data_loader, idx_epoch, context='train')</code>","text":"<p>Helper method invoked during training, validation and testing to perform forward (and backward) propagation</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>an instance of a pytorch DataLoader</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> required <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>'train'</code> Return <p>A dictionary of metrics collected</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _do_epoch(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: int,\n    context: str = \"train\",\n) -&gt; Dict[str, Any]:\n    \"\"\"Helper method invoked during training, validation\n    and testing to perform forward (and backward) propagation\n\n    Arguments:\n        data_loader: an instance of a pytorch DataLoader\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A dictionary of metrics collected\n    \"\"\"\n    cum_metrics = defaultdict(float)\n    num_batches = int(np.ceil(data_loader.dataset.df.shape[0] / data_loader.batch_size))\n\n    #for batch_idx, (x, y) in enumerate(data_loader):\n    for batch_idx, (x, y) in progress.track(enumerate(data_loader), description='', total=num_batches, transient=True):\n        # x is a list with the multiple views\n        x = torch.cat(x, dim=0).to(self.device)\n        y = y.to(self.device)\n        bsz = y.shape[0]\n\n        # forward pass\n        features = self.net(x)\n        # apply L2 normalization\n        features = torch.nn.functional.normalize(features, dim=1)\n\n        # compute loss\n        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n        metrics = self.criterion(features)\n        loss = metrics[\"loss\"]\n\n        if self._is_training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        #print(\".\", end=\"\", flush=True)\n\n        for name, value in metrics.items():\n            cum_metrics[name] += value.item()\n\n    #print(\"\\r\" + \" \" * batch_idx + \" \", end=\"\", flush=True)\n    #print(\"\\r\", end=\"\", flush=True)\n    metrics = {}\n    for name, value in cum_metrics.items():\n        value = value / (batch_idx + 1)\n        if name.startswith(\"acc\"):\n            value *= 100\n        metrics[name] = value\n    self._track_metrics(metrics, epoch=idx_epoch, context=context)\n\n    return metrics\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.train_one_epoch","title":"<code>train_one_epoch(train_loader, idx_epoch, context)</code>","text":"<p>Set the internal model to train, calls _do_epoch() and return the obtained metrics</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_one_epoch(self, train_loader, idx_epoch, context):\n    \"\"\"Set the internal model to train, calls _do_epoch() and return the obtained metrics\"\"\"\n    self.net.train()\n    self._is_training = True\n    out = self._do_epoch(train_loader, idx_epoch, context)\n    self._is_training = False\n    return out\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.train_loop","title":"<code>train_loop(epochs, train_loader, val_loader=None, patience_monitor=None, quiet=False, run_init_train=True, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>number of epochs to run</p> required <code>train_loader</code> <code>DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>DataLoaer</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>if False, no message on the console is reported</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def train_loop(\n    self,\n    epochs: int,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoaer = None,\n    patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n    quiet: bool = False,\n    run_init_train: bool = True,\n    context: str = None,\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    The entry point for triggering training\n\n    Arguments:\n        epochs: number of epochs to run\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        quiet: if False, no message on the console is reported\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        The best model obtained during training\n    \"\"\"\n    assert context is not None\n\n    if run_init_train:\n        self.net, self.optimizer = self.init_train(self.net, self.optimizer)\n        self.net = self.net.to(self.device)\n        x, y = next(iter(train_loader))\n        self.log_msg(\n            \"\\n======= net adapted for contrastive learning training =========\",\n            self.logger\n        )\n        #torchsummary.summary(self.net.float(), tuple(x[0][0].shape))\n        utils.log_torchsummary(self.net.float(), tuple(x[0][0].shape), self.logger)\n        self.net.double()\n\n    t1 = time.perf_counter_ns()\n    res = super().train_loop(\n        epochs=epochs,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        patience_monitor=patience_monitor,\n        quiet=quiet,\n        context=context,\n    )\n    t2 = time.perf_counter_ns()\n    if self.tracker:\n        self.tracker.track((t2 - t1) / 1E9, 'duration', context=dict(subset=f\"{context}train\"))\n\n    return res\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.ContrastiveLearningTrainer.test_loop","title":"<code>test_loop(data_loader, idx_epoch=None, context=None, *args, **kwargs)</code>","text":"<p>Run inference on a model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>ind</code> <p>the current epoch</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second is an empty dictionary</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def test_loop(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: ind = None,\n    context: str = None,\n    *args,\n    **kwargs,\n) -&gt; Tuple[Dict[str, Any], Any]:\n    \"\"\"\n    Run inference on a model (for testing or validation)\n\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second is an empty dictionary\n    \"\"\"\n    self.net.eval()\n\n    if context is None:\n        context = \"val\" if self._is_training else \"test\"\n\n    with torch.no_grad():\n        t1 = time.perf_counter_ns()\n        metrics = self._do_epoch(data_loader, idx_epoch, context=context)\n        t2 = time.perf_counter_ns()\n        metrics['duration'] = (t2 - t1) / 1E9\n\n    # we return an empty report just to have\n    # consistency with the return types\n    # of SimpleTrainer\n    reports = {}\n    return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.MonolithicTrainer","title":"<code>MonolithicTrainer</code>","text":"<p>             Bases: <code>SimpleTrainer</code></p> <p>A wrapper around SimpleTrainer and designed to be used in supervised classification scenarios</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class MonolithicTrainer(SimpleTrainer):\n    \"\"\"A wrapper around SimpleTrainer and designed\n    to be used in supervised classification scenarios\n    \"\"\"\n\n    def __init__(\n        self,\n        net: backbone.BaseNet,\n        optimizer: torch.optim.Optimizer = None,\n        criterion: torch.nn.Module = nn.CrossEntropyLoss(),\n        device: str = \"cuda:0\",\n        deterministic: bool = True,\n        tracker: aim.Run = None,\n        logger: logging.Logger = None,\n        reset_classifier: bool = False,\n        num_classes: int = None,\n        xgboost_model=None,\n    ):\n        \"\"\"\n        Arguments:\n            net: the architecture to use\n            optimizer: the optimizer to use\n            criterion: the instance of the loss to use\n            device: the device to use\n            deterministic: see _make_deterministic()\n            tracker: the AIM run on which register metrics\n            logger: the logging reference\n            reset_classifier: if True, the network is modified\n                to have a new layer\n            num_classes: number of units to use for the new\n                classifier head\n        \"\"\"\n        if reset_classifier:\n            if num_classes is None:\n                raise RuntimeError(\n                    f\"num_classes cannot be None when resetting the model head\"\n                )\n            net = net.reset_classifier(num_classes)\n        super().__init__(\n            net=net,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            deterministic=deterministic,\n            tracker=tracker,\n            logger=logger,\n        )\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.MonolithicTrainer.__init__","title":"<code>__init__(net, optimizer=None, criterion=nn.CrossEntropyLoss(), device='cuda:0', deterministic=True, tracker=None, logger=None, reset_classifier=False, num_classes=None, xgboost_model=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the architecture to use</p> required <code>optimizer</code> <code>Optimizer</code> <p>the optimizer to use</p> <code>None</code> <code>criterion</code> <code>Module</code> <p>the instance of the loss to use</p> <code>CrossEntropyLoss()</code> <code>device</code> <code>str</code> <p>the device to use</p> <code>'cuda:0'</code> <code>deterministic</code> <code>bool</code> <p>see _make_deterministic()</p> <code>True</code> <code>tracker</code> <code>Run</code> <p>the AIM run on which register metrics</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>the logging reference</p> <code>None</code> <code>reset_classifier</code> <code>bool</code> <p>if True, the network is modified to have a new layer</p> <code>False</code> <code>num_classes</code> <code>int</code> <p>number of units to use for the new classifier head</p> <code>None</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(\n    self,\n    net: backbone.BaseNet,\n    optimizer: torch.optim.Optimizer = None,\n    criterion: torch.nn.Module = nn.CrossEntropyLoss(),\n    device: str = \"cuda:0\",\n    deterministic: bool = True,\n    tracker: aim.Run = None,\n    logger: logging.Logger = None,\n    reset_classifier: bool = False,\n    num_classes: int = None,\n    xgboost_model=None,\n):\n    \"\"\"\n    Arguments:\n        net: the architecture to use\n        optimizer: the optimizer to use\n        criterion: the instance of the loss to use\n        device: the device to use\n        deterministic: see _make_deterministic()\n        tracker: the AIM run on which register metrics\n        logger: the logging reference\n        reset_classifier: if True, the network is modified\n            to have a new layer\n        num_classes: number of units to use for the new\n            classifier head\n    \"\"\"\n    if reset_classifier:\n        if num_classes is None:\n            raise RuntimeError(\n                f\"num_classes cannot be None when resetting the model head\"\n            )\n        net = net.reset_classifier(num_classes)\n    super().__init__(\n        net=net,\n        optimizer=optimizer,\n        criterion=criterion,\n        device=device,\n        deterministic=deterministic,\n        tracker=tracker,\n        logger=logger,\n    )\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer","title":"<code>SimCLRTrainer</code>","text":"<p>A trainer designed for SimCLR (https://arxiv.org/abs/2002.05709). Differently from the other trainers which are based on inheritancs, it is based on nesting a ContrastiveLearningTrainer object (for contrastive learning) and a MonolithicTrainer object (for finetune)</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>class SimCLRTrainer:\n    \"\"\"A trainer designed for SimCLR (https://arxiv.org/abs/2002.05709).\n    Differently from the other trainers which are based on\n    inheritancs, it is based on nesting a ContrastiveLearningTrainer\n    object (for contrastive learning) and a MonolithicTrainer object (for finetune)\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrain_config: Dict[str, Any] = None,\n        finetune_config: Dict[str, Any] = None,\n        device: str = \"cuda:0\",\n        deterministic: bool = True,\n        tracker: aim.Run = None,\n        logger: logging.Logger = None,\n        xgboost_model=None,\n    ):\n        \"\"\"\n        Arguments:\n            pretrain_config: a set of configuration required for pretraining.\n                The dictionary should contain an \"optimizer\" and the related instance\n                (None if missing) and a \"loss_temperature\" (0.07 if missing)\n            finetune_config: a set of configuration required for finetune.\n                The dictionary should contain an \"optimizer\" and the related instance\n                (None if missing).\n            device: the device for training and inference\n            tracker: the AIM run for metric tracking\n            logger: a logging object for console and file logging\n        \"\"\"\n        self.tracker = tracker\n        self.logger = logger\n        self.device = device\n        self.pretrain_config = pretrain_config\n        self.pretrain_criterion = None\n        self.pretrain_trainer = None\n        self.pretrain_best_net = None\n\n        self.finetune_config = finetune_config\n        self.finetune_criterion = None\n        self.finetune_trainer = None\n        self.finetune_best_net = None\n\n        trainer_params = dict(\n            net=None,\n            tracker=tracker,\n            logger=logger,\n            deterministic=False,\n            device=device,\n        )\n\n        if pretrain_config is not None:\n            self.pretrain_criterion = losses.SimCLRLoss(\n                temperature=pretrain_config.get(\"loss_temperature\", 0.07),\n                base_temperature=pretrain_config.get(\"loss_base_temperature\", 0.07),\n                contrast_mode=\"all\",\n            )\n            self.pretrain_trainer = ContrastiveLearningTrainer(\n                optimizer=pretrain_config.get(\"optimizer\", None),\n                criterion=self.pretrain_criterion,\n                **trainer_params,\n            )\n\n        if finetune_config is not None:\n            self.finetune_criterion = nn.CrossEntropyLoss()\n            self.finetune_trainer = MonolithicTrainer(\n                optimizer=finetune_config.get(\"optimizer\", None),\n                criterion=self.finetune_criterion,\n                **trainer_params,\n            )\n\n        if deterministic:\n            _make_deterministic()\n\n    @classmethod\n    def init_pretrain(\n        cls,\n        net: backbone.BaseNet,\n        optimizer: torch.optim.Optimizer = None,\n        fname_weights: pathlib.Path = None,\n    ) -&gt; Tuple[backbone.BaseNet, Any]:\n        \"\"\"\n        Clones the input network and prepares it for contrastive learning,\n        and instanciate a new optimized bounding it to the new network weights\n\n        Arguments:\n            net: the network to use\n            optimizer: the optimizer to tuse\n            fname_weights: if provided, the weights are loaded\n                into the new network before returning it\n\n        Return:\n            A tuple with the new updated network and the related optimizer\n        \"\"\"\n        return ContrastiveLearningTrainer.init_train(net, optimizer, fname_weights)\n\n    def pretrain_loop(\n        self,\n        net: backbone.BaseNet,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.Data.DataLoader = None,\n        patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n        epochs: int = 50,\n        run_init_pretrain: bool = True,\n        fname_weights: pathlib.Path = None,\n        context: str = None,\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        The entry point for triggering training\n\n        Arguments:\n            net: the network to use\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            epochs: number of epochs to run\n            run_init_pretrain: if True, invokes .init_pretrain()\n            fname_weights: a file with the weights to load after\n                preparing the model for training\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            The best model obtained during training\n        \"\"\"\n        if run_init_pretrain:\n            net, optimizer = self.init_pretrain(\n                net, self.pretrain_trainer.optimizer, fname_weights\n            )\n            params = list(net.parameters())\n            expected_dtype = params[0].dtype\n            self.pretrain_trainer.net = net.to(self.device)\n            self.pretrain_trainer.optimizer = optimizer\n            x, y = next(iter(train_loader))\n            utils.log_msg(\"\\n==== network adapted for pretrain ====\", self.logger)\n            #torchsummary.summary(net.float(), tuple(x[0][0].shape))\n            utils.log_torchsummary(net.float(), tuple(x[0][0].shape), self.logger)\n            net.double()\n\n        self.pretrain_trainer.net = net\n        t1 = time.perf_counter_ns()\n        self.pretrain_best_model = self.pretrain_trainer.train_loop(\n            train_loader=train_loader,\n            val_loader=val_loader,\n            patience_monitor=patience_monitor,\n            epochs=epochs,\n            run_init_train=False,\n            context=context,\n        )\n        t2 = time.perf_counter_ns()\n        duration = (t2 - t1) / 1E9\n        if self.tracker:\n            self.tracker.track(duration, \"duration\", context=dict(subset=\"pretrain-train\"))\n        return self.pretrain_best_model\n\n    @classmethod\n    def prepare_net_for_finetune(\n        cls,\n        net: backbone.BaseNet,\n        num_classes: int = 5,\n        fname_pretrain_weights: pathlib.Path = None,\n        fname_finetune_weights: pathlib.Path = None,\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        Clone a backbone.BaseNet related to contrastive learning an\n        prepare it for finetune. Specifically, the last linear layer\n        of the newtwork (the projection layer) is masked (via a nn.Identity)\n        and a classifier is added to the network\n\n        Arguments:\n            net: the network to modify\n            num_classes: the number of units for the classifier\n\n        Return:\n            A new instance of the input network with architecture\n            modification required to run training contrastive learning\n            training\n        \"\"\"\n        if not hasattr(net, \"prepare_for_finetune\"):\n            raise RuntimeError(\n                \"Did not find a .prepare_for_finetune() method in the network. Cannot adapt the network for training\"\n            )\n        new_net = backbone.clone_net(net)\n        new_net.prepare_for_finetune(\n            num_classes=num_classes,\n            fname_pretrain_weights=fname_pretrain_weights,\n            fname_finetune_weights=fname_finetune_weights,\n        )\n        return new_net\n\n    @classmethod\n    def init_finetune(\n        cls,\n        net: backbone.BaseNet,\n        num_classes: int,\n        fname_pretrain_weights: pathlib.Path = None,\n        fname_finetune_weights: pathlib.Path = None,\n        optimizer=None,\n    ) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n        \"\"\"\n        Initialize the network for finetuning adapting\n        it from contrastive-learning. Specifically, the\n        input network is the first modified for contrastive-learning,\n        and then further adjusted for finetune.\n\n        Arguments:\n            net: the network to use\n            num_classes: the number of classes for the classifier\n            fname_pretrain_weights: if specified, the weights\n                are loaded before adapting the network from\n                pretraining\n            fname_finetune_weights: if specified, the weights\n                are loaded after adapting the network for\n                finetune\n        \"\"\"\n        new_net = cls.prepare_net_for_finetune(\n            net, num_classes, fname_pretrain_weights, fname_finetune_weights\n        )\n\n        new_optimizer = None\n        if optimizer:\n            new_optimizer = _reset_optimizer(optimizer, new_net.classifier.parameters())\n\n        return new_net, new_optimizer\n\n    def finetune_loop(\n        self,\n        net: backbone.BaseNet,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader = None,\n        patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n        epochs: int = 50,\n        num_classes: int = 5,\n        run_init_finetune: bool = True,\n        fname_pretrain_weights: pathlib.PAth = None,\n        context: str = None,\n    ) -&gt; backbone.BaseNet:\n        \"\"\"\n        The entry point for triggering training\n\n        Arguments:\n            net: the network to use\n            train_loader: the data for training\n            val_loader: the data for validation\n            patience_monitor: the instance of the patience monitor\n            epochs: number of epochs to run\n            num_classes: the number of units for the classifier\n            run_init_finetune: if True, invokes .init_finetune()\n            fname_pretrain_weights: a file with the weights to load after\n                preparing the model for training\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            The best model obtained during training\n        \"\"\"\n        if run_init_finetune:\n            net, optimizer = self.init_finetune(\n                net=net,\n                num_classes=num_classes,\n                fname_pretrain_weights=fname_pretrain_weights,\n                optimizer=self.finetune_trainer.optimizer,\n            )\n            net = net.to(self.device)\n            self.finetune_trainer.net = net\n            self.finetune_trainer.optimizer = optimizer\n            self.finetune_net = net\n\n            x, y = next(iter(train_loader))\n            # x can be a list if the underlining\n            # dataset is multi-view\n            if isinstance(x, list):\n                x = x[0]\n            utils.log_msg(\"\\n==== network adapted for fine-tuning ====\", self.logger)\n            #torchsummary.summary(net.float(), tuple(x[0].shape))\n            utils.log_torchsummary(net.float(), tuple(x[0].shape), self.logger)\n            net.double()\n\n        self.finetune_trainer.net = net\n        t1 = time.perf_counter_ns()\n        self.finetune_best_net = self.finetune_trainer.train_loop(\n            train_loader=train_loader,\n            val_loader=val_loader,\n            patience_monitor=patience_monitor,\n            epochs=epochs,\n            context=context,\n        )\n        t2 = time.perf_counter_ns()\n        duration = (t2 - t1) / 1E9\n        if self.tracker:\n            self.tracker.track(duration, \"duration\", context=dict(subset=\"finetune-train\"))\n        return self.finetune_best_net\n\n    def finetune_test_loop(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: int = None,\n        with_reports: bool = False,\n        context: str = None,\n    ) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n        \"\"\"\n        Run inference on a (supervised) model (for testing or validation)\n\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            with_reports: if True, compute classification report and confusion matrix\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\n        t1 = time.perf_counter_ns()\n        res = self.finetune_trainer.test_loop(\n            data_loader, idx_epoch, with_reports, context\n        )\n        t2 = time.perf_counter_ns()\n        if self.tracker:\n            self.tracker.track((t2 - t1) / 1E9, \"duration\", context=dict(subset=\"finetune-test\"))\n        return res\n\n    def pretrain_test_loop(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        idx_epoch: int = None,\n        context: str = None,\n        *args,\n        **kwargs,\n    ) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n        \"\"\"\n        Run inference on a (unsupervised) model (for testing or validation)\n\n        Arguments:\n            data_loader: the data to use\n            idx_epoch: the current epoch\n            context: a string (for tracking) for extra semantic (train, val, etc.)\n\n        Return:\n            A tuple with two dictionaries. The first contains the metrics collected\n            during inference; the second contains classification report (class_rep)\n            and confusion matrix (conf_mtx) or is empty {} if their computation\n            was not requested\n        \"\"\"\n        t1 = time.perf_counter_ns()\n        res = self.pretrain_trainer.test_loop(data_loader, idx_epoch, context)\n        t2 = time.perf_counter_ns()\n        if self.tracker:\n            self.tracker.track((t2 - t1) / 1E9, \"duration\", context=dict(subset=\"pretrain-test\"))\n        return res\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.__init__","title":"<code>__init__(pretrain_config=None, finetune_config=None, device='cuda:0', deterministic=True, tracker=None, logger=None, xgboost_model=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pretrain_config</code> <code>Dict[str, Any]</code> <p>a set of configuration required for pretraining. The dictionary should contain an \"optimizer\" and the related instance (None if missing) and a \"loss_temperature\" (0.07 if missing)</p> <code>None</code> <code>finetune_config</code> <code>Dict[str, Any]</code> <p>a set of configuration required for finetune. The dictionary should contain an \"optimizer\" and the related instance (None if missing).</p> <code>None</code> <code>device</code> <code>str</code> <p>the device for training and inference</p> <code>'cuda:0'</code> <code>tracker</code> <code>Run</code> <p>the AIM run for metric tracking</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>a logging object for console and file logging</p> <code>None</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def __init__(\n    self,\n    pretrain_config: Dict[str, Any] = None,\n    finetune_config: Dict[str, Any] = None,\n    device: str = \"cuda:0\",\n    deterministic: bool = True,\n    tracker: aim.Run = None,\n    logger: logging.Logger = None,\n    xgboost_model=None,\n):\n    \"\"\"\n    Arguments:\n        pretrain_config: a set of configuration required for pretraining.\n            The dictionary should contain an \"optimizer\" and the related instance\n            (None if missing) and a \"loss_temperature\" (0.07 if missing)\n        finetune_config: a set of configuration required for finetune.\n            The dictionary should contain an \"optimizer\" and the related instance\n            (None if missing).\n        device: the device for training and inference\n        tracker: the AIM run for metric tracking\n        logger: a logging object for console and file logging\n    \"\"\"\n    self.tracker = tracker\n    self.logger = logger\n    self.device = device\n    self.pretrain_config = pretrain_config\n    self.pretrain_criterion = None\n    self.pretrain_trainer = None\n    self.pretrain_best_net = None\n\n    self.finetune_config = finetune_config\n    self.finetune_criterion = None\n    self.finetune_trainer = None\n    self.finetune_best_net = None\n\n    trainer_params = dict(\n        net=None,\n        tracker=tracker,\n        logger=logger,\n        deterministic=False,\n        device=device,\n    )\n\n    if pretrain_config is not None:\n        self.pretrain_criterion = losses.SimCLRLoss(\n            temperature=pretrain_config.get(\"loss_temperature\", 0.07),\n            base_temperature=pretrain_config.get(\"loss_base_temperature\", 0.07),\n            contrast_mode=\"all\",\n        )\n        self.pretrain_trainer = ContrastiveLearningTrainer(\n            optimizer=pretrain_config.get(\"optimizer\", None),\n            criterion=self.pretrain_criterion,\n            **trainer_params,\n        )\n\n    if finetune_config is not None:\n        self.finetune_criterion = nn.CrossEntropyLoss()\n        self.finetune_trainer = MonolithicTrainer(\n            optimizer=finetune_config.get(\"optimizer\", None),\n            criterion=self.finetune_criterion,\n            **trainer_params,\n        )\n\n    if deterministic:\n        _make_deterministic()\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.init_pretrain","title":"<code>init_pretrain(net, optimizer=None, fname_weights=None)</code>  <code>classmethod</code>","text":"<p>Clones the input network and prepares it for contrastive learning, and instanciate a new optimized bounding it to the new network weights</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to use</p> required <code>optimizer</code> <code>Optimizer</code> <p>the optimizer to tuse</p> <code>None</code> <code>fname_weights</code> <code>Path</code> <p>if provided, the weights are loaded into the new network before returning it</p> <code>None</code> Return <p>A tuple with the new updated network and the related optimizer</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_pretrain(\n    cls,\n    net: backbone.BaseNet,\n    optimizer: torch.optim.Optimizer = None,\n    fname_weights: pathlib.Path = None,\n) -&gt; Tuple[backbone.BaseNet, Any]:\n    \"\"\"\n    Clones the input network and prepares it for contrastive learning,\n    and instanciate a new optimized bounding it to the new network weights\n\n    Arguments:\n        net: the network to use\n        optimizer: the optimizer to tuse\n        fname_weights: if provided, the weights are loaded\n            into the new network before returning it\n\n    Return:\n        A tuple with the new updated network and the related optimizer\n    \"\"\"\n    return ContrastiveLearningTrainer.init_train(net, optimizer, fname_weights)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.pretrain_loop","title":"<code>pretrain_loop(net, train_loader, val_loader=None, patience_monitor=None, epochs=50, run_init_pretrain=True, fname_weights=None, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to use</p> required <code>train_loader</code> <code>DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>epochs</code> <code>int</code> <p>number of epochs to run</p> <code>50</code> <code>run_init_pretrain</code> <code>bool</code> <p>if True, invokes .init_pretrain()</p> <code>True</code> <code>fname_weights</code> <code>Path</code> <p>a file with the weights to load after preparing the model for training</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def pretrain_loop(\n    self,\n    net: backbone.BaseNet,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.Data.DataLoader = None,\n    patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n    epochs: int = 50,\n    run_init_pretrain: bool = True,\n    fname_weights: pathlib.Path = None,\n    context: str = None,\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    The entry point for triggering training\n\n    Arguments:\n        net: the network to use\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        epochs: number of epochs to run\n        run_init_pretrain: if True, invokes .init_pretrain()\n        fname_weights: a file with the weights to load after\n            preparing the model for training\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        The best model obtained during training\n    \"\"\"\n    if run_init_pretrain:\n        net, optimizer = self.init_pretrain(\n            net, self.pretrain_trainer.optimizer, fname_weights\n        )\n        params = list(net.parameters())\n        expected_dtype = params[0].dtype\n        self.pretrain_trainer.net = net.to(self.device)\n        self.pretrain_trainer.optimizer = optimizer\n        x, y = next(iter(train_loader))\n        utils.log_msg(\"\\n==== network adapted for pretrain ====\", self.logger)\n        #torchsummary.summary(net.float(), tuple(x[0][0].shape))\n        utils.log_torchsummary(net.float(), tuple(x[0][0].shape), self.logger)\n        net.double()\n\n    self.pretrain_trainer.net = net\n    t1 = time.perf_counter_ns()\n    self.pretrain_best_model = self.pretrain_trainer.train_loop(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        patience_monitor=patience_monitor,\n        epochs=epochs,\n        run_init_train=False,\n        context=context,\n    )\n    t2 = time.perf_counter_ns()\n    duration = (t2 - t1) / 1E9\n    if self.tracker:\n        self.tracker.track(duration, \"duration\", context=dict(subset=\"pretrain-train\"))\n    return self.pretrain_best_model\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.prepare_net_for_finetune","title":"<code>prepare_net_for_finetune(net, num_classes=5, fname_pretrain_weights=None, fname_finetune_weights=None)</code>  <code>classmethod</code>","text":"<p>Clone a backbone.BaseNet related to contrastive learning an prepare it for finetune. Specifically, the last linear layer of the newtwork (the projection layer) is masked (via a nn.Identity) and a classifier is added to the network</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to modify</p> required <code>num_classes</code> <code>int</code> <p>the number of units for the classifier</p> <code>5</code> Return <p>A new instance of the input network with architecture modification required to run training contrastive learning training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef prepare_net_for_finetune(\n    cls,\n    net: backbone.BaseNet,\n    num_classes: int = 5,\n    fname_pretrain_weights: pathlib.Path = None,\n    fname_finetune_weights: pathlib.Path = None,\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    Clone a backbone.BaseNet related to contrastive learning an\n    prepare it for finetune. Specifically, the last linear layer\n    of the newtwork (the projection layer) is masked (via a nn.Identity)\n    and a classifier is added to the network\n\n    Arguments:\n        net: the network to modify\n        num_classes: the number of units for the classifier\n\n    Return:\n        A new instance of the input network with architecture\n        modification required to run training contrastive learning\n        training\n    \"\"\"\n    if not hasattr(net, \"prepare_for_finetune\"):\n        raise RuntimeError(\n            \"Did not find a .prepare_for_finetune() method in the network. Cannot adapt the network for training\"\n        )\n    new_net = backbone.clone_net(net)\n    new_net.prepare_for_finetune(\n        num_classes=num_classes,\n        fname_pretrain_weights=fname_pretrain_weights,\n        fname_finetune_weights=fname_finetune_weights,\n    )\n    return new_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.init_finetune","title":"<code>init_finetune(net, num_classes, fname_pretrain_weights=None, fname_finetune_weights=None, optimizer=None)</code>  <code>classmethod</code>","text":"<p>Initialize the network for finetuning adapting it from contrastive-learning. Specifically, the input network is the first modified for contrastive-learning, and then further adjusted for finetune.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to use</p> required <code>num_classes</code> <code>int</code> <p>the number of classes for the classifier</p> required <code>fname_pretrain_weights</code> <code>Path</code> <p>if specified, the weights are loaded before adapting the network from pretraining</p> <code>None</code> <code>fname_finetune_weights</code> <code>Path</code> <p>if specified, the weights are loaded after adapting the network for finetune</p> <code>None</code> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>@classmethod\ndef init_finetune(\n    cls,\n    net: backbone.BaseNet,\n    num_classes: int,\n    fname_pretrain_weights: pathlib.Path = None,\n    fname_finetune_weights: pathlib.Path = None,\n    optimizer=None,\n) -&gt; Tuple[backbone.BaseNet, torch.optim.Optimizer]:\n    \"\"\"\n    Initialize the network for finetuning adapting\n    it from contrastive-learning. Specifically, the\n    input network is the first modified for contrastive-learning,\n    and then further adjusted for finetune.\n\n    Arguments:\n        net: the network to use\n        num_classes: the number of classes for the classifier\n        fname_pretrain_weights: if specified, the weights\n            are loaded before adapting the network from\n            pretraining\n        fname_finetune_weights: if specified, the weights\n            are loaded after adapting the network for\n            finetune\n    \"\"\"\n    new_net = cls.prepare_net_for_finetune(\n        net, num_classes, fname_pretrain_weights, fname_finetune_weights\n    )\n\n    new_optimizer = None\n    if optimizer:\n        new_optimizer = _reset_optimizer(optimizer, new_net.classifier.parameters())\n\n    return new_net, new_optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.finetune_loop","title":"<code>finetune_loop(net, train_loader, val_loader=None, patience_monitor=None, epochs=50, num_classes=5, run_init_finetune=True, fname_pretrain_weights=None, context=None)</code>","text":"<p>The entry point for triggering training</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the network to use</p> required <code>train_loader</code> <code>DataLoader</code> <p>the data for training</p> required <code>val_loader</code> <code>DataLoader</code> <p>the data for validation</p> <code>None</code> <code>patience_monitor</code> <code>PatienceMonitorLoss | PatienceMonitorAccuracy</code> <p>the instance of the patience monitor</p> <code>None</code> <code>epochs</code> <code>int</code> <p>number of epochs to run</p> <code>50</code> <code>num_classes</code> <code>int</code> <p>the number of units for the classifier</p> <code>5</code> <code>run_init_finetune</code> <code>bool</code> <p>if True, invokes .init_finetune()</p> <code>True</code> <code>fname_pretrain_weights</code> <code>PAth</code> <p>a file with the weights to load after preparing the model for training</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>The best model obtained during training</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def finetune_loop(\n    self,\n    net: backbone.BaseNet,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader = None,\n    patience_monitor: PatienceMonitorLoss | PatienceMonitorAccuracy = None,\n    epochs: int = 50,\n    num_classes: int = 5,\n    run_init_finetune: bool = True,\n    fname_pretrain_weights: pathlib.PAth = None,\n    context: str = None,\n) -&gt; backbone.BaseNet:\n    \"\"\"\n    The entry point for triggering training\n\n    Arguments:\n        net: the network to use\n        train_loader: the data for training\n        val_loader: the data for validation\n        patience_monitor: the instance of the patience monitor\n        epochs: number of epochs to run\n        num_classes: the number of units for the classifier\n        run_init_finetune: if True, invokes .init_finetune()\n        fname_pretrain_weights: a file with the weights to load after\n            preparing the model for training\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        The best model obtained during training\n    \"\"\"\n    if run_init_finetune:\n        net, optimizer = self.init_finetune(\n            net=net,\n            num_classes=num_classes,\n            fname_pretrain_weights=fname_pretrain_weights,\n            optimizer=self.finetune_trainer.optimizer,\n        )\n        net = net.to(self.device)\n        self.finetune_trainer.net = net\n        self.finetune_trainer.optimizer = optimizer\n        self.finetune_net = net\n\n        x, y = next(iter(train_loader))\n        # x can be a list if the underlining\n        # dataset is multi-view\n        if isinstance(x, list):\n            x = x[0]\n        utils.log_msg(\"\\n==== network adapted for fine-tuning ====\", self.logger)\n        #torchsummary.summary(net.float(), tuple(x[0].shape))\n        utils.log_torchsummary(net.float(), tuple(x[0].shape), self.logger)\n        net.double()\n\n    self.finetune_trainer.net = net\n    t1 = time.perf_counter_ns()\n    self.finetune_best_net = self.finetune_trainer.train_loop(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        patience_monitor=patience_monitor,\n        epochs=epochs,\n        context=context,\n    )\n    t2 = time.perf_counter_ns()\n    duration = (t2 - t1) / 1E9\n    if self.tracker:\n        self.tracker.track(duration, \"duration\", context=dict(subset=\"finetune-train\"))\n    return self.finetune_best_net\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.finetune_test_loop","title":"<code>finetune_test_loop(data_loader, idx_epoch=None, with_reports=False, context=None)</code>","text":"<p>Run inference on a (supervised) model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>with_reports</code> <code>bool</code> <p>if True, compute classification report and confusion matrix</p> <code>False</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def finetune_test_loop(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: int = None,\n    with_reports: bool = False,\n    context: str = None,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Run inference on a (supervised) model (for testing or validation)\n\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        with_reports: if True, compute classification report and confusion matrix\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\n    t1 = time.perf_counter_ns()\n    res = self.finetune_trainer.test_loop(\n        data_loader, idx_epoch, with_reports, context\n    )\n    t2 = time.perf_counter_ns()\n    if self.tracker:\n        self.tracker.track((t2 - t1) / 1E9, \"duration\", context=dict(subset=\"finetune-test\"))\n    return res\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.SimCLRTrainer.pretrain_test_loop","title":"<code>pretrain_test_loop(data_loader, idx_epoch=None, context=None, *args, **kwargs)</code>","text":"<p>Run inference on a (unsupervised) model (for testing or validation)</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>the data to use</p> required <code>idx_epoch</code> <code>int</code> <p>the current epoch</p> <code>None</code> <code>context</code> <code>str</code> <p>a string (for tracking) for extra semantic (train, val, etc.)</p> <code>None</code> Return <p>A tuple with two dictionaries. The first contains the metrics collected during inference; the second contains classification report (class_rep) and confusion matrix (conf_mtx) or is empty {} if their computation was not requested</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def pretrain_test_loop(\n    self,\n    data_loader: torch.utils.data.DataLoader,\n    idx_epoch: int = None,\n    context: str = None,\n    *args,\n    **kwargs,\n) -&gt; Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]:\n    \"\"\"\n    Run inference on a (unsupervised) model (for testing or validation)\n\n    Arguments:\n        data_loader: the data to use\n        idx_epoch: the current epoch\n        context: a string (for tracking) for extra semantic (train, val, etc.)\n\n    Return:\n        A tuple with two dictionaries. The first contains the metrics collected\n        during inference; the second contains classification report (class_rep)\n        and confusion matrix (conf_mtx) or is empty {} if their computation\n        was not requested\n    \"\"\"\n    t1 = time.perf_counter_ns()\n    res = self.pretrain_trainer.test_loop(data_loader, idx_epoch, context)\n    t2 = time.perf_counter_ns()\n    if self.tracker:\n        self.tracker.track((t2 - t1) / 1E9, \"duration\", context=dict(subset=\"pretrain-test\"))\n    return res\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods._make_deterministic","title":"<code>_make_deterministic()</code>","text":"<p>Helper method to force pytorch to be deterministic</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _make_deterministic():\n    \"\"\"Helper method to force pytorch to be deterministic\"\"\"\n    torch.use_deterministic_algorithms(True)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods._reset_optimizer","title":"<code>_reset_optimizer(optimizer, net_parameters)</code>","text":"<p>Helper method to recreate a new instance of the optimizer passed as input (via introspection) with the same configuration and bound to the network parameters of a model</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>the optimizer to clone</p> required <code>net_parameters</code> <code>Generator</code> <p>iterator obtained calling .parameters() from a pytorch module</p> required Return <p>a new instance of an optimizer</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def _reset_optimizer(\n    optimizer: torch.optim.Optimizer, net_parameters: Generator\n) -&gt; torch.optim.Optimizer:\n    \"\"\"Helper method to recreate a new instance of\n    the optimizer passed as input (via introspection)\n    with the same configuration and bound to the\n    network parameters of a model\n\n    Arguments:\n        optimizer: the optimizer to clone\n        net_parameters: iterator obtained calling .parameters() from a pytorch module\n\n    Return:\n        a new instance of an optimizer\n    \"\"\"\n    optimizer_class = getattr(torch.optim, optimizer.__class__.__name__)\n    optimizer_params = {\n        name: value\n        for name, value in optimizer.param_groups[0].items()\n        if name != \"params\"\n    }\n    optimizer = optimizer_class(net_parameters, **optimizer_params)\n    return optimizer\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_methods/#tcbench.modeling.methods.trainer_factory","title":"<code>trainer_factory(method, *args, **kwargs)</code>","text":"<p>Helper function to instanciate trainer objects</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>either \"monolithic\" or \"simclr\" or \"xgboost\"</p> required <code>args</code> <code>Any</code> <p>positional arguments to use when instanciating the class (if any)</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>key/value arguments to use when instanciating the class (if any)</p> <code>{}</code> Return <p>a trainer object</p> Source code in <code>src/tcbench/modeling/methods.py</code> <pre><code>def trainer_factory(method: str, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Helper function to instanciate trainer objects\n\n    Arguments:\n        method: either \"monolithic\" or \"simclr\" or \"xgboost\"\n        args: positional arguments to use when instanciating the class (if any)\n        kwargs: key/value arguments to use when instanciating the class (if any)\n\n    Return:\n        a trainer object\n    \"\"\"\n    return METHOD_CLASSES[method](*args, **kwargs)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/","title":"Tcbench modeling run augmentations at loading","text":""},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.train","title":"<code>train(dataset_name=tcbench.DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, learning_rate=0.01, flowpic_dim=32, flowpic_block_duration=15, split_index=0, max_samples_per_class=-1, logger=None, aug_name='noaug', aug_samples=10, device='cuda:0', tracker=None, workers=50, artifacts_folder=None, seed=12345, epochs=50, patience_steps=5, patience_min_delta=0.001, train_val_split_ratio=0.8, suppress_val_augmentation=False, with_dropout=True, state=None)</code>","text":"<p>Model training</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def train(\n    dataset_name: DATASETS = tcbench.DATASETS.UCDAVISICDM19,\n    dataset_minpkts: int = -1,\n    batch_size: int = 32,\n    learning_rate: float = 0.01,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    split_index: int = 0,\n    max_samples_per_class: int = -1,\n    logger=None,\n    aug_name: str = \"noaug\",\n    aug_samples: int = 10,\n    device: str = \"cuda:0\",\n    tracker: aim.Run = None,\n    workers: int = 50,\n    artifacts_folder=None,\n    seed: int = 12345,\n    epochs: int = 50,\n    patience_steps: int = 5,\n    patience_min_delta: float = 0.001,\n    train_val_split_ratio: float = 0.8,\n    suppress_val_augmentation: bool = False,\n    with_dropout: bool = True,\n    state: dict = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Model training\"\"\"\n    if state is None:\n        state = dict()\n\n    aug_config = {aug_name: dict()}\n\n    dset_train, dset_val = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\n        split_idx=split_index,\n        max_samples_per_class=max_samples_per_class,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        aug_config=aug_config,\n        aug_samples=aug_samples,\n        aug_when_loading=True,\n        n_workers=workers,\n        suppress_val_augmentation=suppress_val_augmentation,\n        logger=logger,\n        seed=seed,\n        dataset_minpkts=dataset_minpkts,\n    )\n    train_loader = torch.utils.data.DataLoader(dset_train, batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dset_val, batch_size, shuffle=False)\n\n    net = backbone.net_factory(\n        num_classes=dset_train.num_classes,\n        flowpic_dim=flowpic_dim,\n        with_dropout=with_dropout,\n    )\n\n    #torchsummary.summary(net.to(device), (1, flowpic_dim, flowpic_dim))\n    utils.log_msg(\"\\nnetwork architecture\", logger)\n    utils.log_torchsummary(net.to(device), (1, flowpic_dim, flowpic_dim), logger)\n    optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n\n    trainer_kwargs = dict(\n        net=net, optimizer=optimizer, tracker=tracker, device=device, logger=logger\n    )\n    trainer = methods.trainer_factory(\"monolithic\", **trainer_kwargs)\n\n    best_net = trainer.train_loop(\n        epochs=epochs,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        patience_monitor=methods.PatienceMonitorLoss(\n            steps=patience_steps, min_delta=patience_min_delta\n        ),\n    )\n\n    state = dict(\n        best_net=best_net,\n        dset_train=dset_train,\n        dset_val=dset_val,\n    )\n\n    if artifacts_folder is not None:\n        best_net.save_weights(\n            artifacts_folder / f\"best_model_weights_split_{split_index}.pt\"\n        )\n\n    reports = utils.classification_reports(\n        best_net,\n        dset_train,\n        batch_size,\n        device,\n        context=\"train\",\n        save_to=artifacts_folder,\n        logger=logger,\n    )\n    state[\"train_class_rep\"] = reports[\"class_rep\"]\n    state[\"train_conf_mtx\"] = reports[\"conf_mtx\"]\n\n    report = utils.classification_reports(\n        best_net,\n        dset_val,\n        batch_size,\n        device,\n        context=\"val\",\n        save_to=artifacts_folder,\n        logger=logger,\n    )\n    state[\"val_class_rep\"] = report[\"class_rep\"]\n    state[\"val_conf_mtx\"] = report[\"conf_mtx\"]\n\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.test","title":"<code>test(dataset_name, dataset_minpkts=-1, split_idx=0, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, device='cuda:0', tracker=None, artifacts_folder=None, with_dropout=True, state=None)</code>","text":"<p>Model testing</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def test(\n    dataset_name: tcbench.DATASETS.UCDAVISICDM19,\n    dataset_minpkts: int = -1,\n    split_idx: int = 0,\n    batch_size: int = 32,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    logger=None,\n    device: str = \"cuda:0\",\n    tracker: aim.Run = None,\n    artifacts_folder: pathlib.Path = None,\n    with_dropout: bool = True,\n    state: dict = None,\n):\n    \"\"\"Model testing\"\"\"\n    if state is None:\n        state = dict()\n\n    dset_dict = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TEST,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        logger=logger,\n        dataset_minpkts=dataset_minpkts,\n    )\n\n    # pick the first dataset name\n    # just to identify the number of classes\n    name = next(iter(dset_dict.keys()))\n    num_classes = dset_dict[name].num_classes\n\n    net = backbone.net_factory(\n        num_classes=num_classes, flowpic_dim=flowpic_dim, with_dropout=with_dropout\n    )\n    fname = artifacts_folder / f\"./best_model_weights_split_{split_idx}.pt\"\n    net.load_weights(fname)\n    net = net.to(device)\n    trainer = methods.trainer_factory(\n        method=\"monolithic\", net=net, device=device, tracker=tracker, logger=logger\n    )\n\n    for name, dset in dset_dict.items():\n        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False)\n        context = \"test\"\n        if name != \"test\":\n            context = f\"test-{name}\"\n        metrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\n\n        utils.log_msg(\n            f'Test dataset {name} | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\n            logger,\n        )\n        reports = utils.classification_reports(\n            net,\n            dset,\n            batch_size,\n            device=device,\n            context=context,\n            save_to=artifacts_folder,\n            logger=logger,\n        )\n        state[f\"{name}_class_rep\"] = reports[\"class_rep\"]\n        state[f\"{name}_conf_mtx\"] = reports[\"conf_mtx\"]\n\n        class_rep = reports[\"class_rep\"]\n        precision, recall, f1 = class_rep.loc[\n            \"weighted avg\", [\"precision\", \"recall\", \"f1-score\"]\n        ].values\n        aimutils.track_metrics(\n            tracker, dict(precision=precision, recall=recall, f1=f1), context=context\n        )\n\n    state.update(dset_dict)\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.test_with_train_val_leftover","title":"<code>test_with_train_val_leftover(dataset_name, dset_train, dset_val, split_idx, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, device='cuda:0', tracker=None, artifacts_folder=None, with_dropout=True, state=None, dataset_minpkts=-1)</code>","text":"<p>Model testing on leftover split</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def test_with_train_val_leftover(\n    dataset_name: tcbench.DATASETS.UCDAVISICDM19,\n    dset_train: dataprep.FlowpicDataset,\n    dset_val: dataprep.FlowpicDataset,\n    split_idx: int,\n    batch_size: int = 32,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    logger=None,\n    device: str = \"cuda:0\",\n    tracker: aim.Run = None,\n    artifacts_folder: pathlib.Path = None,\n    with_dropout: bool = True,\n    state: dict = None,\n    dataset_minpkts: int = -1,\n):\n    \"\"\"Model testing on leftover split\"\"\"\n    if state is None:\n        state = dict()\n\n    dset_leftover = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TRAIN_VAL_LEFTOVER,\n        dset_train=dset_train,\n        dset_val=dset_val,\n        flowpic_dim=flowpic_dim,\n        logger=logger,\n        dataset_minpkts=dataset_minpkts,\n    )\n\n    num_classes = dset_train.num_classes\n\n    net = backbone.net_factory(\n        num_classes=num_classes, flowpic_dim=flowpic_dim, with_dropout=with_dropout\n    )\n    fname = artifacts_folder / f\"./best_model_weights_split_{split_idx}.pt\"\n    net.load_weights(fname)\n    net = net.to(device)\n    trainer = methods.trainer_factory(\n        method=\"monolithic\", net=net, device=device, tracker=tracker, logger=logger\n    )\n\n    loader = torch.utils.data.DataLoader(\n        dset_leftover, batch_size=batch_size, shuffle=False\n    )\n    context = f\"test-train-val-leftover\"\n    metrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\n    utils.log_msg(\n        f'Test dataset train-val-leftover | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\n        logger,\n    )\n    reports = utils.classification_reports(\n        net,\n        dset_leftover,\n        batch_size,\n        device=device,\n        context=context,\n        save_to=artifacts_folder,\n        logger=logger,\n    )\n    state[\"dset_leftover\"] = dset_leftover\n    state[\"leftover_class_rep\"] = reports[\"class_rep\"]\n    state[\"leftover_conf_mtx\"] = reports[\"conf_mtx\"]\n\n    class_rep = reports[\"class_rep\"]\n    precision, recall, f1 = class_rep.loc[\n        \"weighted avg\", [\"precision\", \"recall\", \"f1-score\"]\n    ].values\n    aimutils.track_metrics(\n        tracker, dict(precision=precision, recall=recall, f1=f1), context=context\n    )\n\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def main(args, extra_aim_hparams=None) -&gt; Dict[str, Any]:\n    \"\"\"Entry point\"\"\"\n    # bounding to a specific gpu\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_index\n    args.method = \"monolithic\"\n\n    if extra_aim_hparams is None:\n        extra_aim_hparams = {}\n\n    utils.seed_everything(args.seed)\n\n    if str(args.aim_repo).startswith(\"aim://\"):\n        utils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\n        aim_repo_path = args.aim_repo\n    else:\n        aim_repo_path = pathlib.Path(args.aim_repo)\n        args.artifacts_folder = pathlib.Path(args.artifacts_folder)\n\n    aimutils.init_repository(aim_repo_path)\n    aim_run = aim.Run(\n        repo=aim_repo_path,\n        experiment=args.aim_experiment_name,\n        log_system_params=True,\n        capture_terminal_logs=True,\n    )\n    aim_run_hash = utils.get_aim_run_hash(aim_run)\n\n    artifacts_folder = args.artifacts_folder / aim_run_hash\n    logger = utils.get_logger(artifacts_folder / \"log.txt\")\n\n    utils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\n    utils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\n    utils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\n    if artifacts_folder.parent != aim_repo_path:\n        utils.log_msg(\n            f\"WARNING: the artifact folder is not a subfolder of the AIM repo\",\n            logger\n        )\n\n    with_dropout = not args.suppress_dropout\n    run_hparams = dict(\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        split_index=args.split_index,\n        max_samples_per_class=args.max_samples_per_class,\n        aug_name=args.aug_name,\n        patience_steps=args.patience_steps,\n        suppress_val_augmentation=args.suppress_val_augmentation,\n        dataset=args.dataset,\n        dataset_minpkts=args.dataset_minpkts,\n        seed=args.seed,\n        with_dropout=with_dropout,\n        **extra_aim_hparams,\n    )\n    aim_run[\"hparams\"] = run_hparams\n\n    utils.log_msg(\"--- run hparams ---\", logger)\n    for param_name, param_value in run_hparams.items():\n        utils.log_msg(f\"{param_name}: {param_value}\", logger)\n    utils.log_msg(\"-------------------\", logger)\n\n    state = dict()\n\n    state = train(\n        dataset_name=args.dataset,\n        dataset_minpkts=args.dataset_minpkts,\n        batch_size=args.batch_size,\n        learning_rate=args.learning_rate,\n        patience_steps=args.patience_steps,\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        split_index=args.split_index,\n        max_samples_per_class=args.max_samples_per_class,\n        aug_name=args.aug_name,\n        tracker=aim_run,\n        workers=args.workers,\n        artifacts_folder=artifacts_folder,\n        logger=logger,\n        seed=args.seed,\n        epochs=args.epochs,\n        train_val_split_ratio=args.train_val_split_ratio,\n        suppress_val_augmentation=args.suppress_val_augmentation,\n        with_dropout=with_dropout,\n        state=state,\n    )\n    state = test(\n        dataset_name=args.dataset,\n        dataset_minpkts=args.dataset_minpkts,\n        split_idx=args.split_index,\n        batch_size=args.batch_size,\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        tracker=aim_run,\n        artifacts_folder=artifacts_folder,\n        logger=logger,\n        with_dropout=with_dropout,\n        state=state,\n    )\n\n    if not args.suppress_test_train_val_leftover and args.dataset == \"ucdavis-icdm19\":\n        state = test_with_train_val_leftover(\n            dataset_name=args.dataset,\n            dset_train=state[\"dset_train\"],\n            dset_val=state[\"dset_val\"],\n            split_idx=args.split_index,\n            batch_size=args.batch_size,\n            flowpic_dim=args.flowpic_dim,\n            flowpic_block_duration=args.flowpic_block_duration,\n            tracker=aim_run,\n            artifacts_folder=artifacts_folder,\n            logger=logger,\n            with_dropout=with_dropout,\n            state=state,\n        )\n\n    aim_run.close()\n\n    utils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading/#tcbench.modeling.run_augmentations_at_loading.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n\n    ##################\n    # general config\n    ##################\n    #    parser.add_argument(\n    #        \"--config\",\n    #        \"-c\",\n    #        type=pathlib.Path,\n    #        required=True,\n    #        default=\"./config.yml\",\n    #        help=utils.compose_cli_help_string(\"General configuration file\"),\n    #    )\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=DEFAULT_ARTIFACTS_FOLDER,\n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=20,\n        help=utils.compose_cli_help_string(\n            \"Number of parallel worker for loading the data\"\n        ),\n    )\n    parser.add_argument(\n        \"--gpu-index\",\n        default=\"0\",\n        help=utils.compose_cli_help_string(\"The GPU id to use\"),\n    )\n    parser.add_argument(\n        \"--aim-repo\",\n        default=DEFAULT_AIM_REPO,\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"augmentation-at-loading\",\n        help=utils.compose_cli_help_string(\n            \"The name of the experiment for AIM tracking\"\n        ),\n    )\n    parser.add_argument(\"--final\", action=\"store_true\", default=False)\n\n    ###############\n    # flowpic\n    ###############\n    parser.add_argument(\n        \"--flowpic-dim\",\n        type=int,\n        choices=(32, 64, 1500),\n        default=32,\n        help=utils.compose_cli_help_string(\"Flowpic dimension\"),\n    )\n    parser.add_argument(\n        \"--flowpic-block-duration\",\n        type=int,\n        default=15,\n        help=utils.compose_cli_help_string(\"Flowpic block duration (in seconds)\"),\n    )\n\n    ###############\n    # data\n    ###############\n    parser.add_argument(\n        \"--dataset\",\n        choices=tuple(map(str, tcbench.DATASETS.__members__.values())),\n        default=str(tcbench.DATASETS.UCDAVISICDM19),\n        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    )\n    parser.add_argument(\n        \"--dataset-minpkts\",\n        choices=(-1, 10, 100, 1000),\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n        ),\n    )\n    parser.add_argument(\n        \"--split-index\",\n        type=int,\n        default=0,\n        help=utils.compose_cli_help_string(\"Datasplit index\"),\n    )\n    parser.add_argument(\n        \"--max-samples-per-class\",\n        type=int,\n        default=-1,\n        help=utils.compose_cli_help_string(\n            \"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\"\n        ),\n    )\n\n    ###############\n    # training\n    ###############\n    parser.add_argument(\n        \"--train-val-split-ratio\",\n        type=float,\n        default=0.8,\n        help=utils.compose_cli_help_string(\"Training train/val split\"),\n    )\n    parser.add_argument(\n        \"--aug-name\",\n        type=str,\n        choices=(\n            \"noaug\",\n            \"rotate\",\n            \"horizontalflip\",\n            \"colorjitter\",\n            \"packetloss\",\n            \"timeshift\",\n            \"changertt\",\n        ),\n        default=\"noaug\",\n        help=utils.compose_cli_help_string(\"Augmentation policy\"),\n    )\n    parser.add_argument(\n        \"--suppress-val-augmentation\",\n        action=\"store_true\",\n        default=False,\n        help=utils.compose_cli_help_string(\"Do not augment validation set\"),\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=12345,\n        help=utils.compose_cli_help_string(\"Random seed\"),\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=64,\n        help=utils.compose_cli_help_string(\"Training batch size\"),\n    )\n    parser.add_argument(\"--patience-steps\", default=5, type=int)\n    parser.add_argument(\n        \"--learning-rate\",\n        type=float,\n        default=0.001,\n        help=utils.compose_cli_help_string(\"Traning learning rate\"),\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\"Number of epochs for training\"),\n    )\n    parser.add_argument(\n        \"--suppress-test-train-val-leftover\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n    )\n    parser.add_argument(\n        \"--suppress-dropout\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n    )\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/","title":"Tcbench modeling run augmentations at loading xgboost","text":""},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.train","title":"<code>train(dataset_name=DATASETS.UCDAVISICDM19, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, split_index=0, logger=None, tracker=None, workers=50, artifacts_folder=None, seed=12345, train_val_split_ratio=0.8, state=None)</code>","text":"<p>Train an XGBoost model</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def train(\n    dataset_name: DATASETS = DATASETS.UCDAVISICDM19,\n    flow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,\n    max_n_pkts: int = 10,\n    batch_size: int = 32,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    split_index: int = 0,\n    logger=None,\n    tracker: aim.Run = None,\n    workers: int = 50,\n    artifacts_folder=None,\n    seed: int = 12345,\n    train_val_split_ratio: float = 0.8,\n    state: dict = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Train an XGBoost model\"\"\"\n    if state is None:\n        state = dict()\n\n    dset_train, dset_val = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\n        flow_representation=flow_representation,\n        max_n_pkts=max_n_pkts,\n        split_idx=split_index,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        n_workers=workers,\n        logger=logger,\n        seed=seed,\n    )\n\n    train_loader = torch.utils.data.DataLoader(dset_train, batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dset_val, batch_size, shuffle=False)\n\n    xgboost_model = backbone.xgboost_factory(random_state=seed)\n\n    trainer_kwargs = dict(xgboost_model=xgboost_model, tracker=tracker, logger=logger)\n    trainer = methods.trainer_factory(\"xgboost\", **trainer_kwargs)\n\n    trained_model = trainer.train_loop(train_loader=train_loader, val_loader=val_loader)\n\n    xgboost_model.save_model(artifacts_folder / f\"xgb_model_split_{split_index}.json\")\n\n    utils.classification_reports(\n        None,\n        dset_train,\n        batch_size,\n        None,\n        context=\"train\",\n        save_to=artifacts_folder,\n        logger=logger,\n        method=\"xgboost\",\n        xgboost_model=xgboost_model,\n    )\n    utils.classification_reports(\n        None,\n        dset_val,\n        batch_size,\n        None,\n        context=\"val\",\n        save_to=artifacts_folder,\n        logger=logger,\n        method=\"xgboost\",\n        xgboost_model=xgboost_model,\n    )\n\n    state = dict(\n        best_net=xgboost_model,\n        dset_train=dset_train,\n        dset_val=dset_val,\n        scaler=dset_train.scaler,\n    )\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.test","title":"<code>test(dataset_name=DATASETS.UCDAVISICDM19, flow_representation=MODELING_INPUT_REPR_TYPE.FLOWPIC, max_n_pkts=10, split_idx=0, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, tracker=None, artifacts_folder=None, state=None)</code>","text":"<p>Test an XGBoost model</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def test(\n    dataset_name: DATASETS = DATASETS.UCDAVISICDM19,\n    flow_representation: MODELING_INPUT_REPR_TYPE = MODELING_INPUT_REPR_TYPE.FLOWPIC,\n    max_n_pkts: int = 10,\n    split_idx: int = 0,\n    batch_size: int = 32,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    logger=None,\n    tracker: aim.Run = None,\n    artifacts_folder: pathlib.Path = None,\n    state: dict = None,\n):\n    \"\"\"Test an XGBoost model\"\"\"\n    if state is None:\n        state = dict()\n\n    dset_dict = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TEST,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        logger=logger,\n        flow_representation=flow_representation,\n        max_n_pkts=max_n_pkts,\n    )\n\n    # pick the first dataset name\n    # just to identify the number of classes\n    name = next(iter(dset_dict.keys()))\n    num_classes = dset_dict[name].num_classes\n\n    fname = artifacts_folder / f\"./xgb_model_split_{split_idx}.json\"\n    model_xgb_2 = xgb.XGBClassifier()\n    model_xgb_2.load_model(fname)\n    trainer = methods.trainer_factory(\n        method=\"xgboost\", xgboost_model=model_xgb_2, tracker=tracker, logger=logger\n    )\n\n    for name, dset in dset_dict.items():\n        loader = torch.utils.data.DataLoader(dset, batch_size=batch_size, shuffle=False)\n        context = \"test\"\n        dset.set_scaler(state[\"scaler\"])\n        if name != \"test\":\n            context = f\"test-{name}\"\n\n        metrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\n\n        utils.log_msg(\n            f'Test dataset {name} |  acc: {metrics[\"acc\"]:.1f}',\n            logger,\n        )\n        utils.classification_reports(\n            None,\n            dset,\n            batch_size,\n            device=\"-1\",\n            context=context,\n            save_to=artifacts_folder,\n            logger=logger,\n            method=\"xgboost\",\n            xgboost_model=model_xgb_2,\n        )\n    state.update(dset_dict)\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.test_with_train_val_leftover","title":"<code>test_with_train_val_leftover(dataset_name, flow_representation, max_n_pkts, dset_train, dset_val, split_idx, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, logger=None, tracker=None, artifacts_folder=None, state=None)</code>","text":"<p>Test and XGBoost model on a leftover split</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def test_with_train_val_leftover(\n    dataset_name: DATASETS,\n    flow_representation: MODELING_INPUT_REPR_TYPE,\n    max_n_pkts: int,\n    dset_train: dataprep.FlowpicDataset,\n    dset_val: dataprep.FlowpicDataset,\n    split_idx: int,\n    batch_size: int = 32,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    logger=None,\n    tracker: aim.Run = None,\n    artifacts_folder: pathlib.Path = None,\n    state: dict = None,\n):\n    \"\"\"Test and XGBoost model on a leftover split\"\"\"\n    if state is None:\n        state = dict()\n\n    dset_leftover = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TRAIN_VAL_LEFTOVER,\n        dset_train=dset_train,\n        dset_val=dset_val,\n        flowpic_dim=flowpic_dim,\n        logger=logger,\n        flow_representation=flow_representation,\n        max_n_pkts=max_n_pkts,\n    )\n\n    num_classes = dset_train.num_classes\n\n    fname = artifacts_folder / f\"./xgb_model_split_{split_idx}.json\"\n    model_xgb_2 = xgb.XGBClassifier()\n    model_xgb_2.load_model(fname)\n    trainer = methods.trainer_factory(\n        method=\"xgboost\",\n        net=None,\n        device=\"-1\",\n        tracker=tracker,\n        logger=logger,\n        xgboost_model=model_xgb_2,\n    )\n\n    dset_leftover.set_scaler(state[\"scaler\"])\n    loader = torch.utils.data.DataLoader(\n        dset_leftover, batch_size=batch_size, shuffle=False\n    )\n    context = f\"test-train-val-leftover\"\n\n    metrics, reports = trainer.test_loop(loader, with_reports=True, context=context)\n    utils.log_msg(\n        f'Test dataset train-val-leftover |  acc: {metrics[\"acc\"]:.1f}',\n        logger,\n    )\n    utils.classification_reports(\n        None,\n        dset_leftover,\n        batch_size,\n        device=\"-1\",\n        context=context,\n        save_to=artifacts_folder,\n        logger=logger,\n        method=\"xgboost\",\n        xgboost_model=model_xgb_2,\n    )\n    state[\"dset_leftover\"] = dset_leftover\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def main(args, extra_aim_hparams=None) -&gt; Dict[str, Any]:\n    \"\"\"Entry point\"\"\"\n    if extra_aim_hparams is None:\n        extra_aim_hparams = {}\n\n    args.method = \"xgboost\"\n    args.dataset = DATASETS.from_str(str(args.dataset))\n    args.flow_representation = MODELING_INPUT_REPR_TYPE.from_str(\n        str(args.flow_representation)\n    )\n    args.aug_name = \"noaug\"\n\n    if str(args.aim_repo).startswith(\"aim://\"):\n        utils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\n        aim_repo_path = args.aim_repo\n    else:\n        aim_repo_path = pathlib.Path(args.aim_repo)\n        args.artifacts_folder = pathlib.Path(args.artifacts_folder)\n\n    aimutils.init_repository(aim_repo_path)\n    aim_run = aim.Run(\n        repo=aim_repo_path,\n        experiment=args.aim_experiment_name,\n        log_system_params=True,\n        capture_terminal_logs=True,\n    )\n    aim_run_hash = utils.get_aim_run_hash(aim_run)\n\n    artifacts_folder = args.artifacts_folder / aim_run_hash\n    logger = utils.get_logger(artifacts_folder / \"log.txt\")\n\n    utils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\n    utils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\n    utils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\n    if artifacts_folder.parent != aim_repo_path:\n        utils.log_msg(\n            f\"WARNING: the artifact folder is not a subfolder of the AIM repo\"\n        )\n\n    run_hparams = dict(\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        split_index=args.split_index,\n        dataset=str(args.dataset),\n        seed=args.seed,\n        flow_representation=str(args.flow_representation),\n        max_n_pkts=args.max_n_pkts,\n        **extra_aim_hparams,\n    )\n    aim_run[\"hparams\"] = run_hparams\n\n    utils.log_msg(\"--- run hparams ---\")\n    for param_name, param_value in run_hparams.items():\n        utils.log_msg(f\"{param_name}: {param_value}\")\n    utils.log_msg(\"-------------------\")\n\n    state = dict()\n\n    state = train(\n        dataset_name=args.dataset,\n        flow_representation=args.flow_representation,\n        max_n_pkts=args.max_n_pkts,\n        batch_size=args.batch_size,\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        split_index=args.split_index,\n        tracker=aim_run,\n        workers=args.workers,\n        artifacts_folder=artifacts_folder,\n        logger=logger,\n        seed=args.seed,\n        train_val_split_ratio=args.train_val_split_ratio,\n        state=state,\n    )\n    state = test(\n        dataset_name=args.dataset,\n        flow_representation=args.flow_representation,\n        max_n_pkts=args.max_n_pkts,\n        split_idx=args.split_index,\n        batch_size=args.batch_size,\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n        tracker=aim_run,\n        artifacts_folder=artifacts_folder,\n        logger=logger,\n        state=state,\n    )\n\n    if (\n        not args.suppress_test_train_val_leftover\n        and args.dataset == DATASETS.UCDAVISICDM19\n    ):\n        state = test_with_train_val_leftover(\n            dataset_name=args.dataset,\n            flow_representation=args.flow_representation,\n            max_n_pkts=args.max_n_pkts,\n            dset_train=state[\"dset_train\"],\n            dset_val=state[\"dset_val\"],\n            split_idx=args.split_index,\n            batch_size=args.batch_size,\n            flowpic_dim=args.flowpic_dim,\n            flowpic_block_duration=args.flowpic_block_duration,\n            tracker=aim_run,\n            artifacts_folder=artifacts_folder,\n            logger=logger,\n            state=state,\n        )\n\n    aim_run.close()\n\n    utils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_augmentations_at_loading_xgboost/#tcbench.modeling.run_augmentations_at_loading_xgboost.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_augmentations_at_loading_xgboost.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n\n    ##################\n    # general config\n    ##################\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=DEFAULT_ARTIFACTS_FOLDER,\n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=20,\n        help=utils.compose_cli_help_string(\n            \"Number of parallel worker for loading the data\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-repo\",\n        default=DEFAULT_AIM_REPO,\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"xgb_pktseries\",\n        help=utils.compose_cli_help_string(\n            \"The name of the experiment for AIM tracking\"\n        ),\n    )\n    parser.add_argument(\"--final\", action=\"store_true\", default=False)\n\n    ###############\n    # flowpic\n    ###############\n    parser.add_argument(\n        \"--flowpic-dim\",\n        type=int,\n        choices=(32, 64, 1500),\n        default=32,\n        help=utils.compose_cli_help_string(\"Flowpic dimension\"),\n    )\n    parser.add_argument(\n        \"--flowpic-block-duration\",\n        type=int,\n        default=15,\n        help=utils.compose_cli_help_string(\"Flowpic block duration (in seconds)\"),\n    )\n\n    ###############\n    # data\n    ###############\n    parser.add_argument(\n        \"--dataset\",\n        choices=(str(DATASETS.UCDAVISICDM19),),\n        default=str(DATASETS.UCDAVISICDM19),\n        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    )\n    parser.add_argument(\n        \"--split-index\",\n        type=int,\n        default=0,\n        help=utils.compose_cli_help_string(\"Datasplit index\"),\n    )\n    #    parser.add_argument(\n    #        \"--max-samples-per-class\",\n    #        type=int,\n    #        default=-1,\n    #        help=utils.compose_cli_help_string(\"Activated when --split-index is -1 to define how many samples to select for train+val (with a 80/20 split between train and val\")\n    #    )\n\n    ###############\n    # training\n    ###############\n    parser.add_argument(\n        \"--train-val-split-ratio\",\n        type=float,\n        default=0.8,\n        help=utils.compose_cli_help_string(\"Training train/val split\"),\n    )\n    #    parser.add_argument(\n    #        \"--aug-name\",\n    #        type=str,\n    #        choices=(\n    #            \"noaug\",\n    #            \"rotate\",\n    #            \"horizontalflip\",\n    #            \"colorjitter\",\n    #            \"packetloss\",\n    #            \"timeshift\",\n    #            \"changertt\",\n    #        ),\n    #        default=\"noaug\",\n    #        help=utils.compose_cli_help_string(\"Augmentation policy\")\n    #    )\n    #    parser.add_argument(\n    #        \"--suppress-val-augmentation\",\n    #        action='store_true',\n    #        default=False,\n    #        help=utils.compose_cli_help_string('Do not augment validation set')\n    #    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=12345,\n        help=utils.compose_cli_help_string(\"Random seed\"),\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=32,\n        help=utils.compose_cli_help_string(\"Training batch size\"),\n    )\n    parser.add_argument(\n        \"--suppress-test-train-val-leftover\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n    )\n    parser.add_argument(\n        \"--flow-representation\",\n        choices=tuple(\n            map(str, MODELING_INPUT_REPR_TYPE.__members__.values())\n        ),  # ('flowpic', 'pktseries'),\n        default=str(MODELING_INPUT_REPR_TYPE.FLOWPIC),  # \"flowpic\",\n        help=utils.compose_cli_help_string(\n            \"The string representing the flow representation\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-n-pkts\",\n        type=int,\n        default=10,\n        help=utils.compose_cli_help_string(\n            \"The number of packets in case of xgboost on packet series\"\n        ),\n    )\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/","title":"Tcbench modeling run campaign augmentations at loading","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/#tcbench.modeling.run_campaign_augmentations_at_loading.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_campaign_augmentations_at_loading.py</code> <pre><code>def main(args):\n    \"\"\"Entry point\"\"\"\n    torch.multiprocessing.set_start_method(\"spawn\", force=True)\n\n    args.seeds = list(map(int, args.seeds.split(\",\")))\n    args.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\n    args.augmentations = list(args.augmentations.split(\",\"))\n\n    if args.split_indexes is not None:\n        args.split_indexes = list(map(int, args.split_indexes.split(\",\")))\n\n    for dim in args.flowpic_dims:\n        if dim not in DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS: \n            raise RuntimeError(\n                f\"Flowpic can only be of size {DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS}\"\n            )\n\n    for aug_name in args.augmentations:\n        if aug_name not in DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS:\n            raise RuntimeError(\n                f\"Invalid augmentation {arg_name}. Possible choices: {DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS}\"\n            )\n\n    dataset_name = args.dataset\n\n    if args.split_indexes is None:\n        split_indexes = datasets_utils.get_split_indexes(\n            dataset_name, args.dataset_minpkts\n        )\n    else:\n        split_indexes = args.split_indexes\n\n    if args.max_train_splits == -1:\n        args.max_train_splits = len(split_indexes)\n    split_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\n\n    campaign_id = args.campaign_id\n    if campaign_id is None:\n        campaign_id = datetime.now().strftime(\"%s\")\n\n    extra_aim_hparams = dict(\n        campaign_id=campaign_id,\n    )\n\n    experiments_grid = list(\n        itertools.product(\n            split_indexes, args.augmentations, args.flowpic_dims, args.seeds\n        )\n    )\n    cum_run_completion_time = 0\n    avg_run_completion_time = 0\n    for exp_idx, (split_index, aug_name, flowpic_dim, seed) in enumerate(\n        experiments_grid\n    ):\n        time_run_start = time.time()\n\n        time_to_completion = timedelta(\n            seconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n        )\n        print()\n        print(\"#\" * 10)\n        print(\n            f\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n        )\n        print(\"#\" * 10)\n        print()\n        if args.dry_run:\n            print(f\"split_indexes ({len(split_indexes)}): {split_indexes}\")\n            print(f\"augmentations ({len(args.augmentations)}): {args.augmentations}\")\n            print(f\"flowpic_dims  ({len(args.flowpic_dims)}): {args.flowpic_dims}\")\n            print(f\"seeds         ({len(args.seeds)}): {args.seeds}\")\n            sys.exit(0)\n\n        new_params = dict(\n            split_index=split_index,\n            aug_name=aug_name,\n            flowpic_dim=flowpic_dim,\n            seed=seed,\n        )\n\n        extra_aim_hparams[\"campaign_exp_idx\"] = exp_idx + 1\n\n        # creating a \"dummy\" Namespace with all\n        # default values which will be overwritten\n        # based on the campain parameters\n        # cmd = f'--config {args.config_fname}'.split()\n        cmd = \"\"\n        run_args = run_augmentations_at_loading.cli_parser().parse_args(cmd)\n        for attr_name, _ in vars(run_args).items():\n            if attr_name in new_params:\n                setattr(run_args, attr_name, new_params[attr_name])\n\n        # directly passing parameters\n        run_args.final = False\n        run_args.method = \"monolithic\"\n        run_args.experiment_name = args.aim_experiment_name\n        run_args.gpu_index = args.gpu_index\n        run_args.aim_repo = args.aim_repo\n        run_args.artifacts_folder = args.artifacts_folder\n        run_args.patience_steps = args.patience_steps\n        run_args.suppress_test_train_val_leftover = (\n            args.suppress_test_train_val_leftover\n        )\n        run_args.max_samples_per_class = args.max_samples_per_class\n        # run_args.suppress_val_augmentation = args.suppress_val_augmentation\n        run_args.suppress_dropout = args.suppress_dropout\n        run_args.dataset = args.dataset\n        run_args.dataset_minpkts = args.dataset_minpkts\n        run_args.epochs = args.epochs\n        run_args.batch_size = args.batch_size\n\n        run_augmentations_at_loading.main(run_args, extra_aim_hparams)\n\n        time_run_end = time.time()\n        cum_run_completion_time += time_run_end - time_run_start\n        avg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading/#tcbench.modeling.run_campaign_augmentations_at_loading.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_campaign_augmentations_at_loading.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--aim-repo\",\n        default=\"./debug\",\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=\"./debug/artifacts\",\n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--campaign-id\",\n        default=None,\n        help=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"augmentations-at-loading\",\n        help=utils.compose_cli_help_string(\n            \"The experiment name to use for all Aim run in the campaign\"\n        ),\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\n            \"Number of parallel worker for loading the data\"\n        ),\n    )\n    parser.add_argument(\n        \"--gpu-index\",\n        default=\"0\",\n        help=utils.compose_cli_help_string(\"GPU where to operate\"),\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\n            \"Show the number of experiments and then quit\"\n        ),\n    )\n\n    #############################\n    # data options\n    #############################\n    parser.add_argument(\n        \"--split-indexes\",\n        default=None,\n        help=utils.compose_cli_help_string(\n            \"Coma separted list of split indexes. Use -1 to disable predefined split\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-samples-per-class\",\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"Used in conjuction with --split-indexes -1 to dynamically generate a train/val split. The number of samples specified corresponds to train+val (which will be separated with 80/20 for train and val)\"\n        ),\n    )\n    parser.add_argument(\n        \"--augmentations\",\n        default=\",\".join(\n            map(str, DEFAULT_CAMPAIGN_AUGATLOAD_AUGMENTATIONS)\n        ),  # AUGMENTATIONS)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of augmentations for experiments\"\n        ),\n    )\n    #    parser.add_argument(\n    #        \"--dataset\",\n    #        choices=('ucdavis-icdm19', 'utmobilenet21', 'mirage19', 'mirage22'),\n    #        default='ucdavis-icdm19',\n    #        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    #    )\n    parser.add_argument(\n        \"--dataset\",\n        choices=tuple(map(str, DATASETS.__members__.values())),\n        default=str(tcbench.DATASETS.UCDAVISICDM19),\n        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    )\n    parser.add_argument(\n        \"--dataset-minpkts\",\n        choices=(-1, 10, 100, 1000),\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n        ),\n    )\n\n    #############################\n    # train options\n    #############################\n    parser.add_argument(\n        \"--max-train-splits\",\n        type=int,\n        default=-1,\n        help=utils.compose_cli_help_string(\n            \"The maximum number of training splits to experiment with. If -1, use all available\"\n        ),\n    )\n    parser.add_argument(\n        \"--seeds\",\n        default=\",\".join(map(str, DEFAULT_CAMPAIGN_AUGATLOAD_SEEDS)),  # SEEDS)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of seed for experiments\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--flowpic-dims\",\n        default=\",\".join(\n            map(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)\n        ),  # FLOWPIC_DIMS)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of flowpic dimensions for experiments\"\n        ),\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=32,\n        help=utils.compose_cli_help_string(\"Batch size\"),\n    )\n    parser.add_argument(\n        \"--learning-rate\",\n        type=float,\n        default=0.001,\n        help=utils.compose_cli_help_string(\"Learning rate\"),\n    )\n    parser.add_argument(\"--patience-steps\", default=5, type=int)\n    #    parser.add_argument(\n    #        \"--suppress-val-augmentation\",\n    #        action='store_true',\n    #        default=False,\n    #        help=utils.compose_cli_help_string('Do not augment validation set')\n    #    )\n    parser.add_argument(\n        \"--suppress-test-train-val-leftover\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n    )\n    parser.add_argument(\n        \"--suppress-dropout\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\"Number of epochs for training\"),\n    )\n\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/","title":"Tcbench modeling run campaign augmentations at loading xgboost","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/#tcbench.modeling.run_campaign_augmentations_at_loading_xgboost.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_campaign_augmentations_at_loading_xgboost.py</code> <pre><code>def main(args):\n    \"\"\"Entry point\"\"\"\n\n    args.seeds = list(map(int, args.seeds.split(\",\")))\n    args.max_n_pkts = list(map(int, args.max_n_pkts.split(\",\")))\n    args.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\n\n    # forcing specific parameters\n    args.dataset = DATASETS.UCDAVISICDM19\n    args.augmentations = AUGMENTATIONS\n    args.dataset_minpkts = -1\n\n    if args.split_indexes is not None:\n        args.split_indexes = list(map(int, args.split_indexes.split(\",\")))\n\n    for dim in args.flowpic_dims:\n        if (\n            dim not in DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS\n        ):  # FLOWPIC_DIMS: #(32, 64, 1500):\n            raise RuntimeError(\n                f\"Invalid value {dim}: Flowpic can only be of size {DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS}\"\n            )\n\n    if args.split_indexes is None:\n        split_indexes = datasets_utils.get_split_indexes(\n            args.dataset, args.dataset_minpkts\n        )\n    else:\n        split_indexes = args.split_indexes\n\n    if args.max_train_splits == -1:\n        args.max_train_splits = len(split_indexes)\n\n    split_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\n\n    campaign_id = args.campaign_id\n    if campaign_id is None:\n        campaign_id = datetime.now().strftime(\"%s\")\n\n    extra_aim_hparams = dict(\n        campaign_id=campaign_id,\n    )\n\n    l = [split_indexes, args.augmentations, args.seeds]\n    # force a dummy value which will be ignored\n    # in the downstream task\n    if args.flow_representation == \"flowpic\":\n        l.append(args.flowpic_dims)\n        l.append([30])\n    else:\n        l.append([32])\n        l.append(args.max_n_pkts)\n\n    experiments_grid = list(itertools.product(*l))\n\n    cum_run_completion_time = 0\n    avg_run_completion_time = 0\n    for exp_idx, (split_index, aug_name, seed, flowpic_dim, mnp) in enumerate(\n        experiments_grid\n    ):\n        time_run_start = time.time()\n\n        time_to_completion = timedelta(\n            seconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n        )\n        print()\n        print(\"#\" * 10)\n        print(\n            f\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n        )\n        print(\"#\" * 10)\n        print()\n        if args.dry_run:\n            print(f\"split_indexes ({len(split_indexes)}): {split_indexes}\")\n            print(f\"augmentations ({len(args.augmentations)}): {args.augmentations}\")\n            print(f\"seeds         ({len(args.seeds)}): {args.seeds}\")\n            if args.flow_representation == \"flowpic\":\n                print(f\"flowpic_dims  ({len(args.flowpic_dims)}): {args.flowpic_dims}\")\n            else:\n                print(f\"max_n_pkts    ({len(args.max_n_pkts)}): {args.max_n_pkts}\")\n            sys.exit(0)\n\n        new_params = dict(\n            split_index=split_index,\n            aug_name=aug_name,\n            flowpic_dim=flowpic_dim,\n            max_n_pkts=mnp,\n            seed=seed,\n        )\n\n        extra_aim_hparams[\"campaign_exp_idx\"] = exp_idx + 1\n\n        # creating a \"dummy\" Namespace with all\n        # default values which will be overwritten\n        # based on the campain parameters\n        # cmd = f'--config {args.config_fname}'.split()\n        cmd = \"\"\n        run_args = run_augmentations_at_loading_xgboost.cli_parser().parse_args(cmd)\n        for attr_name, _ in vars(run_args).items():\n            if attr_name in new_params:\n                setattr(run_args, attr_name, new_params[attr_name])\n\n        # directly passing parameters\n        # run_args.final = False\n        run_args.method = \"xgboost\"\n        run_args.aim_experiment_name = args.aim_experiment_name\n        # run_args.gpu_index = args.gpu_index\n        run_args.aim_repo = args.aim_repo\n        run_args.artifacts_folder = args.artifacts_folder\n        # run_args.patience_steps = args.patience_steps\n        run_args.suppress_test_train_val_leftover = (\n            args.suppress_test_train_val_leftover\n        )\n        run_args.max_samples_per_class = args.max_samples_per_class\n        # run_args.suppress_val_augmentation = args.suppress_val_augmentation\n        # run_args.suppress_dropout = args.suppress_dropout\n        run_args.dataset = args.dataset\n        run_args.flow_representation = args.flow_representation\n        run_args.batch_size = 32\n\n        run_augmentations_at_loading_xgboost.main(run_args, extra_aim_hparams)\n\n        time_run_end = time.time()\n        cum_run_completion_time += time_run_end - time_run_start\n        avg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_augmentations_at_loading_xgboost/#tcbench.modeling.run_campaign_augmentations_at_loading_xgboost.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_campaign_augmentations_at_loading_xgboost.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--aim-repo\",\n        default=DEFAULT_AIM_REPO,\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=DEFAULT_ARTIFACTS_FOLDER,\n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--campaign-id\",\n        default=None,\n        help=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n    )\n    parser.add_argument(\n        \"--split-indexes\",\n        default=None,\n        help=utils.compose_cli_help_string(\n            \"Comma separted list of split indexes. Use -1 to disable predefined split\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-train-splits\",\n        type=int,\n        default=-1,\n        help=utils.compose_cli_help_string(\n            \"The maximum number of training splits to experiment with. If -1, use all available\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-samples-per-class\",\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"Used in conjuction with --split-indexes -1 to dynamically generate a train/val split. The number of samples specified corresponds to train+val (which will be separated with 80/20 for train and val)\"\n        ),\n    )\n    parser.add_argument(\n        \"--seeds\",\n        default=\",\".join(\n            map(\n                str,\n                DEFAULT_CAMPAIGN_AUGATLOAD_SEEDS,\n            )\n        ),  # SEEDS)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of seed for experiments\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"xgb_pkts\",\n        help=utils.compose_cli_help_string(\n            \"The experiment name to use for all Aim run in the campaign\"\n        ),\n    )\n    parser.add_argument(\n        \"--flow-representation\",\n        choices=(\"flowpic\", \"pktseries\"),\n        default=\"flowpic\",\n        help=utils.compose_cli_help_string(\n            \"The string representing the flow representation (flowpic or pktseries)\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-n-pkts\",\n        default=\",\".join(\n            map(\n                str,\n                DEFAULT_CAMPAIGN_AUGATLOAD_PKTSERIESLEN,\n            )\n        ), \n        help=utils.compose_cli_help_string(\n            \"The number of packets in case of xgboost on packet series\"\n        ),\n    )\n    parser.add_argument(\n        \"--suppress-test-train-val-leftover\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Skip test on leftover split\"),\n    )\n    parser.add_argument(\n        \"--flowpic-dims\",\n        default=\",\".join(\n            map(str, DEFAULT_CAMPAIGN_AUGATLOAD_FLOWPICDIMS)\n        ),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of flowpic dimensions for experiments\"\n        ),\n    )\n    #    parser.add_argument(\n    #        \"--dataset\",\n    #        choices=(str(DATASETS.UCDAVISICDM19),),#('ucdavis-icdm19', #)'utmobilenet21'),\n    #        default=str(DATASETS.UCDAVISICDM19), #'ucdavis-icdm19',\n    #        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    #    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\n            \"Show the number of experiments and then quit\"\n        ),\n    )\n\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/","title":"Tcbench modeling run campaign contrastive learning and finetune","text":""},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/#tcbench.modeling.run_campaign_contrastive_learning_and_finetune.main","title":"<code>main(args)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_campaign_contrastive_learning_and_finetune.py</code> <pre><code>def main(args):\n    \"\"\"Entry point\"\"\"\n    args.contrastive_learning_seeds = list(\n        map(int, args.contrastive_learning_seeds.split(\",\"))\n    )\n    args.finetune_seeds = list(map(int, args.finetune_seeds.split(\",\")))\n    args.augmentations = args.augmentations.split(\",\")\n    args.flowpic_dims = list(map(int, args.flowpic_dims.split(\",\")))\n    args.projection_layer_dims = list(map(int, args.projection_layer_dims.split(\",\")))\n    args.dataset = DATASETS.UCDAVISICDM19\n    args.dropout = args.dropout.split(\",\")\n    args.suppress_dropout = list(item == \"disabled\" for item in args.dropout)\n\n    if args.split_indexes is not None:\n        args.split_indexes = list(map(int, args.split_indexes.split(\",\")))\n\n    for aug_name in args.augmentations:\n        if aug_name not in DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_VALID_AUGMENTATIONS:\n            raise RuntimeError(\n                f\"Invalid augmentation {aug_name}. Possible choices: {list(DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_VALID_AUGMENTATIONS)}\"\n            )\n\n    for dim in args.flowpic_dims:\n        if dim not in DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS:\n            raise RuntimeError(\n                f\"Flowpic can only be of size {DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS}\"\n            )\n\n    if args.split_indexes is None:\n        # NOTE: min_pkts=-1 ...because as is it enforces ucdavis-icdm19\n        split_indexes = datasets_utils.get_split_indexes(args.dataset, min_pkts=-1)\n    else:\n        split_indexes = args.split_indexes\n\n    if args.max_train_splits == -1:\n        args.max_train_splits = len(split_indexes)\n    split_indexes = split_indexes[: min(len(split_indexes), args.max_train_splits)]\n\n    campaign_id = args.campaign_id\n    if campaign_id is None:\n        campaign_id = datetime.now().strftime(\"%s\")\n\n    extra_aim_hparams = dict(\n        campaign_id=campaign_id,\n    )\n\n    experiments_grid = list(\n        itertools.product(\n            split_indexes,\n            args.contrastive_learning_seeds,\n            args.finetune_seeds,\n            args.flowpic_dims,\n            args.suppress_dropout,\n            args.projection_layer_dims,\n        )\n    )\n\n    if len(experiments_grid) == 0:\n        raise RuntimeError(f\"Something wrong: The experiments grid is empty\")\n\n    cum_run_completion_time = 0\n    avg_run_completion_time = 0\n    for exp_idx, (\n        split_index,\n        contrastive_learning_seed,\n        finetune_seed,\n        flowpic_dim,\n        suppress_dropout,\n        projection_layer_dim,\n    ) in enumerate(experiments_grid):\n        time_run_start = time.time()\n\n        time_to_completion = timedelta(\n            seconds=avg_run_completion_time * (len(experiments_grid) - exp_idx)\n        )\n        print()\n        print(\"#\" * 10)\n        print(\n            f\"# campaign_id: {campaign_id} | run {exp_idx+1}/{len(experiments_grid)} - time to completion {time_to_completion}\"\n        )\n        print(\"#\" * 10)\n        print()\n\n        if args.dry_run:\n            print(f\"split_indexes              ({len(split_indexes)}): {split_indexes}\")\n            print(\n                f\"contrastive learning seeds ({len(args.contrastive_learning_seeds)}): {args.contrastive_learning_seeds}\"\n            )\n            print(\n                f\"finetune seeds             ({len(args.finetune_seeds)}): {args.finetune_seeds}\"\n            )\n            print(\n                f\"projection layer dims      ({len(args.projection_layer_dims)}): {args.projection_layer_dims}\"\n            )\n            print(f\"dropout                    ({len(args.dropout)}): {args.dropout}\")\n            print(\n                f\"flowpic dims               ({len(args.flowpic_dims)}): {args.flowpic_dims}\"\n            )\n            sys.exit(0)\n\n        new_params = dict(\n            split_index=split_index,\n            contrastive_learning_seed=contrastive_learning_seed,\n            finetune_seed=finetune_seed,\n        )\n\n        # creating a \"dummy\" Namespace with all\n        # default values which will be overwritten\n        # based on the campain parameters\n        # cmd = f'--config {args.config_fname}'.split()\n        cmd = \"\"\n        run_args = run_contrastive_learning_and_finetune.cli_parser().parse_args(cmd)\n        extra_aim_hparams[\"campaign_exp_idx\"] = exp_idx\n\n        for attr_name, _ in vars(run_args).items():\n            if attr_name in new_params:\n                setattr(run_args, attr_name, new_params[attr_name])\n        run_args.final = False\n        run_args.method = \"simclr\"\n        # run_args.config = args.config\n        run_args.aim_experiment_name = args.aim_experiment_name\n        run_args.aim_repo = args.aim_repo\n        run_args.artifacts_folder = args.artifacts_folder\n        run_args.gpu_index = args.gpu_index\n        run_args.suppress_dropout = suppress_dropout\n        run_args.flowpic_dim = flowpic_dim\n        run_args.projection_layer_dim = projection_layer_dim\n        # run_args.finetune_augmentation = args.finetune_augmentation\n        run_args.augmentations = args.augmentations\n        run_args.batch_size = args.batch_size\n        run_args.train_val_split_ratio = args.train_val_split_ratio\n\n        run_contrastive_learning_and_finetune.main(run_args, extra_aim_hparams)\n\n        time_run_end = time.time()\n        cum_run_completion_time += time_run_end - time_run_start\n        avg_run_completion_time = cum_run_completion_time / (exp_idx + 1)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_campaign_contrastive_learning_and_finetune/#tcbench.modeling.run_campaign_contrastive_learning_and_finetune.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_campaign_contrastive_learning_and_finetune.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n    ###################\n    ## general options\n    ###################\n    parser.add_argument(\n        \"--campaign-id\",\n        default=None,\n        help=utils.compose_cli_help_string(\"A campaign id to mark all experiments\"),\n    )\n    parser.add_argument(\n        \"--aim-repo\",\n        default=DEFAULT_AIM_REPO, \n        type=pathlib.Path,\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"contrastive-learning-and-finetune\",\n        help=utils.compose_cli_help_string(\n            \"The experiment name to use for all Aim run in the campaign\"\n        ),\n    )\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=DEFAULT_ARTIFACTS_FOLDER, \n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\n            \"Number of parallel worker for loading the data\"\n        ),\n    )\n    parser.add_argument(\n        \"--gpu-index\",\n        default=\"0\",\n        help=utils.compose_cli_help_string(\"GPU where to operate\"),\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\n            \"Show the number of experiments and then quit\"\n        ),\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=32,\n        help=utils.compose_cli_help_string(\"Training batch size\"),\n    )\n    parser.add_argument(\n        \"--split-indexes\",\n        default=None,\n        help=utils.compose_cli_help_string(\n            \"Coma separted list of split indexes. Use -1 to disable predefined split\"\n        ),\n    )\n\n    ###################\n    ## data options\n    ###################\n    parser.add_argument(\n        \"--max-train-splits\",\n        type=int,\n        default=-1,\n        help=utils.compose_cli_help_string(\n            \"The maximum number of training splits to experiment with. If -1, use all available\"\n        ),\n    )\n    parser.add_argument(\n        \"--augmentations\",\n        default=\"changertt,timeshift\",\n        help=utils.compose_cli_help_string(\n            \"A pair of augmentations to use for contrastive learning\"\n        ),\n    )\n    parser.add_argument(\n        \"--train-val-split-ratio\",\n        default=0.8,\n        type=float,\n        help=utils.compose_cli_help_string(\n            \"Fraction of samples to dedicate for training\"\n        )\n    )\n\n    ###################\n    ## flowpic options\n    ###################\n    parser.add_argument(\n        \"--flowpic-dims\",\n        default=\",\".join(\n            map(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_FLOWPICDIMS)\n        ),  # FLOWPIC_DIMS)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of flowpic dimensions for experiments\"\n        ),\n    )\n    ###################\n    ## training options\n    ###################\n    parser.add_argument(\n        \"--contrastive-learning-seeds\",\n        default=\",\".join(\n            map(\n                str,\n                DEFAULT_CAMPAING_CONTRALEARNANDFINETUNE_SEEDS_CONTRALEARN,\n            )\n        ),  # SEEDS_CONTRASTIVELEARNING)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of seeds to use for contrastive learning pretraining\"\n        ),\n    )\n    parser.add_argument(\n        \"--finetune-seeds\",\n        default=\",\".join(\n            map(str, DEFAULT_CAMPAIGN_CONTRALEARNANDFINETUNE_SEEDS_FINETUNE)\n        ),  ##SEEDS_FINETUNING)),\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of seeds to use for finetune training\"\n        ),\n    )\n    parser.add_argument(\n        \"--dropout\",\n        type=str,\n        default=\"disabled\",\n        help=utils.compose_cli_help_string(\n            \"Coma separated list. Choices: (enabled, disabled)\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--projection-layer-dims\",\n        default=\"30\",\n        help=utils.compose_cli_help_string(\n            \"Coma separated list of contrastive learning projection layer dimensions\"\n        ),\n    )\n    #    parser.add_argument(\n    #        \"--finetune-augmentation\", default=\"none\",\n    #        choices=(\"none\", \"only-views\", \"views-and-original\"),\n    #        help=utils.compose_cli_help_string(\"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\")\n    #    )\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/","title":"Tcbench modeling run contrastive learning and finetune","text":""},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.pretrain","title":"<code>pretrain(dataset_name=DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, learning_rate=0.001, flowpic_dim=32, flowpic_block_duration=15, split_idx=0, aug_samples=2, aug_config=None, logger=None, device='cuda:0', tracker=None, workers=50, artifacts_folder=None, seed=12345, epochs=50, patience_steps=3, loss_temperature=0.07, with_dropout=True, projection_layer_dim=30, max_samples_per_class=-1, state=None, train_val_split_ratio=0.8)</code>","text":"<p>Pretrain a model</p> Source code in <code>src/tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def pretrain(\n    dataset_name: str = DATASETS.UCDAVISICDM19,\n    dataset_minpkts: int = -1,\n    batch_size: int = 32,\n    learning_rate: float = 0.001,\n    flowpic_dim: int = 32,\n    flowpic_block_duration: int = 15,\n    split_idx: int = 0,\n    aug_samples: int = 2,\n    aug_config: dict = None,\n    logger=None,\n    device: str = \"cuda:0\",\n    tracker: aim.Run = None,\n    workers: int = 50,\n    artifacts_folder=None,\n    seed: int = 12345,\n    epochs: int = 50,\n    patience_steps: int = 3,\n    loss_temperature: float = 0.07,\n    with_dropout: bool = True,\n    projection_layer_dim: int = 30,\n    max_samples_per_class: int = -1,\n    state: dict = None,\n    train_val_split_ratio: float = 0.8,\n):\n    \"\"\"Pretrain a model\"\"\"\n    assert aug_samples &gt;= 2, \"aug_samples cannot be smaller than 2\"\n\n    if state is None:\n        state = dict()\n\n    utils.seed_everything(seed)\n\n    if aug_config is None:\n        aug_config = dict(changertt={}, timeshift={})\n\n    dset_train, dset_val = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.TRAIN_VAL,\n        split_idx=split_idx,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        aug_config=aug_config,\n        aug_samples=aug_samples,\n        aug_when_loading=False,\n        n_workers=workers,\n        logger=logger,\n        dataset_minpkts=dataset_minpkts,\n        max_samples_per_class=max_samples_per_class,\n        seed=seed,\n        train_val_split_ratio=train_val_split_ratio,\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        dset_train,\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    val_loader = torch.utils.data.DataLoader(dset_val, batch_size=batch_size)\n\n    net = backbone.net_factory(\n        num_classes=None,\n        flowpic_dim=32,\n        with_dropout=with_dropout,\n        projection_layer_dim=projection_layer_dim,\n    )\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n    trainer = methods.SimCLRTrainer(\n        pretrain_config=dict(\n            optimizer=optimizer,\n            loss_temperature=loss_temperature,\n        ),\n        deterministic=True,\n        device=device,\n        tracker=tracker,\n        logger=logger,\n    )\n    best_net = trainer.pretrain_loop(\n        net=net,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        patience_monitor=methods.PatienceMonitorAccuracy(\n            \"acc_top_5\", steps=patience_steps\n        ),\n        epochs=epochs,\n        context=\"contrastivelearning\",\n    )\n    if artifacts_folder:\n        fname = artifacts_folder / f\"best_model_weights_pretrain_split_{split_idx}.pt\"\n        utils.log_msg(f\"saving: {fname}\", logger)\n        best_net.save_weights(fname)\n\n    state[\"best_net\"] = best_net\n    state[\"dset_train\"] = dset_train\n    state[\"dset_val\"] = dset_val\n\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.finetune_test","title":"<code>finetune_test(net, dset, dset_name, trainer, logger=None, batch_size=32, device='cuda:0', tracker=None, artifacts_folder=None)</code>","text":"<p>Test after finetune</p> Source code in <code>src/tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def finetune_test(\n    net: backbone.BaseNet,\n    dset: dataprep.FlowpicDataset,\n    dset_name: str,\n    trainer: methods.SimCLRTrainer,\n    logger: logging.Logger = None,\n    batch_size=32,\n    device: str = \"cuda:0\",\n    tracker: aim.Run = None,\n    artifacts_folder=None,\n):\n    \"\"\"Test after finetune\"\"\"\n    utils.log_msg(f\"\\n--- finetune (test) on {dset_name} ---\", logger)\n    utils.log_msg(dset.samples_count(), logger)\n\n    loader = torch.utils.data.DataLoader(dset, batch_size, shuffle=False)\n    context = f\"test-{dset_name}\"\n    metrics, reports = trainer.finetune_test_loop(\n        loader, with_reports=True, context=context\n    )\n\n    utils.log_msg(\n        f'Test dataset {dset_name} | loss: {metrics[\"loss\"]:.6f} | acc: {metrics[\"acc\"]:.1f}',\n        logger,\n    )\n    utils.classification_reports(\n        net,\n        dset,\n        batch_size,\n        device=device,\n        context=context,\n        save_to=artifacts_folder,\n        logger=logger,\n    )\n    return metrics, reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.finetune","title":"<code>finetune(dataset_name=tcbench.DATASETS.UCDAVISICDM19, dataset_minpkts=-1, batch_size=32, flowpic_dim=32, flowpic_block_duration=15, learning_rate=0.01, tracker=None, logger=None, seed=12345, epochs=50, device='cuda:0', split_idx=None, artifacts_folder=None, fname_pretrain_weights=None, train_samples=10, patience_steps=5, patience_min_delta=0.001, with_dropout=True, projection_layer_dim=30, aug_config=None, aug_samples=2, aug_yield_also_original=False, state=None)</code>","text":"<p>Finetune a model</p> Source code in <code>src/tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def finetune(\n    # config,\n    dataset_name: str = tcbench.DATASETS.UCDAVISICDM19,\n    dataset_minpkts: int = -1,\n    batch_size:int=32,\n    flowpic_dim:int=32,\n    flowpic_block_duration:int=15,\n    learning_rate:float=0.01,\n    tracker:aim.Run=None,\n    logger:logging.Logger=None,\n    seed:int=12345,\n    epochs:int=50,\n    device:str=\"cuda:0\",\n    split_idx:int=None,\n    artifacts_folder:pathlib.Path=None,\n    fname_pretrain_weights:pathlib.Path=None,\n    train_samples:int=10,\n    patience_steps:int=5,\n    patience_min_delta:float=0.001,\n    with_dropout: bool = True,\n    projection_layer_dim: int = 30,\n    aug_config: dict = None,\n    aug_samples: int = 2,\n    aug_yield_also_original: bool = False,\n    state: dict = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Finetune a model\"\"\"\n    utils.seed_everything(seed)\n\n    if (artifacts_folder is None and fname_pretrain_weights is None) or (\n        artifacts_folder is not None and fname_pretrain_weights is not None\n    ):\n        raise RuntimeError(\"Provide either artifact_folder or fname_weights\")\n\n    if artifacts_folder and split_idx is None:\n        raise RuntimeError(\"When using artifact_folder, split_idx cannot be None\")\n\n    fname_pretrain_weights = (\n        artifacts_folder / f\"best_model_weights_pretrain_split_{split_idx}.pt\"\n    )\n\n    dset_dict = dataprep.load_dataset(\n        dataset_name=dataset_name,\n        dataset_type=MODELING_DATASET_TYPE.FINETUNING,\n        flowpic_dim=flowpic_dim,\n        flowpic_block_duration=flowpic_block_duration,\n        logger=logger,\n        train_samples=train_samples,\n        seed=seed,\n        aug_config=aug_config,\n        aug_samples=aug_samples,\n        aug_yield_also_original=aug_yield_also_original,\n        dataset_minpkts=dataset_minpkts,\n    )\n\n    dataset_names = [name.split(\"_\")[0] for name in dset_dict if name.endswith(\"train\")]\n\n    if state is None:\n        state = dict()\n\n    for dset_name in dataset_names:\n        dset_train = dset_dict[f\"{dset_name}_train\"]\n        dset_test = dset_dict[f\"{dset_name}_test\"]\n        state[f\"{dset_name}_dset_train\"] = dset_train\n        state[f\"{dset_name}_dset_test\"] = dset_test\n        num_classes = dset_train.num_classes\n\n        train_loader = torch.utils.data.DataLoader(\n            dset_train, batch_size=batch_size, shuffle=True\n        )\n\n        utils.log_msg(f\"\\n--- finetune (train) on {dset_name} ---\", logger)\n\n        utils.log_msg(dset_train.samples_count(), logger)\n\n        # the network here is just a dummy object\n        # the actual network is created by the trainer\n        net = backbone.net_factory(\n            num_classes=None,\n            flowpic_dim=flowpic_dim,\n            with_dropout=with_dropout,\n            projection_layer_dim=projection_layer_dim,\n        )\n        # the optimizer here is just a dummy\n        # reference object. The final optimizer\n        # is recreated when triggering the training\n        # to adapt to the network and loaded weights\n        finetune_config = dict(\n            optimizer=torch.optim.Adam(net.parameters(), learning_rate),\n        )\n\n        trainer = methods.SimCLRTrainer(\n            finetune_config=finetune_config,\n            deterministic=True,\n            device=device,\n            tracker=tracker,\n            logger=logger,\n        )\n        best_net = trainer.finetune_loop(\n            net=net,\n            train_loader=train_loader,\n            val_loader=None,\n            epochs=epochs,\n            num_classes=dset_train.num_classes,\n            fname_pretrain_weights=fname_pretrain_weights,\n            patience_monitor=methods.PatienceMonitorLoss(\n                steps=patience_steps, min_delta=patience_min_delta\n            ),\n            context=f\"finetune_{dset_name}\",\n        )\n        if artifacts_folder:\n            fname = (\n                artifacts_folder\n                / f\"best_model_weights_finetune_{dset_name}_from_split_{split_idx}.pt\"\n            )\n            utils.log_msg(f\"saving: {fname}\", logger)\n            best_net.save_weights(fname)\n\n        state[f\"{dset_name}_best_net\"] = best_net\n\n        metrics, reports = finetune_test(\n            net=best_net,\n            dset=dset_test,\n            dset_name=dset_name,\n            trainer=trainer,\n            logger=logger,\n            batch_size=batch_size,\n            device=device,\n            tracker=tracker,\n            artifacts_folder=artifacts_folder,\n        )\n        state[f\"{dset_name}_class_rep\"] = reports[\"class_rep\"]\n        state[f\"{dset_name}_conf_mtx\"] = reports[\"conf_mtx\"]\n\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.main","title":"<code>main(args, extra_aim_hparams=None)</code>","text":"<p>Entry point</p> Source code in <code>src/tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def main(args, extra_aim_hparams=None):\n    \"\"\"Entry point\"\"\"\n    # bounding to a specific gpu\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_index\n    args.method = \"simclr\"\n\n    if extra_aim_hparams is None:\n        extra_aim_hparams = {}\n\n    if str(args.aim_repo).startswith(\"aim://\"):\n        utils.log_msg(f\"Connecting to remote AIM server {args.aim_repo}\")\n        aim_repo_path = args.aim_repo\n    else:\n        aim_repo_path = pathlib.Path(args.aim_repo)\n        args.artifacts_folder = pathlib.Path(args.artifacts_folder)\n\n    aimutils.init_repository(aim_repo_path)\n    aim_run = aim.Run(\n        repo=aim_repo_path,\n        experiment=args.aim_experiment_name,\n        log_system_params=True,\n        capture_terminal_logs=True,\n    )\n    aim_run_hash = utils.get_aim_run_hash(aim_run)\n    # artifacts_folder = args.artifacts_folder / args.dataset / aim_run_hash\n    artifacts_folder = args.artifacts_folder / aim_run_hash\n\n    logger = utils.get_logger(artifacts_folder / \"log.txt\")\n\n    utils.log_msg(f\"\\nconnecting to AIM repo at: {aim_repo_path}\", logger)\n    utils.log_msg(f\"created aim run hash={aim_run_hash}\", logger)\n    utils.log_msg(f\"artifacts folder at: {artifacts_folder}\", logger)\n    if artifacts_folder.parent != aim_repo_path:\n        utils.log_msg(\n            f\"WARNING: the artifact folder is not a subfolder of the AIM repo\",\n            logger\n        )\n\n    with_dropout = not args.suppress_dropout\n    run_hparams = dict(\n        flowpic_dim=args.flowpic_dim,\n        split_index=args.split_index,\n        dataset=args.dataset,\n        dataset_minpkts=args.dataset_minpkts,\n        contrastive_learning_seed=args.contrastive_learning_seed,\n        finetune_seed=args.finetune_seed,\n        finetune_train_samples=args.finetune_train_samples,\n        with_dropout=with_dropout,\n        projection_layer_dim=args.projection_layer_dim,\n        finetune_augmentation=args.finetune_augmentation,\n        augmentations=args.augmentations,\n        train_val_split_ratio=args.train_val_split_ratio,\n        **extra_aim_hparams,\n    )\n    aim_run[\"hparams\"] = run_hparams\n\n    utils.log_msg(\"--- run hparams ---\", logger)\n    for param_name, param_value in run_hparams.items():\n        utils.log_msg(f\"{param_name}: {param_value}\", logger)\n    utils.log_msg(\"-------------------\", logger)\n\n    generic_params = dict(\n        dataset_name=args.dataset,\n        dataset_minpkts=args.dataset_minpkts,\n        tracker=aim_run,\n        logger=logger,\n        artifacts_folder=artifacts_folder,\n        with_dropout=with_dropout,\n        projection_layer_dim=args.projection_layer_dim,\n    )\n    flowpic_params = dict(\n        flowpic_dim=args.flowpic_dim,\n        flowpic_block_duration=args.flowpic_block_duration,\n    )\n\n    aug_config = dict(changertt={}, timeshift={})\n    if args.augmentations:\n        aug_config = {aug_name: {} for aug_name in args.augmentations}\n\n    contrastive_learning_params = dict(\n        split_idx=args.split_index,\n        batch_size=args.batch_size,\n        learning_rate=args.contrastive_learning_lr,\n        seed=args.contrastive_learning_seed,\n        aug_config=aug_config,\n        aug_samples=2,\n        epochs=args.contrastive_learning_epochs,\n        patience_steps=args.contrastive_learning_patience_steps,\n        loss_temperature=args.contrastive_learning_temperature,\n        max_samples_per_class=args.max_samples_per_class,\n        train_val_split_ratio=args.train_val_split_ratio,\n    )\n    state_pretrain = pretrain(\n        **generic_params, **flowpic_params, **contrastive_learning_params\n    )\n\n    finetune_params = dict(\n        batch_size=args.batch_size,\n        learning_rate=args.finetune_lr,\n        seed=args.finetune_seed,\n        epochs=args.finetune_epochs,\n        split_idx=args.split_index,\n        train_samples=args.finetune_train_samples,\n        patience_steps=args.finetune_patience_steps,\n        patience_min_delta=args.finetune_patience_min_delta,\n    )\n    if args.finetune_augmentation != \"none\":\n        finetune_params[\"aug_config\"] = contrastive_learning_params[\"aug_config\"]\n        finetune_params[\"aug_samples\"] = contrastive_learning_params[\"aug_samples\"]\n        finetune_params[\"aug_yield_also_original\"] = (\n            args.finetune_augmentation == \"views-and-original\"\n        )\n    state_finetune = finetune(**generic_params, **flowpic_params, **finetune_params)\n\n    state = dict()\n    for key, value in state_pretrain.items():\n        state[f\"pretrain_{key}\"] = value\n    for key, valu in state_finetune.items():\n        state[f\"finetune_{key}\"] = value\n\n    aim_run.close()\n\n    utils.dump_cli_args(args, artifacts_folder / \"params.yml\", logger=logger)\n    return state\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_run_contrastive_learning_and_finetune/#tcbench.modeling.run_contrastive_learning_and_finetune.cli_parser","title":"<code>cli_parser()</code>","text":"<p>Create an ArgumentParser</p> Source code in <code>src/tcbench/modeling/run_contrastive_learning_and_finetune.py</code> <pre><code>def cli_parser():\n    \"\"\"Create an ArgumentParser\"\"\"\n    parser = argparse.ArgumentParser()\n\n    ####################################\n    # general configs\n    ####################################\n    # parser.add_argument(\n    #    \"--config\", \"-c\", type=pathlib.Path, required=True, default=\"./config.yml\",\n    #    help=utils.compose_cli_help_string(\"General configuration file\"),\n    # )\n    parser.add_argument(\n        \"--artifacts-folder\",\n        type=pathlib.Path,\n        default=\"./debug/artifacts\",\n        help=utils.compose_cli_help_string(\"Artifact folder\"),\n    )\n    parser.add_argument(\n        \"--device\",\n        default=\"cuda:0\",\n        help=utils.compose_cli_help_string(\"Device where to run experiments\"),\n    )\n    parser.add_argument(\n        \"--aim-repo\",\n        default=\"./debug\",\n        help=utils.compose_cli_help_string(\n            \"Local aim folder or URL of AIM remote server\"\n        ),\n    )\n    parser.add_argument(\n        \"--aim-experiment-name\",\n        default=\"contrastive-learning-and-finetune\",\n        help=utils.compose_cli_help_string(\n            \"The experiment name to use for the Aim run\"\n        ),\n    )\n    parser.add_argument(\n        \"--gpu-index\",\n        default=\"0\",\n        help=utils.compose_cli_help_string(\"The GPU id to use\"),\n    )\n    parser.add_argument(\"--final\", action=\"store_true\", default=False)\n\n    ####################################\n    # data and dataset configs\n    ####################################\n    parser.add_argument(\n        \"--dataset\",\n        choices=tuple(map(str, tcbench.DATASETS.__members__.values())),\n        default=str(tcbench.DATASETS.UCDAVISICDM19),\n        help=utils.compose_cli_help_string(\"Dataset to use for modeling\"),\n    )\n    parser.add_argument(\n        \"--dataset-minpkts\",\n        choices=(-1, 10, 100, 1000),\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"When used in combination with --dataset can refine the dataset and split to use for modeling\"\n        ),\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\n            \"Number of parallel worker for loading the data\"\n        ),\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=32,\n        help=utils.compose_cli_help_string(\"Training batch size\"),\n    )\n    parser.add_argument(\n        \"--split-index\",\n        type=int,\n        default=0,\n        help=utils.compose_cli_help_string(\"Datasplit index\"),\n    )\n    parser.add_argument(\n        \"--augmentations\",\n        default=\"changertt,timeshift\",\n        help=utils.compose_cli_help_string(\n            \"A pair of augmentations to use for contrastive learning\"\n        ),\n    )\n    parser.add_argument(\n        \"--max-samples-per-class\",\n        default=-1,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"Balance the dataset with the specified number of samples per class\"\n        ),\n    )\n    parser.add_argument(\n        \"--train-val-split-ratio\",\n        default=0.8,\n        type=float,\n        help=utils.compose_cli_help_string(\n            \"Fraction of samples to dedicate for training\"\n        )\n    )\n\n    ####################################\n    # flowpic configs\n    ####################################\n    parser.add_argument(\n        \"--flowpic-dim\",\n        type=int,\n        default=32,\n        choices=(32,),\n        help=utils.compose_cli_help_string(\"Flowpic dimension\"),\n    )\n    parser.add_argument(\n        \"--flowpic-block-duration\",\n        type=int,\n        default=15,\n        help=utils.compose_cli_help_string(\"Time window from which extract a flowpic\"),\n    )\n\n    ####################################\n    # model architecture\n    ####################################\n    parser.add_argument(\n        \"--suppress-dropout\",\n        default=False,\n        action=\"store_true\",\n        help=utils.compose_cli_help_string(\"Mask dropout layers with Identity\"),\n    )\n    parser.add_argument(\n        \"--finetune-augmentation\",\n        default=\"none\",\n        choices=(\"none\", \"only-views\", \"views-and-original\"),\n        help=utils.compose_cli_help_string(\n            \"Optional augmentation for finetuning training data. With 'only-views' finetuning is performed only using augmented data; with 'views-and-original' finetuning is performed using augmentation and original data. By default, no augmentation is performed\"\n        ),\n    )\n    parser.add_argument(\n        \"--projection-layer-dim\",\n        default=30,\n        type=int,\n        help=utils.compose_cli_help_string(\n            \"The number of units in the contrastive learning projection layer\"\n        ),\n    )\n\n    ####################################\n    # contrastive learning configs\n    ####################################\n    parser.add_argument(\n        \"--contrastive-learning-lr\",\n        type=float,\n        default=0.001,\n        help=utils.compose_cli_help_string(\"Learning rate for pretraining\"),\n    )\n    parser.add_argument(\n        \"--contrastive-learning-seed\",\n        type=int,\n        default=12345,\n        help=utils.compose_cli_help_string(\"Seed for contrastive learning pretraining\"),\n    )\n    parser.add_argument(\n        \"--contrastive-learning-patience-steps\",\n        type=int,\n        default=3,\n        help=utils.compose_cli_help_string(\n            \"Max steps to wait before stopping training if the top5 validation accuracy does not improve\"\n        ),\n    )\n    parser.add_argument(\n        \"--contrastive-learning-temperature\",\n        type=float,\n        default=0.07,\n        help=utils.compose_cli_help_string(\"Temperature for InfoNCE loss\"),\n    )\n    parser.add_argument(\n        \"--contrastive-learning-epochs\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\n            \"Epochs for contrastive learning pretraining\"\n        ),\n    )\n\n    ####################################\n    # finetune configs\n    ####################################\n    parser.add_argument(\n        \"--finetune-lr\",\n        type=float,\n        default=0.01,\n        help=utils.compose_cli_help_string(\"Learning for for finetune\"),\n    )\n    parser.add_argument(\n        \"--finetune-patience-steps\",\n        type=int,\n        default=5,\n        help=utils.compose_cli_help_string(\n            \"Max steps to wait before stopping training training loss does not improve\"\n        ),\n    )\n    parser.add_argument(\n        \"--finetune-patience-min-delta\",\n        type=float,\n        default=0.001,\n        help=utils.compose_cli_help_string(\"Min improvement for training loss\"),\n    )\n    parser.add_argument(\n        \"--finetune-train-samples\",\n        type=int,\n        default=10,\n        help=utils.compose_cli_help_string(\n            \"Number of samples per-class for finetune training\"\n        ),\n    )\n    parser.add_argument(\n        \"--finetune-epochs\",\n        type=int,\n        default=50,\n        help=utils.compose_cli_help_string(\"Epochs for finetune training\"),\n    )\n    parser.add_argument(\n        \"--finetune-seed\",\n        type=int,\n        default=12345,\n        help=utils.compose_cli_help_string(\"Sed for finetune training\"),\n    )\n\n    return parser\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/","title":"Tcbench modeling utils","text":"<p>This modules contains a set of utility function to support a variety of tasks</p>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.compose_cli_help_string","title":"<code>compose_cli_help_string(text)</code>","text":"<p>Attach to the input text the default formatting string used by argparse to print help strings</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>the input text to process</p> required Return <p>The input text modified by appending \"(default: %(default)s)\"</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def compose_cli_help_string(text: str) -&gt; str:\n    \"\"\"Attach to the input text the default formatting string used by argparse to print help strings\n\n    Arguments:\n        text: the input text to process\n\n    Return:\n        The input text modified by appending \"(default: %(default)s)\"\n    \"\"\"\n    return text + \" \" + ARGPARSE_HELP_DEFAULT\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.load_yaml","title":"<code>load_yaml(fname)</code>","text":"<p>Load an input YAML file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the YAML filename to load</p> required Return <p>The YAML object loaded</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def load_yaml(fname: pathlib.Path) -&gt; Dict[Any, Any]:\n    \"\"\"Load an input YAML file\n\n    Arguments:\n        fname: the YAML filename to load\n\n    Return:\n        The YAML object loaded\n    \"\"\"\n    with open(fname) as fin:\n        return yaml.safe_load(fin)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.load_config","title":"<code>load_config(fname)</code>","text":"<p>Load the configuration file of the framework</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the YAML config file to load</p> required Return <p>The loaded config file</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def load_config(fname: pathlib.Path) -&gt; Dict:\n    \"\"\"Load the configuration file of the framework\n\n    Arguments:\n        fname: the YAML config file to load\n\n    Return:\n        The loaded config file\n    \"\"\"\n    return load_yaml(fname)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.get_logger","title":"<code>get_logger(fname)</code>","text":"<p>Create a logger attached to the console and also binds it to the filename passed as input. Anything printed via the logger will appear on the console and in the file</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Path</code> <p>the file name to bind to the logger</p> required Return <p>A new logger object associated to both console and the specified filename</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def get_logger(fname: pathlib.Path) -&gt; logging.Logger:\n    \"\"\"Create a logger attached to the console\n    and also binds it to the filename passed as input.\n    Anything printed via the logger will appear on the\n    console and in the file\n\n    Arguments:\n        fname: the file name to bind to the logger\n\n    Return:\n        A new logger object associated to both\n        console and the specified filename\n    \"\"\"\n    fname = pathlib.Path(fname)\n\n    logger = logging.getLogger(\"tcbench\")\n\n    # loggers are kept internally\n    # and consistently returned based on name\n    # thus, if handlers are set, it means\n    # the logger was previously created.\n    # In this case we close them\n    # and create new ones\n    if logger.handlers:\n        logger.handlers[0].close()\n        logger.handlers[1].close()\n        logger.removeHandler(logger.handlers[0])\n        logger.removeHandler(logger.handlers[0])\n\n    # logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n\n    if not fname.parent.exists():\n        fname.parent.mkdir(parents=True)\n\n    fh = logging.FileHandler(fname)\n    fh.setLevel(logging.DEBUG)\n    print(f\"opened log at {fname}\")\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n    return logger\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.log_msg","title":"<code>log_msg(msg, logger=None)</code>","text":"<p>A generic function to print a message via a logger</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>the text to print</p> required <code>logger</code> <code>Logger</code> <p>the logger handling the printing</p> <code>None</code> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def log_msg(msg: str, logger: logging.Logger = None) -&gt; None:\n    \"\"\"A generic function to print a message via a logger\n\n    Arguments:\n        msg: the text to print\n        logger: the logger handling the printing\n    \"\"\"\n    if logger is None:\n        print(msg, flush=True)\n    else:\n        logger.debug(msg)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.seed_everything","title":"<code>seed_everything(seed)</code>","text":"<p>Set the seed for pytorch, numpy and python</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the seed to use for the initialization</p> required Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def seed_everything(seed: int) -&gt; None:\n    \"\"\"Set the seed for pytorch, numpy and python\n\n    Arguments:\n        seed: the seed to use for the initialization\n    \"\"\"\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.get_aim_run_hash","title":"<code>get_aim_run_hash(run)</code>","text":"<p>Return the hash number of an AIM run based on its name</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>Run</code> <p>the AIM run from which extract the hash</p> required Return <p>The hash of the run</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def get_aim_run_hash(run: aim.Run) -&gt; str:\n    \"\"\"Return the hash number of an AIM run based on its name\n\n    Arguments:\n        run: the AIM run from which extract the hash\n\n    Return:\n        The hash of the run\n    \"\"\"\n    return run.name.split(\" \")[1]\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.dump_cli_args","title":"<code>dump_cli_args(cli_args, save_as, logger)</code>","text":"<p>Transform a argparse Namespace object into a dictionary which is saved as YAML file as well as printed by the logger</p> <p>Parameters:</p> Name Type Description Default <code>cli_args</code> <code>Namespace</code> <p>a Namespace objected obtained by calling .parse_args() on a argparse parser</p> required <code>save_as</code> <code>Path</code> <p>a file where to save the arguments</p> required <code>logger</code> <code>Logger</code> <p>a logger instance use for printing the parsed CLI arguments</p> required Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def dump_cli_args(\n    cli_args: argparse.Namespace, save_as: pathlib.Path, logger: logging.Logger\n) -&gt; None:\n    \"\"\"Transform a argparse Namespace object into a dictionary\n    which is saved as YAML file as well as printed by the logger\n\n    Arguments:\n        cli_args: a Namespace objected obtained by calling .parse_args() on a argparse parser\n        save_as: a file where to save the arguments\n        logger: a logger instance use for printing the parsed CLI arguments\n    \"\"\"\n    params = dict()\n    for key in dir(cli_args):\n        if key[0] == \"_\":\n            continue\n        params[key] = getattr(cli_args, key)\n        if isinstance(params[key], (pathlib.Path, DATASETS, MODELING_INPUT_REPR_TYPE)):\n            params[key] = str(params[key])\n\n    save_as = pathlib.Path(save_as)\n    if not save_as.parent.exists():\n        save_as.parent.mkdir(parents=True)\n    log_msg(f\"saving: {save_as}\", logger)\n    with open(save_as, \"w\") as fout:\n        yaml.dump(params, fout)\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.classification_reports","title":"<code>classification_reports(net, dset, batch_size, device='cuda:0', context='train', save_to=None, logger=None, method='monolithic', xgboost_model=None)</code>","text":"<p>Compute scikit learn classification report and confusion matrix</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>BaseNet</code> <p>the trained network to use</p> required <code>dset</code> <code>FlowpicDataset</code> <p>the dataset to use</p> required <code>batch_size</code> <code>int</code> <p>the batch_size to use</p> required <code>device</code> <code>str</code> <p>the device where to operate inferece</p> <code>'cuda:0'</code> <code>context</code> <code>str</code> <p>a text string used for AIM tracking</p> <code>'train'</code> <code>save_to</code> <code>Path</code> <p>a folder where to store the report</p> <code>None</code> <code>logger</code> <code>Path</code> <p>the logger where to print the reports</p> <code>None</code> <code>method</code> <code>str</code> <p>e.g. 'monolithic' (NN) or 'xgboost'</p> <code>'monolithic'</code> <code>xgboost_model</code> <code>Any</code> <p>the xgboost model in case method=='xgboost'</p> <code>None</code> Return <p>A dictionary with two keys: \"class_rep\" and \"conf_mtx\". Each key is associated to a pandas DataFrame containing the classification report and confusion matrix computed based on inference</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def classification_reports(\n    net: backbone.BaseNet,\n    dset: dataprep.FlowpicDataset,\n    batch_size: int,\n    device: str = \"cuda:0\",\n    context: str = \"train\",\n    save_to: pathlib.Path = None,\n    logger: pathlib.Path = None,\n    method: str = \"monolithic\",\n    xgboost_model: Any=None,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"Compute scikit learn classification report\n    and confusion matrix\n\n    Arguments:\n        net: the trained network to use\n        dset: the dataset to use\n        batch_size: the batch_size to use\n        device: the device where to operate inferece\n        context: a text string used for AIM tracking\n        save_to: a folder where to store the report\n        logger: the logger where to print the reports\n        method: e.g. 'monolithic' (NN) or 'xgboost'\n        xgboost_model: the xgboost model in case method=='xgboost'\n\n    Return:\n        A dictionary with two keys: \"class_rep\"\n        and \"conf_mtx\". Each key is associated\n        to a pandas DataFrame containing the\n        classification report and confusion matrix\n        computed based on inference\n    \"\"\"\n\n    save_to = pathlib.Path(save_to)\n    if not save_to.exists():\n        save_to.mkdir(parents=True)\n\n    dummy_trainer = methods.trainer_factory(\n        method, net=net, device=device, logger=logger, xgboost_model=xgboost_model\n    )\n\n    loader = torch.utils.data.DataLoader(dset, batch_size, shuffle=False)\n    _, reports = dummy_trainer.test_loop(loader, with_reports=True)\n\n    labels = dset.df[\"app\"].dtype.categories\n    mapping = {str(idx): lab for idx, lab in enumerate(labels)}\n    reports[\"class_rep\"] = reports[\"class_rep\"].rename(mapping, axis=0)\n    ###\n    mapping = {idx: lab for idx, lab in enumerate(labels)}\n    reports[\"conf_mtx\"] = reports[\"conf_mtx\"].rename(mapping).rename(mapping, axis=1)\n\n    log_msg(\"\", logger)\n    log_msg(f\"---{context} reports---\", logger)\n    log_msg(\"\", logger)\n    log_msg(reports[\"class_rep\"], logger)\n    log_msg(\"\", logger)\n    log_msg(reports[\"conf_mtx\"], logger)\n\n    if save_to:\n        log_msg(\"\")\n        fname = save_to / f\"./{context}_class_rep.csv\"\n        log_msg(f\"saving: {fname}\", logger)\n        reports[\"class_rep\"].reset_index().to_csv(fname, index=None)\n\n        fname = save_to / f\"./{context}_conf_mtx.csv\"\n        log_msg(f\"saving: {fname}\", logger)\n        reports[\"conf_mtx\"].reset_index().to_csv(fname, index=None)\n    return reports\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.compute_confidence_intervals","title":"<code>compute_confidence_intervals(array, alpha=0.05)</code>","text":"<p>Computes the confidence intervasl from an array of values.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>a list of values to process</p> required <code>alpha</code> <code>float</code> <p>the alpha of the confidence interval</p> <code>0.05</code> Return <p>The function is based on statsmodels.stats.api.DescrStatsW() which reports lower and upper values of the interval. However we return the difference between mean value of the input array and the upper value of the confidence interval</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def compute_confidence_intervals(array: NDArray, alpha: float = 0.05) -&gt; float:\n    \"\"\"Computes the confidence intervasl from an array of values.\n\n    Arguments:\n        array: a list of values to process\n        alpha: the alpha of the confidence interval\n\n    Return:\n        The function is based on statsmodels.stats.api.DescrStatsW()\n        which reports lower and upper values of the interval.\n        However we return the difference between mean value of the\n        input array and the upper value of the confidence interval\n    \"\"\"\n    array = np.array(array)\n    low, high = sms.DescrStatsW(array).tconfint_mean(alpha)\n    mean = array.mean()\n    ci = high - mean\n    return ci\n</code></pre>"},{"location":"tcbench/api/tcbench_modeling_utils/#tcbench.modeling.utils.log_torchsummary","title":"<code>log_torchsummary(net, input_shape, logger=None)</code>","text":"<p>Log a model backbone architecture</p> Source code in <code>src/tcbench/modeling/utils.py</code> <pre><code>def log_torchsummary(net:Any, input_shape:Any, logger:logging.Logger=None) -&gt; None:\n    \"\"\"Log a model backbone architecture\"\"\"\n\n    f_capture = io.StringIO()\n    with contextlib.redirect_stdout(f_capture):\n        torchsummary.summary(net, input_shape)\n\n    log_msg(f_capture.getvalue(), logger)\n</code></pre>"}]}